{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "61v_NqelklQD"
      },
      "outputs": [],
      "source": [
        "DATAPATH = \"./results/\" #directory with the detectors results\n",
        "optimize_threshold = False #whether to find optimal classification threshold or to use a default 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_24oOhymdnLc"
      },
      "source": [
        "# Installs and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hLnb85ykywL",
        "outputId": "438ecb57-b622-4913-a8c2-1a7461a6abf3"
      },
      "outputs": [],
      "source": [
        "#mount GDrive if DATAPATH is on it\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G7wQ0uZ3k3pK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, f1_score, accuracy_score\n",
        "import pickle\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlUYW0wYdqQK"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#casting dtypes to reduce RAM usage\n",
        "CAST_DICT = {'text': 'string', 'language': 'string', 'label': 'string', 'length': 'int16', 'source': 'string', 'domain': 'string', 'topic': 'string', 'split': 'string', 'multi_label': 'string', 'predictions': 'string', 'prediction_probs': np.float16}\n",
        "#modification of labels from textual to numeric form (and backwards) for evaluation purpose\n",
        "label_names = [\"human\", \"machine\"] #0, 1\n",
        "id2label = {idx:label for idx, label in enumerate(label_names)}\n",
        "label2id = {v:k for k,v in id2label.items()}\n",
        "\n",
        "test_results = []"
      ],
      "metadata": {
        "id": "zKdqV78bLVxm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kv82HNCERZdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0f729ef-5d24-4098-e45b-c20b8befb81d"
      },
      "outputs": [],
      "source": [
        "#Load results from finetuned detectors\n",
        "files = glob.glob(DATAPATH + '*.csv.gz')\n",
        "for f in tqdm(files, total= len(files)):\n",
        "  df = pd.read_csv(f, dtype = CAST_DICT)\n",
        "  df.drop(columns=['text', 'domain', 'topic', 'split'], inplace=True)\n",
        "  test_results.append({f.split('/')[-1] : df})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multitude_test = pd.read_csv(DATAPATH + 'multitude.csv', dtype = CAST_DICT)\n",
        "multitude_test = multitude_test[multitude_test.split == 'test']\n",
        "multitude_test.drop(columns=['text', 'domain', 'topic', 'split'], inplace=True)\n",
        "\n",
        "#Add results from statistical detectors\n",
        "files = glob.glob(DATAPATH + \"statistical/*.csv\")\n",
        "for f in tqdm(files, total= len(files)):\n",
        "  df = pd.read_csv(f)\n",
        "  temp = multitude_test.copy().reset_index()\n",
        "  temp['predictions'] = [id2label[x] for x in df['Predictions']]\n",
        "  temp['predictions'] = temp['predictions'].astype('string')\n",
        "  temp['label'] = [id2label[int(x)] for x in temp['label']]\n",
        "  temp['label'] = temp['label'].astype('string')\n",
        "  test_results.append({f.split('/')[-1].replace('predictions_', 'statistical-').replace('.csv', '') : temp})\n",
        "\n",
        "#Add results from black-box detectors\n",
        "files = glob.glob(DATAPATH + \"blackbox/*.csv\")\n",
        "for f in tqdm(files, total= len(files)):\n",
        "  df = pd.read_csv(f)\n",
        "  temp = multitude_test.copy().reset_index()\n",
        "  temp['predictions'] = [id2label[x] for x in df['predictions']]\n",
        "  temp['predictions'] = temp['predictions'].astype('string')\n",
        "  temp['label'] = [id2label[int(x)] for x in temp['label']]\n",
        "  temp['label'] = temp['label'].astype('string')\n",
        "  test_results.append({f.split('/')[-1].replace('predictions_', 'blackbox-').replace('.csv', '') : temp})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7d-koU7hylq",
        "outputId": "4422924c-ca59-4b75-f3d4-7e1b7fb3f3ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-539875c9dba9>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  multitude_test.drop(columns=['text', 'domain', 'topic', 'split'], inplace=True)\n",
            "100%|██████████| 7/7 [00:00<00:00, 20.25it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 21.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#limit detectors to those in EMNLP paper\n",
        "EMNLP_detectors = ['bert-base-multilingual-cased-finetuned',\n",
        "                   'roberta-large-openai-detector-finetuned',\n",
        "                   'mdeberta-v3-base-finetuned',\n",
        "                   'xlm-roberta-large-finetuned',\n",
        "                   'electra-large-discriminator-finetuned',\n",
        "                   'gpt2-medium-finetuned',\n",
        "                   'mGPT-finetuned',\n",
        "                   'statistical-entropy',\n",
        "                   'statistical-rank',\n",
        "                   'statistical-log_rank',\n",
        "                   'statistical-rank_GLTR',\n",
        "                   'statistical-likelihood',\n",
        "                   'statistical-detectgpt',\n",
        "                   'statistical-entropy_RF-tuned',\n",
        "                   'blackbox-GPTZero',\n",
        "                   'blackbox-ZeroGPT']\n",
        "def is_EMNLP_detector(detector):\n",
        "  for k,v in detector.items():\n",
        "    #print(k);\n",
        "    temp = k.split('-finetuned-')[0]\n",
        "    if len(k.split('-finetuned-')) > 1:\n",
        "      temp += '-finetuned'\n",
        "    return temp in EMNLP_detectors\n",
        "\n",
        "test_results = [x for x in test_results if is_EMNLP_detector(x)]"
      ],
      "metadata": {
        "id": "24jp1g6_Cex9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MBU8JvqeHbC"
      },
      "source": [
        "#Results analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HvBYfrfrYliV"
      },
      "outputs": [],
      "source": [
        "def cr2df(labels, predictions, detector):\n",
        "  cr = classification_report(labels, predictions, digits=4, output_dict=True, zero_division=0)\n",
        "  cm = confusion_matrix(labels, predictions)\n",
        "\n",
        "  #based on https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
        "  TN = cm[0][0]\n",
        "  FN = cm[1][0]\n",
        "  TP = cm[1][1]\n",
        "  FP = cm[0][1]\n",
        "  # Fall out or false positive rate\n",
        "  FPR = FP/(FP+TN) if (FP+TN) > 0 else 0\n",
        "  # False negative rate\n",
        "  FNR = FN/(TP+FN) if (TP+FN) > 0 else 0\n",
        "\n",
        "  train_language = 'N/A'\n",
        "  train_llm = 'N/A'\n",
        "  if 'finetuned' in detector:\n",
        "    train_language = detector.split('-finetuned-')[1].split('-')[0]\n",
        "    train_llm = detector.split(f'-{train_language}-')[1].replace('.csv.gz', '')\n",
        "  return pd.DataFrame({'Model': detector.split('-finetuned-')[0], 'Train Language': train_language, 'Train LLM': train_llm, 'Detector': detector, 'Macro avg F1-score': cr['macro avg']['f1-score'], 'Weighted avg F1-score': cr['weighted avg']['f1-score'], 'Weighted avg Precision': cr['weighted avg']['precision'], 'Weighted avg Recall': cr['weighted avg']['recall'], 'Accuracy': cr['accuracy'], 'FPR': FPR, 'FNR': FNR, 'Human samples': cr['human']['support'], 'Machine samples': cr['machine']['support']}, index=[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "j51uVqVsNjCm"
      },
      "outputs": [],
      "source": [
        "def analyze(results_list):\n",
        "  results = pd.DataFrame()\n",
        "  for detector in tqdm(results_list, total=len(results_list)):\n",
        "    for detector_name, detector_data in detector.items():\n",
        "        temp = detector_data\n",
        "        if len(temp.label.unique()) < 2: continue\n",
        "        optimal_threshold = 0.5\n",
        "        if optimize_threshold and 'prediction_probs' in temp.columns:\n",
        "          labels = [label2id[x] for x in temp['label']]\n",
        "          predictions = [label2id[x] for x in temp['predictions']]\n",
        "          temp = temp.fillna(0.0)\n",
        "          temp['prediction_probs'] = temp['prediction_probs'].astype(float)\n",
        "          temp.loc[temp.predictions == 'human', 'prediction_probs'] = 1 - temp['prediction_probs']\n",
        "          if (optimize_threshold):\n",
        "            fpr, tpr, thresholds = roc_curve(labels, temp['prediction_probs'])\n",
        "            optimal_threshold = thresholds[np.argmax(tpr - fpr)]\n",
        "            #optimal_threshold = thresholds[fpr <= 0.05][-1] #get threshold for 5% FPR\n",
        "          preds = [\"machine\" if ((y > optimal_threshold)) else \"human\" for x,y in zip(temp['predictions'],temp['prediction_probs'])]\n",
        "        else:\n",
        "          preds = temp['predictions']\n",
        "          if 'prediction_probs' in temp.columns:\n",
        "            temp = temp.fillna(0.0)\n",
        "            temp['prediction_probs'] = temp['prediction_probs'].astype(float)\n",
        "            temp.loc[temp.predictions == 'human', 'prediction_probs'] = 1 - temp['prediction_probs']\n",
        "        scores = cr2df(temp['label'], preds, detector_name)\n",
        "        try:\n",
        "          scores['AUC_preds'] = roc_auc_score([label2id[x] for x in temp['label']], [label2id[x] for x in preds])\n",
        "          scores['AUC_probs'] = roc_auc_score([label2id[x] for x in temp['label']], temp['prediction_probs'])\n",
        "          scores['threshold'] = optimal_threshold\n",
        "        except:\n",
        "          pass\n",
        "        results = pd.concat([results, scores], copy=False, ignore_index=True)\n",
        "  temp = results.sort_values(by=['Macro avg F1-score'], ascending=False).reset_index(drop=True)\n",
        "  return temp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNg-fyHUeTwn"
      },
      "source": [
        "\n",
        "## General Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GpDl_AToRPMs",
        "outputId": "99aa8f83-0e07-42ac-a8bb-5e26438635cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 324/324 [09:00<00:00,  1.67s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             Model Train Language         Train LLM  \\\n",
              "0                 mdeberta-v3-base            all               all   \n",
              "1                xlm-roberta-large            all               all   \n",
              "2                xlm-roberta-large             es               all   \n",
              "3                xlm-roberta-large             ru               all   \n",
              "4                 mdeberta-v3-base             es               all   \n",
              "5     bert-base-multilingual-cased            all               all   \n",
              "6                 mdeberta-v3-base             ru               all   \n",
              "7     bert-base-multilingual-cased             es               all   \n",
              "8    roberta-large-openai-detector            all               all   \n",
              "9                             mGPT             ru               all   \n",
              "10                            mGPT            all               all   \n",
              "11                            mGPT             es               all   \n",
              "12    bert-base-multilingual-cased             ru               all   \n",
              "13                     gpt2-medium            all               all   \n",
              "14                mdeberta-v3-base             ru   alpaca-lora-30b   \n",
              "15   roberta-large-openai-detector             en  opt-iml-max-1.3b   \n",
              "16   roberta-large-openai-detector            en3  opt-iml-max-1.3b   \n",
              "17   roberta-large-openai-detector             es               all   \n",
              "18    bert-base-multilingual-cased             en               all   \n",
              "19                mdeberta-v3-base             ru        vicuna-13b   \n",
              "20                            mGPT             ru        vicuna-13b   \n",
              "21                mdeberta-v3-base             en               all   \n",
              "22   roberta-large-openai-detector            en3           opt-66b   \n",
              "23   roberta-large-openai-detector            en3               all   \n",
              "24                mdeberta-v3-base            en3  opt-iml-max-1.3b   \n",
              "25               xlm-roberta-large            en3               all   \n",
              "26                            mGPT             en  opt-iml-max-1.3b   \n",
              "27                     gpt2-medium             ru               all   \n",
              "28               xlm-roberta-large             ru        vicuna-13b   \n",
              "29   roberta-large-openai-detector             en   alpaca-lora-30b   \n",
              "30                mdeberta-v3-base             en         llama-65b   \n",
              "31   roberta-large-openai-detector             en           opt-66b   \n",
              "32    bert-base-multilingual-cased             en  opt-iml-max-1.3b   \n",
              "33               xlm-roberta-large             ru   alpaca-lora-30b   \n",
              "34               xlm-roberta-large            all        vicuna-13b   \n",
              "35   roberta-large-openai-detector             en        vicuna-13b   \n",
              "36   roberta-large-openai-detector             es         llama-65b   \n",
              "37               xlm-roberta-large             es           opt-66b   \n",
              "38   roberta-large-openai-detector            en3   alpaca-lora-30b   \n",
              "39                mdeberta-v3-base            en3               all   \n",
              "40                            mGPT             en               all   \n",
              "41   roberta-large-openai-detector            en3        vicuna-13b   \n",
              "42                     gpt2-medium             es               all   \n",
              "43    bert-base-multilingual-cased             ru   alpaca-lora-30b   \n",
              "44               xlm-roberta-large             en               all   \n",
              "45    bert-base-multilingual-cased            en3  opt-iml-max-1.3b   \n",
              "46                            mGPT            en3  opt-iml-max-1.3b   \n",
              "47   roberta-large-openai-detector             en  text-davinci-003   \n",
              "48   roberta-large-openai-detector             ru               all   \n",
              "49    bert-base-multilingual-cased             ru        vicuna-13b   \n",
              "50    bert-base-multilingual-cased            en3        vicuna-13b   \n",
              "51                            mGPT             es        vicuna-13b   \n",
              "52   roberta-large-openai-detector             en     gpt-3.5-turbo   \n",
              "53                            mGPT            en3               all   \n",
              "54               xlm-roberta-large            en3         llama-65b   \n",
              "55                mdeberta-v3-base            all           opt-66b   \n",
              "56     electra-large-discriminator             en               all   \n",
              "57                            mGPT             es   alpaca-lora-30b   \n",
              "58   roberta-large-openai-detector             en               all   \n",
              "59    bert-base-multilingual-cased            all   alpaca-lora-30b   \n",
              "60               xlm-roberta-large             en   alpaca-lora-30b   \n",
              "61                mdeberta-v3-base            all   alpaca-lora-30b   \n",
              "62                mdeberta-v3-base             ru         llama-65b   \n",
              "63   roberta-large-openai-detector            en3         llama-65b   \n",
              "64                            mGPT             es           opt-66b   \n",
              "65                mdeberta-v3-base             es   alpaca-lora-30b   \n",
              "66     electra-large-discriminator            en3         llama-65b   \n",
              "67    bert-base-multilingual-cased             es   alpaca-lora-30b   \n",
              "68    bert-base-multilingual-cased            all        vicuna-13b   \n",
              "69                            mGPT             ru   alpaca-lora-30b   \n",
              "70               xlm-roberta-large             ru     gpt-3.5-turbo   \n",
              "71   roberta-large-openai-detector             es           opt-66b   \n",
              "72               xlm-roberta-large            all   alpaca-lora-30b   \n",
              "73                            mGPT            en3         llama-65b   \n",
              "74     electra-large-discriminator            en3               all   \n",
              "75                            mGPT             en         llama-65b   \n",
              "76   roberta-large-openai-detector             es  opt-iml-max-1.3b   \n",
              "77    bert-base-multilingual-cased             en   alpaca-lora-30b   \n",
              "78    bert-base-multilingual-cased            en3   alpaca-lora-30b   \n",
              "79   roberta-large-openai-detector             en         llama-65b   \n",
              "80               xlm-roberta-large            en3   alpaca-lora-30b   \n",
              "81                            mGPT             ru         llama-65b   \n",
              "82    bert-base-multilingual-cased             es        vicuna-13b   \n",
              "83                mdeberta-v3-base             en  opt-iml-max-1.3b   \n",
              "84                mdeberta-v3-base             es        vicuna-13b   \n",
              "85                mdeberta-v3-base            all        vicuna-13b   \n",
              "86     electra-large-discriminator             en         llama-65b   \n",
              "87    bert-base-multilingual-cased             es           opt-66b   \n",
              "88                mdeberta-v3-base            en3           opt-66b   \n",
              "89    bert-base-multilingual-cased             en        vicuna-13b   \n",
              "90     electra-large-discriminator            en3           opt-66b   \n",
              "91   roberta-large-openai-detector            all        vicuna-13b   \n",
              "92                mdeberta-v3-base             es           opt-66b   \n",
              "93               xlm-roberta-large             en         llama-65b   \n",
              "94    bert-base-multilingual-cased            en3               all   \n",
              "95    bert-base-multilingual-cased             ru     gpt-3.5-turbo   \n",
              "96                            mGPT             es         llama-65b   \n",
              "97                            mGPT            all   alpaca-lora-30b   \n",
              "98                            mGPT            all           opt-66b   \n",
              "99                            mGPT            all        vicuna-13b   \n",
              "100   bert-base-multilingual-cased             ru             gpt-4   \n",
              "101  roberta-large-openai-detector            en3     gpt-3.5-turbo   \n",
              "102  roberta-large-openai-detector            all  text-davinci-003   \n",
              "103  roberta-large-openai-detector             ru  opt-iml-max-1.3b   \n",
              "104              xlm-roberta-large            en3  opt-iml-max-1.3b   \n",
              "105              xlm-roberta-large            en3        vicuna-13b   \n",
              "106              xlm-roberta-large             en  opt-iml-max-1.3b   \n",
              "107              xlm-roberta-large             ru         llama-65b   \n",
              "108   bert-base-multilingual-cased            en3           opt-66b   \n",
              "109   bert-base-multilingual-cased            all  text-davinci-003   \n",
              "110   bert-base-multilingual-cased             es             gpt-4   \n",
              "111               mdeberta-v3-base             ru     gpt-3.5-turbo   \n",
              "112  roberta-large-openai-detector            all   alpaca-lora-30b   \n",
              "113                           mGPT            en3        vicuna-13b   \n",
              "114    electra-large-discriminator             ru        vicuna-13b   \n",
              "115   bert-base-multilingual-cased             es  text-davinci-003   \n",
              "116   bert-base-multilingual-cased             en     gpt-3.5-turbo   \n",
              "117    electra-large-discriminator             en  opt-iml-max-1.3b   \n",
              "118   bert-base-multilingual-cased            en3     gpt-3.5-turbo   \n",
              "119              xlm-roberta-large             es  text-davinci-003   \n",
              "120               mdeberta-v3-base            all         llama-65b   \n",
              "121    electra-large-discriminator            all               all   \n",
              "122  roberta-large-openai-detector             ru   alpaca-lora-30b   \n",
              "123                           mGPT            en3           opt-66b   \n",
              "124              xlm-roberta-large             ru             gpt-4   \n",
              "125  roberta-large-openai-detector             en             gpt-4   \n",
              "126  roberta-large-openai-detector             ru        vicuna-13b   \n",
              "127    electra-large-discriminator             es  opt-iml-max-1.3b   \n",
              "128               mdeberta-v3-base            all     gpt-3.5-turbo   \n",
              "129                           mGPT             en        vicuna-13b   \n",
              "130    electra-large-discriminator            en3  opt-iml-max-1.3b   \n",
              "131    electra-large-discriminator             ru           opt-66b   \n",
              "132              xlm-roberta-large             en           opt-66b   \n",
              "133                           mGPT             en   alpaca-lora-30b   \n",
              "134   bert-base-multilingual-cased            all             gpt-4   \n",
              "135               mdeberta-v3-base            en3   alpaca-lora-30b   \n",
              "136   bert-base-multilingual-cased             ru         llama-65b   \n",
              "137                    gpt2-medium            en3         llama-65b   \n",
              "138              xlm-roberta-large            en3             gpt-4   \n",
              "139              xlm-roberta-large            all             gpt-4   \n",
              "140                           mGPT            en3   alpaca-lora-30b   \n",
              "141               mdeberta-v3-base             es     gpt-3.5-turbo   \n",
              "142              xlm-roberta-large            all     gpt-3.5-turbo   \n",
              "143    electra-large-discriminator             es               all   \n",
              "144    electra-large-discriminator             ru         llama-65b   \n",
              "145              xlm-roberta-large             es        vicuna-13b   \n",
              "146              xlm-roberta-large            en3     gpt-3.5-turbo   \n",
              "147   bert-base-multilingual-cased            all     gpt-3.5-turbo   \n",
              "148   bert-base-multilingual-cased            all  opt-iml-max-1.3b   \n",
              "149               mdeberta-v3-base            all  text-davinci-003   \n",
              "150   bert-base-multilingual-cased            en3  text-davinci-003   \n",
              "151               mdeberta-v3-base            all  opt-iml-max-1.3b   \n",
              "152               mdeberta-v3-base            all             gpt-4   \n",
              "153   statistical-entropy_RF-tuned            N/A               N/A   \n",
              "154  roberta-large-openai-detector             ru           opt-66b   \n",
              "155  roberta-large-openai-detector            all           opt-66b   \n",
              "156                    gpt2-medium             en               all   \n",
              "157               mdeberta-v3-base             es         llama-65b   \n",
              "158               mdeberta-v3-base             es  text-davinci-003   \n",
              "159   bert-base-multilingual-cased             en           opt-66b   \n",
              "160  roberta-large-openai-detector             ru         llama-65b   \n",
              "161  roberta-large-openai-detector            all         llama-65b   \n",
              "162               mdeberta-v3-base             en   alpaca-lora-30b   \n",
              "163              xlm-roberta-large            en3           opt-66b   \n",
              "164    electra-large-discriminator             es           opt-66b   \n",
              "165               mdeberta-v3-base             en  text-davinci-003   \n",
              "166                           mGPT             en           opt-66b   \n",
              "167               mdeberta-v3-base            en3  text-davinci-003   \n",
              "168               mdeberta-v3-base            en3        vicuna-13b   \n",
              "169   bert-base-multilingual-cased            en3             gpt-4   \n",
              "170               mdeberta-v3-base             es             gpt-4   \n",
              "171  roberta-large-openai-detector             ru             gpt-4   \n",
              "172               mdeberta-v3-base            en3     gpt-3.5-turbo   \n",
              "173   bert-base-multilingual-cased             es     gpt-3.5-turbo   \n",
              "174    electra-large-discriminator             ru               all   \n",
              "175               statistical-rank            N/A               N/A   \n",
              "176          statistical-detectgpt            N/A               N/A   \n",
              "177            statistical-entropy            N/A               N/A   \n",
              "178    electra-large-discriminator             en           opt-66b   \n",
              "179         statistical-likelihood            N/A               N/A   \n",
              "180           statistical-log_rank            N/A               N/A   \n",
              "181                           mGPT             es  text-davinci-003   \n",
              "182               mdeberta-v3-base            en3         llama-65b   \n",
              "183                    gpt2-medium             en         llama-65b   \n",
              "184  roberta-large-openai-detector            en3             gpt-4   \n",
              "185   bert-base-multilingual-cased             en  text-davinci-003   \n",
              "186          statistical-rank_GLTR            N/A               N/A   \n",
              "187              xlm-roberta-large             es         llama-65b   \n",
              "188              xlm-roberta-large             en        vicuna-13b   \n",
              "189              xlm-roberta-large             es   alpaca-lora-30b   \n",
              "190                           mGPT             es     gpt-3.5-turbo   \n",
              "191  roberta-large-openai-detector             ru  text-davinci-003   \n",
              "192              xlm-roberta-large            all         llama-65b   \n",
              "193                           mGPT             en  text-davinci-003   \n",
              "194  roberta-large-openai-detector            en3  text-davinci-003   \n",
              "195                    gpt2-medium            en3               all   \n",
              "196              xlm-roberta-large            all           opt-66b   \n",
              "197                           mGPT            all  text-davinci-003   \n",
              "198                           mGPT             ru     gpt-3.5-turbo   \n",
              "199               mdeberta-v3-base             ru             gpt-4   \n",
              "200               mdeberta-v3-base            en3             gpt-4   \n",
              "201              xlm-roberta-large            all  text-davinci-003   \n",
              "202               mdeberta-v3-base             en        vicuna-13b   \n",
              "203                           mGPT            all             gpt-4   \n",
              "204              xlm-roberta-large             en  text-davinci-003   \n",
              "205   bert-base-multilingual-cased            all           opt-66b   \n",
              "206  roberta-large-openai-detector             ru     gpt-3.5-turbo   \n",
              "207    electra-large-discriminator             es         llama-65b   \n",
              "208  roberta-large-openai-detector            all  opt-iml-max-1.3b   \n",
              "209                           mGPT             en     gpt-3.5-turbo   \n",
              "210                    gpt2-medium             es        vicuna-13b   \n",
              "211    electra-large-discriminator             en        vicuna-13b   \n",
              "212               mdeberta-v3-base             en           opt-66b   \n",
              "213    electra-large-discriminator            en3        vicuna-13b   \n",
              "214                           mGPT             en             gpt-4   \n",
              "215                           mGPT            all         llama-65b   \n",
              "216              xlm-roberta-large             es     gpt-3.5-turbo   \n",
              "217                           mGPT             ru             gpt-4   \n",
              "218  roberta-large-openai-detector             es        vicuna-13b   \n",
              "219               mdeberta-v3-base             en     gpt-3.5-turbo   \n",
              "220                    gpt2-medium             es           opt-66b   \n",
              "221                           mGPT             es  opt-iml-max-1.3b   \n",
              "222    electra-large-discriminator             ru  opt-iml-max-1.3b   \n",
              "223   bert-base-multilingual-cased            en3         llama-65b   \n",
              "224                    gpt2-medium            all           opt-66b   \n",
              "225                           mGPT             es             gpt-4   \n",
              "226                           mGPT            all  opt-iml-max-1.3b   \n",
              "227                           mGPT            en3  text-davinci-003   \n",
              "228               blackbox-ZeroGPT            N/A               N/A   \n",
              "229  roberta-large-openai-detector            all             gpt-4   \n",
              "230                    gpt2-medium            en3  opt-iml-max-1.3b   \n",
              "231                    gpt2-medium             es         llama-65b   \n",
              "232              xlm-roberta-large             en     gpt-3.5-turbo   \n",
              "233                           mGPT            all     gpt-3.5-turbo   \n",
              "234                           mGPT            en3     gpt-3.5-turbo   \n",
              "235                    gpt2-medium            all  text-davinci-003   \n",
              "236                    gpt2-medium             en  opt-iml-max-1.3b   \n",
              "237   bert-base-multilingual-cased            all         llama-65b   \n",
              "238   bert-base-multilingual-cased             en         llama-65b   \n",
              "239                    gpt2-medium            en3           opt-66b   \n",
              "240              xlm-roberta-large            en3  text-davinci-003   \n",
              "241   bert-base-multilingual-cased             es         llama-65b   \n",
              "242  roberta-large-openai-detector             es   alpaca-lora-30b   \n",
              "243                    gpt2-medium            all        vicuna-13b   \n",
              "244                    gpt2-medium             ru         llama-65b   \n",
              "245   bert-base-multilingual-cased             en             gpt-4   \n",
              "246                           mGPT            en3             gpt-4   \n",
              "247    electra-large-discriminator            all           opt-66b   \n",
              "248   bert-base-multilingual-cased             ru  text-davinci-003   \n",
              "249                    gpt2-medium             es   alpaca-lora-30b   \n",
              "250                    gpt2-medium            all  opt-iml-max-1.3b   \n",
              "251  roberta-large-openai-detector             es     gpt-3.5-turbo   \n",
              "252    electra-large-discriminator            en3   alpaca-lora-30b   \n",
              "253                           mGPT             ru  text-davinci-003   \n",
              "254                    gpt2-medium             es  text-davinci-003   \n",
              "255                    gpt2-medium            all   alpaca-lora-30b   \n",
              "256  roberta-large-openai-detector             es  text-davinci-003   \n",
              "257                           mGPT             ru           opt-66b   \n",
              "258    electra-large-discriminator             ru   alpaca-lora-30b   \n",
              "259               mdeberta-v3-base             es  opt-iml-max-1.3b   \n",
              "260                    gpt2-medium            all             gpt-4   \n",
              "261                    gpt2-medium            all         llama-65b   \n",
              "262              xlm-roberta-large             es             gpt-4   \n",
              "263  roberta-large-openai-detector             es             gpt-4   \n",
              "264    electra-large-discriminator             es        vicuna-13b   \n",
              "265              xlm-roberta-large             ru  text-davinci-003   \n",
              "266                    gpt2-medium            en3        vicuna-13b   \n",
              "267                    gpt2-medium            all     gpt-3.5-turbo   \n",
              "268              xlm-roberta-large             en             gpt-4   \n",
              "269               mdeberta-v3-base             ru  text-davinci-003   \n",
              "270    electra-large-discriminator             en  text-davinci-003   \n",
              "271               mdeberta-v3-base             en             gpt-4   \n",
              "272    electra-large-discriminator             en     gpt-3.5-turbo   \n",
              "273   bert-base-multilingual-cased             es  opt-iml-max-1.3b   \n",
              "274              xlm-roberta-large            all  opt-iml-max-1.3b   \n",
              "275                    gpt2-medium             en           opt-66b   \n",
              "276    electra-large-discriminator             es  text-davinci-003   \n",
              "277              xlm-roberta-large             ru           opt-66b   \n",
              "278  roberta-large-openai-detector            all     gpt-3.5-turbo   \n",
              "279                    gpt2-medium             es  opt-iml-max-1.3b   \n",
              "280    electra-large-discriminator             es   alpaca-lora-30b   \n",
              "281    electra-large-discriminator            all         llama-65b   \n",
              "282    electra-large-discriminator            all        vicuna-13b   \n",
              "283    electra-large-discriminator             es             gpt-4   \n",
              "284    electra-large-discriminator            all  opt-iml-max-1.3b   \n",
              "285    electra-large-discriminator            all   alpaca-lora-30b   \n",
              "286    electra-large-discriminator             es     gpt-3.5-turbo   \n",
              "287    electra-large-discriminator             en   alpaca-lora-30b   \n",
              "288                    gpt2-medium             en        vicuna-13b   \n",
              "289   bert-base-multilingual-cased             ru           opt-66b   \n",
              "290               mdeberta-v3-base             ru           opt-66b   \n",
              "291    electra-large-discriminator            all             gpt-4   \n",
              "292                    gpt2-medium             ru        vicuna-13b   \n",
              "293    electra-large-discriminator            en3             gpt-4   \n",
              "294                    gpt2-medium             es     gpt-3.5-turbo   \n",
              "295    electra-large-discriminator            all  text-davinci-003   \n",
              "296              xlm-roberta-large             es  opt-iml-max-1.3b   \n",
              "297                    gpt2-medium             ru           opt-66b   \n",
              "298                    gpt2-medium             en   alpaca-lora-30b   \n",
              "299    electra-large-discriminator            en3     gpt-3.5-turbo   \n",
              "300    electra-large-discriminator             ru             gpt-4   \n",
              "301    electra-large-discriminator            all     gpt-3.5-turbo   \n",
              "302                    gpt2-medium             ru             gpt-4   \n",
              "303                    gpt2-medium             ru   alpaca-lora-30b   \n",
              "304    electra-large-discriminator             en             gpt-4   \n",
              "305                    gpt2-medium             ru  opt-iml-max-1.3b   \n",
              "306    electra-large-discriminator             ru     gpt-3.5-turbo   \n",
              "307    electra-large-discriminator             ru  text-davinci-003   \n",
              "308                    gpt2-medium             es             gpt-4   \n",
              "309                    gpt2-medium             ru  text-davinci-003   \n",
              "310              xlm-roberta-large             ru  opt-iml-max-1.3b   \n",
              "311                    gpt2-medium            en3   alpaca-lora-30b   \n",
              "312               mdeberta-v3-base             ru  opt-iml-max-1.3b   \n",
              "313                    gpt2-medium             ru     gpt-3.5-turbo   \n",
              "314                           mGPT             ru  opt-iml-max-1.3b   \n",
              "315                    gpt2-medium             en             gpt-4   \n",
              "316                    gpt2-medium             en     gpt-3.5-turbo   \n",
              "317                    gpt2-medium             en  text-davinci-003   \n",
              "318                    gpt2-medium            en3  text-davinci-003   \n",
              "319                    gpt2-medium            en3             gpt-4   \n",
              "320                    gpt2-medium            en3     gpt-3.5-turbo   \n",
              "321   bert-base-multilingual-cased             ru  opt-iml-max-1.3b   \n",
              "322    electra-large-discriminator            en3  text-davinci-003   \n",
              "323               blackbox-GPTZero            N/A               N/A   \n",
              "\n",
              "                                                                Detector  \\\n",
              "0                              mdeberta-v3-base-finetuned-all-all.csv.gz   \n",
              "1                             xlm-roberta-large-finetuned-all-all.csv.gz   \n",
              "2                              xlm-roberta-large-finetuned-es-all.csv.gz   \n",
              "3                              xlm-roberta-large-finetuned-ru-all.csv.gz   \n",
              "4                               mdeberta-v3-base-finetuned-es-all.csv.gz   \n",
              "5                  bert-base-multilingual-cased-finetuned-all-all.csv.gz   \n",
              "6                               mdeberta-v3-base-finetuned-ru-all.csv.gz   \n",
              "7                   bert-base-multilingual-cased-finetuned-es-all.csv.gz   \n",
              "8                 roberta-large-openai-detector-finetuned-all-all.csv.gz   \n",
              "9                                           mGPT-finetuned-ru-all.csv.gz   \n",
              "10                                         mGPT-finetuned-all-all.csv.gz   \n",
              "11                                          mGPT-finetuned-es-all.csv.gz   \n",
              "12                  bert-base-multilingual-cased-finetuned-ru-all.csv.gz   \n",
              "13                                  gpt2-medium-finetuned-all-all.csv.gz   \n",
              "14                  mdeberta-v3-base-finetuned-ru-alpaca-lora-30b.csv.gz   \n",
              "15    roberta-large-openai-detector-finetuned-en-opt-iml-max-1.3b.csv.gz   \n",
              "16   roberta-large-openai-detector-finetuned-en3-opt-iml-max-1.3b.csv.gz   \n",
              "17                 roberta-large-openai-detector-finetuned-es-all.csv.gz   \n",
              "18                  bert-base-multilingual-cased-finetuned-en-all.csv.gz   \n",
              "19                       mdeberta-v3-base-finetuned-ru-vicuna-13b.csv.gz   \n",
              "20                                   mGPT-finetuned-ru-vicuna-13b.csv.gz   \n",
              "21                              mdeberta-v3-base-finetuned-en-all.csv.gz   \n",
              "22            roberta-large-openai-detector-finetuned-en3-opt-66b.csv.gz   \n",
              "23                roberta-large-openai-detector-finetuned-en3-all.csv.gz   \n",
              "24                mdeberta-v3-base-finetuned-en3-opt-iml-max-1.3b.csv.gz   \n",
              "25                            xlm-roberta-large-finetuned-en3-all.csv.gz   \n",
              "26                             mGPT-finetuned-en-opt-iml-max-1.3b.csv.gz   \n",
              "27                                   gpt2-medium-finetuned-ru-all.csv.gz   \n",
              "28                      xlm-roberta-large-finetuned-ru-vicuna-13b.csv.gz   \n",
              "29     roberta-large-openai-detector-finetuned-en-alpaca-lora-30b.csv.gz   \n",
              "30                        mdeberta-v3-base-finetuned-en-llama-65b.csv.gz   \n",
              "31             roberta-large-openai-detector-finetuned-en-opt-66b.csv.gz   \n",
              "32     bert-base-multilingual-cased-finetuned-en-opt-iml-max-1.3b.csv.gz   \n",
              "33                 xlm-roberta-large-finetuned-ru-alpaca-lora-30b.csv.gz   \n",
              "34                     xlm-roberta-large-finetuned-all-vicuna-13b.csv.gz   \n",
              "35          roberta-large-openai-detector-finetuned-en-vicuna-13b.csv.gz   \n",
              "36           roberta-large-openai-detector-finetuned-es-llama-65b.csv.gz   \n",
              "37                         xlm-roberta-large-finetuned-es-opt-66b.csv.gz   \n",
              "38    roberta-large-openai-detector-finetuned-en3-alpaca-lora-30b.csv.gz   \n",
              "39                             mdeberta-v3-base-finetuned-en3-all.csv.gz   \n",
              "40                                          mGPT-finetuned-en-all.csv.gz   \n",
              "41         roberta-large-openai-detector-finetuned-en3-vicuna-13b.csv.gz   \n",
              "42                                   gpt2-medium-finetuned-es-all.csv.gz   \n",
              "43      bert-base-multilingual-cased-finetuned-ru-alpaca-lora-30b.csv.gz   \n",
              "44                             xlm-roberta-large-finetuned-en-all.csv.gz   \n",
              "45    bert-base-multilingual-cased-finetuned-en3-opt-iml-max-1.3b.csv.gz   \n",
              "46                            mGPT-finetuned-en3-opt-iml-max-1.3b.csv.gz   \n",
              "47    roberta-large-openai-detector-finetuned-en-text-davinci-003.csv.gz   \n",
              "48                 roberta-large-openai-detector-finetuned-ru-all.csv.gz   \n",
              "49           bert-base-multilingual-cased-finetuned-ru-vicuna-13b.csv.gz   \n",
              "50          bert-base-multilingual-cased-finetuned-en3-vicuna-13b.csv.gz   \n",
              "51                                   mGPT-finetuned-es-vicuna-13b.csv.gz   \n",
              "52       roberta-large-openai-detector-finetuned-en-gpt-3.5-turbo.csv.gz   \n",
              "53                                         mGPT-finetuned-en3-all.csv.gz   \n",
              "54                      xlm-roberta-large-finetuned-en3-llama-65b.csv.gz   \n",
              "55                         mdeberta-v3-base-finetuned-all-opt-66b.csv.gz   \n",
              "56                   electra-large-discriminator-finetuned-en-all.csv.gz   \n",
              "57                              mGPT-finetuned-es-alpaca-lora-30b.csv.gz   \n",
              "58                 roberta-large-openai-detector-finetuned-en-all.csv.gz   \n",
              "59     bert-base-multilingual-cased-finetuned-all-alpaca-lora-30b.csv.gz   \n",
              "60                 xlm-roberta-large-finetuned-en-alpaca-lora-30b.csv.gz   \n",
              "61                 mdeberta-v3-base-finetuned-all-alpaca-lora-30b.csv.gz   \n",
              "62                        mdeberta-v3-base-finetuned-ru-llama-65b.csv.gz   \n",
              "63          roberta-large-openai-detector-finetuned-en3-llama-65b.csv.gz   \n",
              "64                                      mGPT-finetuned-es-opt-66b.csv.gz   \n",
              "65                  mdeberta-v3-base-finetuned-es-alpaca-lora-30b.csv.gz   \n",
              "66            electra-large-discriminator-finetuned-en3-llama-65b.csv.gz   \n",
              "67      bert-base-multilingual-cased-finetuned-es-alpaca-lora-30b.csv.gz   \n",
              "68          bert-base-multilingual-cased-finetuned-all-vicuna-13b.csv.gz   \n",
              "69                              mGPT-finetuned-ru-alpaca-lora-30b.csv.gz   \n",
              "70                   xlm-roberta-large-finetuned-ru-gpt-3.5-turbo.csv.gz   \n",
              "71             roberta-large-openai-detector-finetuned-es-opt-66b.csv.gz   \n",
              "72                xlm-roberta-large-finetuned-all-alpaca-lora-30b.csv.gz   \n",
              "73                                   mGPT-finetuned-en3-llama-65b.csv.gz   \n",
              "74                  electra-large-discriminator-finetuned-en3-all.csv.gz   \n",
              "75                                    mGPT-finetuned-en-llama-65b.csv.gz   \n",
              "76    roberta-large-openai-detector-finetuned-es-opt-iml-max-1.3b.csv.gz   \n",
              "77      bert-base-multilingual-cased-finetuned-en-alpaca-lora-30b.csv.gz   \n",
              "78     bert-base-multilingual-cased-finetuned-en3-alpaca-lora-30b.csv.gz   \n",
              "79           roberta-large-openai-detector-finetuned-en-llama-65b.csv.gz   \n",
              "80                xlm-roberta-large-finetuned-en3-alpaca-lora-30b.csv.gz   \n",
              "81                                    mGPT-finetuned-ru-llama-65b.csv.gz   \n",
              "82           bert-base-multilingual-cased-finetuned-es-vicuna-13b.csv.gz   \n",
              "83                 mdeberta-v3-base-finetuned-en-opt-iml-max-1.3b.csv.gz   \n",
              "84                       mdeberta-v3-base-finetuned-es-vicuna-13b.csv.gz   \n",
              "85                      mdeberta-v3-base-finetuned-all-vicuna-13b.csv.gz   \n",
              "86             electra-large-discriminator-finetuned-en-llama-65b.csv.gz   \n",
              "87              bert-base-multilingual-cased-finetuned-es-opt-66b.csv.gz   \n",
              "88                         mdeberta-v3-base-finetuned-en3-opt-66b.csv.gz   \n",
              "89           bert-base-multilingual-cased-finetuned-en-vicuna-13b.csv.gz   \n",
              "90              electra-large-discriminator-finetuned-en3-opt-66b.csv.gz   \n",
              "91         roberta-large-openai-detector-finetuned-all-vicuna-13b.csv.gz   \n",
              "92                          mdeberta-v3-base-finetuned-es-opt-66b.csv.gz   \n",
              "93                       xlm-roberta-large-finetuned-en-llama-65b.csv.gz   \n",
              "94                 bert-base-multilingual-cased-finetuned-en3-all.csv.gz   \n",
              "95        bert-base-multilingual-cased-finetuned-ru-gpt-3.5-turbo.csv.gz   \n",
              "96                                    mGPT-finetuned-es-llama-65b.csv.gz   \n",
              "97                             mGPT-finetuned-all-alpaca-lora-30b.csv.gz   \n",
              "98                                     mGPT-finetuned-all-opt-66b.csv.gz   \n",
              "99                                  mGPT-finetuned-all-vicuna-13b.csv.gz   \n",
              "100               bert-base-multilingual-cased-finetuned-ru-gpt-4.csv.gz   \n",
              "101     roberta-large-openai-detector-finetuned-en3-gpt-3.5-turbo.csv.gz   \n",
              "102  roberta-large-openai-detector-finetuned-all-text-davinci-003.csv.gz   \n",
              "103   roberta-large-openai-detector-finetuned-ru-opt-iml-max-1.3b.csv.gz   \n",
              "104              xlm-roberta-large-finetuned-en3-opt-iml-max-1.3b.csv.gz   \n",
              "105                    xlm-roberta-large-finetuned-en3-vicuna-13b.csv.gz   \n",
              "106               xlm-roberta-large-finetuned-en-opt-iml-max-1.3b.csv.gz   \n",
              "107                      xlm-roberta-large-finetuned-ru-llama-65b.csv.gz   \n",
              "108            bert-base-multilingual-cased-finetuned-en3-opt-66b.csv.gz   \n",
              "109   bert-base-multilingual-cased-finetuned-all-text-davinci-003.csv.gz   \n",
              "110               bert-base-multilingual-cased-finetuned-es-gpt-4.csv.gz   \n",
              "111                   mdeberta-v3-base-finetuned-ru-gpt-3.5-turbo.csv.gz   \n",
              "112   roberta-large-openai-detector-finetuned-all-alpaca-lora-30b.csv.gz   \n",
              "113                                 mGPT-finetuned-en3-vicuna-13b.csv.gz   \n",
              "114           electra-large-discriminator-finetuned-ru-vicuna-13b.csv.gz   \n",
              "115    bert-base-multilingual-cased-finetuned-es-text-davinci-003.csv.gz   \n",
              "116       bert-base-multilingual-cased-finetuned-en-gpt-3.5-turbo.csv.gz   \n",
              "117     electra-large-discriminator-finetuned-en-opt-iml-max-1.3b.csv.gz   \n",
              "118      bert-base-multilingual-cased-finetuned-en3-gpt-3.5-turbo.csv.gz   \n",
              "119               xlm-roberta-large-finetuned-es-text-davinci-003.csv.gz   \n",
              "120                      mdeberta-v3-base-finetuned-all-llama-65b.csv.gz   \n",
              "121                 electra-large-discriminator-finetuned-all-all.csv.gz   \n",
              "122    roberta-large-openai-detector-finetuned-ru-alpaca-lora-30b.csv.gz   \n",
              "123                                    mGPT-finetuned-en3-opt-66b.csv.gz   \n",
              "124                          xlm-roberta-large-finetuned-ru-gpt-4.csv.gz   \n",
              "125              roberta-large-openai-detector-finetuned-en-gpt-4.csv.gz   \n",
              "126         roberta-large-openai-detector-finetuned-ru-vicuna-13b.csv.gz   \n",
              "127     electra-large-discriminator-finetuned-es-opt-iml-max-1.3b.csv.gz   \n",
              "128                  mdeberta-v3-base-finetuned-all-gpt-3.5-turbo.csv.gz   \n",
              "129                                  mGPT-finetuned-en-vicuna-13b.csv.gz   \n",
              "130    electra-large-discriminator-finetuned-en3-opt-iml-max-1.3b.csv.gz   \n",
              "131              electra-large-discriminator-finetuned-ru-opt-66b.csv.gz   \n",
              "132                        xlm-roberta-large-finetuned-en-opt-66b.csv.gz   \n",
              "133                             mGPT-finetuned-en-alpaca-lora-30b.csv.gz   \n",
              "134              bert-base-multilingual-cased-finetuned-all-gpt-4.csv.gz   \n",
              "135                mdeberta-v3-base-finetuned-en3-alpaca-lora-30b.csv.gz   \n",
              "136           bert-base-multilingual-cased-finetuned-ru-llama-65b.csv.gz   \n",
              "137                           gpt2-medium-finetuned-en3-llama-65b.csv.gz   \n",
              "138                         xlm-roberta-large-finetuned-en3-gpt-4.csv.gz   \n",
              "139                         xlm-roberta-large-finetuned-all-gpt-4.csv.gz   \n",
              "140                            mGPT-finetuned-en3-alpaca-lora-30b.csv.gz   \n",
              "141                   mdeberta-v3-base-finetuned-es-gpt-3.5-turbo.csv.gz   \n",
              "142                 xlm-roberta-large-finetuned-all-gpt-3.5-turbo.csv.gz   \n",
              "143                  electra-large-discriminator-finetuned-es-all.csv.gz   \n",
              "144            electra-large-discriminator-finetuned-ru-llama-65b.csv.gz   \n",
              "145                     xlm-roberta-large-finetuned-es-vicuna-13b.csv.gz   \n",
              "146                 xlm-roberta-large-finetuned-en3-gpt-3.5-turbo.csv.gz   \n",
              "147      bert-base-multilingual-cased-finetuned-all-gpt-3.5-turbo.csv.gz   \n",
              "148   bert-base-multilingual-cased-finetuned-all-opt-iml-max-1.3b.csv.gz   \n",
              "149               mdeberta-v3-base-finetuned-all-text-davinci-003.csv.gz   \n",
              "150   bert-base-multilingual-cased-finetuned-en3-text-davinci-003.csv.gz   \n",
              "151               mdeberta-v3-base-finetuned-all-opt-iml-max-1.3b.csv.gz   \n",
              "152                          mdeberta-v3-base-finetuned-all-gpt-4.csv.gz   \n",
              "153                                         statistical-entropy_RF-tuned   \n",
              "154            roberta-large-openai-detector-finetuned-ru-opt-66b.csv.gz   \n",
              "155           roberta-large-openai-detector-finetuned-all-opt-66b.csv.gz   \n",
              "156                                  gpt2-medium-finetuned-en-all.csv.gz   \n",
              "157                       mdeberta-v3-base-finetuned-es-llama-65b.csv.gz   \n",
              "158                mdeberta-v3-base-finetuned-es-text-davinci-003.csv.gz   \n",
              "159             bert-base-multilingual-cased-finetuned-en-opt-66b.csv.gz   \n",
              "160          roberta-large-openai-detector-finetuned-ru-llama-65b.csv.gz   \n",
              "161         roberta-large-openai-detector-finetuned-all-llama-65b.csv.gz   \n",
              "162                 mdeberta-v3-base-finetuned-en-alpaca-lora-30b.csv.gz   \n",
              "163                       xlm-roberta-large-finetuned-en3-opt-66b.csv.gz   \n",
              "164              electra-large-discriminator-finetuned-es-opt-66b.csv.gz   \n",
              "165                mdeberta-v3-base-finetuned-en-text-davinci-003.csv.gz   \n",
              "166                                     mGPT-finetuned-en-opt-66b.csv.gz   \n",
              "167               mdeberta-v3-base-finetuned-en3-text-davinci-003.csv.gz   \n",
              "168                     mdeberta-v3-base-finetuned-en3-vicuna-13b.csv.gz   \n",
              "169              bert-base-multilingual-cased-finetuned-en3-gpt-4.csv.gz   \n",
              "170                           mdeberta-v3-base-finetuned-es-gpt-4.csv.gz   \n",
              "171              roberta-large-openai-detector-finetuned-ru-gpt-4.csv.gz   \n",
              "172                  mdeberta-v3-base-finetuned-en3-gpt-3.5-turbo.csv.gz   \n",
              "173       bert-base-multilingual-cased-finetuned-es-gpt-3.5-turbo.csv.gz   \n",
              "174                  electra-large-discriminator-finetuned-ru-all.csv.gz   \n",
              "175                                                     statistical-rank   \n",
              "176                                                statistical-detectgpt   \n",
              "177                                                  statistical-entropy   \n",
              "178              electra-large-discriminator-finetuned-en-opt-66b.csv.gz   \n",
              "179                                               statistical-likelihood   \n",
              "180                                                 statistical-log_rank   \n",
              "181                            mGPT-finetuned-es-text-davinci-003.csv.gz   \n",
              "182                      mdeberta-v3-base-finetuned-en3-llama-65b.csv.gz   \n",
              "183                            gpt2-medium-finetuned-en-llama-65b.csv.gz   \n",
              "184             roberta-large-openai-detector-finetuned-en3-gpt-4.csv.gz   \n",
              "185    bert-base-multilingual-cased-finetuned-en-text-davinci-003.csv.gz   \n",
              "186                                                statistical-rank_GLTR   \n",
              "187                      xlm-roberta-large-finetuned-es-llama-65b.csv.gz   \n",
              "188                     xlm-roberta-large-finetuned-en-vicuna-13b.csv.gz   \n",
              "189                xlm-roberta-large-finetuned-es-alpaca-lora-30b.csv.gz   \n",
              "190                               mGPT-finetuned-es-gpt-3.5-turbo.csv.gz   \n",
              "191   roberta-large-openai-detector-finetuned-ru-text-davinci-003.csv.gz   \n",
              "192                     xlm-roberta-large-finetuned-all-llama-65b.csv.gz   \n",
              "193                            mGPT-finetuned-en-text-davinci-003.csv.gz   \n",
              "194  roberta-large-openai-detector-finetuned-en3-text-davinci-003.csv.gz   \n",
              "195                                 gpt2-medium-finetuned-en3-all.csv.gz   \n",
              "196                       xlm-roberta-large-finetuned-all-opt-66b.csv.gz   \n",
              "197                           mGPT-finetuned-all-text-davinci-003.csv.gz   \n",
              "198                               mGPT-finetuned-ru-gpt-3.5-turbo.csv.gz   \n",
              "199                           mdeberta-v3-base-finetuned-ru-gpt-4.csv.gz   \n",
              "200                          mdeberta-v3-base-finetuned-en3-gpt-4.csv.gz   \n",
              "201              xlm-roberta-large-finetuned-all-text-davinci-003.csv.gz   \n",
              "202                      mdeberta-v3-base-finetuned-en-vicuna-13b.csv.gz   \n",
              "203                                      mGPT-finetuned-all-gpt-4.csv.gz   \n",
              "204               xlm-roberta-large-finetuned-en-text-davinci-003.csv.gz   \n",
              "205            bert-base-multilingual-cased-finetuned-all-opt-66b.csv.gz   \n",
              "206      roberta-large-openai-detector-finetuned-ru-gpt-3.5-turbo.csv.gz   \n",
              "207            electra-large-discriminator-finetuned-es-llama-65b.csv.gz   \n",
              "208  roberta-large-openai-detector-finetuned-all-opt-iml-max-1.3b.csv.gz   \n",
              "209                               mGPT-finetuned-en-gpt-3.5-turbo.csv.gz   \n",
              "210                           gpt2-medium-finetuned-es-vicuna-13b.csv.gz   \n",
              "211           electra-large-discriminator-finetuned-en-vicuna-13b.csv.gz   \n",
              "212                         mdeberta-v3-base-finetuned-en-opt-66b.csv.gz   \n",
              "213          electra-large-discriminator-finetuned-en3-vicuna-13b.csv.gz   \n",
              "214                                       mGPT-finetuned-en-gpt-4.csv.gz   \n",
              "215                                  mGPT-finetuned-all-llama-65b.csv.gz   \n",
              "216                  xlm-roberta-large-finetuned-es-gpt-3.5-turbo.csv.gz   \n",
              "217                                       mGPT-finetuned-ru-gpt-4.csv.gz   \n",
              "218         roberta-large-openai-detector-finetuned-es-vicuna-13b.csv.gz   \n",
              "219                   mdeberta-v3-base-finetuned-en-gpt-3.5-turbo.csv.gz   \n",
              "220                              gpt2-medium-finetuned-es-opt-66b.csv.gz   \n",
              "221                            mGPT-finetuned-es-opt-iml-max-1.3b.csv.gz   \n",
              "222     electra-large-discriminator-finetuned-ru-opt-iml-max-1.3b.csv.gz   \n",
              "223          bert-base-multilingual-cased-finetuned-en3-llama-65b.csv.gz   \n",
              "224                             gpt2-medium-finetuned-all-opt-66b.csv.gz   \n",
              "225                                       mGPT-finetuned-es-gpt-4.csv.gz   \n",
              "226                           mGPT-finetuned-all-opt-iml-max-1.3b.csv.gz   \n",
              "227                           mGPT-finetuned-en3-text-davinci-003.csv.gz   \n",
              "228                                                     blackbox-ZeroGPT   \n",
              "229             roberta-large-openai-detector-finetuned-all-gpt-4.csv.gz   \n",
              "230                    gpt2-medium-finetuned-en3-opt-iml-max-1.3b.csv.gz   \n",
              "231                            gpt2-medium-finetuned-es-llama-65b.csv.gz   \n",
              "232                  xlm-roberta-large-finetuned-en-gpt-3.5-turbo.csv.gz   \n",
              "233                              mGPT-finetuned-all-gpt-3.5-turbo.csv.gz   \n",
              "234                              mGPT-finetuned-en3-gpt-3.5-turbo.csv.gz   \n",
              "235                    gpt2-medium-finetuned-all-text-davinci-003.csv.gz   \n",
              "236                     gpt2-medium-finetuned-en-opt-iml-max-1.3b.csv.gz   \n",
              "237          bert-base-multilingual-cased-finetuned-all-llama-65b.csv.gz   \n",
              "238           bert-base-multilingual-cased-finetuned-en-llama-65b.csv.gz   \n",
              "239                             gpt2-medium-finetuned-en3-opt-66b.csv.gz   \n",
              "240              xlm-roberta-large-finetuned-en3-text-davinci-003.csv.gz   \n",
              "241           bert-base-multilingual-cased-finetuned-es-llama-65b.csv.gz   \n",
              "242    roberta-large-openai-detector-finetuned-es-alpaca-lora-30b.csv.gz   \n",
              "243                          gpt2-medium-finetuned-all-vicuna-13b.csv.gz   \n",
              "244                            gpt2-medium-finetuned-ru-llama-65b.csv.gz   \n",
              "245               bert-base-multilingual-cased-finetuned-en-gpt-4.csv.gz   \n",
              "246                                      mGPT-finetuned-en3-gpt-4.csv.gz   \n",
              "247             electra-large-discriminator-finetuned-all-opt-66b.csv.gz   \n",
              "248    bert-base-multilingual-cased-finetuned-ru-text-davinci-003.csv.gz   \n",
              "249                      gpt2-medium-finetuned-es-alpaca-lora-30b.csv.gz   \n",
              "250                    gpt2-medium-finetuned-all-opt-iml-max-1.3b.csv.gz   \n",
              "251      roberta-large-openai-detector-finetuned-es-gpt-3.5-turbo.csv.gz   \n",
              "252     electra-large-discriminator-finetuned-en3-alpaca-lora-30b.csv.gz   \n",
              "253                            mGPT-finetuned-ru-text-davinci-003.csv.gz   \n",
              "254                     gpt2-medium-finetuned-es-text-davinci-003.csv.gz   \n",
              "255                     gpt2-medium-finetuned-all-alpaca-lora-30b.csv.gz   \n",
              "256   roberta-large-openai-detector-finetuned-es-text-davinci-003.csv.gz   \n",
              "257                                     mGPT-finetuned-ru-opt-66b.csv.gz   \n",
              "258      electra-large-discriminator-finetuned-ru-alpaca-lora-30b.csv.gz   \n",
              "259                mdeberta-v3-base-finetuned-es-opt-iml-max-1.3b.csv.gz   \n",
              "260                               gpt2-medium-finetuned-all-gpt-4.csv.gz   \n",
              "261                           gpt2-medium-finetuned-all-llama-65b.csv.gz   \n",
              "262                          xlm-roberta-large-finetuned-es-gpt-4.csv.gz   \n",
              "263              roberta-large-openai-detector-finetuned-es-gpt-4.csv.gz   \n",
              "264           electra-large-discriminator-finetuned-es-vicuna-13b.csv.gz   \n",
              "265               xlm-roberta-large-finetuned-ru-text-davinci-003.csv.gz   \n",
              "266                          gpt2-medium-finetuned-en3-vicuna-13b.csv.gz   \n",
              "267                       gpt2-medium-finetuned-all-gpt-3.5-turbo.csv.gz   \n",
              "268                          xlm-roberta-large-finetuned-en-gpt-4.csv.gz   \n",
              "269                mdeberta-v3-base-finetuned-ru-text-davinci-003.csv.gz   \n",
              "270     electra-large-discriminator-finetuned-en-text-davinci-003.csv.gz   \n",
              "271                           mdeberta-v3-base-finetuned-en-gpt-4.csv.gz   \n",
              "272        electra-large-discriminator-finetuned-en-gpt-3.5-turbo.csv.gz   \n",
              "273    bert-base-multilingual-cased-finetuned-es-opt-iml-max-1.3b.csv.gz   \n",
              "274              xlm-roberta-large-finetuned-all-opt-iml-max-1.3b.csv.gz   \n",
              "275                              gpt2-medium-finetuned-en-opt-66b.csv.gz   \n",
              "276     electra-large-discriminator-finetuned-es-text-davinci-003.csv.gz   \n",
              "277                        xlm-roberta-large-finetuned-ru-opt-66b.csv.gz   \n",
              "278     roberta-large-openai-detector-finetuned-all-gpt-3.5-turbo.csv.gz   \n",
              "279                     gpt2-medium-finetuned-es-opt-iml-max-1.3b.csv.gz   \n",
              "280      electra-large-discriminator-finetuned-es-alpaca-lora-30b.csv.gz   \n",
              "281           electra-large-discriminator-finetuned-all-llama-65b.csv.gz   \n",
              "282          electra-large-discriminator-finetuned-all-vicuna-13b.csv.gz   \n",
              "283                electra-large-discriminator-finetuned-es-gpt-4.csv.gz   \n",
              "284    electra-large-discriminator-finetuned-all-opt-iml-max-1.3b.csv.gz   \n",
              "285     electra-large-discriminator-finetuned-all-alpaca-lora-30b.csv.gz   \n",
              "286        electra-large-discriminator-finetuned-es-gpt-3.5-turbo.csv.gz   \n",
              "287      electra-large-discriminator-finetuned-en-alpaca-lora-30b.csv.gz   \n",
              "288                           gpt2-medium-finetuned-en-vicuna-13b.csv.gz   \n",
              "289             bert-base-multilingual-cased-finetuned-ru-opt-66b.csv.gz   \n",
              "290                         mdeberta-v3-base-finetuned-ru-opt-66b.csv.gz   \n",
              "291               electra-large-discriminator-finetuned-all-gpt-4.csv.gz   \n",
              "292                           gpt2-medium-finetuned-ru-vicuna-13b.csv.gz   \n",
              "293               electra-large-discriminator-finetuned-en3-gpt-4.csv.gz   \n",
              "294                        gpt2-medium-finetuned-es-gpt-3.5-turbo.csv.gz   \n",
              "295    electra-large-discriminator-finetuned-all-text-davinci-003.csv.gz   \n",
              "296               xlm-roberta-large-finetuned-es-opt-iml-max-1.3b.csv.gz   \n",
              "297                              gpt2-medium-finetuned-ru-opt-66b.csv.gz   \n",
              "298                      gpt2-medium-finetuned-en-alpaca-lora-30b.csv.gz   \n",
              "299       electra-large-discriminator-finetuned-en3-gpt-3.5-turbo.csv.gz   \n",
              "300                electra-large-discriminator-finetuned-ru-gpt-4.csv.gz   \n",
              "301       electra-large-discriminator-finetuned-all-gpt-3.5-turbo.csv.gz   \n",
              "302                                gpt2-medium-finetuned-ru-gpt-4.csv.gz   \n",
              "303                      gpt2-medium-finetuned-ru-alpaca-lora-30b.csv.gz   \n",
              "304                electra-large-discriminator-finetuned-en-gpt-4.csv.gz   \n",
              "305                     gpt2-medium-finetuned-ru-opt-iml-max-1.3b.csv.gz   \n",
              "306        electra-large-discriminator-finetuned-ru-gpt-3.5-turbo.csv.gz   \n",
              "307     electra-large-discriminator-finetuned-ru-text-davinci-003.csv.gz   \n",
              "308                                gpt2-medium-finetuned-es-gpt-4.csv.gz   \n",
              "309                     gpt2-medium-finetuned-ru-text-davinci-003.csv.gz   \n",
              "310               xlm-roberta-large-finetuned-ru-opt-iml-max-1.3b.csv.gz   \n",
              "311                     gpt2-medium-finetuned-en3-alpaca-lora-30b.csv.gz   \n",
              "312                mdeberta-v3-base-finetuned-ru-opt-iml-max-1.3b.csv.gz   \n",
              "313                        gpt2-medium-finetuned-ru-gpt-3.5-turbo.csv.gz   \n",
              "314                            mGPT-finetuned-ru-opt-iml-max-1.3b.csv.gz   \n",
              "315                                gpt2-medium-finetuned-en-gpt-4.csv.gz   \n",
              "316                        gpt2-medium-finetuned-en-gpt-3.5-turbo.csv.gz   \n",
              "317                     gpt2-medium-finetuned-en-text-davinci-003.csv.gz   \n",
              "318                    gpt2-medium-finetuned-en3-text-davinci-003.csv.gz   \n",
              "319                               gpt2-medium-finetuned-en3-gpt-4.csv.gz   \n",
              "320                       gpt2-medium-finetuned-en3-gpt-3.5-turbo.csv.gz   \n",
              "321    bert-base-multilingual-cased-finetuned-ru-opt-iml-max-1.3b.csv.gz   \n",
              "322    electra-large-discriminator-finetuned-en3-text-davinci-003.csv.gz   \n",
              "323                                                     blackbox-GPTZero   \n",
              "\n",
              "     Macro avg F1-score  Weighted avg F1-score  Weighted avg Precision  \\\n",
              "0              0.848011               0.939955                0.940286   \n",
              "1              0.824012               0.935233                0.935675   \n",
              "2              0.811016               0.925465                0.925731   \n",
              "3              0.798669               0.923438                0.921867   \n",
              "4              0.796051               0.922763                0.921114   \n",
              "5              0.756344               0.907265                0.905064   \n",
              "6              0.746595               0.910254                0.912071   \n",
              "7              0.740026               0.894183                0.898589   \n",
              "8              0.736044               0.893278                0.896759   \n",
              "9              0.721899               0.897587                0.894128   \n",
              "10             0.695916               0.873422                0.882171   \n",
              "11             0.684915               0.865265                0.879489   \n",
              "12             0.675434               0.887761                0.887928   \n",
              "13             0.664612               0.866769                0.868235   \n",
              "14             0.659287               0.815688                0.903477   \n",
              "15             0.652380               0.857916                0.864047   \n",
              "16             0.642569               0.849638                0.861358   \n",
              "17             0.634903               0.831519                0.865395   \n",
              "18             0.628257               0.856153                0.853998   \n",
              "19             0.622840               0.778630                0.904622   \n",
              "20             0.621305               0.835325                0.854650   \n",
              "21             0.614824               0.856630                0.849994   \n",
              "22             0.613234               0.867569                0.861752   \n",
              "23             0.610576               0.870014                0.870844   \n",
              "24             0.606918               0.789167                0.873799   \n",
              "25             0.606566               0.867796                0.865365   \n",
              "26             0.603522               0.825304                0.848154   \n",
              "27             0.594514               0.836389                0.840853   \n",
              "28             0.594081               0.756104                0.893803   \n",
              "29             0.594013               0.844587                0.840689   \n",
              "30             0.588254               0.821369                0.840924   \n",
              "31             0.579981               0.858662                0.850717   \n",
              "32             0.579916               0.831538                0.835010   \n",
              "33             0.579426               0.731606                0.904486   \n",
              "34             0.579232               0.732948                0.902283   \n",
              "35             0.579163               0.816171                0.837542   \n",
              "36             0.579094               0.805933                0.841042   \n",
              "37             0.577967               0.794830                0.845341   \n",
              "38             0.577122               0.848703                0.837530   \n",
              "39             0.573683               0.861951                0.870090   \n",
              "40             0.572694               0.837324                0.832379   \n",
              "41             0.572519               0.848716                0.836718   \n",
              "42             0.567999               0.784699                0.843346   \n",
              "43             0.567941               0.773019                0.850052   \n",
              "44             0.567889               0.861098                0.874124   \n",
              "45             0.567327               0.777897                0.846575   \n",
              "46             0.564812               0.787520                0.839684   \n",
              "47             0.564681               0.819524                0.829622   \n",
              "48             0.564216               0.858202                0.858897   \n",
              "49             0.564186               0.827698                0.828716   \n",
              "50             0.563109               0.735961                0.875729   \n",
              "51             0.561170               0.742678                0.866803   \n",
              "52             0.560357               0.800240                0.831930   \n",
              "53             0.560242               0.811661                0.828887   \n",
              "54             0.560237               0.799402                0.832127   \n",
              "55             0.555966               0.717663                0.886598   \n",
              "56             0.555896               0.795225                0.830961   \n",
              "57             0.555471               0.732468                0.869725   \n",
              "58             0.554132               0.859082                0.893326   \n",
              "59             0.552802               0.703734                0.899928   \n",
              "60             0.552626               0.732969                0.865824   \n",
              "61             0.551927               0.703762                0.898405   \n",
              "62             0.551419               0.764172                0.841678   \n",
              "63             0.551131               0.855902                0.861753   \n",
              "64             0.549055               0.777883                0.833172   \n",
              "65             0.546349               0.699507                0.894939   \n",
              "66             0.544748               0.853562                0.854318   \n",
              "67             0.544745               0.702597                0.888260   \n",
              "68             0.544604               0.696329                0.896384   \n",
              "69             0.544351               0.746981                0.846096   \n",
              "70             0.543448               0.689973                0.903533   \n",
              "71             0.542597               0.737869                0.850788   \n",
              "72             0.541894               0.687712                0.904093   \n",
              "73             0.541871               0.790086                0.824488   \n",
              "74             0.541786               0.854998                0.875666   \n",
              "75             0.541660               0.799265                0.822158   \n",
              "76             0.541318               0.740796                0.847427   \n",
              "77             0.540083               0.717446                0.865234   \n",
              "78             0.539153               0.708725                0.872864   \n",
              "79             0.537760               0.851597                0.850363   \n",
              "80             0.537613               0.708735                0.870756   \n",
              "81             0.535351               0.786728                0.821769   \n",
              "82             0.533970               0.688444                0.889205   \n",
              "83             0.533466               0.691296                0.884708   \n",
              "84             0.533426               0.687895                0.889027   \n",
              "85             0.532327               0.679635                0.898595   \n",
              "86             0.531405               0.794388                0.817985   \n",
              "87             0.531365               0.731171                0.844387   \n",
              "88             0.530852               0.763341                0.826744   \n",
              "89             0.529381               0.692110                0.877388   \n",
              "90             0.529356               0.741596                0.836044   \n",
              "91             0.529117               0.676535                0.897271   \n",
              "92             0.528281               0.691955                0.875918   \n",
              "93             0.527563               0.756254                0.827372   \n",
              "94             0.526371               0.851480                0.880885   \n",
              "95             0.525860               0.688357                0.876443   \n",
              "96             0.524760               0.743180                0.831363   \n",
              "97             0.522982               0.681149                0.880603   \n",
              "98             0.522330               0.681450                0.879195   \n",
              "99             0.522266               0.673810                0.888907   \n",
              "100            0.521967               0.715584                0.845904   \n",
              "101            0.521696               0.694117                0.864225   \n",
              "102            0.521483               0.688458                0.869850   \n",
              "103            0.521333               0.780650                0.815624   \n",
              "104            0.519412               0.706383                0.850236   \n",
              "105            0.518315               0.684640                0.869409   \n",
              "106            0.517943               0.705280                0.849442   \n",
              "107            0.517053               0.721882                0.836658   \n",
              "108            0.516472               0.726279                0.833474   \n",
              "109            0.516124               0.659103                0.898740   \n",
              "110            0.515462               0.674628                0.876541   \n",
              "111            0.513331               0.657628                0.895577   \n",
              "112            0.513234               0.663798                0.886642   \n",
              "113            0.513120               0.697748                0.849991   \n",
              "114            0.513115               0.785767                0.810492   \n",
              "115            0.511852               0.667855                0.878989   \n",
              "116            0.510298               0.658502                0.888655   \n",
              "117            0.509822               0.748808                0.817895   \n",
              "118            0.508449               0.653644                0.892137   \n",
              "119            0.506892               0.643275                0.905064   \n",
              "120            0.506674               0.671128                0.866959   \n",
              "121            0.505921               0.804059                0.805817   \n",
              "122            0.505909               0.759549                0.812115   \n",
              "123            0.504160               0.708932                0.832300   \n",
              "124            0.503719               0.640914                0.902234   \n",
              "125            0.503498               0.734582                0.818806   \n",
              "126            0.502183               0.750256                0.812442   \n",
              "127            0.501950               0.709198                0.830021   \n",
              "128            0.501057               0.643419                0.892931   \n",
              "129            0.500709               0.684201                0.846302   \n",
              "130            0.499904               0.723550                0.820767   \n",
              "131            0.499178               0.778436                0.804854   \n",
              "132            0.498294               0.701219                0.831309   \n",
              "133            0.498148               0.667606                0.858153   \n",
              "134            0.497424               0.635878                0.897160   \n",
              "135            0.496785               0.694191                0.834328   \n",
              "136            0.495636               0.707671                0.825016   \n",
              "137            0.495248               0.771771                0.803881   \n",
              "138            0.495228               0.639683                0.887072   \n",
              "139            0.494915               0.627250                0.905910   \n",
              "140            0.494811               0.665745                0.855264   \n",
              "141            0.494788               0.639302                0.886773   \n",
              "142            0.494016               0.624528                0.908593   \n",
              "143            0.491472               0.779179                0.801189   \n",
              "144            0.489651               0.749275                0.805122   \n",
              "145            0.489621               0.631758                0.887638   \n",
              "146            0.489415               0.626042                0.895942   \n",
              "147            0.489238               0.620887                0.904049   \n",
              "148            0.488158               0.630158                0.887087   \n",
              "149            0.487793               0.618915                0.904157   \n",
              "150            0.487528               0.633029                0.881766   \n",
              "151            0.487085               0.622277                0.896997   \n",
              "152            0.486767               0.622288                0.896294   \n",
              "153            0.486330               0.833501                0.805047   \n",
              "154            0.485909               0.761204                0.800798   \n",
              "155            0.485099               0.636301                0.872940   \n",
              "156            0.484911               0.707523                0.815748   \n",
              "157            0.484322               0.644680                0.861610   \n",
              "158            0.483200               0.618935                0.894004   \n",
              "159            0.482495               0.711106                0.812258   \n",
              "160            0.481693               0.742843                0.801937   \n",
              "161            0.481338               0.630707                0.873388   \n",
              "162            0.480343               0.641137                0.859192   \n",
              "163            0.479673               0.685480                0.822261   \n",
              "164            0.478770               0.693570                0.817049   \n",
              "165            0.478074               0.669787                0.830600   \n",
              "166            0.477264               0.674710                0.826364   \n",
              "167            0.476244               0.645452                0.848484   \n",
              "168            0.475553               0.657620                0.836760   \n",
              "169            0.474859               0.623915                0.870374   \n",
              "170            0.473011               0.611365                0.884211   \n",
              "171            0.472522               0.722666                0.801210   \n",
              "172            0.471563               0.647356                0.840150   \n",
              "173            0.471506               0.604620                0.891461   \n",
              "174            0.470887               0.836797                0.795170   \n",
              "175            0.470770               0.837535                0.791277   \n",
              "176            0.470770               0.837535                0.791277   \n",
              "177            0.470770               0.837535                0.791277   \n",
              "178            0.470752               0.669335                0.822786   \n",
              "179            0.470330               0.836752                0.791122   \n",
              "180            0.470157               0.836445                0.791062   \n",
              "181            0.469657               0.608342                0.881768   \n",
              "182            0.467960               0.648078                0.834753   \n",
              "183            0.466852               0.715445                0.799417   \n",
              "184            0.466843               0.657562                0.826214   \n",
              "185            0.466336               0.602412                0.883702   \n",
              "186            0.466236               0.828237                0.790060   \n",
              "187            0.465634               0.622665                0.855773   \n",
              "188            0.465287               0.611641                0.868552   \n",
              "189            0.464705               0.594435                0.892800   \n",
              "190            0.464688               0.601554                0.881505   \n",
              "191            0.463071               0.703432                0.800319   \n",
              "192            0.462752               0.622502                0.851212   \n",
              "193            0.462483               0.614994                0.859259   \n",
              "194            0.462450               0.605507                0.871300   \n",
              "195            0.461908               0.697166                0.801521   \n",
              "196            0.458762               0.585729                0.893595   \n",
              "197            0.458002               0.581530                0.899115   \n",
              "198            0.457593               0.594990                0.876324   \n",
              "199            0.456370               0.594568                0.874389   \n",
              "200            0.454663               0.617703                0.843320   \n",
              "201            0.454262               0.576727                0.898443   \n",
              "202            0.453134               0.688505                0.797733   \n",
              "203            0.452888               0.576577                0.895286   \n",
              "204            0.452250               0.587897                0.875471   \n",
              "205            0.451727               0.586571                0.876324   \n",
              "206            0.448999               0.670265                0.801161   \n",
              "207            0.447883               0.632802                0.820906   \n",
              "208            0.445988               0.580136                0.873524   \n",
              "209            0.445926               0.581224                0.871785   \n",
              "210            0.445570               0.578360                0.875282   \n",
              "211            0.445433               0.617309                0.829889   \n",
              "212            0.443604               0.620146                0.824974   \n",
              "213            0.443065               0.603959                0.838759   \n",
              "214            0.442825               0.589060                0.854993   \n",
              "215            0.441762               0.580140                0.864585   \n",
              "216            0.441229               0.555413                0.904442   \n",
              "217            0.440709               0.563984                0.887216   \n",
              "218            0.439047               0.577322                0.862878   \n",
              "219            0.437736               0.603669                0.830719   \n",
              "220            0.437147               0.603912                0.829605   \n",
              "221            0.434361               0.576725                0.854244   \n",
              "222            0.433427               0.633463                0.803664   \n",
              "223            0.432415               0.618272                0.811803   \n",
              "224            0.431336               0.565599                0.863066   \n",
              "225            0.430409               0.548773                0.887923   \n",
              "226            0.427575               0.543453                0.890173   \n",
              "227            0.427558               0.549788                0.878900   \n",
              "228            0.425926               0.555909                0.865302   \n",
              "229            0.421871               0.534461                0.891580   \n",
              "230            0.421614               0.574354                0.833418   \n",
              "231            0.420968               0.588508                0.818893   \n",
              "232            0.418850               0.531225                0.889369   \n",
              "233            0.417104               0.529118                0.888522   \n",
              "234            0.417025               0.539024                0.870755   \n",
              "235            0.414019               0.524990                0.887775   \n",
              "236            0.413160               0.563076                0.830495   \n",
              "237            0.411371               0.541472                0.853727   \n",
              "238            0.411308               0.600116                0.797511   \n",
              "239            0.409883               0.562598                0.825212   \n",
              "240            0.409586               0.518806                0.887125   \n",
              "241            0.409573               0.556951                0.830638   \n",
              "242            0.406698               0.528444                0.862437   \n",
              "243            0.405648               0.508230                0.896928   \n",
              "244            0.404211               0.595898                0.791470   \n",
              "245            0.402692               0.509692                0.884971   \n",
              "246            0.402445               0.515719                0.872945   \n",
              "247            0.401549               0.520420                0.862555   \n",
              "248            0.397715               0.502740                0.884032   \n",
              "249            0.396313               0.514619                0.858740   \n",
              "250            0.395710               0.502724                0.878138   \n",
              "251            0.393647               0.513379                0.854028   \n",
              "252            0.388707               0.531230                0.819730   \n",
              "253            0.388129               0.484379                0.892598   \n",
              "254            0.382733               0.483492                0.877149   \n",
              "255            0.380017               0.472399                0.892273   \n",
              "256            0.376126               0.482742                0.858898   \n",
              "257            0.371776               0.486537                0.840733   \n",
              "258            0.369966               0.498562                0.819608   \n",
              "259            0.369579               0.454965                0.896501   \n",
              "260            0.366916               0.453017                0.891398   \n",
              "261            0.365997               0.462306                0.867476   \n",
              "262            0.365840               0.446707                0.903333   \n",
              "263            0.365708               0.467121                0.857130   \n",
              "264            0.362544               0.468701                0.845076   \n",
              "265            0.358429               0.441555                0.887791   \n",
              "266            0.357012               0.467920                0.831253   \n",
              "267            0.355082               0.431747                0.900067   \n",
              "268            0.354579               0.433655                0.892921   \n",
              "269            0.354182               0.429168                0.903451   \n",
              "270            0.352606               0.454392                0.841436   \n",
              "271            0.350923               0.427030                0.895724   \n",
              "272            0.350254               0.442734                0.856264   \n",
              "273            0.348358               0.425944                0.888153   \n",
              "274            0.345646               0.421223                0.889589   \n",
              "275            0.345167               0.446399                0.833628   \n",
              "276            0.344822               0.444330                0.836225   \n",
              "277            0.340420               0.409619                0.899645   \n",
              "278            0.338767               0.408192                0.896429   \n",
              "279            0.335497               0.427709                0.837945   \n",
              "280            0.335094               0.425205                0.841499   \n",
              "281            0.331128               0.405731                0.870699   \n",
              "282            0.329236               0.393918                0.895246   \n",
              "283            0.325855               0.412467                0.835938   \n",
              "284            0.324427               0.387578                0.891864   \n",
              "285            0.321353               0.381345                0.896411   \n",
              "286            0.319260               0.396526                0.846361   \n",
              "287            0.314809               0.392909                0.837753   \n",
              "288            0.313205               0.390665                0.836711   \n",
              "289            0.309555               0.366109                0.886159   \n",
              "290            0.308479               0.361756                0.894933   \n",
              "291            0.303492               0.352319                0.900737   \n",
              "292            0.299265               0.358544                0.858971   \n",
              "293            0.294223               0.351677                0.854833   \n",
              "294            0.292554               0.340774                0.880562   \n",
              "295            0.290891               0.333114                0.898862   \n",
              "296            0.290052               0.331344                0.900632   \n",
              "297            0.288770               0.343528                0.852219   \n",
              "298            0.278989               0.325852                0.856611   \n",
              "299            0.266367               0.311631                0.834244   \n",
              "300            0.260882               0.310298                0.810358   \n",
              "301            0.259361               0.284140                0.894640   \n",
              "302            0.252007               0.282362                0.850926   \n",
              "303            0.249810               0.271696                0.880788   \n",
              "304            0.243161               0.267113                0.852350   \n",
              "305            0.236609               0.251246                0.875229   \n",
              "306            0.235810               0.266619                0.806339   \n",
              "307            0.233410               0.265601                0.795507   \n",
              "308            0.233255               0.245081                0.878635   \n",
              "309            0.223241               0.231421                0.863093   \n",
              "310            0.223035               0.224730                0.900575   \n",
              "311            0.211332               0.207221                0.890686   \n",
              "312            0.209753               0.203637                0.897897   \n",
              "313            0.205929               0.199784                0.880748   \n",
              "314            0.203065               0.198572                0.856898   \n",
              "315            0.197699               0.185879                0.883156   \n",
              "316            0.197480               0.184696                0.889629   \n",
              "317            0.193042               0.177290                0.890651   \n",
              "318            0.187736               0.168382                0.892424   \n",
              "319            0.184907               0.163490                0.894758   \n",
              "320            0.178861               0.153683                0.893620   \n",
              "321            0.176563               0.149606                0.896816   \n",
              "322            0.176044               0.150648                0.877163   \n",
              "323            0.160532               0.125840                0.863584   \n",
              "\n",
              "     Weighted avg Recall  Accuracy       FPR       FNR  Human samples  \\\n",
              "0               0.939648  0.939648  0.261434  0.035381           3236   \n",
              "1               0.939819  0.939819  0.417800  0.015772           3236   \n",
              "2               0.925209  0.925209  0.330655  0.043018           3236   \n",
              "3               0.926062  0.926062  0.408838  0.032350           3236   \n",
              "4               0.925755  0.925755  0.419345  0.031390           3236   \n",
              "5               0.910360  0.910360  0.478059  0.041406           3236   \n",
              "6               0.920737  0.920737  0.584672  0.016501           3236   \n",
              "7               0.890630  0.890630  0.413782  0.071568           3236   \n",
              "8               0.890357  0.890357  0.430779  0.069765           3236   \n",
              "9               0.904762  0.904762  0.575093  0.035650           3236   \n",
              "10              0.866633  0.866633  0.468789  0.091715           3236   \n",
              "11              0.854992  0.854992  0.455501  0.106451           3236   \n",
              "12              0.904284  0.904284  0.707664  0.019724           3236   \n",
              "13              0.865370  0.865370  0.584981  0.078706           3236   \n",
              "14              0.777573  0.777573  0.147404  0.231743           3236   \n",
              "15              0.852603  0.852603  0.576020  0.094171           3236   \n",
              "16              0.840212  0.840212  0.562732  0.109751           3236   \n",
              "17              0.809353  0.809353  0.465389  0.156529           3236   \n",
              "18              0.858440  0.858440  0.676143  0.075176           3236   \n",
              "19              0.728862  0.728862  0.100742  0.292298           3236   \n",
              "20              0.820584  0.820584  0.569221  0.131010           3236   \n",
              "21              0.864926  0.864926  0.732386  0.060900           3236   \n",
              "22              0.888786  0.888786  0.797590  0.025980           3236   \n",
              "23              0.895341  0.895341  0.817985  0.016079           3236   \n",
              "24              0.746134  0.746134  0.316440  0.246095           3236   \n",
              "25              0.892337  0.892337  0.818603  0.019379           3236   \n",
              "26              0.807919  0.807919  0.593016  0.142292           3236   \n",
              "27              0.832224  0.832224  0.698702  0.101846           3236   \n",
              "28              0.700632  0.700632  0.147713  0.318201           3236   \n",
              "29              0.848814  0.848814  0.743820  0.077593           3236   \n",
              "30              0.805701  0.805701  0.642460  0.138647           3236   \n",
              "31              0.884622  0.884622  0.850742  0.024061           3236   \n",
              "32              0.828230  0.828230  0.731150  0.102306           3236   \n",
              "33              0.670148  0.670148  0.068912  0.362255           3236   \n",
              "34              0.671753  0.671753  0.081891  0.358840           3236   \n",
              "35              0.799078  0.799078  0.655130  0.144518           3236   \n",
              "36              0.780509  0.780509  0.598269  0.172455           3236   \n",
              "37              0.761563  0.761563  0.538319  0.201197           3236   \n",
              "38              0.864482  0.864482  0.818294  0.050731           3236   \n",
              "39              0.894828  0.894828  0.878245  0.009171           3236   \n",
              "40              0.842704  0.842704  0.783684  0.079512           3236   \n",
              "41              0.866530  0.866530  0.831582  0.046778           3236   \n",
              "42              0.747090  0.747090  0.532756  0.218159           3236   \n",
              "43              0.728315  0.728315  0.461063  0.248168           3236   \n",
              "44              0.895682  0.895682  0.888133  0.006984           3236   \n",
              "45              0.736167  0.736167  0.495365  0.235082           3236   \n",
              "46              0.752586  0.752586  0.566749  0.207759           3236   \n",
              "47              0.810480  0.810480  0.732695  0.122069           3236   \n",
              "48              0.890971  0.890971  0.886588  0.012472           3236   \n",
              "49              0.826694  0.826694  0.771014  0.099083           3236   \n",
              "50              0.675781  0.675781  0.239802  0.334702           3236   \n",
              "51              0.684861  0.684861  0.303152  0.316628           3236   \n",
              "52              0.776003  0.776003  0.657602  0.170152           3236   \n",
              "53              0.797064  0.797064  0.713844  0.139491           3236   \n",
              "54              0.774535  0.774535  0.653894  0.172263           3236   \n",
              "55              0.652978  0.652978  0.160074  0.370237           3236   \n",
              "56              0.768391  0.768391  0.652967  0.179285           3236   \n",
              "57              0.671582  0.671582  0.273486  0.335239           3236   \n",
              "58              0.897764  0.897764  0.910074  0.001919           3236   \n",
              "59              0.636730  0.636730  0.078801  0.398595           3236   \n",
              "60              0.672401  0.672401  0.298517  0.331210           3236   \n",
              "61              0.636696  0.636696  0.086836  0.397636           3236   \n",
              "62              0.717665  0.717665  0.507108  0.254423           3236   \n",
              "63              0.892063  0.892063  0.906984  0.008711           3236   \n",
              "64              0.740365  0.740365  0.597033  0.217737           3236   \n",
              "65              0.631541  0.631541  0.102905  0.401435           3236   \n",
              "66              0.889879  0.889879  0.913164  0.010399           3236   \n",
              "67              0.634921  0.634921  0.140606  0.392954           3236   \n",
              "68              0.627889  0.627889  0.093634  0.406693           3236   \n",
              "69              0.692814  0.692814  0.447775  0.289727           3236   \n",
              "70              0.620925  0.620925  0.054079  0.419433           3236   \n",
              "71              0.679945  0.679945  0.402658  0.309797           3236   \n",
              "72              0.618365  0.618365  0.050371  0.422771           3236   \n",
              "73              0.763441  0.763441  0.692213  0.179976           3236   \n",
              "74              0.894521  0.894521  0.922435  0.004029           3236   \n",
              "75              0.780201  0.780201  0.733931  0.155954           3236   \n",
              "76              0.684246  0.684246  0.429543  0.301623           3236   \n",
              "77              0.652774  0.652774  0.285847  0.354849           3236   \n",
              "78              0.641953  0.641953  0.232077  0.373690           3236   \n",
              "79              0.888821  0.888821  0.921508  0.010553           3236   \n",
              "80              0.641953  0.641953  0.244438  0.372155           3236   \n",
              "81              0.759413  0.759413  0.705810  0.182816           3236   \n",
              "82              0.618331  0.618331  0.127009  0.413293           3236   \n",
              "83              0.621437  0.621437  0.152658  0.406616           3236   \n",
              "84              0.617682  0.617682  0.127627  0.413945           3236   \n",
              "85              0.608773  0.608773  0.074475  0.430561           3236   \n",
              "86              0.774569  0.774569  0.754635  0.159714           3236   \n",
              "87              0.671719  0.671719  0.436650  0.314824           3236   \n",
              "88              0.720669  0.720669  0.617120  0.237384           3236   \n",
              "89              0.622086  0.622086  0.193140  0.400860           3236   \n",
              "90              0.687046  0.687046  0.510198  0.288461           3236   \n",
              "91              0.605154  0.605154  0.079728  0.433977           3236   \n",
              "92              0.621847  0.621847  0.201174  0.400130           3236   \n",
              "93              0.709951  0.709951  0.598888  0.251698           3236   \n",
              "94              0.894043  0.894043  0.941286  0.002226           3236   \n",
              "95              0.617614  0.617614  0.195612  0.405580           3236   \n",
              "96              0.690152  0.690152  0.546354  0.280479           3236   \n",
              "97              0.609387  0.609387  0.168109  0.418243           3236   \n",
              "98              0.609660  0.609660  0.175834  0.416977           3236   \n",
              "99              0.601468  0.601468  0.120519  0.433056           3236   \n",
              "100             0.651169  0.651169  0.405748  0.341763           3236   \n",
              "101             0.624100  0.624100  0.269468  0.389117           3236   \n",
              "102             0.617477  0.617477  0.232386  0.401167           3236   \n",
              "103             0.752791  0.752791  0.740111  0.186001           3236   \n",
              "104             0.639256  0.639256  0.366811  0.359991           3236   \n",
              "105             0.612937  0.612937  0.231768  0.406347           3236   \n",
              "106             0.637890  0.637890  0.370519  0.361065           3236   \n",
              "107             0.660181  0.660181  0.475896  0.322921           3236   \n",
              "108             0.666462  0.666462  0.504326  0.312330           3236   \n",
              "109             0.585731  0.585731  0.065513  0.457577           3236   \n",
              "110             0.601604  0.601604  0.185414  0.424844           3236   \n",
              "111             0.583820  0.583820  0.080037  0.457922           3236   \n",
              "112             0.589964  0.589964  0.126700  0.445220           3236   \n",
              "113             0.628469  0.628469  0.358467  0.373153           3236   \n",
              "114             0.764670  0.764670  0.792336  0.166161           3236   \n",
              "115             0.593992  0.593992  0.168109  0.435550           3236   \n",
              "116             0.584195  0.584195  0.114030  0.453279           3236   \n",
              "117             0.701792  0.701792  0.656057  0.253770           3236   \n",
              "118             0.579109  0.579109  0.094870  0.461376           3236   \n",
              "119             0.569039  0.569039  0.031211  0.480602           3236   \n",
              "120             0.596996  0.596996  0.234549  0.423923           3236   \n",
              "121             0.802321  0.802321  0.874227  0.113665           3236   \n",
              "122             0.720430  0.720430  0.721570  0.224682           3236   \n",
              "123             0.643489  0.643489  0.486712  0.340343           3236   \n",
              "124             0.566206  0.566206  0.043263  0.482290           3236   \n",
              "125             0.680696  0.680696  0.622991  0.281592           3236   \n",
              "126             0.705854  0.705854  0.700247  0.243716           3236   \n",
              "127             0.644035  0.644035  0.502472  0.337772           3236   \n",
              "128             0.567981  0.567981  0.086836  0.474884           3236   \n",
              "129             0.611811  0.611811  0.365884  0.390959           3236   \n",
              "130             0.664687  0.664687  0.589617  0.303734           3236   \n",
              "131             0.755726  0.755726  0.818912  0.172915           3236   \n",
              "132             0.633521  0.633521  0.482386  0.352086           3236   \n",
              "133             0.592422  0.592422  0.280284  0.423385           3236   \n",
              "134             0.560266  0.560266  0.064586  0.486320           3236   \n",
              "135             0.624373  0.624373  0.453028  0.366016           3236   \n",
              "136             0.642499  0.642499  0.534302  0.335546           3236   \n",
              "137             0.744837  0.744837  0.811496  0.186078           3236   \n",
              "138             0.563338  0.563338  0.112794  0.476879           3236   \n",
              "139             0.552040  0.552040  0.023486  0.500672           3236   \n",
              "140             0.590101  0.590101  0.294808  0.424191           3236   \n",
              "141             0.562895  0.562895  0.114030  0.477225           3236   \n",
              "142             0.549479  0.549479  0.011434  0.505046           3236   \n",
              "143             0.759652  0.759652  0.848578  0.164818           3236   \n",
              "144             0.707254  0.707254  0.754326  0.235427           3236   \n",
              "145             0.554839  0.554839  0.106613  0.487202           3236   \n",
              "146             0.549650  0.549650  0.066749  0.497985           3236   \n",
              "147             0.545144  0.545144  0.029975  0.507617           3236   \n",
              "148             0.553064  0.553064  0.108467  0.488967           3236   \n",
              "149             0.543096  0.543096  0.029048  0.510035           3236   \n",
              "150             0.555590  0.555590  0.134734  0.482866           3236   \n",
              "151             0.545793  0.545793  0.060878  0.503051           3236   \n",
              "152             0.545725  0.545725  0.063968  0.502744           3236   \n",
              "153             0.872913  0.872913  0.975587  0.021720           3236   \n",
              "154             0.728793  0.728793  0.812423  0.203999           3236   \n",
              "155             0.558252  0.558252  0.179234  0.474347           3236   \n",
              "156             0.643420  0.643420  0.598578  0.326528           3236   \n",
              "157             0.566479  0.566479  0.242583  0.457232           3236   \n",
              "158             0.541935  0.541935  0.072930  0.505890           3236   \n",
              "159             0.648882  0.648882  0.629481  0.316551           3236   \n",
              "160             0.698481  0.698481  0.765760  0.243870           3236   \n",
              "161             0.552210  0.552210  0.173671  0.481830           3236   \n",
              "162             0.562314  0.562314  0.252472  0.460685           3236   \n",
              "163             0.613791  0.613791  0.519778  0.369623           3236   \n",
              "164             0.624612  0.624612  0.567058  0.351587           3236   \n",
              "165             0.594095  0.594095  0.444994  0.401051           3236   \n",
              "166             0.600137  0.600137  0.478059  0.390153           3236   \n",
              "167             0.566308  0.566308  0.313659  0.448597           3236   \n",
              "168             0.579689  0.579689  0.393078  0.423692           3236   \n",
              "169             0.544564  0.544564  0.184178  0.489121           3236   \n",
              "170             0.532855  0.532855  0.113412  0.511071           3236   \n",
              "171             0.667964  0.667964  0.731768  0.282398           3236   \n",
              "172             0.567913  0.567913  0.362176  0.440769           3236   \n",
              "173             0.526745  0.526745  0.079110  0.522200           3236   \n",
              "174             0.887797  0.887797  0.999382  0.002034           3236   \n",
              "175             0.889537  0.889537  1.000000  0.000000           3236   \n",
              "176             0.889537  0.889537  1.000000  0.000000           3236   \n",
              "177             0.889537  0.889537  1.000000  0.000000           3236   \n",
              "178             0.593514  0.593514  0.493511  0.395679           3236   \n",
              "179             0.887967  0.887967  1.000000  0.001765           3236   \n",
              "180             0.887353  0.887353  1.000000  0.002456           3236   \n",
              "181             0.529408  0.529408  0.122991  0.513757           3236   \n",
              "182             0.568425  0.568425  0.394005  0.436241           3236   \n",
              "183             0.657826  0.657826  0.731459  0.293833           3236   \n",
              "184             0.579246  0.579246  0.456428  0.416324           3236   \n",
              "185             0.523502  0.523502  0.111867  0.521778           3236   \n",
              "186             0.870729  0.870729  0.999073  0.021259           3236   \n",
              "187             0.541662  0.541662  0.255562  0.483518           3236   \n",
              "188             0.531285  0.531285  0.185414  0.503895           3236   \n",
              "189             0.516505  0.516505  0.070148  0.534825           3236   \n",
              "190             0.522342  0.522342  0.121137  0.521931           3236   \n",
              "191             0.640348  0.640348  0.702410  0.317088           3236   \n",
              "192             0.541014  0.541014  0.278739  0.481369           3236   \n",
              "193             0.533777  0.533777  0.232386  0.495261           3236   \n",
              "194             0.525175  0.525175  0.169036  0.512798           3236   \n",
              "195             0.631371  0.631371  0.682324  0.329675           3236   \n",
              "196             0.507834  0.507834  0.064277  0.545301           3236   \n",
              "197             0.504386  0.504386  0.041100  0.552055           3236   \n",
              "198             0.514934  0.514934  0.140915  0.527802           3236   \n",
              "199             0.514252  0.514252  0.149258  0.527534           3236   \n",
              "200             0.534972  0.534972  0.315513  0.483595           3236   \n",
              "201             0.499539  0.499539  0.042645  0.557312           3236   \n",
              "202             0.620038  0.620038  0.694067  0.340957           3236   \n",
              "203             0.498959  0.498959  0.055006  0.556430           3236   \n",
              "204             0.507595  0.507595  0.141224  0.536014           3236   \n",
              "205             0.506366  0.506366  0.136897  0.537933           3236   \n",
              "206             0.595392  0.595392  0.638133  0.375609           3236   \n",
              "207             0.549923  0.549923  0.456737  0.449250           3236   \n",
              "208             0.499505  0.499505  0.145859  0.544534           3236   \n",
              "209             0.500358  0.500358  0.153894  0.542576           3236   \n",
              "210             0.497969  0.497969  0.137515  0.547297           3236   \n",
              "211             0.533197  0.533197  0.387206  0.476688           3236   \n",
              "212             0.535893  0.535893  0.417800  0.469857           3236   \n",
              "213             0.519645  0.519645  0.326329  0.499482           3236   \n",
              "214             0.506059  0.506059  0.234240  0.526191           3236   \n",
              "215             0.498276  0.498276  0.184796  0.541080           3236   \n",
              "216             0.479672  0.479672  0.016069  0.582946           3236   \n",
              "217             0.485475  0.485475  0.083127  0.568095           3236   \n",
              "218             0.495204  0.495204  0.190667  0.543804           3236   \n",
              "219             0.518416  0.518416  0.368047  0.495683           3236   \n",
              "220             0.518553  0.518553  0.374227  0.494762           3236   \n",
              "221             0.493395  0.493395  0.228986  0.541080           3236   \n",
              "222             0.549787  0.549787  0.562732  0.436241           3236   \n",
              "223             0.532685  0.532685  0.491347  0.464331           3236   \n",
              "224             0.483564  0.483564  0.182942  0.557849           3236   \n",
              "225             0.470934  0.470934  0.075711  0.585364           3236   \n",
              "226             0.466223  0.466223  0.065822  0.591888           3236   \n",
              "227             0.470558  0.470558  0.110630  0.581450           3236   \n",
              "228             0.474415  0.474415  0.168109  0.569976           3236   \n",
              "229             0.457996  0.457996  0.058405  0.602057           3236   \n",
              "230             0.488070  0.488070  0.325093  0.535132           3236   \n",
              "231             0.500836  0.500836  0.414091  0.509728           3236   \n",
              "232             0.454651  0.454651  0.065513  0.604935           3236   \n",
              "233             0.452569  0.452569  0.067985  0.606969           3236   \n",
              "234             0.459089  0.459089  0.137824  0.590967           3236   \n",
              "235             0.448643  0.448643  0.069530  0.611190           3236   \n",
              "236             0.476259  0.476259  0.328492  0.547987           3236   \n",
              "237             0.458747  0.458747  0.207664  0.582678           3236   \n",
              "238             0.511077  0.511077  0.550062  0.481331           3236   \n",
              "239             0.474996  0.474996  0.353523  0.546299           3236   \n",
              "240             0.442874  0.442874  0.070148  0.617599           3236   \n",
              "241             0.470183  0.470183  0.322002  0.555624           3236   \n",
              "242             0.447858  0.447858  0.165019  0.600215           3236   \n",
              "243             0.434818  0.434818  0.034611  0.631068           3236   \n",
              "244             0.505820  0.505820  0.579728  0.483557           3236   \n",
              "245             0.434272  0.434272  0.075093  0.626655           3236   \n",
              "246             0.437822  0.437822  0.119592  0.617138           3236   \n",
              "247             0.440451  0.440451  0.160383  0.609118           3236   \n",
              "248             0.427889  0.427889  0.076329  0.633677           3236   \n",
              "249             0.434511  0.434511  0.171817  0.614375           3236   \n",
              "250             0.426933  0.426933  0.096415  0.632258           3236   \n",
              "251             0.432599  0.432599  0.189122  0.614375           3236   \n",
              "252             0.443455  0.443455  0.347342  0.582524           3236   \n",
              "253             0.413074  0.413074  0.044190  0.654323           3236   \n",
              "254             0.409831  0.409831  0.093325  0.651867           3236   \n",
              "255             0.402697  0.402697  0.042954  0.666142           3236   \n",
              "256             0.406144  0.406144  0.154512  0.648413           3236   \n",
              "257             0.406315  0.406315  0.222188  0.639817           3236   \n",
              "258             0.413210  0.413210  0.315513  0.620477           3236   \n",
              "259             0.388633  0.388633  0.027812  0.683833           3236   \n",
              "260             0.386209  0.386209  0.042027  0.684792           3236   \n",
              "261             0.390101  0.390101  0.116811  0.671131           3236   \n",
              "262             0.382830  0.382830  0.008035  0.692812           3236   \n",
              "263             0.392422  0.392422  0.152658  0.664070           3236   \n",
              "264             0.391671  0.391671  0.194685  0.659695           3236   \n",
              "265             0.376173  0.376173  0.050062  0.695077           3236   \n",
              "266             0.388530  0.388530  0.243511  0.657163           3236   \n",
              "267             0.370097  0.370097  0.015451  0.706205           3236   \n",
              "268             0.370541  0.370541  0.034611  0.703327           3236   \n",
              "269             0.368527  0.368527  0.006489  0.709083           3236   \n",
              "270             0.378973  0.378973  0.198084  0.673548           3236   \n",
              "271             0.365626  0.365626  0.026267  0.709889           3236   \n",
              "272             0.371941  0.371941  0.143387  0.688246           3236   \n",
              "273             0.363577  0.363577  0.046044  0.709736           3236   \n",
              "274             0.360027  0.360027  0.041409  0.714302           3236   \n",
              "275             0.370951  0.370951  0.219098  0.679957           3236   \n",
              "276             0.369722  0.369722  0.208900  0.682605           3236   \n",
              "277             0.352381  0.352381  0.014524  0.726237           3236   \n",
              "278             0.350777  0.350777  0.022250  0.727081           3236   \n",
              "279             0.356580  0.356580  0.192213  0.699451           3236   \n",
              "280             0.355214  0.355214  0.179543  0.702560           3236   \n",
              "281             0.344837  0.344837  0.087145  0.725699           3236   \n",
              "282             0.339512  0.339512  0.023486  0.739591           3236   \n",
              "283             0.344188  0.344188  0.188504  0.713842           3236   \n",
              "284             0.334153  0.334153  0.030593  0.744733           3236   \n",
              "285             0.330090  0.330090  0.019468  0.750681           3236   \n",
              "286             0.333709  0.333709  0.148949  0.730535           3236   \n",
              "287             0.329476  0.329476  0.170890  0.732568           3236   \n",
              "288             0.327599  0.327599  0.172435  0.734487           3236   \n",
              "289             0.317187  0.317187  0.040173  0.762616           3236   \n",
              "290             0.315242  0.315242  0.020705  0.767221           3236   \n",
              "291             0.309131  0.309131  0.008035  0.775663           3236   \n",
              "292             0.307527  0.307527  0.099506  0.766108           3236   \n",
              "293             0.301929  0.301929  0.106304  0.771557           3236   \n",
              "294             0.297969  0.297969  0.047281  0.783338           3236   \n",
              "295             0.295033  0.295033  0.010507  0.791205           3236   \n",
              "296             0.294009  0.294009  0.007108  0.792778           3236   \n",
              "297             0.295716  0.295716  0.108776  0.778234           3236   \n",
              "298             0.284008  0.284008  0.092089  0.793469           3236   \n",
              "299             0.270968  0.270968  0.131953  0.803177           3236   \n",
              "300             0.266325  0.266325  0.182942  0.802065           3236   \n",
              "301             0.260727  0.260727  0.014215  0.829310           3236   \n",
              "302             0.254037  0.254037  0.085909  0.827929           3236   \n",
              "303             0.250862  0.250862  0.033993  0.837945           3236   \n",
              "304             0.244410  0.244410  0.077565  0.839787           3236   \n",
              "305             0.237071  0.237071  0.038319  0.852911           3236   \n",
              "306             0.237856  0.237856  0.157602  0.837216           3236   \n",
              "307             0.235637  0.235637  0.177379  0.837254           3236   \n",
              "308             0.233555  0.233555  0.032447  0.857592           3236   \n",
              "309             0.223383  0.223383  0.050062  0.866841           3236   \n",
              "310             0.223041  0.223041  0.003090  0.873057           3236   \n",
              "311             0.211367  0.211367  0.012979  0.884953           3236   \n",
              "312             0.209831  0.209831  0.005253  0.887640           3236   \n",
              "313             0.206008  0.206008  0.022559  0.889789           3236   \n",
              "314             0.203106  0.203106  0.047899  0.889904           3236   \n",
              "315             0.197986  0.197986  0.018232  0.899344           3236   \n",
              "316             0.197815  0.197815  0.012052  0.900303           3236   \n",
              "317             0.193548  0.193548  0.010507  0.905292           3236   \n",
              "318             0.188496  0.188496  0.008344  0.911240           3236   \n",
              "319             0.185834  0.185834  0.006180  0.914502           3236   \n",
              "320             0.180133  0.180133  0.006489  0.920872           3236   \n",
              "321             0.178017  0.178017  0.004017  0.923558           3236   \n",
              "322             0.177334  0.177334  0.018232  0.922560           3236   \n",
              "323             0.162895  0.162895  0.022559  0.938255           3236   \n",
              "\n",
              "     Machine samples  AUC_preds  AUC_probs  threshold  \n",
              "0              26059   0.851592   0.960706        0.5  \n",
              "1              26059   0.783214   0.965781        0.5  \n",
              "2              26059   0.813164   0.922260        0.5  \n",
              "3              26059   0.779406   0.944882        0.5  \n",
              "4              26059   0.774632   0.930882        0.5  \n",
              "5              26059   0.740267   0.918754        0.5  \n",
              "6              26059   0.699413   0.926895        0.5  \n",
              "7              26059   0.757325   0.881894        0.5  \n",
              "8              26059   0.749728   0.864470        0.5  \n",
              "9              26059   0.694629   0.878017        0.5  \n",
              "10             26059   0.719748   0.870902        0.5  \n",
              "11             26059   0.719024   0.862852        0.5  \n",
              "12             26059   0.636306   0.855513        0.5  \n",
              "13             26059   0.668156   0.789929        0.5  \n",
              "14             26059   0.810426   0.887485        0.5  \n",
              "15             26059   0.664905   0.755152        0.5  \n",
              "16             26059   0.663759   0.724382        0.5  \n",
              "17             26059   0.689041   0.745071        0.5  \n",
              "18             26059   0.624341   0.780937        0.5  \n",
              "19             26059   0.803480   0.889736        0.5  \n",
              "20             26059   0.649884   0.747929        0.5  \n",
              "21             26059   0.603357   0.733930        0.5  \n",
              "22             26059   0.588215   0.712085        0.5  \n",
              "23             26059   0.582968   0.695187        0.5  \n",
              "24             26059   0.718732   0.781433        0.5  \n",
              "25             26059   0.581009   0.760209        0.5  \n",
              "26             26059   0.632346   0.723809        0.5  \n",
              "27             26059   0.599726   0.698380        0.5  \n",
              "28             26059   0.767043   0.849992        0.5  \n",
              "29             26059   0.589294   0.669721        0.5  \n",
              "30             26059   0.609447   0.635741        0.5  \n",
              "31             26059   0.562599   0.651288        0.5  \n",
              "32             26059   0.583272   0.625012        0.5  \n",
              "33             26059   0.784416   0.869572        0.5  \n",
              "34             26059   0.779635   0.884636        0.5  \n",
              "35             26059   0.600176   0.669345        0.5  \n",
              "36             26059   0.614638   0.643611        0.5  \n",
              "37             26059   0.630242   0.719857        0.5  \n",
              "38             26059   0.565487   0.657949        0.5  \n",
              "39             26059   0.556292   0.795436        0.5  \n",
              "40             26059   0.568402   0.701581        0.5  \n",
              "41             26059   0.560820   0.646861        0.5  \n",
              "42             26059   0.624542   0.692207        0.5  \n",
              "43             26059   0.645385   0.746760        0.5  \n",
              "44             26059   0.552441   0.779727        0.5  \n",
              "45             26059   0.634777   0.660347        0.5  \n",
              "46             26059   0.612746   0.674759        0.5  \n",
              "47             26059   0.572618   0.648633        0.5  \n",
              "48             26059   0.550470   0.584697        0.5  \n",
              "49             26059   0.564952   0.709849        0.5  \n",
              "50             26059   0.712748   0.778700        0.5  \n",
              "51             26059   0.690110   0.775206        0.5  \n",
              "52             26059   0.586123   0.644935        0.5  \n",
              "53             26059   0.573332   0.717078        0.5  \n",
              "54             26059   0.586922   0.618038        0.5  \n",
              "55             26059   0.734845   0.801388        0.5  \n",
              "56             26059   0.583874   0.605279        0.5  \n",
              "57             26059   0.695637   0.772116        0.5  \n",
              "58             26059   0.544004   0.692618        0.5  \n",
              "59             26059   0.761302   0.822377        0.5  \n",
              "60             26059   0.685137   0.754744        0.5  \n",
              "61             26059   0.757764   0.904026        0.5  \n",
              "62             26059   0.619235   0.699780        0.5  \n",
              "63             26059   0.542153   0.561775        0.5  \n",
              "64             26059   0.592615   0.654873        0.5  \n",
              "65             26059   0.747830   0.877235        0.5  \n",
              "66             26059   0.538218   0.549112        0.5  \n",
              "67             26059   0.733220   0.806364        0.5  \n",
              "68             26059   0.749837   0.831581        0.5  \n",
              "69             26059   0.631249   0.735858        0.5  \n",
              "70             26059   0.763244   0.859526        0.5  \n",
              "71             26059   0.643773   0.684635        0.5  \n",
              "72             26059   0.763429   0.866735        0.5  \n",
              "73             26059   0.563906   0.581040        0.5  \n",
              "74             26059   0.536768   0.655048        0.5  \n",
              "75             26059   0.555058   0.583853        0.5  \n",
              "76             26059   0.634417   0.699382        0.5  \n",
              "77             26059   0.679652   0.748058        0.5  \n",
              "78             26059   0.697116   0.761980        0.5  \n",
              "79             26059   0.533969   0.568730        0.5  \n",
              "80             26059   0.691703   0.729961        0.5  \n",
              "81             26059   0.555687   0.593804        0.5  \n",
              "82             26059   0.729849   0.809739        0.5  \n",
              "83             26059   0.720363   0.799561        0.5  \n",
              "84             26059   0.729214   0.790451        0.5  \n",
              "85             26059   0.747482   0.897301        0.5  \n",
              "86             26059   0.542825   0.544350        0.5  \n",
              "87             26059   0.624263   0.692956        0.5  \n",
              "88             26059   0.572748   0.590467        0.5  \n",
              "89             26059   0.703000   0.768892        0.5  \n",
              "90             26059   0.600671   0.657734        0.5  \n",
              "91             26059   0.743148   0.842250        0.5  \n",
              "92             26059   0.699348   0.746321        0.5  \n",
              "93             26059   0.574707   0.615944        0.5  \n",
              "94             26059   0.528244   0.750318        0.5  \n",
              "95             26059   0.699404   0.741409        0.5  \n",
              "96             26059   0.586584   0.627180        0.5  \n",
              "97             26059   0.706824   0.795345        0.5  \n",
              "98             26059   0.703594   0.761374        0.5  \n",
              "99             26059   0.723213   0.813149        0.5  \n",
              "100            26059   0.626245   0.699374        0.5  \n",
              "101            26059   0.670707   0.731856        0.5  \n",
              "102            26059   0.683224   0.742683        0.5  \n",
              "103            26059   0.536944   0.502054        0.5  \n",
              "104            26059   0.636599   0.697180        0.5  \n",
              "105            26059   0.680943   0.733247        0.5  \n",
              "106            26059   0.634208   0.714988        0.5  \n",
              "107            26059   0.600591   0.656779        0.5  \n",
              "108            26059   0.591672   0.616230        0.5  \n",
              "109            26059   0.738455   0.788112        0.5  \n",
              "110            26059   0.694871   0.750145        0.5  \n",
              "111            26059   0.731020   0.840883        0.5  \n",
              "112            26059   0.714040   0.810285        0.5  \n",
              "113            26059   0.634190   0.716675        0.5  \n",
              "114            26059   0.520751   0.523704        0.5  \n",
              "115            26059   0.698171   0.759722        0.5  \n",
              "116            26059   0.716346   0.763013        0.5  \n",
              "117            26059   0.545086   0.597917        0.5  \n",
              "118            26059   0.721877   0.755431        0.5  \n",
              "119            26059   0.744093   0.853889        0.5  \n",
              "120            26059   0.670764   0.754289        0.5  \n",
              "121            26059   0.506054   0.512044        0.5  \n",
              "122            26059   0.526874   0.477450        0.5  \n",
              "123            26059   0.586472   0.635100        0.5  \n",
              "124            26059   0.737223   0.832728        0.5  \n",
              "125            26059   0.547708   0.583410        0.5  \n",
              "126            26059   0.528018   0.487323        0.5  \n",
              "127            26059   0.579878   0.630541        0.5  \n",
              "128            26059   0.719140   0.801897        0.5  \n",
              "129            26059   0.621579   0.716454        0.5  \n",
              "130            26059   0.553325   0.574046        0.5  \n",
              "131            26059   0.504086   0.502312        0.5  \n",
              "132            26059   0.582764   0.648291        0.5  \n",
              "133            26059   0.648165   0.728190        0.5  \n",
              "134            26059   0.724547   0.767448        0.5  \n",
              "135            26059   0.590478   0.701507        0.5  \n",
              "136            26059   0.565076   0.616450        0.5  \n",
              "137            26059   0.501213   0.507860        0.5  \n",
              "138            26059   0.705164   0.736256        0.5  \n",
              "139            26059   0.737921   0.764362        0.5  \n",
              "140            26059   0.640500   0.710134        0.5  \n",
              "141            26059   0.704373   0.695489        0.5  \n",
              "142            26059   0.741760   0.795428        0.5  \n",
              "143            26059   0.493302   0.495433        0.5  \n",
              "144            26059   0.505123   0.510775        0.5  \n",
              "145            26059   0.703092   0.824232        0.5  \n",
              "146            26059   0.717633   0.792722        0.5  \n",
              "147            26059   0.731204   0.807163        0.5  \n",
              "148            26059   0.701283   0.760717        0.5  \n",
              "149            26059   0.730458   0.832195        0.5  \n",
              "150            26059   0.691200   0.725784        0.5  \n",
              "151            26059   0.718036   0.807079        0.5  \n",
              "152            26059   0.716644   0.803885        0.5  \n",
              "153            26059   0.501346        NaN        NaN  \n",
              "154            26059   0.491789   0.443959        0.5  \n",
              "155            26059   0.673210   0.747711        0.5  \n",
              "156            26059   0.537447   0.562276        0.5  \n",
              "157            26059   0.650092   0.694914        0.5  \n",
              "158            26059   0.710590   0.809962        0.5  \n",
              "159            26059   0.526984   0.532684        0.5  \n",
              "160            26059   0.495185   0.486053        0.5  \n",
              "161            26059   0.672250   0.789353        0.5  \n",
              "162            26059   0.643421   0.744190        0.5  \n",
              "163            26059   0.555300   0.594744        0.5  \n",
              "164            26059   0.540678   0.601624        0.5  \n",
              "165            26059   0.576977   0.690535        0.5  \n",
              "166            26059   0.565894   0.620299        0.5  \n",
              "167            26059   0.618872   0.696166        0.5  \n",
              "168            26059   0.591615   0.690941        0.5  \n",
              "169            26059   0.663351   0.709188        0.5  \n",
              "170            26059   0.687759   0.749026        0.5  \n",
              "171            26059   0.492917   0.462085        0.5  \n",
              "172            26059   0.598528   0.691391        0.5  \n",
              "173            26059   0.699345   0.773888        0.5  \n",
              "174            26059   0.499292   0.506971        0.5  \n",
              "175            26059   0.500000        NaN        NaN  \n",
              "176            26059   0.500000        NaN        NaN  \n",
              "177            26059   0.500000        NaN        NaN  \n",
              "178            26059   0.555405   0.602249        0.5  \n",
              "179            26059   0.499117        NaN        NaN  \n",
              "180            26059   0.498772        NaN        NaN  \n",
              "181            26059   0.681626   0.742618        0.5  \n",
              "182            26059   0.584877   0.638281        0.5  \n",
              "183            26059   0.487354   0.505357        0.5  \n",
              "184            26059   0.563624   0.591772        0.5  \n",
              "185            26059   0.683178   0.726928        0.5  \n",
              "186            26059   0.489834        NaN        NaN  \n",
              "187            26059   0.630460   0.660296        0.5  \n",
              "188            26059   0.655345   0.768221        0.5  \n",
              "189            26059   0.697513   0.842146        0.5  \n",
              "190            26059   0.678466   0.739678        0.5  \n",
              "191            26059   0.490251   0.487587        0.5  \n",
              "192            26059   0.619946   0.671150        0.5  \n",
              "193            26059   0.636177   0.696095        0.5  \n",
              "194            26059   0.659083   0.724217        0.5  \n",
              "195            26059   0.494001   0.516002        0.5  \n",
              "196            26059   0.695211   0.753989        0.5  \n",
              "197            26059   0.703422   0.785601        0.5  \n",
              "198            26059   0.665641   0.714516        0.5  \n",
              "199            26059   0.661604   0.759367        0.5  \n",
              "200            26059   0.600446   0.668121        0.5  \n",
              "201            26059   0.700021   0.852871        0.5  \n",
              "202            26059   0.482488   0.629460        0.5  \n",
              "203            26059   0.694282   0.750463        0.5  \n",
              "204            26059   0.661381   0.778180        0.5  \n",
              "205            26059   0.662585   0.768075        0.5  \n",
              "206            26059   0.493129   0.488656        0.5  \n",
              "207            26059   0.547007   0.609385        0.5  \n",
              "208            26059   0.654804   0.742389        0.5  \n",
              "209            26059   0.651765   0.711667        0.5  \n",
              "210            26059   0.657594   0.748483        0.5  \n",
              "211            26059   0.568053   0.637477        0.5  \n",
              "212            26059   0.556172   0.593869        0.5  \n",
              "213            26059   0.587095   0.668084        0.5  \n",
              "214            26059   0.619785   0.680873        0.5  \n",
              "215            26059   0.637062   0.675300        0.5  \n",
              "216            26059   0.700492   0.842410        0.5  \n",
              "217            26059   0.674389   0.723332        0.5  \n",
              "218            26059   0.632764   0.712582        0.5  \n",
              "219            26059   0.568135   0.684327        0.5  \n",
              "220            26059   0.565505   0.600430        0.5  \n",
              "221            26059   0.614967   0.628957        0.5  \n",
              "222            26059   0.500514   0.532835        0.5  \n",
              "223            26059   0.522161   0.545700        0.5  \n",
              "224            26059   0.629604   0.701977        0.5  \n",
              "225            26059   0.669463   0.739242        0.5  \n",
              "226            26059   0.671145   0.720559        0.5  \n",
              "227            26059   0.653960   0.679092        0.5  \n",
              "228            26059   0.630958        NaN        NaN  \n",
              "229            26059   0.669769   0.755480        0.5  \n",
              "230            26059   0.569888   0.630157        0.5  \n",
              "231            26059   0.538090   0.559947        0.5  \n",
              "232            26059   0.664776   0.775137        0.5  \n",
              "233            26059   0.662523   0.755304        0.5  \n",
              "234            26059   0.635604   0.690062        0.5  \n",
              "235            26059   0.659640   0.766416        0.5  \n",
              "236            26059   0.561760   0.598876        0.5  \n",
              "237            26059   0.604829   0.675881        0.5  \n",
              "238            26059   0.484304   0.511104        0.5  \n",
              "239            26059   0.550089   0.563874        0.5  \n",
              "240            26059   0.656127   0.773114        0.5  \n",
              "241            26059   0.561187   0.600818        0.5  \n",
              "242            26059   0.617383   0.704118        0.5  \n",
              "243            26059   0.667161   0.832440        0.5  \n",
              "244            26059   0.468358   0.482927        0.5  \n",
              "245            26059   0.649126   0.691477        0.5  \n",
              "246            26059   0.631635   0.646943        0.5  \n",
              "247            26059   0.615250   0.687791        0.5  \n",
              "248            26059   0.644997   0.732773        0.5  \n",
              "249            26059   0.606904   0.643591        0.5  \n",
              "250            26059   0.635664   0.762180        0.5  \n",
              "251            26059   0.598251   0.653135        0.5  \n",
              "252            26059   0.535067   0.635839        0.5  \n",
              "253            26059   0.650743   0.745371        0.5  \n",
              "254            26059   0.627404   0.672555        0.5  \n",
              "255            26059   0.645452   0.785381        0.5  \n",
              "256            26059   0.598538   0.670794        0.5  \n",
              "257            26059   0.568997   0.570702        0.5  \n",
              "258            26059   0.532005   0.529911        0.5  \n",
              "259            26059   0.644178   0.679795        0.5  \n",
              "260            26059   0.636590   0.776197        0.5  \n",
              "261            26059   0.606029   0.634321        0.5  \n",
              "262            26059   0.649576   0.824223        0.5  \n",
              "263            26059   0.591636   0.642544        0.5  \n",
              "264            26059   0.572810   0.636909        0.5  \n",
              "265            26059   0.627431   0.788449        0.5  \n",
              "266            26059   0.549663   0.635090        0.5  \n",
              "267            26059   0.639172   0.804542        0.5  \n",
              "268            26059   0.631031   0.755875        0.5  \n",
              "269            26059   0.642214   0.833264        0.5  \n",
              "270            26059   0.564184   0.653209        0.5  \n",
              "271            26059   0.631922   0.661799        0.5  \n",
              "272            26059   0.584184   0.671382        0.5  \n",
              "273            26059   0.622110   0.670766        0.5  \n",
              "274            26059   0.622144   0.688418        0.5  \n",
              "275            26059   0.550473   0.580848        0.5  \n",
              "276            26059   0.554248   0.643464        0.5  \n",
              "277            26059   0.629620   0.701259        0.5  \n",
              "278            26059   0.625335   0.774304        0.5  \n",
              "279            26059   0.554168   0.637338        0.5  \n",
              "280            26059   0.558949   0.639474        0.5  \n",
              "281            26059   0.593578   0.624965        0.5  \n",
              "282            26059   0.618462   0.707840        0.5  \n",
              "283            26059   0.548827   0.633436        0.5  \n",
              "284            26059   0.612337   0.700732        0.5  \n",
              "285            26059   0.614925   0.715804        0.5  \n",
              "286            26059   0.560258   0.644083        0.5  \n",
              "287            26059   0.548271   0.640889        0.5  \n",
              "288            26059   0.546539   0.628785        0.5  \n",
              "289            26059   0.598606   0.569724        0.5  \n",
              "290            26059   0.606037   0.699036        0.5  \n",
              "291            26059   0.608151   0.705440        0.5  \n",
              "292            26059   0.567193   0.648312        0.5  \n",
              "293            26059   0.561070   0.636948        0.5  \n",
              "294            26059   0.584691   0.658344        0.5  \n",
              "295            26059   0.599144   0.702262        0.5  \n",
              "296            26059   0.600057   0.679857        0.5  \n",
              "297            26059   0.556495   0.600062        0.5  \n",
              "298            26059   0.557221   0.610913        0.5  \n",
              "299            26059   0.532435   0.620925        0.5  \n",
              "300            26059   0.507497   0.523619        0.5  \n",
              "301            26059   0.578237   0.687410        0.5  \n",
              "302            26059   0.543081   0.585849        0.5  \n",
              "303            26059   0.564031   0.697837        0.5  \n",
              "304            26059   0.541324   0.628192        0.5  \n",
              "305            26059   0.554385   0.672273        0.5  \n",
              "306            26059   0.502591   0.527629        0.5  \n",
              "307            26059   0.492683   0.493666        0.5  \n",
              "308            26059   0.554980   0.644483        0.5  \n",
              "309            26059   0.541549   0.582568        0.5  \n",
              "310            26059   0.561926   0.568527        0.5  \n",
              "311            26059   0.551034   0.629469        0.5  \n",
              "312            26059   0.553554   0.591348        0.5  \n",
              "313            26059   0.543826   0.661491        0.5  \n",
              "314            26059   0.531099   0.563328        0.5  \n",
              "315            26059   0.541212   0.623898        0.5  \n",
              "316            26059   0.543822   0.613916        0.5  \n",
              "317            26059   0.542101   0.599857        0.5  \n",
              "318            26059   0.540208   0.604055        0.5  \n",
              "319            26059   0.539659   0.631604        0.5  \n",
              "320            26059   0.536319   0.599225        0.5  \n",
              "321            26059   0.536212   0.631759        0.5  \n",
              "322            26059   0.529604   0.618568        0.5  \n",
              "323            26059   0.519593        NaN        NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f9e8c56-b23f-49b2-af38-928c18882334\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Train Language</th>\n",
              "      <th>Train LLM</th>\n",
              "      <th>Detector</th>\n",
              "      <th>Macro avg F1-score</th>\n",
              "      <th>Weighted avg F1-score</th>\n",
              "      <th>Weighted avg Precision</th>\n",
              "      <th>Weighted avg Recall</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>FPR</th>\n",
              "      <th>FNR</th>\n",
              "      <th>Human samples</th>\n",
              "      <th>Machine samples</th>\n",
              "      <th>AUC_preds</th>\n",
              "      <th>AUC_probs</th>\n",
              "      <th>threshold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>all</td>\n",
              "      <td>all</td>\n",
              "      <td>mdeberta-v3-base-finetuned-all-all.csv.gz</td>\n",
              "      <td>0.848011</td>\n",
              "      <td>0.939955</td>\n",
              "      <td>0.940286</td>\n",
              "      <td>0.939648</td>\n",
              "      <td>0.939648</td>\n",
              "      <td>0.261434</td>\n",
              "      <td>0.035381</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.851592</td>\n",
              "      <td>0.960706</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>all</td>\n",
              "      <td>all</td>\n",
              "      <td>xlm-roberta-large-finetuned-all-all.csv.gz</td>\n",
              "      <td>0.824012</td>\n",
              "      <td>0.935233</td>\n",
              "      <td>0.935675</td>\n",
              "      <td>0.939819</td>\n",
              "      <td>0.939819</td>\n",
              "      <td>0.417800</td>\n",
              "      <td>0.015772</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.783214</td>\n",
              "      <td>0.965781</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>es</td>\n",
              "      <td>all</td>\n",
              "      <td>xlm-roberta-large-finetuned-es-all.csv.gz</td>\n",
              "      <td>0.811016</td>\n",
              "      <td>0.925465</td>\n",
              "      <td>0.925731</td>\n",
              "      <td>0.925209</td>\n",
              "      <td>0.925209</td>\n",
              "      <td>0.330655</td>\n",
              "      <td>0.043018</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.813164</td>\n",
              "      <td>0.922260</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>ru</td>\n",
              "      <td>all</td>\n",
              "      <td>xlm-roberta-large-finetuned-ru-all.csv.gz</td>\n",
              "      <td>0.798669</td>\n",
              "      <td>0.923438</td>\n",
              "      <td>0.921867</td>\n",
              "      <td>0.926062</td>\n",
              "      <td>0.926062</td>\n",
              "      <td>0.408838</td>\n",
              "      <td>0.032350</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.779406</td>\n",
              "      <td>0.944882</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>es</td>\n",
              "      <td>all</td>\n",
              "      <td>mdeberta-v3-base-finetuned-es-all.csv.gz</td>\n",
              "      <td>0.796051</td>\n",
              "      <td>0.922763</td>\n",
              "      <td>0.921114</td>\n",
              "      <td>0.925755</td>\n",
              "      <td>0.925755</td>\n",
              "      <td>0.419345</td>\n",
              "      <td>0.031390</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.774632</td>\n",
              "      <td>0.930882</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>all</td>\n",
              "      <td>all</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-all-all.csv.gz</td>\n",
              "      <td>0.756344</td>\n",
              "      <td>0.907265</td>\n",
              "      <td>0.905064</td>\n",
              "      <td>0.910360</td>\n",
              "      <td>0.910360</td>\n",
              "      <td>0.478059</td>\n",
              "      <td>0.041406</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.740267</td>\n",
              "      <td>0.918754</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>ru</td>\n",
              "      <td>all</td>\n",
              "      <td>mdeberta-v3-base-finetuned-ru-all.csv.gz</td>\n",
              "      <td>0.746595</td>\n",
              "      <td>0.910254</td>\n",
              "      <td>0.912071</td>\n",
              "      <td>0.920737</td>\n",
              "      <td>0.920737</td>\n",
              "      <td>0.584672</td>\n",
              "      <td>0.016501</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.699413</td>\n",
              "      <td>0.926895</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>es</td>\n",
              "      <td>all</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-es-all.csv.gz</td>\n",
              "      <td>0.740026</td>\n",
              "      <td>0.894183</td>\n",
              "      <td>0.898589</td>\n",
              "      <td>0.890630</td>\n",
              "      <td>0.890630</td>\n",
              "      <td>0.413782</td>\n",
              "      <td>0.071568</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.757325</td>\n",
              "      <td>0.881894</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>all</td>\n",
              "      <td>all</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-all-all.csv.gz</td>\n",
              "      <td>0.736044</td>\n",
              "      <td>0.893278</td>\n",
              "      <td>0.896759</td>\n",
              "      <td>0.890357</td>\n",
              "      <td>0.890357</td>\n",
              "      <td>0.430779</td>\n",
              "      <td>0.069765</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.749728</td>\n",
              "      <td>0.864470</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>ru</td>\n",
              "      <td>all</td>\n",
              "      <td>mGPT-finetuned-ru-all.csv.gz</td>\n",
              "      <td>0.721899</td>\n",
              "      <td>0.897587</td>\n",
              "      <td>0.894128</td>\n",
              "      <td>0.904762</td>\n",
              "      <td>0.904762</td>\n",
              "      <td>0.575093</td>\n",
              "      <td>0.035650</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.694629</td>\n",
              "      <td>0.878017</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>all</td>\n",
              "      <td>all</td>\n",
              "      <td>mGPT-finetuned-all-all.csv.gz</td>\n",
              "      <td>0.695916</td>\n",
              "      <td>0.873422</td>\n",
              "      <td>0.882171</td>\n",
              "      <td>0.866633</td>\n",
              "      <td>0.866633</td>\n",
              "      <td>0.468789</td>\n",
              "      <td>0.091715</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.719748</td>\n",
              "      <td>0.870902</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>es</td>\n",
              "      <td>all</td>\n",
              "      <td>mGPT-finetuned-es-all.csv.gz</td>\n",
              "      <td>0.684915</td>\n",
              "      <td>0.865265</td>\n",
              "      <td>0.879489</td>\n",
              "      <td>0.854992</td>\n",
              "      <td>0.854992</td>\n",
              "      <td>0.455501</td>\n",
              "      <td>0.106451</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.719024</td>\n",
              "      <td>0.862852</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>ru</td>\n",
              "      <td>all</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-ru-all.csv.gz</td>\n",
              "      <td>0.675434</td>\n",
              "      <td>0.887761</td>\n",
              "      <td>0.887928</td>\n",
              "      <td>0.904284</td>\n",
              "      <td>0.904284</td>\n",
              "      <td>0.707664</td>\n",
              "      <td>0.019724</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.636306</td>\n",
              "      <td>0.855513</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>all</td>\n",
              "      <td>all</td>\n",
              "      <td>gpt2-medium-finetuned-all-all.csv.gz</td>\n",
              "      <td>0.664612</td>\n",
              "      <td>0.866769</td>\n",
              "      <td>0.868235</td>\n",
              "      <td>0.865370</td>\n",
              "      <td>0.865370</td>\n",
              "      <td>0.584981</td>\n",
              "      <td>0.078706</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.668156</td>\n",
              "      <td>0.789929</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>ru</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-ru-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.659287</td>\n",
              "      <td>0.815688</td>\n",
              "      <td>0.903477</td>\n",
              "      <td>0.777573</td>\n",
              "      <td>0.777573</td>\n",
              "      <td>0.147404</td>\n",
              "      <td>0.231743</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.810426</td>\n",
              "      <td>0.887485</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>en</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-en-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.652380</td>\n",
              "      <td>0.857916</td>\n",
              "      <td>0.864047</td>\n",
              "      <td>0.852603</td>\n",
              "      <td>0.852603</td>\n",
              "      <td>0.576020</td>\n",
              "      <td>0.094171</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.664905</td>\n",
              "      <td>0.755152</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>en3</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-en3-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.642569</td>\n",
              "      <td>0.849638</td>\n",
              "      <td>0.861358</td>\n",
              "      <td>0.840212</td>\n",
              "      <td>0.840212</td>\n",
              "      <td>0.562732</td>\n",
              "      <td>0.109751</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.663759</td>\n",
              "      <td>0.724382</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>es</td>\n",
              "      <td>all</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-es-all.csv.gz</td>\n",
              "      <td>0.634903</td>\n",
              "      <td>0.831519</td>\n",
              "      <td>0.865395</td>\n",
              "      <td>0.809353</td>\n",
              "      <td>0.809353</td>\n",
              "      <td>0.465389</td>\n",
              "      <td>0.156529</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.689041</td>\n",
              "      <td>0.745071</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>en</td>\n",
              "      <td>all</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-en-all.csv.gz</td>\n",
              "      <td>0.628257</td>\n",
              "      <td>0.856153</td>\n",
              "      <td>0.853998</td>\n",
              "      <td>0.858440</td>\n",
              "      <td>0.858440</td>\n",
              "      <td>0.676143</td>\n",
              "      <td>0.075176</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.624341</td>\n",
              "      <td>0.780937</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>ru</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-ru-vicuna-13b.csv.gz</td>\n",
              "      <td>0.622840</td>\n",
              "      <td>0.778630</td>\n",
              "      <td>0.904622</td>\n",
              "      <td>0.728862</td>\n",
              "      <td>0.728862</td>\n",
              "      <td>0.100742</td>\n",
              "      <td>0.292298</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.803480</td>\n",
              "      <td>0.889736</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>ru</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mGPT-finetuned-ru-vicuna-13b.csv.gz</td>\n",
              "      <td>0.621305</td>\n",
              "      <td>0.835325</td>\n",
              "      <td>0.854650</td>\n",
              "      <td>0.820584</td>\n",
              "      <td>0.820584</td>\n",
              "      <td>0.569221</td>\n",
              "      <td>0.131010</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.649884</td>\n",
              "      <td>0.747929</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>en</td>\n",
              "      <td>all</td>\n",
              "      <td>mdeberta-v3-base-finetuned-en-all.csv.gz</td>\n",
              "      <td>0.614824</td>\n",
              "      <td>0.856630</td>\n",
              "      <td>0.849994</td>\n",
              "      <td>0.864926</td>\n",
              "      <td>0.864926</td>\n",
              "      <td>0.732386</td>\n",
              "      <td>0.060900</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.603357</td>\n",
              "      <td>0.733930</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>en3</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-en3-opt-66b.csv.gz</td>\n",
              "      <td>0.613234</td>\n",
              "      <td>0.867569</td>\n",
              "      <td>0.861752</td>\n",
              "      <td>0.888786</td>\n",
              "      <td>0.888786</td>\n",
              "      <td>0.797590</td>\n",
              "      <td>0.025980</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.588215</td>\n",
              "      <td>0.712085</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>en3</td>\n",
              "      <td>all</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-en3-all.csv.gz</td>\n",
              "      <td>0.610576</td>\n",
              "      <td>0.870014</td>\n",
              "      <td>0.870844</td>\n",
              "      <td>0.895341</td>\n",
              "      <td>0.895341</td>\n",
              "      <td>0.817985</td>\n",
              "      <td>0.016079</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.582968</td>\n",
              "      <td>0.695187</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>en3</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-en3-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.606918</td>\n",
              "      <td>0.789167</td>\n",
              "      <td>0.873799</td>\n",
              "      <td>0.746134</td>\n",
              "      <td>0.746134</td>\n",
              "      <td>0.316440</td>\n",
              "      <td>0.246095</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.718732</td>\n",
              "      <td>0.781433</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>en3</td>\n",
              "      <td>all</td>\n",
              "      <td>xlm-roberta-large-finetuned-en3-all.csv.gz</td>\n",
              "      <td>0.606566</td>\n",
              "      <td>0.867796</td>\n",
              "      <td>0.865365</td>\n",
              "      <td>0.892337</td>\n",
              "      <td>0.892337</td>\n",
              "      <td>0.818603</td>\n",
              "      <td>0.019379</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.581009</td>\n",
              "      <td>0.760209</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>en</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mGPT-finetuned-en-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.603522</td>\n",
              "      <td>0.825304</td>\n",
              "      <td>0.848154</td>\n",
              "      <td>0.807919</td>\n",
              "      <td>0.807919</td>\n",
              "      <td>0.593016</td>\n",
              "      <td>0.142292</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.632346</td>\n",
              "      <td>0.723809</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>ru</td>\n",
              "      <td>all</td>\n",
              "      <td>gpt2-medium-finetuned-ru-all.csv.gz</td>\n",
              "      <td>0.594514</td>\n",
              "      <td>0.836389</td>\n",
              "      <td>0.840853</td>\n",
              "      <td>0.832224</td>\n",
              "      <td>0.832224</td>\n",
              "      <td>0.698702</td>\n",
              "      <td>0.101846</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.599726</td>\n",
              "      <td>0.698380</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>ru</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>xlm-roberta-large-finetuned-ru-vicuna-13b.csv.gz</td>\n",
              "      <td>0.594081</td>\n",
              "      <td>0.756104</td>\n",
              "      <td>0.893803</td>\n",
              "      <td>0.700632</td>\n",
              "      <td>0.700632</td>\n",
              "      <td>0.147713</td>\n",
              "      <td>0.318201</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.767043</td>\n",
              "      <td>0.849992</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>en</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-en-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.594013</td>\n",
              "      <td>0.844587</td>\n",
              "      <td>0.840689</td>\n",
              "      <td>0.848814</td>\n",
              "      <td>0.848814</td>\n",
              "      <td>0.743820</td>\n",
              "      <td>0.077593</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.589294</td>\n",
              "      <td>0.669721</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>en</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-en-llama-65b.csv.gz</td>\n",
              "      <td>0.588254</td>\n",
              "      <td>0.821369</td>\n",
              "      <td>0.840924</td>\n",
              "      <td>0.805701</td>\n",
              "      <td>0.805701</td>\n",
              "      <td>0.642460</td>\n",
              "      <td>0.138647</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.609447</td>\n",
              "      <td>0.635741</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>en</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-en-opt-66b.csv.gz</td>\n",
              "      <td>0.579981</td>\n",
              "      <td>0.858662</td>\n",
              "      <td>0.850717</td>\n",
              "      <td>0.884622</td>\n",
              "      <td>0.884622</td>\n",
              "      <td>0.850742</td>\n",
              "      <td>0.024061</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.562599</td>\n",
              "      <td>0.651288</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>en</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-en-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.579916</td>\n",
              "      <td>0.831538</td>\n",
              "      <td>0.835010</td>\n",
              "      <td>0.828230</td>\n",
              "      <td>0.828230</td>\n",
              "      <td>0.731150</td>\n",
              "      <td>0.102306</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.583272</td>\n",
              "      <td>0.625012</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>ru</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>xlm-roberta-large-finetuned-ru-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.579426</td>\n",
              "      <td>0.731606</td>\n",
              "      <td>0.904486</td>\n",
              "      <td>0.670148</td>\n",
              "      <td>0.670148</td>\n",
              "      <td>0.068912</td>\n",
              "      <td>0.362255</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.784416</td>\n",
              "      <td>0.869572</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>all</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>xlm-roberta-large-finetuned-all-vicuna-13b.csv.gz</td>\n",
              "      <td>0.579232</td>\n",
              "      <td>0.732948</td>\n",
              "      <td>0.902283</td>\n",
              "      <td>0.671753</td>\n",
              "      <td>0.671753</td>\n",
              "      <td>0.081891</td>\n",
              "      <td>0.358840</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.779635</td>\n",
              "      <td>0.884636</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>en</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-en-vicuna-13b.csv.gz</td>\n",
              "      <td>0.579163</td>\n",
              "      <td>0.816171</td>\n",
              "      <td>0.837542</td>\n",
              "      <td>0.799078</td>\n",
              "      <td>0.799078</td>\n",
              "      <td>0.655130</td>\n",
              "      <td>0.144518</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.600176</td>\n",
              "      <td>0.669345</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>es</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-es-llama-65b.csv.gz</td>\n",
              "      <td>0.579094</td>\n",
              "      <td>0.805933</td>\n",
              "      <td>0.841042</td>\n",
              "      <td>0.780509</td>\n",
              "      <td>0.780509</td>\n",
              "      <td>0.598269</td>\n",
              "      <td>0.172455</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.614638</td>\n",
              "      <td>0.643611</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>es</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>xlm-roberta-large-finetuned-es-opt-66b.csv.gz</td>\n",
              "      <td>0.577967</td>\n",
              "      <td>0.794830</td>\n",
              "      <td>0.845341</td>\n",
              "      <td>0.761563</td>\n",
              "      <td>0.761563</td>\n",
              "      <td>0.538319</td>\n",
              "      <td>0.201197</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.630242</td>\n",
              "      <td>0.719857</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>en3</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-en3-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.577122</td>\n",
              "      <td>0.848703</td>\n",
              "      <td>0.837530</td>\n",
              "      <td>0.864482</td>\n",
              "      <td>0.864482</td>\n",
              "      <td>0.818294</td>\n",
              "      <td>0.050731</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.565487</td>\n",
              "      <td>0.657949</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>en3</td>\n",
              "      <td>all</td>\n",
              "      <td>mdeberta-v3-base-finetuned-en3-all.csv.gz</td>\n",
              "      <td>0.573683</td>\n",
              "      <td>0.861951</td>\n",
              "      <td>0.870090</td>\n",
              "      <td>0.894828</td>\n",
              "      <td>0.894828</td>\n",
              "      <td>0.878245</td>\n",
              "      <td>0.009171</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.556292</td>\n",
              "      <td>0.795436</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>en</td>\n",
              "      <td>all</td>\n",
              "      <td>mGPT-finetuned-en-all.csv.gz</td>\n",
              "      <td>0.572694</td>\n",
              "      <td>0.837324</td>\n",
              "      <td>0.832379</td>\n",
              "      <td>0.842704</td>\n",
              "      <td>0.842704</td>\n",
              "      <td>0.783684</td>\n",
              "      <td>0.079512</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.568402</td>\n",
              "      <td>0.701581</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>en3</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-en3-vicuna-13b.csv.gz</td>\n",
              "      <td>0.572519</td>\n",
              "      <td>0.848716</td>\n",
              "      <td>0.836718</td>\n",
              "      <td>0.866530</td>\n",
              "      <td>0.866530</td>\n",
              "      <td>0.831582</td>\n",
              "      <td>0.046778</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.560820</td>\n",
              "      <td>0.646861</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>es</td>\n",
              "      <td>all</td>\n",
              "      <td>gpt2-medium-finetuned-es-all.csv.gz</td>\n",
              "      <td>0.567999</td>\n",
              "      <td>0.784699</td>\n",
              "      <td>0.843346</td>\n",
              "      <td>0.747090</td>\n",
              "      <td>0.747090</td>\n",
              "      <td>0.532756</td>\n",
              "      <td>0.218159</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.624542</td>\n",
              "      <td>0.692207</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>ru</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-ru-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.567941</td>\n",
              "      <td>0.773019</td>\n",
              "      <td>0.850052</td>\n",
              "      <td>0.728315</td>\n",
              "      <td>0.728315</td>\n",
              "      <td>0.461063</td>\n",
              "      <td>0.248168</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.645385</td>\n",
              "      <td>0.746760</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>en</td>\n",
              "      <td>all</td>\n",
              "      <td>xlm-roberta-large-finetuned-en-all.csv.gz</td>\n",
              "      <td>0.567889</td>\n",
              "      <td>0.861098</td>\n",
              "      <td>0.874124</td>\n",
              "      <td>0.895682</td>\n",
              "      <td>0.895682</td>\n",
              "      <td>0.888133</td>\n",
              "      <td>0.006984</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.552441</td>\n",
              "      <td>0.779727</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>en3</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-en3-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.567327</td>\n",
              "      <td>0.777897</td>\n",
              "      <td>0.846575</td>\n",
              "      <td>0.736167</td>\n",
              "      <td>0.736167</td>\n",
              "      <td>0.495365</td>\n",
              "      <td>0.235082</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.634777</td>\n",
              "      <td>0.660347</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>en3</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mGPT-finetuned-en3-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.564812</td>\n",
              "      <td>0.787520</td>\n",
              "      <td>0.839684</td>\n",
              "      <td>0.752586</td>\n",
              "      <td>0.752586</td>\n",
              "      <td>0.566749</td>\n",
              "      <td>0.207759</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.612746</td>\n",
              "      <td>0.674759</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>en</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-en-text-davinci-003.csv.gz</td>\n",
              "      <td>0.564681</td>\n",
              "      <td>0.819524</td>\n",
              "      <td>0.829622</td>\n",
              "      <td>0.810480</td>\n",
              "      <td>0.810480</td>\n",
              "      <td>0.732695</td>\n",
              "      <td>0.122069</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.572618</td>\n",
              "      <td>0.648633</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>ru</td>\n",
              "      <td>all</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-ru-all.csv.gz</td>\n",
              "      <td>0.564216</td>\n",
              "      <td>0.858202</td>\n",
              "      <td>0.858897</td>\n",
              "      <td>0.890971</td>\n",
              "      <td>0.890971</td>\n",
              "      <td>0.886588</td>\n",
              "      <td>0.012472</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.550470</td>\n",
              "      <td>0.584697</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>ru</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-ru-vicuna-13b.csv.gz</td>\n",
              "      <td>0.564186</td>\n",
              "      <td>0.827698</td>\n",
              "      <td>0.828716</td>\n",
              "      <td>0.826694</td>\n",
              "      <td>0.826694</td>\n",
              "      <td>0.771014</td>\n",
              "      <td>0.099083</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.564952</td>\n",
              "      <td>0.709849</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>en3</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-en3-vicuna-13b.csv.gz</td>\n",
              "      <td>0.563109</td>\n",
              "      <td>0.735961</td>\n",
              "      <td>0.875729</td>\n",
              "      <td>0.675781</td>\n",
              "      <td>0.675781</td>\n",
              "      <td>0.239802</td>\n",
              "      <td>0.334702</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.712748</td>\n",
              "      <td>0.778700</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>es</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mGPT-finetuned-es-vicuna-13b.csv.gz</td>\n",
              "      <td>0.561170</td>\n",
              "      <td>0.742678</td>\n",
              "      <td>0.866803</td>\n",
              "      <td>0.684861</td>\n",
              "      <td>0.684861</td>\n",
              "      <td>0.303152</td>\n",
              "      <td>0.316628</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.690110</td>\n",
              "      <td>0.775206</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>en</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-en-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.560357</td>\n",
              "      <td>0.800240</td>\n",
              "      <td>0.831930</td>\n",
              "      <td>0.776003</td>\n",
              "      <td>0.776003</td>\n",
              "      <td>0.657602</td>\n",
              "      <td>0.170152</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.586123</td>\n",
              "      <td>0.644935</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>en3</td>\n",
              "      <td>all</td>\n",
              "      <td>mGPT-finetuned-en3-all.csv.gz</td>\n",
              "      <td>0.560242</td>\n",
              "      <td>0.811661</td>\n",
              "      <td>0.828887</td>\n",
              "      <td>0.797064</td>\n",
              "      <td>0.797064</td>\n",
              "      <td>0.713844</td>\n",
              "      <td>0.139491</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.573332</td>\n",
              "      <td>0.717078</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>en3</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>xlm-roberta-large-finetuned-en3-llama-65b.csv.gz</td>\n",
              "      <td>0.560237</td>\n",
              "      <td>0.799402</td>\n",
              "      <td>0.832127</td>\n",
              "      <td>0.774535</td>\n",
              "      <td>0.774535</td>\n",
              "      <td>0.653894</td>\n",
              "      <td>0.172263</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.586922</td>\n",
              "      <td>0.618038</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>all</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-all-opt-66b.csv.gz</td>\n",
              "      <td>0.555966</td>\n",
              "      <td>0.717663</td>\n",
              "      <td>0.886598</td>\n",
              "      <td>0.652978</td>\n",
              "      <td>0.652978</td>\n",
              "      <td>0.160074</td>\n",
              "      <td>0.370237</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.734845</td>\n",
              "      <td>0.801388</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>en</td>\n",
              "      <td>all</td>\n",
              "      <td>electra-large-discriminator-finetuned-en-all.csv.gz</td>\n",
              "      <td>0.555896</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.830961</td>\n",
              "      <td>0.768391</td>\n",
              "      <td>0.768391</td>\n",
              "      <td>0.652967</td>\n",
              "      <td>0.179285</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.583874</td>\n",
              "      <td>0.605279</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>es</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mGPT-finetuned-es-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.555471</td>\n",
              "      <td>0.732468</td>\n",
              "      <td>0.869725</td>\n",
              "      <td>0.671582</td>\n",
              "      <td>0.671582</td>\n",
              "      <td>0.273486</td>\n",
              "      <td>0.335239</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.695637</td>\n",
              "      <td>0.772116</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>en</td>\n",
              "      <td>all</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-en-all.csv.gz</td>\n",
              "      <td>0.554132</td>\n",
              "      <td>0.859082</td>\n",
              "      <td>0.893326</td>\n",
              "      <td>0.897764</td>\n",
              "      <td>0.897764</td>\n",
              "      <td>0.910074</td>\n",
              "      <td>0.001919</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.544004</td>\n",
              "      <td>0.692618</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>all</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-all-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.552802</td>\n",
              "      <td>0.703734</td>\n",
              "      <td>0.899928</td>\n",
              "      <td>0.636730</td>\n",
              "      <td>0.636730</td>\n",
              "      <td>0.078801</td>\n",
              "      <td>0.398595</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.761302</td>\n",
              "      <td>0.822377</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>en</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>xlm-roberta-large-finetuned-en-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.552626</td>\n",
              "      <td>0.732969</td>\n",
              "      <td>0.865824</td>\n",
              "      <td>0.672401</td>\n",
              "      <td>0.672401</td>\n",
              "      <td>0.298517</td>\n",
              "      <td>0.331210</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.685137</td>\n",
              "      <td>0.754744</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>all</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-all-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.551927</td>\n",
              "      <td>0.703762</td>\n",
              "      <td>0.898405</td>\n",
              "      <td>0.636696</td>\n",
              "      <td>0.636696</td>\n",
              "      <td>0.086836</td>\n",
              "      <td>0.397636</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.757764</td>\n",
              "      <td>0.904026</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>ru</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-ru-llama-65b.csv.gz</td>\n",
              "      <td>0.551419</td>\n",
              "      <td>0.764172</td>\n",
              "      <td>0.841678</td>\n",
              "      <td>0.717665</td>\n",
              "      <td>0.717665</td>\n",
              "      <td>0.507108</td>\n",
              "      <td>0.254423</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.619235</td>\n",
              "      <td>0.699780</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>en3</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-en3-llama-65b.csv.gz</td>\n",
              "      <td>0.551131</td>\n",
              "      <td>0.855902</td>\n",
              "      <td>0.861753</td>\n",
              "      <td>0.892063</td>\n",
              "      <td>0.892063</td>\n",
              "      <td>0.906984</td>\n",
              "      <td>0.008711</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.542153</td>\n",
              "      <td>0.561775</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>es</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mGPT-finetuned-es-opt-66b.csv.gz</td>\n",
              "      <td>0.549055</td>\n",
              "      <td>0.777883</td>\n",
              "      <td>0.833172</td>\n",
              "      <td>0.740365</td>\n",
              "      <td>0.740365</td>\n",
              "      <td>0.597033</td>\n",
              "      <td>0.217737</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.592615</td>\n",
              "      <td>0.654873</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>es</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-es-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.546349</td>\n",
              "      <td>0.699507</td>\n",
              "      <td>0.894939</td>\n",
              "      <td>0.631541</td>\n",
              "      <td>0.631541</td>\n",
              "      <td>0.102905</td>\n",
              "      <td>0.401435</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.747830</td>\n",
              "      <td>0.877235</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>en3</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>electra-large-discriminator-finetuned-en3-llama-65b.csv.gz</td>\n",
              "      <td>0.544748</td>\n",
              "      <td>0.853562</td>\n",
              "      <td>0.854318</td>\n",
              "      <td>0.889879</td>\n",
              "      <td>0.889879</td>\n",
              "      <td>0.913164</td>\n",
              "      <td>0.010399</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.538218</td>\n",
              "      <td>0.549112</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>es</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-es-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.544745</td>\n",
              "      <td>0.702597</td>\n",
              "      <td>0.888260</td>\n",
              "      <td>0.634921</td>\n",
              "      <td>0.634921</td>\n",
              "      <td>0.140606</td>\n",
              "      <td>0.392954</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.733220</td>\n",
              "      <td>0.806364</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>all</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-all-vicuna-13b.csv.gz</td>\n",
              "      <td>0.544604</td>\n",
              "      <td>0.696329</td>\n",
              "      <td>0.896384</td>\n",
              "      <td>0.627889</td>\n",
              "      <td>0.627889</td>\n",
              "      <td>0.093634</td>\n",
              "      <td>0.406693</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.749837</td>\n",
              "      <td>0.831581</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>ru</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mGPT-finetuned-ru-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.544351</td>\n",
              "      <td>0.746981</td>\n",
              "      <td>0.846096</td>\n",
              "      <td>0.692814</td>\n",
              "      <td>0.692814</td>\n",
              "      <td>0.447775</td>\n",
              "      <td>0.289727</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.631249</td>\n",
              "      <td>0.735858</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>xlm-roberta-large-finetuned-ru-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.543448</td>\n",
              "      <td>0.689973</td>\n",
              "      <td>0.903533</td>\n",
              "      <td>0.620925</td>\n",
              "      <td>0.620925</td>\n",
              "      <td>0.054079</td>\n",
              "      <td>0.419433</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.763244</td>\n",
              "      <td>0.859526</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>es</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-es-opt-66b.csv.gz</td>\n",
              "      <td>0.542597</td>\n",
              "      <td>0.737869</td>\n",
              "      <td>0.850788</td>\n",
              "      <td>0.679945</td>\n",
              "      <td>0.679945</td>\n",
              "      <td>0.402658</td>\n",
              "      <td>0.309797</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.643773</td>\n",
              "      <td>0.684635</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>all</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>xlm-roberta-large-finetuned-all-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.541894</td>\n",
              "      <td>0.687712</td>\n",
              "      <td>0.904093</td>\n",
              "      <td>0.618365</td>\n",
              "      <td>0.618365</td>\n",
              "      <td>0.050371</td>\n",
              "      <td>0.422771</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.763429</td>\n",
              "      <td>0.866735</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>en3</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mGPT-finetuned-en3-llama-65b.csv.gz</td>\n",
              "      <td>0.541871</td>\n",
              "      <td>0.790086</td>\n",
              "      <td>0.824488</td>\n",
              "      <td>0.763441</td>\n",
              "      <td>0.763441</td>\n",
              "      <td>0.692213</td>\n",
              "      <td>0.179976</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.563906</td>\n",
              "      <td>0.581040</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>en3</td>\n",
              "      <td>all</td>\n",
              "      <td>electra-large-discriminator-finetuned-en3-all.csv.gz</td>\n",
              "      <td>0.541786</td>\n",
              "      <td>0.854998</td>\n",
              "      <td>0.875666</td>\n",
              "      <td>0.894521</td>\n",
              "      <td>0.894521</td>\n",
              "      <td>0.922435</td>\n",
              "      <td>0.004029</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.536768</td>\n",
              "      <td>0.655048</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>en</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mGPT-finetuned-en-llama-65b.csv.gz</td>\n",
              "      <td>0.541660</td>\n",
              "      <td>0.799265</td>\n",
              "      <td>0.822158</td>\n",
              "      <td>0.780201</td>\n",
              "      <td>0.780201</td>\n",
              "      <td>0.733931</td>\n",
              "      <td>0.155954</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.555058</td>\n",
              "      <td>0.583853</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>es</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-es-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.541318</td>\n",
              "      <td>0.740796</td>\n",
              "      <td>0.847427</td>\n",
              "      <td>0.684246</td>\n",
              "      <td>0.684246</td>\n",
              "      <td>0.429543</td>\n",
              "      <td>0.301623</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.634417</td>\n",
              "      <td>0.699382</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>en</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-en-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.540083</td>\n",
              "      <td>0.717446</td>\n",
              "      <td>0.865234</td>\n",
              "      <td>0.652774</td>\n",
              "      <td>0.652774</td>\n",
              "      <td>0.285847</td>\n",
              "      <td>0.354849</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.679652</td>\n",
              "      <td>0.748058</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>en3</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-en3-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.539153</td>\n",
              "      <td>0.708725</td>\n",
              "      <td>0.872864</td>\n",
              "      <td>0.641953</td>\n",
              "      <td>0.641953</td>\n",
              "      <td>0.232077</td>\n",
              "      <td>0.373690</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.697116</td>\n",
              "      <td>0.761980</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>en</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-en-llama-65b.csv.gz</td>\n",
              "      <td>0.537760</td>\n",
              "      <td>0.851597</td>\n",
              "      <td>0.850363</td>\n",
              "      <td>0.888821</td>\n",
              "      <td>0.888821</td>\n",
              "      <td>0.921508</td>\n",
              "      <td>0.010553</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.533969</td>\n",
              "      <td>0.568730</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>en3</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>xlm-roberta-large-finetuned-en3-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.537613</td>\n",
              "      <td>0.708735</td>\n",
              "      <td>0.870756</td>\n",
              "      <td>0.641953</td>\n",
              "      <td>0.641953</td>\n",
              "      <td>0.244438</td>\n",
              "      <td>0.372155</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.691703</td>\n",
              "      <td>0.729961</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>ru</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mGPT-finetuned-ru-llama-65b.csv.gz</td>\n",
              "      <td>0.535351</td>\n",
              "      <td>0.786728</td>\n",
              "      <td>0.821769</td>\n",
              "      <td>0.759413</td>\n",
              "      <td>0.759413</td>\n",
              "      <td>0.705810</td>\n",
              "      <td>0.182816</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.555687</td>\n",
              "      <td>0.593804</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>es</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-es-vicuna-13b.csv.gz</td>\n",
              "      <td>0.533970</td>\n",
              "      <td>0.688444</td>\n",
              "      <td>0.889205</td>\n",
              "      <td>0.618331</td>\n",
              "      <td>0.618331</td>\n",
              "      <td>0.127009</td>\n",
              "      <td>0.413293</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.729849</td>\n",
              "      <td>0.809739</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>en</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-en-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.533466</td>\n",
              "      <td>0.691296</td>\n",
              "      <td>0.884708</td>\n",
              "      <td>0.621437</td>\n",
              "      <td>0.621437</td>\n",
              "      <td>0.152658</td>\n",
              "      <td>0.406616</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.720363</td>\n",
              "      <td>0.799561</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>es</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-es-vicuna-13b.csv.gz</td>\n",
              "      <td>0.533426</td>\n",
              "      <td>0.687895</td>\n",
              "      <td>0.889027</td>\n",
              "      <td>0.617682</td>\n",
              "      <td>0.617682</td>\n",
              "      <td>0.127627</td>\n",
              "      <td>0.413945</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.729214</td>\n",
              "      <td>0.790451</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>all</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-all-vicuna-13b.csv.gz</td>\n",
              "      <td>0.532327</td>\n",
              "      <td>0.679635</td>\n",
              "      <td>0.898595</td>\n",
              "      <td>0.608773</td>\n",
              "      <td>0.608773</td>\n",
              "      <td>0.074475</td>\n",
              "      <td>0.430561</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.747482</td>\n",
              "      <td>0.897301</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>en</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>electra-large-discriminator-finetuned-en-llama-65b.csv.gz</td>\n",
              "      <td>0.531405</td>\n",
              "      <td>0.794388</td>\n",
              "      <td>0.817985</td>\n",
              "      <td>0.774569</td>\n",
              "      <td>0.774569</td>\n",
              "      <td>0.754635</td>\n",
              "      <td>0.159714</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.542825</td>\n",
              "      <td>0.544350</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>es</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-es-opt-66b.csv.gz</td>\n",
              "      <td>0.531365</td>\n",
              "      <td>0.731171</td>\n",
              "      <td>0.844387</td>\n",
              "      <td>0.671719</td>\n",
              "      <td>0.671719</td>\n",
              "      <td>0.436650</td>\n",
              "      <td>0.314824</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.624263</td>\n",
              "      <td>0.692956</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>en3</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-en3-opt-66b.csv.gz</td>\n",
              "      <td>0.530852</td>\n",
              "      <td>0.763341</td>\n",
              "      <td>0.826744</td>\n",
              "      <td>0.720669</td>\n",
              "      <td>0.720669</td>\n",
              "      <td>0.617120</td>\n",
              "      <td>0.237384</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.572748</td>\n",
              "      <td>0.590467</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>en</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-en-vicuna-13b.csv.gz</td>\n",
              "      <td>0.529381</td>\n",
              "      <td>0.692110</td>\n",
              "      <td>0.877388</td>\n",
              "      <td>0.622086</td>\n",
              "      <td>0.622086</td>\n",
              "      <td>0.193140</td>\n",
              "      <td>0.400860</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.703000</td>\n",
              "      <td>0.768892</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>en3</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>electra-large-discriminator-finetuned-en3-opt-66b.csv.gz</td>\n",
              "      <td>0.529356</td>\n",
              "      <td>0.741596</td>\n",
              "      <td>0.836044</td>\n",
              "      <td>0.687046</td>\n",
              "      <td>0.687046</td>\n",
              "      <td>0.510198</td>\n",
              "      <td>0.288461</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.600671</td>\n",
              "      <td>0.657734</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>all</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-all-vicuna-13b.csv.gz</td>\n",
              "      <td>0.529117</td>\n",
              "      <td>0.676535</td>\n",
              "      <td>0.897271</td>\n",
              "      <td>0.605154</td>\n",
              "      <td>0.605154</td>\n",
              "      <td>0.079728</td>\n",
              "      <td>0.433977</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.743148</td>\n",
              "      <td>0.842250</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>es</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-es-opt-66b.csv.gz</td>\n",
              "      <td>0.528281</td>\n",
              "      <td>0.691955</td>\n",
              "      <td>0.875918</td>\n",
              "      <td>0.621847</td>\n",
              "      <td>0.621847</td>\n",
              "      <td>0.201174</td>\n",
              "      <td>0.400130</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.699348</td>\n",
              "      <td>0.746321</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>en</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>xlm-roberta-large-finetuned-en-llama-65b.csv.gz</td>\n",
              "      <td>0.527563</td>\n",
              "      <td>0.756254</td>\n",
              "      <td>0.827372</td>\n",
              "      <td>0.709951</td>\n",
              "      <td>0.709951</td>\n",
              "      <td>0.598888</td>\n",
              "      <td>0.251698</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.574707</td>\n",
              "      <td>0.615944</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>en3</td>\n",
              "      <td>all</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-en3-all.csv.gz</td>\n",
              "      <td>0.526371</td>\n",
              "      <td>0.851480</td>\n",
              "      <td>0.880885</td>\n",
              "      <td>0.894043</td>\n",
              "      <td>0.894043</td>\n",
              "      <td>0.941286</td>\n",
              "      <td>0.002226</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.528244</td>\n",
              "      <td>0.750318</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-ru-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.525860</td>\n",
              "      <td>0.688357</td>\n",
              "      <td>0.876443</td>\n",
              "      <td>0.617614</td>\n",
              "      <td>0.617614</td>\n",
              "      <td>0.195612</td>\n",
              "      <td>0.405580</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.699404</td>\n",
              "      <td>0.741409</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>es</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mGPT-finetuned-es-llama-65b.csv.gz</td>\n",
              "      <td>0.524760</td>\n",
              "      <td>0.743180</td>\n",
              "      <td>0.831363</td>\n",
              "      <td>0.690152</td>\n",
              "      <td>0.690152</td>\n",
              "      <td>0.546354</td>\n",
              "      <td>0.280479</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.586584</td>\n",
              "      <td>0.627180</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>all</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mGPT-finetuned-all-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.522982</td>\n",
              "      <td>0.681149</td>\n",
              "      <td>0.880603</td>\n",
              "      <td>0.609387</td>\n",
              "      <td>0.609387</td>\n",
              "      <td>0.168109</td>\n",
              "      <td>0.418243</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.706824</td>\n",
              "      <td>0.795345</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>all</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mGPT-finetuned-all-opt-66b.csv.gz</td>\n",
              "      <td>0.522330</td>\n",
              "      <td>0.681450</td>\n",
              "      <td>0.879195</td>\n",
              "      <td>0.609660</td>\n",
              "      <td>0.609660</td>\n",
              "      <td>0.175834</td>\n",
              "      <td>0.416977</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.703594</td>\n",
              "      <td>0.761374</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>all</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mGPT-finetuned-all-vicuna-13b.csv.gz</td>\n",
              "      <td>0.522266</td>\n",
              "      <td>0.673810</td>\n",
              "      <td>0.888907</td>\n",
              "      <td>0.601468</td>\n",
              "      <td>0.601468</td>\n",
              "      <td>0.120519</td>\n",
              "      <td>0.433056</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.723213</td>\n",
              "      <td>0.813149</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-ru-gpt-4.csv.gz</td>\n",
              "      <td>0.521967</td>\n",
              "      <td>0.715584</td>\n",
              "      <td>0.845904</td>\n",
              "      <td>0.651169</td>\n",
              "      <td>0.651169</td>\n",
              "      <td>0.405748</td>\n",
              "      <td>0.341763</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.626245</td>\n",
              "      <td>0.699374</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-en3-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.521696</td>\n",
              "      <td>0.694117</td>\n",
              "      <td>0.864225</td>\n",
              "      <td>0.624100</td>\n",
              "      <td>0.624100</td>\n",
              "      <td>0.269468</td>\n",
              "      <td>0.389117</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.670707</td>\n",
              "      <td>0.731856</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>all</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-all-text-davinci-003.csv.gz</td>\n",
              "      <td>0.521483</td>\n",
              "      <td>0.688458</td>\n",
              "      <td>0.869850</td>\n",
              "      <td>0.617477</td>\n",
              "      <td>0.617477</td>\n",
              "      <td>0.232386</td>\n",
              "      <td>0.401167</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.683224</td>\n",
              "      <td>0.742683</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>ru</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-ru-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.521333</td>\n",
              "      <td>0.780650</td>\n",
              "      <td>0.815624</td>\n",
              "      <td>0.752791</td>\n",
              "      <td>0.752791</td>\n",
              "      <td>0.740111</td>\n",
              "      <td>0.186001</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.536944</td>\n",
              "      <td>0.502054</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>en3</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>xlm-roberta-large-finetuned-en3-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.519412</td>\n",
              "      <td>0.706383</td>\n",
              "      <td>0.850236</td>\n",
              "      <td>0.639256</td>\n",
              "      <td>0.639256</td>\n",
              "      <td>0.366811</td>\n",
              "      <td>0.359991</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.636599</td>\n",
              "      <td>0.697180</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>en3</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>xlm-roberta-large-finetuned-en3-vicuna-13b.csv.gz</td>\n",
              "      <td>0.518315</td>\n",
              "      <td>0.684640</td>\n",
              "      <td>0.869409</td>\n",
              "      <td>0.612937</td>\n",
              "      <td>0.612937</td>\n",
              "      <td>0.231768</td>\n",
              "      <td>0.406347</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.680943</td>\n",
              "      <td>0.733247</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>en</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>xlm-roberta-large-finetuned-en-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.517943</td>\n",
              "      <td>0.705280</td>\n",
              "      <td>0.849442</td>\n",
              "      <td>0.637890</td>\n",
              "      <td>0.637890</td>\n",
              "      <td>0.370519</td>\n",
              "      <td>0.361065</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.634208</td>\n",
              "      <td>0.714988</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>ru</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>xlm-roberta-large-finetuned-ru-llama-65b.csv.gz</td>\n",
              "      <td>0.517053</td>\n",
              "      <td>0.721882</td>\n",
              "      <td>0.836658</td>\n",
              "      <td>0.660181</td>\n",
              "      <td>0.660181</td>\n",
              "      <td>0.475896</td>\n",
              "      <td>0.322921</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.600591</td>\n",
              "      <td>0.656779</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>en3</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-en3-opt-66b.csv.gz</td>\n",
              "      <td>0.516472</td>\n",
              "      <td>0.726279</td>\n",
              "      <td>0.833474</td>\n",
              "      <td>0.666462</td>\n",
              "      <td>0.666462</td>\n",
              "      <td>0.504326</td>\n",
              "      <td>0.312330</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.591672</td>\n",
              "      <td>0.616230</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>all</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-all-text-davinci-003.csv.gz</td>\n",
              "      <td>0.516124</td>\n",
              "      <td>0.659103</td>\n",
              "      <td>0.898740</td>\n",
              "      <td>0.585731</td>\n",
              "      <td>0.585731</td>\n",
              "      <td>0.065513</td>\n",
              "      <td>0.457577</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.738455</td>\n",
              "      <td>0.788112</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>es</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-es-gpt-4.csv.gz</td>\n",
              "      <td>0.515462</td>\n",
              "      <td>0.674628</td>\n",
              "      <td>0.876541</td>\n",
              "      <td>0.601604</td>\n",
              "      <td>0.601604</td>\n",
              "      <td>0.185414</td>\n",
              "      <td>0.424844</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.694871</td>\n",
              "      <td>0.750145</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mdeberta-v3-base-finetuned-ru-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.513331</td>\n",
              "      <td>0.657628</td>\n",
              "      <td>0.895577</td>\n",
              "      <td>0.583820</td>\n",
              "      <td>0.583820</td>\n",
              "      <td>0.080037</td>\n",
              "      <td>0.457922</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.731020</td>\n",
              "      <td>0.840883</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>all</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-all-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.513234</td>\n",
              "      <td>0.663798</td>\n",
              "      <td>0.886642</td>\n",
              "      <td>0.589964</td>\n",
              "      <td>0.589964</td>\n",
              "      <td>0.126700</td>\n",
              "      <td>0.445220</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.714040</td>\n",
              "      <td>0.810285</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>en3</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mGPT-finetuned-en3-vicuna-13b.csv.gz</td>\n",
              "      <td>0.513120</td>\n",
              "      <td>0.697748</td>\n",
              "      <td>0.849991</td>\n",
              "      <td>0.628469</td>\n",
              "      <td>0.628469</td>\n",
              "      <td>0.358467</td>\n",
              "      <td>0.373153</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.634190</td>\n",
              "      <td>0.716675</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>ru</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>electra-large-discriminator-finetuned-ru-vicuna-13b.csv.gz</td>\n",
              "      <td>0.513115</td>\n",
              "      <td>0.785767</td>\n",
              "      <td>0.810492</td>\n",
              "      <td>0.764670</td>\n",
              "      <td>0.764670</td>\n",
              "      <td>0.792336</td>\n",
              "      <td>0.166161</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.520751</td>\n",
              "      <td>0.523704</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>es</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-es-text-davinci-003.csv.gz</td>\n",
              "      <td>0.511852</td>\n",
              "      <td>0.667855</td>\n",
              "      <td>0.878989</td>\n",
              "      <td>0.593992</td>\n",
              "      <td>0.593992</td>\n",
              "      <td>0.168109</td>\n",
              "      <td>0.435550</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.698171</td>\n",
              "      <td>0.759722</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>en</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-en-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.510298</td>\n",
              "      <td>0.658502</td>\n",
              "      <td>0.888655</td>\n",
              "      <td>0.584195</td>\n",
              "      <td>0.584195</td>\n",
              "      <td>0.114030</td>\n",
              "      <td>0.453279</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.716346</td>\n",
              "      <td>0.763013</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>en</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>electra-large-discriminator-finetuned-en-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.509822</td>\n",
              "      <td>0.748808</td>\n",
              "      <td>0.817895</td>\n",
              "      <td>0.701792</td>\n",
              "      <td>0.701792</td>\n",
              "      <td>0.656057</td>\n",
              "      <td>0.253770</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.545086</td>\n",
              "      <td>0.597917</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-en3-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.508449</td>\n",
              "      <td>0.653644</td>\n",
              "      <td>0.892137</td>\n",
              "      <td>0.579109</td>\n",
              "      <td>0.579109</td>\n",
              "      <td>0.094870</td>\n",
              "      <td>0.461376</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.721877</td>\n",
              "      <td>0.755431</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>es</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>xlm-roberta-large-finetuned-es-text-davinci-003.csv.gz</td>\n",
              "      <td>0.506892</td>\n",
              "      <td>0.643275</td>\n",
              "      <td>0.905064</td>\n",
              "      <td>0.569039</td>\n",
              "      <td>0.569039</td>\n",
              "      <td>0.031211</td>\n",
              "      <td>0.480602</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.744093</td>\n",
              "      <td>0.853889</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>all</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-all-llama-65b.csv.gz</td>\n",
              "      <td>0.506674</td>\n",
              "      <td>0.671128</td>\n",
              "      <td>0.866959</td>\n",
              "      <td>0.596996</td>\n",
              "      <td>0.596996</td>\n",
              "      <td>0.234549</td>\n",
              "      <td>0.423923</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.670764</td>\n",
              "      <td>0.754289</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>all</td>\n",
              "      <td>all</td>\n",
              "      <td>electra-large-discriminator-finetuned-all-all.csv.gz</td>\n",
              "      <td>0.505921</td>\n",
              "      <td>0.804059</td>\n",
              "      <td>0.805817</td>\n",
              "      <td>0.802321</td>\n",
              "      <td>0.802321</td>\n",
              "      <td>0.874227</td>\n",
              "      <td>0.113665</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.506054</td>\n",
              "      <td>0.512044</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>ru</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-ru-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.505909</td>\n",
              "      <td>0.759549</td>\n",
              "      <td>0.812115</td>\n",
              "      <td>0.720430</td>\n",
              "      <td>0.720430</td>\n",
              "      <td>0.721570</td>\n",
              "      <td>0.224682</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.526874</td>\n",
              "      <td>0.477450</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>en3</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mGPT-finetuned-en3-opt-66b.csv.gz</td>\n",
              "      <td>0.504160</td>\n",
              "      <td>0.708932</td>\n",
              "      <td>0.832300</td>\n",
              "      <td>0.643489</td>\n",
              "      <td>0.643489</td>\n",
              "      <td>0.486712</td>\n",
              "      <td>0.340343</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.586472</td>\n",
              "      <td>0.635100</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>xlm-roberta-large-finetuned-ru-gpt-4.csv.gz</td>\n",
              "      <td>0.503719</td>\n",
              "      <td>0.640914</td>\n",
              "      <td>0.902234</td>\n",
              "      <td>0.566206</td>\n",
              "      <td>0.566206</td>\n",
              "      <td>0.043263</td>\n",
              "      <td>0.482290</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.737223</td>\n",
              "      <td>0.832728</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>en</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-en-gpt-4.csv.gz</td>\n",
              "      <td>0.503498</td>\n",
              "      <td>0.734582</td>\n",
              "      <td>0.818806</td>\n",
              "      <td>0.680696</td>\n",
              "      <td>0.680696</td>\n",
              "      <td>0.622991</td>\n",
              "      <td>0.281592</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.547708</td>\n",
              "      <td>0.583410</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>ru</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-ru-vicuna-13b.csv.gz</td>\n",
              "      <td>0.502183</td>\n",
              "      <td>0.750256</td>\n",
              "      <td>0.812442</td>\n",
              "      <td>0.705854</td>\n",
              "      <td>0.705854</td>\n",
              "      <td>0.700247</td>\n",
              "      <td>0.243716</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.528018</td>\n",
              "      <td>0.487323</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>es</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>electra-large-discriminator-finetuned-es-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.501950</td>\n",
              "      <td>0.709198</td>\n",
              "      <td>0.830021</td>\n",
              "      <td>0.644035</td>\n",
              "      <td>0.644035</td>\n",
              "      <td>0.502472</td>\n",
              "      <td>0.337772</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.579878</td>\n",
              "      <td>0.630541</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>all</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mdeberta-v3-base-finetuned-all-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.501057</td>\n",
              "      <td>0.643419</td>\n",
              "      <td>0.892931</td>\n",
              "      <td>0.567981</td>\n",
              "      <td>0.567981</td>\n",
              "      <td>0.086836</td>\n",
              "      <td>0.474884</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.719140</td>\n",
              "      <td>0.801897</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>en</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mGPT-finetuned-en-vicuna-13b.csv.gz</td>\n",
              "      <td>0.500709</td>\n",
              "      <td>0.684201</td>\n",
              "      <td>0.846302</td>\n",
              "      <td>0.611811</td>\n",
              "      <td>0.611811</td>\n",
              "      <td>0.365884</td>\n",
              "      <td>0.390959</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.621579</td>\n",
              "      <td>0.716454</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>en3</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>electra-large-discriminator-finetuned-en3-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.499904</td>\n",
              "      <td>0.723550</td>\n",
              "      <td>0.820767</td>\n",
              "      <td>0.664687</td>\n",
              "      <td>0.664687</td>\n",
              "      <td>0.589617</td>\n",
              "      <td>0.303734</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.553325</td>\n",
              "      <td>0.574046</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>ru</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>electra-large-discriminator-finetuned-ru-opt-66b.csv.gz</td>\n",
              "      <td>0.499178</td>\n",
              "      <td>0.778436</td>\n",
              "      <td>0.804854</td>\n",
              "      <td>0.755726</td>\n",
              "      <td>0.755726</td>\n",
              "      <td>0.818912</td>\n",
              "      <td>0.172915</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.504086</td>\n",
              "      <td>0.502312</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>en</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>xlm-roberta-large-finetuned-en-opt-66b.csv.gz</td>\n",
              "      <td>0.498294</td>\n",
              "      <td>0.701219</td>\n",
              "      <td>0.831309</td>\n",
              "      <td>0.633521</td>\n",
              "      <td>0.633521</td>\n",
              "      <td>0.482386</td>\n",
              "      <td>0.352086</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.582764</td>\n",
              "      <td>0.648291</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>en</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mGPT-finetuned-en-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.498148</td>\n",
              "      <td>0.667606</td>\n",
              "      <td>0.858153</td>\n",
              "      <td>0.592422</td>\n",
              "      <td>0.592422</td>\n",
              "      <td>0.280284</td>\n",
              "      <td>0.423385</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.648165</td>\n",
              "      <td>0.728190</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>all</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-all-gpt-4.csv.gz</td>\n",
              "      <td>0.497424</td>\n",
              "      <td>0.635878</td>\n",
              "      <td>0.897160</td>\n",
              "      <td>0.560266</td>\n",
              "      <td>0.560266</td>\n",
              "      <td>0.064586</td>\n",
              "      <td>0.486320</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.724547</td>\n",
              "      <td>0.767448</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>en3</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-en3-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.496785</td>\n",
              "      <td>0.694191</td>\n",
              "      <td>0.834328</td>\n",
              "      <td>0.624373</td>\n",
              "      <td>0.624373</td>\n",
              "      <td>0.453028</td>\n",
              "      <td>0.366016</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.590478</td>\n",
              "      <td>0.701507</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>ru</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-ru-llama-65b.csv.gz</td>\n",
              "      <td>0.495636</td>\n",
              "      <td>0.707671</td>\n",
              "      <td>0.825016</td>\n",
              "      <td>0.642499</td>\n",
              "      <td>0.642499</td>\n",
              "      <td>0.534302</td>\n",
              "      <td>0.335546</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.565076</td>\n",
              "      <td>0.616450</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>en3</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>gpt2-medium-finetuned-en3-llama-65b.csv.gz</td>\n",
              "      <td>0.495248</td>\n",
              "      <td>0.771771</td>\n",
              "      <td>0.803881</td>\n",
              "      <td>0.744837</td>\n",
              "      <td>0.744837</td>\n",
              "      <td>0.811496</td>\n",
              "      <td>0.186078</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.501213</td>\n",
              "      <td>0.507860</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>xlm-roberta-large-finetuned-en3-gpt-4.csv.gz</td>\n",
              "      <td>0.495228</td>\n",
              "      <td>0.639683</td>\n",
              "      <td>0.887072</td>\n",
              "      <td>0.563338</td>\n",
              "      <td>0.563338</td>\n",
              "      <td>0.112794</td>\n",
              "      <td>0.476879</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.705164</td>\n",
              "      <td>0.736256</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>all</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>xlm-roberta-large-finetuned-all-gpt-4.csv.gz</td>\n",
              "      <td>0.494915</td>\n",
              "      <td>0.627250</td>\n",
              "      <td>0.905910</td>\n",
              "      <td>0.552040</td>\n",
              "      <td>0.552040</td>\n",
              "      <td>0.023486</td>\n",
              "      <td>0.500672</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.737921</td>\n",
              "      <td>0.764362</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>en3</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mGPT-finetuned-en3-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.494811</td>\n",
              "      <td>0.665745</td>\n",
              "      <td>0.855264</td>\n",
              "      <td>0.590101</td>\n",
              "      <td>0.590101</td>\n",
              "      <td>0.294808</td>\n",
              "      <td>0.424191</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.640500</td>\n",
              "      <td>0.710134</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>es</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mdeberta-v3-base-finetuned-es-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.494788</td>\n",
              "      <td>0.639302</td>\n",
              "      <td>0.886773</td>\n",
              "      <td>0.562895</td>\n",
              "      <td>0.562895</td>\n",
              "      <td>0.114030</td>\n",
              "      <td>0.477225</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.704373</td>\n",
              "      <td>0.695489</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>all</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>xlm-roberta-large-finetuned-all-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.494016</td>\n",
              "      <td>0.624528</td>\n",
              "      <td>0.908593</td>\n",
              "      <td>0.549479</td>\n",
              "      <td>0.549479</td>\n",
              "      <td>0.011434</td>\n",
              "      <td>0.505046</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.741760</td>\n",
              "      <td>0.795428</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>es</td>\n",
              "      <td>all</td>\n",
              "      <td>electra-large-discriminator-finetuned-es-all.csv.gz</td>\n",
              "      <td>0.491472</td>\n",
              "      <td>0.779179</td>\n",
              "      <td>0.801189</td>\n",
              "      <td>0.759652</td>\n",
              "      <td>0.759652</td>\n",
              "      <td>0.848578</td>\n",
              "      <td>0.164818</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.493302</td>\n",
              "      <td>0.495433</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>ru</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>electra-large-discriminator-finetuned-ru-llama-65b.csv.gz</td>\n",
              "      <td>0.489651</td>\n",
              "      <td>0.749275</td>\n",
              "      <td>0.805122</td>\n",
              "      <td>0.707254</td>\n",
              "      <td>0.707254</td>\n",
              "      <td>0.754326</td>\n",
              "      <td>0.235427</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.505123</td>\n",
              "      <td>0.510775</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>es</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>xlm-roberta-large-finetuned-es-vicuna-13b.csv.gz</td>\n",
              "      <td>0.489621</td>\n",
              "      <td>0.631758</td>\n",
              "      <td>0.887638</td>\n",
              "      <td>0.554839</td>\n",
              "      <td>0.554839</td>\n",
              "      <td>0.106613</td>\n",
              "      <td>0.487202</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.703092</td>\n",
              "      <td>0.824232</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>xlm-roberta-large-finetuned-en3-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.489415</td>\n",
              "      <td>0.626042</td>\n",
              "      <td>0.895942</td>\n",
              "      <td>0.549650</td>\n",
              "      <td>0.549650</td>\n",
              "      <td>0.066749</td>\n",
              "      <td>0.497985</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.717633</td>\n",
              "      <td>0.792722</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>all</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-all-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.489238</td>\n",
              "      <td>0.620887</td>\n",
              "      <td>0.904049</td>\n",
              "      <td>0.545144</td>\n",
              "      <td>0.545144</td>\n",
              "      <td>0.029975</td>\n",
              "      <td>0.507617</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.731204</td>\n",
              "      <td>0.807163</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>all</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-all-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.488158</td>\n",
              "      <td>0.630158</td>\n",
              "      <td>0.887087</td>\n",
              "      <td>0.553064</td>\n",
              "      <td>0.553064</td>\n",
              "      <td>0.108467</td>\n",
              "      <td>0.488967</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.701283</td>\n",
              "      <td>0.760717</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>all</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mdeberta-v3-base-finetuned-all-text-davinci-003.csv.gz</td>\n",
              "      <td>0.487793</td>\n",
              "      <td>0.618915</td>\n",
              "      <td>0.904157</td>\n",
              "      <td>0.543096</td>\n",
              "      <td>0.543096</td>\n",
              "      <td>0.029048</td>\n",
              "      <td>0.510035</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.730458</td>\n",
              "      <td>0.832195</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>en3</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-en3-text-davinci-003.csv.gz</td>\n",
              "      <td>0.487528</td>\n",
              "      <td>0.633029</td>\n",
              "      <td>0.881766</td>\n",
              "      <td>0.555590</td>\n",
              "      <td>0.555590</td>\n",
              "      <td>0.134734</td>\n",
              "      <td>0.482866</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.691200</td>\n",
              "      <td>0.725784</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>all</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-all-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.487085</td>\n",
              "      <td>0.622277</td>\n",
              "      <td>0.896997</td>\n",
              "      <td>0.545793</td>\n",
              "      <td>0.545793</td>\n",
              "      <td>0.060878</td>\n",
              "      <td>0.503051</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.718036</td>\n",
              "      <td>0.807079</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>all</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mdeberta-v3-base-finetuned-all-gpt-4.csv.gz</td>\n",
              "      <td>0.486767</td>\n",
              "      <td>0.622288</td>\n",
              "      <td>0.896294</td>\n",
              "      <td>0.545725</td>\n",
              "      <td>0.545725</td>\n",
              "      <td>0.063968</td>\n",
              "      <td>0.502744</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.716644</td>\n",
              "      <td>0.803885</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>statistical-entropy_RF-tuned</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>statistical-entropy_RF-tuned</td>\n",
              "      <td>0.486330</td>\n",
              "      <td>0.833501</td>\n",
              "      <td>0.805047</td>\n",
              "      <td>0.872913</td>\n",
              "      <td>0.872913</td>\n",
              "      <td>0.975587</td>\n",
              "      <td>0.021720</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.501346</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>ru</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-ru-opt-66b.csv.gz</td>\n",
              "      <td>0.485909</td>\n",
              "      <td>0.761204</td>\n",
              "      <td>0.800798</td>\n",
              "      <td>0.728793</td>\n",
              "      <td>0.728793</td>\n",
              "      <td>0.812423</td>\n",
              "      <td>0.203999</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.491789</td>\n",
              "      <td>0.443959</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>all</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-all-opt-66b.csv.gz</td>\n",
              "      <td>0.485099</td>\n",
              "      <td>0.636301</td>\n",
              "      <td>0.872940</td>\n",
              "      <td>0.558252</td>\n",
              "      <td>0.558252</td>\n",
              "      <td>0.179234</td>\n",
              "      <td>0.474347</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.673210</td>\n",
              "      <td>0.747711</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>en</td>\n",
              "      <td>all</td>\n",
              "      <td>gpt2-medium-finetuned-en-all.csv.gz</td>\n",
              "      <td>0.484911</td>\n",
              "      <td>0.707523</td>\n",
              "      <td>0.815748</td>\n",
              "      <td>0.643420</td>\n",
              "      <td>0.643420</td>\n",
              "      <td>0.598578</td>\n",
              "      <td>0.326528</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.537447</td>\n",
              "      <td>0.562276</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>es</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-es-llama-65b.csv.gz</td>\n",
              "      <td>0.484322</td>\n",
              "      <td>0.644680</td>\n",
              "      <td>0.861610</td>\n",
              "      <td>0.566479</td>\n",
              "      <td>0.566479</td>\n",
              "      <td>0.242583</td>\n",
              "      <td>0.457232</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.650092</td>\n",
              "      <td>0.694914</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>es</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mdeberta-v3-base-finetuned-es-text-davinci-003.csv.gz</td>\n",
              "      <td>0.483200</td>\n",
              "      <td>0.618935</td>\n",
              "      <td>0.894004</td>\n",
              "      <td>0.541935</td>\n",
              "      <td>0.541935</td>\n",
              "      <td>0.072930</td>\n",
              "      <td>0.505890</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.710590</td>\n",
              "      <td>0.809962</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>en</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-en-opt-66b.csv.gz</td>\n",
              "      <td>0.482495</td>\n",
              "      <td>0.711106</td>\n",
              "      <td>0.812258</td>\n",
              "      <td>0.648882</td>\n",
              "      <td>0.648882</td>\n",
              "      <td>0.629481</td>\n",
              "      <td>0.316551</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.526984</td>\n",
              "      <td>0.532684</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>ru</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-ru-llama-65b.csv.gz</td>\n",
              "      <td>0.481693</td>\n",
              "      <td>0.742843</td>\n",
              "      <td>0.801937</td>\n",
              "      <td>0.698481</td>\n",
              "      <td>0.698481</td>\n",
              "      <td>0.765760</td>\n",
              "      <td>0.243870</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.495185</td>\n",
              "      <td>0.486053</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>all</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-all-llama-65b.csv.gz</td>\n",
              "      <td>0.481338</td>\n",
              "      <td>0.630707</td>\n",
              "      <td>0.873388</td>\n",
              "      <td>0.552210</td>\n",
              "      <td>0.552210</td>\n",
              "      <td>0.173671</td>\n",
              "      <td>0.481830</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.672250</td>\n",
              "      <td>0.789353</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>en</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-en-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.480343</td>\n",
              "      <td>0.641137</td>\n",
              "      <td>0.859192</td>\n",
              "      <td>0.562314</td>\n",
              "      <td>0.562314</td>\n",
              "      <td>0.252472</td>\n",
              "      <td>0.460685</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.643421</td>\n",
              "      <td>0.744190</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>en3</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>xlm-roberta-large-finetuned-en3-opt-66b.csv.gz</td>\n",
              "      <td>0.479673</td>\n",
              "      <td>0.685480</td>\n",
              "      <td>0.822261</td>\n",
              "      <td>0.613791</td>\n",
              "      <td>0.613791</td>\n",
              "      <td>0.519778</td>\n",
              "      <td>0.369623</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.555300</td>\n",
              "      <td>0.594744</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>es</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>electra-large-discriminator-finetuned-es-opt-66b.csv.gz</td>\n",
              "      <td>0.478770</td>\n",
              "      <td>0.693570</td>\n",
              "      <td>0.817049</td>\n",
              "      <td>0.624612</td>\n",
              "      <td>0.624612</td>\n",
              "      <td>0.567058</td>\n",
              "      <td>0.351587</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.540678</td>\n",
              "      <td>0.601624</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>en</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mdeberta-v3-base-finetuned-en-text-davinci-003.csv.gz</td>\n",
              "      <td>0.478074</td>\n",
              "      <td>0.669787</td>\n",
              "      <td>0.830600</td>\n",
              "      <td>0.594095</td>\n",
              "      <td>0.594095</td>\n",
              "      <td>0.444994</td>\n",
              "      <td>0.401051</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.576977</td>\n",
              "      <td>0.690535</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>en</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mGPT-finetuned-en-opt-66b.csv.gz</td>\n",
              "      <td>0.477264</td>\n",
              "      <td>0.674710</td>\n",
              "      <td>0.826364</td>\n",
              "      <td>0.600137</td>\n",
              "      <td>0.600137</td>\n",
              "      <td>0.478059</td>\n",
              "      <td>0.390153</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.565894</td>\n",
              "      <td>0.620299</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>en3</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mdeberta-v3-base-finetuned-en3-text-davinci-003.csv.gz</td>\n",
              "      <td>0.476244</td>\n",
              "      <td>0.645452</td>\n",
              "      <td>0.848484</td>\n",
              "      <td>0.566308</td>\n",
              "      <td>0.566308</td>\n",
              "      <td>0.313659</td>\n",
              "      <td>0.448597</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.618872</td>\n",
              "      <td>0.696166</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>en3</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-en3-vicuna-13b.csv.gz</td>\n",
              "      <td>0.475553</td>\n",
              "      <td>0.657620</td>\n",
              "      <td>0.836760</td>\n",
              "      <td>0.579689</td>\n",
              "      <td>0.579689</td>\n",
              "      <td>0.393078</td>\n",
              "      <td>0.423692</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.591615</td>\n",
              "      <td>0.690941</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-en3-gpt-4.csv.gz</td>\n",
              "      <td>0.474859</td>\n",
              "      <td>0.623915</td>\n",
              "      <td>0.870374</td>\n",
              "      <td>0.544564</td>\n",
              "      <td>0.544564</td>\n",
              "      <td>0.184178</td>\n",
              "      <td>0.489121</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.663351</td>\n",
              "      <td>0.709188</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>es</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mdeberta-v3-base-finetuned-es-gpt-4.csv.gz</td>\n",
              "      <td>0.473011</td>\n",
              "      <td>0.611365</td>\n",
              "      <td>0.884211</td>\n",
              "      <td>0.532855</td>\n",
              "      <td>0.532855</td>\n",
              "      <td>0.113412</td>\n",
              "      <td>0.511071</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.687759</td>\n",
              "      <td>0.749026</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-ru-gpt-4.csv.gz</td>\n",
              "      <td>0.472522</td>\n",
              "      <td>0.722666</td>\n",
              "      <td>0.801210</td>\n",
              "      <td>0.667964</td>\n",
              "      <td>0.667964</td>\n",
              "      <td>0.731768</td>\n",
              "      <td>0.282398</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.492917</td>\n",
              "      <td>0.462085</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mdeberta-v3-base-finetuned-en3-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.471563</td>\n",
              "      <td>0.647356</td>\n",
              "      <td>0.840150</td>\n",
              "      <td>0.567913</td>\n",
              "      <td>0.567913</td>\n",
              "      <td>0.362176</td>\n",
              "      <td>0.440769</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.598528</td>\n",
              "      <td>0.691391</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>es</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-es-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.471506</td>\n",
              "      <td>0.604620</td>\n",
              "      <td>0.891461</td>\n",
              "      <td>0.526745</td>\n",
              "      <td>0.526745</td>\n",
              "      <td>0.079110</td>\n",
              "      <td>0.522200</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.699345</td>\n",
              "      <td>0.773888</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>ru</td>\n",
              "      <td>all</td>\n",
              "      <td>electra-large-discriminator-finetuned-ru-all.csv.gz</td>\n",
              "      <td>0.470887</td>\n",
              "      <td>0.836797</td>\n",
              "      <td>0.795170</td>\n",
              "      <td>0.887797</td>\n",
              "      <td>0.887797</td>\n",
              "      <td>0.999382</td>\n",
              "      <td>0.002034</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.499292</td>\n",
              "      <td>0.506971</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>statistical-rank</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>statistical-rank</td>\n",
              "      <td>0.470770</td>\n",
              "      <td>0.837535</td>\n",
              "      <td>0.791277</td>\n",
              "      <td>0.889537</td>\n",
              "      <td>0.889537</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>statistical-detectgpt</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>statistical-detectgpt</td>\n",
              "      <td>0.470770</td>\n",
              "      <td>0.837535</td>\n",
              "      <td>0.791277</td>\n",
              "      <td>0.889537</td>\n",
              "      <td>0.889537</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>statistical-entropy</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>statistical-entropy</td>\n",
              "      <td>0.470770</td>\n",
              "      <td>0.837535</td>\n",
              "      <td>0.791277</td>\n",
              "      <td>0.889537</td>\n",
              "      <td>0.889537</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>en</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>electra-large-discriminator-finetuned-en-opt-66b.csv.gz</td>\n",
              "      <td>0.470752</td>\n",
              "      <td>0.669335</td>\n",
              "      <td>0.822786</td>\n",
              "      <td>0.593514</td>\n",
              "      <td>0.593514</td>\n",
              "      <td>0.493511</td>\n",
              "      <td>0.395679</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.555405</td>\n",
              "      <td>0.602249</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>statistical-likelihood</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>statistical-likelihood</td>\n",
              "      <td>0.470330</td>\n",
              "      <td>0.836752</td>\n",
              "      <td>0.791122</td>\n",
              "      <td>0.887967</td>\n",
              "      <td>0.887967</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.001765</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.499117</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>statistical-log_rank</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>statistical-log_rank</td>\n",
              "      <td>0.470157</td>\n",
              "      <td>0.836445</td>\n",
              "      <td>0.791062</td>\n",
              "      <td>0.887353</td>\n",
              "      <td>0.887353</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.002456</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.498772</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>es</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mGPT-finetuned-es-text-davinci-003.csv.gz</td>\n",
              "      <td>0.469657</td>\n",
              "      <td>0.608342</td>\n",
              "      <td>0.881768</td>\n",
              "      <td>0.529408</td>\n",
              "      <td>0.529408</td>\n",
              "      <td>0.122991</td>\n",
              "      <td>0.513757</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.681626</td>\n",
              "      <td>0.742618</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>en3</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-en3-llama-65b.csv.gz</td>\n",
              "      <td>0.467960</td>\n",
              "      <td>0.648078</td>\n",
              "      <td>0.834753</td>\n",
              "      <td>0.568425</td>\n",
              "      <td>0.568425</td>\n",
              "      <td>0.394005</td>\n",
              "      <td>0.436241</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.584877</td>\n",
              "      <td>0.638281</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>en</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>gpt2-medium-finetuned-en-llama-65b.csv.gz</td>\n",
              "      <td>0.466852</td>\n",
              "      <td>0.715445</td>\n",
              "      <td>0.799417</td>\n",
              "      <td>0.657826</td>\n",
              "      <td>0.657826</td>\n",
              "      <td>0.731459</td>\n",
              "      <td>0.293833</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.487354</td>\n",
              "      <td>0.505357</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-en3-gpt-4.csv.gz</td>\n",
              "      <td>0.466843</td>\n",
              "      <td>0.657562</td>\n",
              "      <td>0.826214</td>\n",
              "      <td>0.579246</td>\n",
              "      <td>0.579246</td>\n",
              "      <td>0.456428</td>\n",
              "      <td>0.416324</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.563624</td>\n",
              "      <td>0.591772</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>en</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-en-text-davinci-003.csv.gz</td>\n",
              "      <td>0.466336</td>\n",
              "      <td>0.602412</td>\n",
              "      <td>0.883702</td>\n",
              "      <td>0.523502</td>\n",
              "      <td>0.523502</td>\n",
              "      <td>0.111867</td>\n",
              "      <td>0.521778</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.683178</td>\n",
              "      <td>0.726928</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>statistical-rank_GLTR</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>statistical-rank_GLTR</td>\n",
              "      <td>0.466236</td>\n",
              "      <td>0.828237</td>\n",
              "      <td>0.790060</td>\n",
              "      <td>0.870729</td>\n",
              "      <td>0.870729</td>\n",
              "      <td>0.999073</td>\n",
              "      <td>0.021259</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.489834</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>es</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>xlm-roberta-large-finetuned-es-llama-65b.csv.gz</td>\n",
              "      <td>0.465634</td>\n",
              "      <td>0.622665</td>\n",
              "      <td>0.855773</td>\n",
              "      <td>0.541662</td>\n",
              "      <td>0.541662</td>\n",
              "      <td>0.255562</td>\n",
              "      <td>0.483518</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.630460</td>\n",
              "      <td>0.660296</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>en</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>xlm-roberta-large-finetuned-en-vicuna-13b.csv.gz</td>\n",
              "      <td>0.465287</td>\n",
              "      <td>0.611641</td>\n",
              "      <td>0.868552</td>\n",
              "      <td>0.531285</td>\n",
              "      <td>0.531285</td>\n",
              "      <td>0.185414</td>\n",
              "      <td>0.503895</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.655345</td>\n",
              "      <td>0.768221</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>es</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>xlm-roberta-large-finetuned-es-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.464705</td>\n",
              "      <td>0.594435</td>\n",
              "      <td>0.892800</td>\n",
              "      <td>0.516505</td>\n",
              "      <td>0.516505</td>\n",
              "      <td>0.070148</td>\n",
              "      <td>0.534825</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.697513</td>\n",
              "      <td>0.842146</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>es</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mGPT-finetuned-es-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.464688</td>\n",
              "      <td>0.601554</td>\n",
              "      <td>0.881505</td>\n",
              "      <td>0.522342</td>\n",
              "      <td>0.522342</td>\n",
              "      <td>0.121137</td>\n",
              "      <td>0.521931</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.678466</td>\n",
              "      <td>0.739678</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>ru</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-ru-text-davinci-003.csv.gz</td>\n",
              "      <td>0.463071</td>\n",
              "      <td>0.703432</td>\n",
              "      <td>0.800319</td>\n",
              "      <td>0.640348</td>\n",
              "      <td>0.640348</td>\n",
              "      <td>0.702410</td>\n",
              "      <td>0.317088</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.490251</td>\n",
              "      <td>0.487587</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>all</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>xlm-roberta-large-finetuned-all-llama-65b.csv.gz</td>\n",
              "      <td>0.462752</td>\n",
              "      <td>0.622502</td>\n",
              "      <td>0.851212</td>\n",
              "      <td>0.541014</td>\n",
              "      <td>0.541014</td>\n",
              "      <td>0.278739</td>\n",
              "      <td>0.481369</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.619946</td>\n",
              "      <td>0.671150</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>en</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mGPT-finetuned-en-text-davinci-003.csv.gz</td>\n",
              "      <td>0.462483</td>\n",
              "      <td>0.614994</td>\n",
              "      <td>0.859259</td>\n",
              "      <td>0.533777</td>\n",
              "      <td>0.533777</td>\n",
              "      <td>0.232386</td>\n",
              "      <td>0.495261</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.636177</td>\n",
              "      <td>0.696095</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>en3</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-en3-text-davinci-003.csv.gz</td>\n",
              "      <td>0.462450</td>\n",
              "      <td>0.605507</td>\n",
              "      <td>0.871300</td>\n",
              "      <td>0.525175</td>\n",
              "      <td>0.525175</td>\n",
              "      <td>0.169036</td>\n",
              "      <td>0.512798</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.659083</td>\n",
              "      <td>0.724217</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>en3</td>\n",
              "      <td>all</td>\n",
              "      <td>gpt2-medium-finetuned-en3-all.csv.gz</td>\n",
              "      <td>0.461908</td>\n",
              "      <td>0.697166</td>\n",
              "      <td>0.801521</td>\n",
              "      <td>0.631371</td>\n",
              "      <td>0.631371</td>\n",
              "      <td>0.682324</td>\n",
              "      <td>0.329675</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.494001</td>\n",
              "      <td>0.516002</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>all</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>xlm-roberta-large-finetuned-all-opt-66b.csv.gz</td>\n",
              "      <td>0.458762</td>\n",
              "      <td>0.585729</td>\n",
              "      <td>0.893595</td>\n",
              "      <td>0.507834</td>\n",
              "      <td>0.507834</td>\n",
              "      <td>0.064277</td>\n",
              "      <td>0.545301</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.695211</td>\n",
              "      <td>0.753989</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>all</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mGPT-finetuned-all-text-davinci-003.csv.gz</td>\n",
              "      <td>0.458002</td>\n",
              "      <td>0.581530</td>\n",
              "      <td>0.899115</td>\n",
              "      <td>0.504386</td>\n",
              "      <td>0.504386</td>\n",
              "      <td>0.041100</td>\n",
              "      <td>0.552055</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.703422</td>\n",
              "      <td>0.785601</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mGPT-finetuned-ru-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.457593</td>\n",
              "      <td>0.594990</td>\n",
              "      <td>0.876324</td>\n",
              "      <td>0.514934</td>\n",
              "      <td>0.514934</td>\n",
              "      <td>0.140915</td>\n",
              "      <td>0.527802</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.665641</td>\n",
              "      <td>0.714516</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mdeberta-v3-base-finetuned-ru-gpt-4.csv.gz</td>\n",
              "      <td>0.456370</td>\n",
              "      <td>0.594568</td>\n",
              "      <td>0.874389</td>\n",
              "      <td>0.514252</td>\n",
              "      <td>0.514252</td>\n",
              "      <td>0.149258</td>\n",
              "      <td>0.527534</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.661604</td>\n",
              "      <td>0.759367</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mdeberta-v3-base-finetuned-en3-gpt-4.csv.gz</td>\n",
              "      <td>0.454663</td>\n",
              "      <td>0.617703</td>\n",
              "      <td>0.843320</td>\n",
              "      <td>0.534972</td>\n",
              "      <td>0.534972</td>\n",
              "      <td>0.315513</td>\n",
              "      <td>0.483595</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.600446</td>\n",
              "      <td>0.668121</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>all</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>xlm-roberta-large-finetuned-all-text-davinci-003.csv.gz</td>\n",
              "      <td>0.454262</td>\n",
              "      <td>0.576727</td>\n",
              "      <td>0.898443</td>\n",
              "      <td>0.499539</td>\n",
              "      <td>0.499539</td>\n",
              "      <td>0.042645</td>\n",
              "      <td>0.557312</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.700021</td>\n",
              "      <td>0.852871</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>en</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-en-vicuna-13b.csv.gz</td>\n",
              "      <td>0.453134</td>\n",
              "      <td>0.688505</td>\n",
              "      <td>0.797733</td>\n",
              "      <td>0.620038</td>\n",
              "      <td>0.620038</td>\n",
              "      <td>0.694067</td>\n",
              "      <td>0.340957</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.482488</td>\n",
              "      <td>0.629460</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>all</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mGPT-finetuned-all-gpt-4.csv.gz</td>\n",
              "      <td>0.452888</td>\n",
              "      <td>0.576577</td>\n",
              "      <td>0.895286</td>\n",
              "      <td>0.498959</td>\n",
              "      <td>0.498959</td>\n",
              "      <td>0.055006</td>\n",
              "      <td>0.556430</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.694282</td>\n",
              "      <td>0.750463</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>en</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>xlm-roberta-large-finetuned-en-text-davinci-003.csv.gz</td>\n",
              "      <td>0.452250</td>\n",
              "      <td>0.587897</td>\n",
              "      <td>0.875471</td>\n",
              "      <td>0.507595</td>\n",
              "      <td>0.507595</td>\n",
              "      <td>0.141224</td>\n",
              "      <td>0.536014</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.661381</td>\n",
              "      <td>0.778180</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>all</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-all-opt-66b.csv.gz</td>\n",
              "      <td>0.451727</td>\n",
              "      <td>0.586571</td>\n",
              "      <td>0.876324</td>\n",
              "      <td>0.506366</td>\n",
              "      <td>0.506366</td>\n",
              "      <td>0.136897</td>\n",
              "      <td>0.537933</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.662585</td>\n",
              "      <td>0.768075</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-ru-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.448999</td>\n",
              "      <td>0.670265</td>\n",
              "      <td>0.801161</td>\n",
              "      <td>0.595392</td>\n",
              "      <td>0.595392</td>\n",
              "      <td>0.638133</td>\n",
              "      <td>0.375609</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.493129</td>\n",
              "      <td>0.488656</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>es</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>electra-large-discriminator-finetuned-es-llama-65b.csv.gz</td>\n",
              "      <td>0.447883</td>\n",
              "      <td>0.632802</td>\n",
              "      <td>0.820906</td>\n",
              "      <td>0.549923</td>\n",
              "      <td>0.549923</td>\n",
              "      <td>0.456737</td>\n",
              "      <td>0.449250</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.547007</td>\n",
              "      <td>0.609385</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>all</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-all-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.445988</td>\n",
              "      <td>0.580136</td>\n",
              "      <td>0.873524</td>\n",
              "      <td>0.499505</td>\n",
              "      <td>0.499505</td>\n",
              "      <td>0.145859</td>\n",
              "      <td>0.544534</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.654804</td>\n",
              "      <td>0.742389</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>en</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mGPT-finetuned-en-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.445926</td>\n",
              "      <td>0.581224</td>\n",
              "      <td>0.871785</td>\n",
              "      <td>0.500358</td>\n",
              "      <td>0.500358</td>\n",
              "      <td>0.153894</td>\n",
              "      <td>0.542576</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.651765</td>\n",
              "      <td>0.711667</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>es</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>gpt2-medium-finetuned-es-vicuna-13b.csv.gz</td>\n",
              "      <td>0.445570</td>\n",
              "      <td>0.578360</td>\n",
              "      <td>0.875282</td>\n",
              "      <td>0.497969</td>\n",
              "      <td>0.497969</td>\n",
              "      <td>0.137515</td>\n",
              "      <td>0.547297</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.657594</td>\n",
              "      <td>0.748483</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>en</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>electra-large-discriminator-finetuned-en-vicuna-13b.csv.gz</td>\n",
              "      <td>0.445433</td>\n",
              "      <td>0.617309</td>\n",
              "      <td>0.829889</td>\n",
              "      <td>0.533197</td>\n",
              "      <td>0.533197</td>\n",
              "      <td>0.387206</td>\n",
              "      <td>0.476688</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.568053</td>\n",
              "      <td>0.637477</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>en</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-en-opt-66b.csv.gz</td>\n",
              "      <td>0.443604</td>\n",
              "      <td>0.620146</td>\n",
              "      <td>0.824974</td>\n",
              "      <td>0.535893</td>\n",
              "      <td>0.535893</td>\n",
              "      <td>0.417800</td>\n",
              "      <td>0.469857</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.556172</td>\n",
              "      <td>0.593869</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>en3</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>electra-large-discriminator-finetuned-en3-vicuna-13b.csv.gz</td>\n",
              "      <td>0.443065</td>\n",
              "      <td>0.603959</td>\n",
              "      <td>0.838759</td>\n",
              "      <td>0.519645</td>\n",
              "      <td>0.519645</td>\n",
              "      <td>0.326329</td>\n",
              "      <td>0.499482</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.587095</td>\n",
              "      <td>0.668084</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>en</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mGPT-finetuned-en-gpt-4.csv.gz</td>\n",
              "      <td>0.442825</td>\n",
              "      <td>0.589060</td>\n",
              "      <td>0.854993</td>\n",
              "      <td>0.506059</td>\n",
              "      <td>0.506059</td>\n",
              "      <td>0.234240</td>\n",
              "      <td>0.526191</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.619785</td>\n",
              "      <td>0.680873</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>all</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mGPT-finetuned-all-llama-65b.csv.gz</td>\n",
              "      <td>0.441762</td>\n",
              "      <td>0.580140</td>\n",
              "      <td>0.864585</td>\n",
              "      <td>0.498276</td>\n",
              "      <td>0.498276</td>\n",
              "      <td>0.184796</td>\n",
              "      <td>0.541080</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.637062</td>\n",
              "      <td>0.675300</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>es</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>xlm-roberta-large-finetuned-es-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.441229</td>\n",
              "      <td>0.555413</td>\n",
              "      <td>0.904442</td>\n",
              "      <td>0.479672</td>\n",
              "      <td>0.479672</td>\n",
              "      <td>0.016069</td>\n",
              "      <td>0.582946</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.700492</td>\n",
              "      <td>0.842410</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mGPT-finetuned-ru-gpt-4.csv.gz</td>\n",
              "      <td>0.440709</td>\n",
              "      <td>0.563984</td>\n",
              "      <td>0.887216</td>\n",
              "      <td>0.485475</td>\n",
              "      <td>0.485475</td>\n",
              "      <td>0.083127</td>\n",
              "      <td>0.568095</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.674389</td>\n",
              "      <td>0.723332</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>es</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-es-vicuna-13b.csv.gz</td>\n",
              "      <td>0.439047</td>\n",
              "      <td>0.577322</td>\n",
              "      <td>0.862878</td>\n",
              "      <td>0.495204</td>\n",
              "      <td>0.495204</td>\n",
              "      <td>0.190667</td>\n",
              "      <td>0.543804</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.632764</td>\n",
              "      <td>0.712582</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>en</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mdeberta-v3-base-finetuned-en-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.437736</td>\n",
              "      <td>0.603669</td>\n",
              "      <td>0.830719</td>\n",
              "      <td>0.518416</td>\n",
              "      <td>0.518416</td>\n",
              "      <td>0.368047</td>\n",
              "      <td>0.495683</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.568135</td>\n",
              "      <td>0.684327</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>es</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>gpt2-medium-finetuned-es-opt-66b.csv.gz</td>\n",
              "      <td>0.437147</td>\n",
              "      <td>0.603912</td>\n",
              "      <td>0.829605</td>\n",
              "      <td>0.518553</td>\n",
              "      <td>0.518553</td>\n",
              "      <td>0.374227</td>\n",
              "      <td>0.494762</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.565505</td>\n",
              "      <td>0.600430</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>es</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mGPT-finetuned-es-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.434361</td>\n",
              "      <td>0.576725</td>\n",
              "      <td>0.854244</td>\n",
              "      <td>0.493395</td>\n",
              "      <td>0.493395</td>\n",
              "      <td>0.228986</td>\n",
              "      <td>0.541080</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.614967</td>\n",
              "      <td>0.628957</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>ru</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>electra-large-discriminator-finetuned-ru-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.433427</td>\n",
              "      <td>0.633463</td>\n",
              "      <td>0.803664</td>\n",
              "      <td>0.549787</td>\n",
              "      <td>0.549787</td>\n",
              "      <td>0.562732</td>\n",
              "      <td>0.436241</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.500514</td>\n",
              "      <td>0.532835</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>en3</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-en3-llama-65b.csv.gz</td>\n",
              "      <td>0.432415</td>\n",
              "      <td>0.618272</td>\n",
              "      <td>0.811803</td>\n",
              "      <td>0.532685</td>\n",
              "      <td>0.532685</td>\n",
              "      <td>0.491347</td>\n",
              "      <td>0.464331</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.522161</td>\n",
              "      <td>0.545700</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>all</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>gpt2-medium-finetuned-all-opt-66b.csv.gz</td>\n",
              "      <td>0.431336</td>\n",
              "      <td>0.565599</td>\n",
              "      <td>0.863066</td>\n",
              "      <td>0.483564</td>\n",
              "      <td>0.483564</td>\n",
              "      <td>0.182942</td>\n",
              "      <td>0.557849</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.629604</td>\n",
              "      <td>0.701977</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>es</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mGPT-finetuned-es-gpt-4.csv.gz</td>\n",
              "      <td>0.430409</td>\n",
              "      <td>0.548773</td>\n",
              "      <td>0.887923</td>\n",
              "      <td>0.470934</td>\n",
              "      <td>0.470934</td>\n",
              "      <td>0.075711</td>\n",
              "      <td>0.585364</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.669463</td>\n",
              "      <td>0.739242</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>all</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mGPT-finetuned-all-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.427575</td>\n",
              "      <td>0.543453</td>\n",
              "      <td>0.890173</td>\n",
              "      <td>0.466223</td>\n",
              "      <td>0.466223</td>\n",
              "      <td>0.065822</td>\n",
              "      <td>0.591888</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.671145</td>\n",
              "      <td>0.720559</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>en3</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mGPT-finetuned-en3-text-davinci-003.csv.gz</td>\n",
              "      <td>0.427558</td>\n",
              "      <td>0.549788</td>\n",
              "      <td>0.878900</td>\n",
              "      <td>0.470558</td>\n",
              "      <td>0.470558</td>\n",
              "      <td>0.110630</td>\n",
              "      <td>0.581450</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.653960</td>\n",
              "      <td>0.679092</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>blackbox-ZeroGPT</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>blackbox-ZeroGPT</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.555909</td>\n",
              "      <td>0.865302</td>\n",
              "      <td>0.474415</td>\n",
              "      <td>0.474415</td>\n",
              "      <td>0.168109</td>\n",
              "      <td>0.569976</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.630958</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>all</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-all-gpt-4.csv.gz</td>\n",
              "      <td>0.421871</td>\n",
              "      <td>0.534461</td>\n",
              "      <td>0.891580</td>\n",
              "      <td>0.457996</td>\n",
              "      <td>0.457996</td>\n",
              "      <td>0.058405</td>\n",
              "      <td>0.602057</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.669769</td>\n",
              "      <td>0.755480</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>en3</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>gpt2-medium-finetuned-en3-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.421614</td>\n",
              "      <td>0.574354</td>\n",
              "      <td>0.833418</td>\n",
              "      <td>0.488070</td>\n",
              "      <td>0.488070</td>\n",
              "      <td>0.325093</td>\n",
              "      <td>0.535132</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.569888</td>\n",
              "      <td>0.630157</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>es</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>gpt2-medium-finetuned-es-llama-65b.csv.gz</td>\n",
              "      <td>0.420968</td>\n",
              "      <td>0.588508</td>\n",
              "      <td>0.818893</td>\n",
              "      <td>0.500836</td>\n",
              "      <td>0.500836</td>\n",
              "      <td>0.414091</td>\n",
              "      <td>0.509728</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.538090</td>\n",
              "      <td>0.559947</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>en</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>xlm-roberta-large-finetuned-en-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.418850</td>\n",
              "      <td>0.531225</td>\n",
              "      <td>0.889369</td>\n",
              "      <td>0.454651</td>\n",
              "      <td>0.454651</td>\n",
              "      <td>0.065513</td>\n",
              "      <td>0.604935</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.664776</td>\n",
              "      <td>0.775137</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>all</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mGPT-finetuned-all-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.417104</td>\n",
              "      <td>0.529118</td>\n",
              "      <td>0.888522</td>\n",
              "      <td>0.452569</td>\n",
              "      <td>0.452569</td>\n",
              "      <td>0.067985</td>\n",
              "      <td>0.606969</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.662523</td>\n",
              "      <td>0.755304</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mGPT-finetuned-en3-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.417025</td>\n",
              "      <td>0.539024</td>\n",
              "      <td>0.870755</td>\n",
              "      <td>0.459089</td>\n",
              "      <td>0.459089</td>\n",
              "      <td>0.137824</td>\n",
              "      <td>0.590967</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.635604</td>\n",
              "      <td>0.690062</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>all</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>gpt2-medium-finetuned-all-text-davinci-003.csv.gz</td>\n",
              "      <td>0.414019</td>\n",
              "      <td>0.524990</td>\n",
              "      <td>0.887775</td>\n",
              "      <td>0.448643</td>\n",
              "      <td>0.448643</td>\n",
              "      <td>0.069530</td>\n",
              "      <td>0.611190</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.659640</td>\n",
              "      <td>0.766416</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>236</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>en</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>gpt2-medium-finetuned-en-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.413160</td>\n",
              "      <td>0.563076</td>\n",
              "      <td>0.830495</td>\n",
              "      <td>0.476259</td>\n",
              "      <td>0.476259</td>\n",
              "      <td>0.328492</td>\n",
              "      <td>0.547987</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.561760</td>\n",
              "      <td>0.598876</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>all</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-all-llama-65b.csv.gz</td>\n",
              "      <td>0.411371</td>\n",
              "      <td>0.541472</td>\n",
              "      <td>0.853727</td>\n",
              "      <td>0.458747</td>\n",
              "      <td>0.458747</td>\n",
              "      <td>0.207664</td>\n",
              "      <td>0.582678</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.604829</td>\n",
              "      <td>0.675881</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>238</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>en</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-en-llama-65b.csv.gz</td>\n",
              "      <td>0.411308</td>\n",
              "      <td>0.600116</td>\n",
              "      <td>0.797511</td>\n",
              "      <td>0.511077</td>\n",
              "      <td>0.511077</td>\n",
              "      <td>0.550062</td>\n",
              "      <td>0.481331</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.484304</td>\n",
              "      <td>0.511104</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>en3</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>gpt2-medium-finetuned-en3-opt-66b.csv.gz</td>\n",
              "      <td>0.409883</td>\n",
              "      <td>0.562598</td>\n",
              "      <td>0.825212</td>\n",
              "      <td>0.474996</td>\n",
              "      <td>0.474996</td>\n",
              "      <td>0.353523</td>\n",
              "      <td>0.546299</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.550089</td>\n",
              "      <td>0.563874</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>en3</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>xlm-roberta-large-finetuned-en3-text-davinci-003.csv.gz</td>\n",
              "      <td>0.409586</td>\n",
              "      <td>0.518806</td>\n",
              "      <td>0.887125</td>\n",
              "      <td>0.442874</td>\n",
              "      <td>0.442874</td>\n",
              "      <td>0.070148</td>\n",
              "      <td>0.617599</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.656127</td>\n",
              "      <td>0.773114</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>es</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-es-llama-65b.csv.gz</td>\n",
              "      <td>0.409573</td>\n",
              "      <td>0.556951</td>\n",
              "      <td>0.830638</td>\n",
              "      <td>0.470183</td>\n",
              "      <td>0.470183</td>\n",
              "      <td>0.322002</td>\n",
              "      <td>0.555624</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.561187</td>\n",
              "      <td>0.600818</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>es</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-es-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.406698</td>\n",
              "      <td>0.528444</td>\n",
              "      <td>0.862437</td>\n",
              "      <td>0.447858</td>\n",
              "      <td>0.447858</td>\n",
              "      <td>0.165019</td>\n",
              "      <td>0.600215</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.617383</td>\n",
              "      <td>0.704118</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>all</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>gpt2-medium-finetuned-all-vicuna-13b.csv.gz</td>\n",
              "      <td>0.405648</td>\n",
              "      <td>0.508230</td>\n",
              "      <td>0.896928</td>\n",
              "      <td>0.434818</td>\n",
              "      <td>0.434818</td>\n",
              "      <td>0.034611</td>\n",
              "      <td>0.631068</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.667161</td>\n",
              "      <td>0.832440</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>ru</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>gpt2-medium-finetuned-ru-llama-65b.csv.gz</td>\n",
              "      <td>0.404211</td>\n",
              "      <td>0.595898</td>\n",
              "      <td>0.791470</td>\n",
              "      <td>0.505820</td>\n",
              "      <td>0.505820</td>\n",
              "      <td>0.579728</td>\n",
              "      <td>0.483557</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.468358</td>\n",
              "      <td>0.482927</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>en</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-en-gpt-4.csv.gz</td>\n",
              "      <td>0.402692</td>\n",
              "      <td>0.509692</td>\n",
              "      <td>0.884971</td>\n",
              "      <td>0.434272</td>\n",
              "      <td>0.434272</td>\n",
              "      <td>0.075093</td>\n",
              "      <td>0.626655</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.649126</td>\n",
              "      <td>0.691477</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mGPT-finetuned-en3-gpt-4.csv.gz</td>\n",
              "      <td>0.402445</td>\n",
              "      <td>0.515719</td>\n",
              "      <td>0.872945</td>\n",
              "      <td>0.437822</td>\n",
              "      <td>0.437822</td>\n",
              "      <td>0.119592</td>\n",
              "      <td>0.617138</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.631635</td>\n",
              "      <td>0.646943</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>all</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>electra-large-discriminator-finetuned-all-opt-66b.csv.gz</td>\n",
              "      <td>0.401549</td>\n",
              "      <td>0.520420</td>\n",
              "      <td>0.862555</td>\n",
              "      <td>0.440451</td>\n",
              "      <td>0.440451</td>\n",
              "      <td>0.160383</td>\n",
              "      <td>0.609118</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.615250</td>\n",
              "      <td>0.687791</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>ru</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-ru-text-davinci-003.csv.gz</td>\n",
              "      <td>0.397715</td>\n",
              "      <td>0.502740</td>\n",
              "      <td>0.884032</td>\n",
              "      <td>0.427889</td>\n",
              "      <td>0.427889</td>\n",
              "      <td>0.076329</td>\n",
              "      <td>0.633677</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.644997</td>\n",
              "      <td>0.732773</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>es</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>gpt2-medium-finetuned-es-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.396313</td>\n",
              "      <td>0.514619</td>\n",
              "      <td>0.858740</td>\n",
              "      <td>0.434511</td>\n",
              "      <td>0.434511</td>\n",
              "      <td>0.171817</td>\n",
              "      <td>0.614375</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.606904</td>\n",
              "      <td>0.643591</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>all</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>gpt2-medium-finetuned-all-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.395710</td>\n",
              "      <td>0.502724</td>\n",
              "      <td>0.878138</td>\n",
              "      <td>0.426933</td>\n",
              "      <td>0.426933</td>\n",
              "      <td>0.096415</td>\n",
              "      <td>0.632258</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.635664</td>\n",
              "      <td>0.762180</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>es</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-es-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.393647</td>\n",
              "      <td>0.513379</td>\n",
              "      <td>0.854028</td>\n",
              "      <td>0.432599</td>\n",
              "      <td>0.432599</td>\n",
              "      <td>0.189122</td>\n",
              "      <td>0.614375</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.598251</td>\n",
              "      <td>0.653135</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>en3</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>electra-large-discriminator-finetuned-en3-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.388707</td>\n",
              "      <td>0.531230</td>\n",
              "      <td>0.819730</td>\n",
              "      <td>0.443455</td>\n",
              "      <td>0.443455</td>\n",
              "      <td>0.347342</td>\n",
              "      <td>0.582524</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.535067</td>\n",
              "      <td>0.635839</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>ru</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mGPT-finetuned-ru-text-davinci-003.csv.gz</td>\n",
              "      <td>0.388129</td>\n",
              "      <td>0.484379</td>\n",
              "      <td>0.892598</td>\n",
              "      <td>0.413074</td>\n",
              "      <td>0.413074</td>\n",
              "      <td>0.044190</td>\n",
              "      <td>0.654323</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.650743</td>\n",
              "      <td>0.745371</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>es</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>gpt2-medium-finetuned-es-text-davinci-003.csv.gz</td>\n",
              "      <td>0.382733</td>\n",
              "      <td>0.483492</td>\n",
              "      <td>0.877149</td>\n",
              "      <td>0.409831</td>\n",
              "      <td>0.409831</td>\n",
              "      <td>0.093325</td>\n",
              "      <td>0.651867</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.627404</td>\n",
              "      <td>0.672555</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>all</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>gpt2-medium-finetuned-all-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.380017</td>\n",
              "      <td>0.472399</td>\n",
              "      <td>0.892273</td>\n",
              "      <td>0.402697</td>\n",
              "      <td>0.402697</td>\n",
              "      <td>0.042954</td>\n",
              "      <td>0.666142</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.645452</td>\n",
              "      <td>0.785381</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>es</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-es-text-davinci-003.csv.gz</td>\n",
              "      <td>0.376126</td>\n",
              "      <td>0.482742</td>\n",
              "      <td>0.858898</td>\n",
              "      <td>0.406144</td>\n",
              "      <td>0.406144</td>\n",
              "      <td>0.154512</td>\n",
              "      <td>0.648413</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.598538</td>\n",
              "      <td>0.670794</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>257</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>ru</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mGPT-finetuned-ru-opt-66b.csv.gz</td>\n",
              "      <td>0.371776</td>\n",
              "      <td>0.486537</td>\n",
              "      <td>0.840733</td>\n",
              "      <td>0.406315</td>\n",
              "      <td>0.406315</td>\n",
              "      <td>0.222188</td>\n",
              "      <td>0.639817</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.568997</td>\n",
              "      <td>0.570702</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>ru</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>electra-large-discriminator-finetuned-ru-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.369966</td>\n",
              "      <td>0.498562</td>\n",
              "      <td>0.819608</td>\n",
              "      <td>0.413210</td>\n",
              "      <td>0.413210</td>\n",
              "      <td>0.315513</td>\n",
              "      <td>0.620477</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.532005</td>\n",
              "      <td>0.529911</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>es</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-es-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.369579</td>\n",
              "      <td>0.454965</td>\n",
              "      <td>0.896501</td>\n",
              "      <td>0.388633</td>\n",
              "      <td>0.388633</td>\n",
              "      <td>0.027812</td>\n",
              "      <td>0.683833</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.644178</td>\n",
              "      <td>0.679795</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>all</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>gpt2-medium-finetuned-all-gpt-4.csv.gz</td>\n",
              "      <td>0.366916</td>\n",
              "      <td>0.453017</td>\n",
              "      <td>0.891398</td>\n",
              "      <td>0.386209</td>\n",
              "      <td>0.386209</td>\n",
              "      <td>0.042027</td>\n",
              "      <td>0.684792</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.636590</td>\n",
              "      <td>0.776197</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>all</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>gpt2-medium-finetuned-all-llama-65b.csv.gz</td>\n",
              "      <td>0.365997</td>\n",
              "      <td>0.462306</td>\n",
              "      <td>0.867476</td>\n",
              "      <td>0.390101</td>\n",
              "      <td>0.390101</td>\n",
              "      <td>0.116811</td>\n",
              "      <td>0.671131</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.606029</td>\n",
              "      <td>0.634321</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>es</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>xlm-roberta-large-finetuned-es-gpt-4.csv.gz</td>\n",
              "      <td>0.365840</td>\n",
              "      <td>0.446707</td>\n",
              "      <td>0.903333</td>\n",
              "      <td>0.382830</td>\n",
              "      <td>0.382830</td>\n",
              "      <td>0.008035</td>\n",
              "      <td>0.692812</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.649576</td>\n",
              "      <td>0.824223</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>es</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-es-gpt-4.csv.gz</td>\n",
              "      <td>0.365708</td>\n",
              "      <td>0.467121</td>\n",
              "      <td>0.857130</td>\n",
              "      <td>0.392422</td>\n",
              "      <td>0.392422</td>\n",
              "      <td>0.152658</td>\n",
              "      <td>0.664070</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.591636</td>\n",
              "      <td>0.642544</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>es</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>electra-large-discriminator-finetuned-es-vicuna-13b.csv.gz</td>\n",
              "      <td>0.362544</td>\n",
              "      <td>0.468701</td>\n",
              "      <td>0.845076</td>\n",
              "      <td>0.391671</td>\n",
              "      <td>0.391671</td>\n",
              "      <td>0.194685</td>\n",
              "      <td>0.659695</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.572810</td>\n",
              "      <td>0.636909</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>ru</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>xlm-roberta-large-finetuned-ru-text-davinci-003.csv.gz</td>\n",
              "      <td>0.358429</td>\n",
              "      <td>0.441555</td>\n",
              "      <td>0.887791</td>\n",
              "      <td>0.376173</td>\n",
              "      <td>0.376173</td>\n",
              "      <td>0.050062</td>\n",
              "      <td>0.695077</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.627431</td>\n",
              "      <td>0.788449</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>en3</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>gpt2-medium-finetuned-en3-vicuna-13b.csv.gz</td>\n",
              "      <td>0.357012</td>\n",
              "      <td>0.467920</td>\n",
              "      <td>0.831253</td>\n",
              "      <td>0.388530</td>\n",
              "      <td>0.388530</td>\n",
              "      <td>0.243511</td>\n",
              "      <td>0.657163</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.549663</td>\n",
              "      <td>0.635090</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>all</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>gpt2-medium-finetuned-all-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.355082</td>\n",
              "      <td>0.431747</td>\n",
              "      <td>0.900067</td>\n",
              "      <td>0.370097</td>\n",
              "      <td>0.370097</td>\n",
              "      <td>0.015451</td>\n",
              "      <td>0.706205</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.639172</td>\n",
              "      <td>0.804542</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>en</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>xlm-roberta-large-finetuned-en-gpt-4.csv.gz</td>\n",
              "      <td>0.354579</td>\n",
              "      <td>0.433655</td>\n",
              "      <td>0.892921</td>\n",
              "      <td>0.370541</td>\n",
              "      <td>0.370541</td>\n",
              "      <td>0.034611</td>\n",
              "      <td>0.703327</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.631031</td>\n",
              "      <td>0.755875</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>ru</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mdeberta-v3-base-finetuned-ru-text-davinci-003.csv.gz</td>\n",
              "      <td>0.354182</td>\n",
              "      <td>0.429168</td>\n",
              "      <td>0.903451</td>\n",
              "      <td>0.368527</td>\n",
              "      <td>0.368527</td>\n",
              "      <td>0.006489</td>\n",
              "      <td>0.709083</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.642214</td>\n",
              "      <td>0.833264</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>en</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>electra-large-discriminator-finetuned-en-text-davinci-003.csv.gz</td>\n",
              "      <td>0.352606</td>\n",
              "      <td>0.454392</td>\n",
              "      <td>0.841436</td>\n",
              "      <td>0.378973</td>\n",
              "      <td>0.378973</td>\n",
              "      <td>0.198084</td>\n",
              "      <td>0.673548</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.564184</td>\n",
              "      <td>0.653209</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>en</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mdeberta-v3-base-finetuned-en-gpt-4.csv.gz</td>\n",
              "      <td>0.350923</td>\n",
              "      <td>0.427030</td>\n",
              "      <td>0.895724</td>\n",
              "      <td>0.365626</td>\n",
              "      <td>0.365626</td>\n",
              "      <td>0.026267</td>\n",
              "      <td>0.709889</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.631922</td>\n",
              "      <td>0.661799</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>en</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>electra-large-discriminator-finetuned-en-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.350254</td>\n",
              "      <td>0.442734</td>\n",
              "      <td>0.856264</td>\n",
              "      <td>0.371941</td>\n",
              "      <td>0.371941</td>\n",
              "      <td>0.143387</td>\n",
              "      <td>0.688246</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.584184</td>\n",
              "      <td>0.671382</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>es</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-es-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.348358</td>\n",
              "      <td>0.425944</td>\n",
              "      <td>0.888153</td>\n",
              "      <td>0.363577</td>\n",
              "      <td>0.363577</td>\n",
              "      <td>0.046044</td>\n",
              "      <td>0.709736</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.622110</td>\n",
              "      <td>0.670766</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>all</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>xlm-roberta-large-finetuned-all-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.345646</td>\n",
              "      <td>0.421223</td>\n",
              "      <td>0.889589</td>\n",
              "      <td>0.360027</td>\n",
              "      <td>0.360027</td>\n",
              "      <td>0.041409</td>\n",
              "      <td>0.714302</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.622144</td>\n",
              "      <td>0.688418</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>en</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>gpt2-medium-finetuned-en-opt-66b.csv.gz</td>\n",
              "      <td>0.345167</td>\n",
              "      <td>0.446399</td>\n",
              "      <td>0.833628</td>\n",
              "      <td>0.370951</td>\n",
              "      <td>0.370951</td>\n",
              "      <td>0.219098</td>\n",
              "      <td>0.679957</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.550473</td>\n",
              "      <td>0.580848</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>es</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>electra-large-discriminator-finetuned-es-text-davinci-003.csv.gz</td>\n",
              "      <td>0.344822</td>\n",
              "      <td>0.444330</td>\n",
              "      <td>0.836225</td>\n",
              "      <td>0.369722</td>\n",
              "      <td>0.369722</td>\n",
              "      <td>0.208900</td>\n",
              "      <td>0.682605</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.554248</td>\n",
              "      <td>0.643464</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>ru</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>xlm-roberta-large-finetuned-ru-opt-66b.csv.gz</td>\n",
              "      <td>0.340420</td>\n",
              "      <td>0.409619</td>\n",
              "      <td>0.899645</td>\n",
              "      <td>0.352381</td>\n",
              "      <td>0.352381</td>\n",
              "      <td>0.014524</td>\n",
              "      <td>0.726237</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.629620</td>\n",
              "      <td>0.701259</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>roberta-large-openai-detector</td>\n",
              "      <td>all</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>roberta-large-openai-detector-finetuned-all-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.338767</td>\n",
              "      <td>0.408192</td>\n",
              "      <td>0.896429</td>\n",
              "      <td>0.350777</td>\n",
              "      <td>0.350777</td>\n",
              "      <td>0.022250</td>\n",
              "      <td>0.727081</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.625335</td>\n",
              "      <td>0.774304</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>es</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>gpt2-medium-finetuned-es-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.335497</td>\n",
              "      <td>0.427709</td>\n",
              "      <td>0.837945</td>\n",
              "      <td>0.356580</td>\n",
              "      <td>0.356580</td>\n",
              "      <td>0.192213</td>\n",
              "      <td>0.699451</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.554168</td>\n",
              "      <td>0.637338</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>es</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>electra-large-discriminator-finetuned-es-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.335094</td>\n",
              "      <td>0.425205</td>\n",
              "      <td>0.841499</td>\n",
              "      <td>0.355214</td>\n",
              "      <td>0.355214</td>\n",
              "      <td>0.179543</td>\n",
              "      <td>0.702560</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.558949</td>\n",
              "      <td>0.639474</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>all</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>electra-large-discriminator-finetuned-all-llama-65b.csv.gz</td>\n",
              "      <td>0.331128</td>\n",
              "      <td>0.405731</td>\n",
              "      <td>0.870699</td>\n",
              "      <td>0.344837</td>\n",
              "      <td>0.344837</td>\n",
              "      <td>0.087145</td>\n",
              "      <td>0.725699</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.593578</td>\n",
              "      <td>0.624965</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>all</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>electra-large-discriminator-finetuned-all-vicuna-13b.csv.gz</td>\n",
              "      <td>0.329236</td>\n",
              "      <td>0.393918</td>\n",
              "      <td>0.895246</td>\n",
              "      <td>0.339512</td>\n",
              "      <td>0.339512</td>\n",
              "      <td>0.023486</td>\n",
              "      <td>0.739591</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.618462</td>\n",
              "      <td>0.707840</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>es</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>electra-large-discriminator-finetuned-es-gpt-4.csv.gz</td>\n",
              "      <td>0.325855</td>\n",
              "      <td>0.412467</td>\n",
              "      <td>0.835938</td>\n",
              "      <td>0.344188</td>\n",
              "      <td>0.344188</td>\n",
              "      <td>0.188504</td>\n",
              "      <td>0.713842</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.548827</td>\n",
              "      <td>0.633436</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>all</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>electra-large-discriminator-finetuned-all-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.324427</td>\n",
              "      <td>0.387578</td>\n",
              "      <td>0.891864</td>\n",
              "      <td>0.334153</td>\n",
              "      <td>0.334153</td>\n",
              "      <td>0.030593</td>\n",
              "      <td>0.744733</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.612337</td>\n",
              "      <td>0.700732</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>all</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>electra-large-discriminator-finetuned-all-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.321353</td>\n",
              "      <td>0.381345</td>\n",
              "      <td>0.896411</td>\n",
              "      <td>0.330090</td>\n",
              "      <td>0.330090</td>\n",
              "      <td>0.019468</td>\n",
              "      <td>0.750681</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.614925</td>\n",
              "      <td>0.715804</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>es</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>electra-large-discriminator-finetuned-es-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.319260</td>\n",
              "      <td>0.396526</td>\n",
              "      <td>0.846361</td>\n",
              "      <td>0.333709</td>\n",
              "      <td>0.333709</td>\n",
              "      <td>0.148949</td>\n",
              "      <td>0.730535</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.560258</td>\n",
              "      <td>0.644083</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>en</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>electra-large-discriminator-finetuned-en-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.314809</td>\n",
              "      <td>0.392909</td>\n",
              "      <td>0.837753</td>\n",
              "      <td>0.329476</td>\n",
              "      <td>0.329476</td>\n",
              "      <td>0.170890</td>\n",
              "      <td>0.732568</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.548271</td>\n",
              "      <td>0.640889</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>en</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>gpt2-medium-finetuned-en-vicuna-13b.csv.gz</td>\n",
              "      <td>0.313205</td>\n",
              "      <td>0.390665</td>\n",
              "      <td>0.836711</td>\n",
              "      <td>0.327599</td>\n",
              "      <td>0.327599</td>\n",
              "      <td>0.172435</td>\n",
              "      <td>0.734487</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.546539</td>\n",
              "      <td>0.628785</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>ru</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-ru-opt-66b.csv.gz</td>\n",
              "      <td>0.309555</td>\n",
              "      <td>0.366109</td>\n",
              "      <td>0.886159</td>\n",
              "      <td>0.317187</td>\n",
              "      <td>0.317187</td>\n",
              "      <td>0.040173</td>\n",
              "      <td>0.762616</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.598606</td>\n",
              "      <td>0.569724</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>ru</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-ru-opt-66b.csv.gz</td>\n",
              "      <td>0.308479</td>\n",
              "      <td>0.361756</td>\n",
              "      <td>0.894933</td>\n",
              "      <td>0.315242</td>\n",
              "      <td>0.315242</td>\n",
              "      <td>0.020705</td>\n",
              "      <td>0.767221</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.606037</td>\n",
              "      <td>0.699036</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>all</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>electra-large-discriminator-finetuned-all-gpt-4.csv.gz</td>\n",
              "      <td>0.303492</td>\n",
              "      <td>0.352319</td>\n",
              "      <td>0.900737</td>\n",
              "      <td>0.309131</td>\n",
              "      <td>0.309131</td>\n",
              "      <td>0.008035</td>\n",
              "      <td>0.775663</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.608151</td>\n",
              "      <td>0.705440</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>ru</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>gpt2-medium-finetuned-ru-vicuna-13b.csv.gz</td>\n",
              "      <td>0.299265</td>\n",
              "      <td>0.358544</td>\n",
              "      <td>0.858971</td>\n",
              "      <td>0.307527</td>\n",
              "      <td>0.307527</td>\n",
              "      <td>0.099506</td>\n",
              "      <td>0.766108</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.567193</td>\n",
              "      <td>0.648312</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>electra-large-discriminator-finetuned-en3-gpt-4.csv.gz</td>\n",
              "      <td>0.294223</td>\n",
              "      <td>0.351677</td>\n",
              "      <td>0.854833</td>\n",
              "      <td>0.301929</td>\n",
              "      <td>0.301929</td>\n",
              "      <td>0.106304</td>\n",
              "      <td>0.771557</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.561070</td>\n",
              "      <td>0.636948</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>es</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>gpt2-medium-finetuned-es-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.292554</td>\n",
              "      <td>0.340774</td>\n",
              "      <td>0.880562</td>\n",
              "      <td>0.297969</td>\n",
              "      <td>0.297969</td>\n",
              "      <td>0.047281</td>\n",
              "      <td>0.783338</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.584691</td>\n",
              "      <td>0.658344</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>all</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>electra-large-discriminator-finetuned-all-text-davinci-003.csv.gz</td>\n",
              "      <td>0.290891</td>\n",
              "      <td>0.333114</td>\n",
              "      <td>0.898862</td>\n",
              "      <td>0.295033</td>\n",
              "      <td>0.295033</td>\n",
              "      <td>0.010507</td>\n",
              "      <td>0.791205</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.599144</td>\n",
              "      <td>0.702262</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>es</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>xlm-roberta-large-finetuned-es-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.290052</td>\n",
              "      <td>0.331344</td>\n",
              "      <td>0.900632</td>\n",
              "      <td>0.294009</td>\n",
              "      <td>0.294009</td>\n",
              "      <td>0.007108</td>\n",
              "      <td>0.792778</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.600057</td>\n",
              "      <td>0.679857</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>ru</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>gpt2-medium-finetuned-ru-opt-66b.csv.gz</td>\n",
              "      <td>0.288770</td>\n",
              "      <td>0.343528</td>\n",
              "      <td>0.852219</td>\n",
              "      <td>0.295716</td>\n",
              "      <td>0.295716</td>\n",
              "      <td>0.108776</td>\n",
              "      <td>0.778234</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.556495</td>\n",
              "      <td>0.600062</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>en</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>gpt2-medium-finetuned-en-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.278989</td>\n",
              "      <td>0.325852</td>\n",
              "      <td>0.856611</td>\n",
              "      <td>0.284008</td>\n",
              "      <td>0.284008</td>\n",
              "      <td>0.092089</td>\n",
              "      <td>0.793469</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.557221</td>\n",
              "      <td>0.610913</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>electra-large-discriminator-finetuned-en3-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.266367</td>\n",
              "      <td>0.311631</td>\n",
              "      <td>0.834244</td>\n",
              "      <td>0.270968</td>\n",
              "      <td>0.270968</td>\n",
              "      <td>0.131953</td>\n",
              "      <td>0.803177</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.532435</td>\n",
              "      <td>0.620925</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>electra-large-discriminator-finetuned-ru-gpt-4.csv.gz</td>\n",
              "      <td>0.260882</td>\n",
              "      <td>0.310298</td>\n",
              "      <td>0.810358</td>\n",
              "      <td>0.266325</td>\n",
              "      <td>0.266325</td>\n",
              "      <td>0.182942</td>\n",
              "      <td>0.802065</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.507497</td>\n",
              "      <td>0.523619</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>all</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>electra-large-discriminator-finetuned-all-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.259361</td>\n",
              "      <td>0.284140</td>\n",
              "      <td>0.894640</td>\n",
              "      <td>0.260727</td>\n",
              "      <td>0.260727</td>\n",
              "      <td>0.014215</td>\n",
              "      <td>0.829310</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.578237</td>\n",
              "      <td>0.687410</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>gpt2-medium-finetuned-ru-gpt-4.csv.gz</td>\n",
              "      <td>0.252007</td>\n",
              "      <td>0.282362</td>\n",
              "      <td>0.850926</td>\n",
              "      <td>0.254037</td>\n",
              "      <td>0.254037</td>\n",
              "      <td>0.085909</td>\n",
              "      <td>0.827929</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.543081</td>\n",
              "      <td>0.585849</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>ru</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>gpt2-medium-finetuned-ru-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.249810</td>\n",
              "      <td>0.271696</td>\n",
              "      <td>0.880788</td>\n",
              "      <td>0.250862</td>\n",
              "      <td>0.250862</td>\n",
              "      <td>0.033993</td>\n",
              "      <td>0.837945</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.564031</td>\n",
              "      <td>0.697837</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>en</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>electra-large-discriminator-finetuned-en-gpt-4.csv.gz</td>\n",
              "      <td>0.243161</td>\n",
              "      <td>0.267113</td>\n",
              "      <td>0.852350</td>\n",
              "      <td>0.244410</td>\n",
              "      <td>0.244410</td>\n",
              "      <td>0.077565</td>\n",
              "      <td>0.839787</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.541324</td>\n",
              "      <td>0.628192</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>ru</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>gpt2-medium-finetuned-ru-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.236609</td>\n",
              "      <td>0.251246</td>\n",
              "      <td>0.875229</td>\n",
              "      <td>0.237071</td>\n",
              "      <td>0.237071</td>\n",
              "      <td>0.038319</td>\n",
              "      <td>0.852911</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.554385</td>\n",
              "      <td>0.672273</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>306</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>electra-large-discriminator-finetuned-ru-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.235810</td>\n",
              "      <td>0.266619</td>\n",
              "      <td>0.806339</td>\n",
              "      <td>0.237856</td>\n",
              "      <td>0.237856</td>\n",
              "      <td>0.157602</td>\n",
              "      <td>0.837216</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.502591</td>\n",
              "      <td>0.527629</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>ru</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>electra-large-discriminator-finetuned-ru-text-davinci-003.csv.gz</td>\n",
              "      <td>0.233410</td>\n",
              "      <td>0.265601</td>\n",
              "      <td>0.795507</td>\n",
              "      <td>0.235637</td>\n",
              "      <td>0.235637</td>\n",
              "      <td>0.177379</td>\n",
              "      <td>0.837254</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.492683</td>\n",
              "      <td>0.493666</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>es</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>gpt2-medium-finetuned-es-gpt-4.csv.gz</td>\n",
              "      <td>0.233255</td>\n",
              "      <td>0.245081</td>\n",
              "      <td>0.878635</td>\n",
              "      <td>0.233555</td>\n",
              "      <td>0.233555</td>\n",
              "      <td>0.032447</td>\n",
              "      <td>0.857592</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.554980</td>\n",
              "      <td>0.644483</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>ru</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>gpt2-medium-finetuned-ru-text-davinci-003.csv.gz</td>\n",
              "      <td>0.223241</td>\n",
              "      <td>0.231421</td>\n",
              "      <td>0.863093</td>\n",
              "      <td>0.223383</td>\n",
              "      <td>0.223383</td>\n",
              "      <td>0.050062</td>\n",
              "      <td>0.866841</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.541549</td>\n",
              "      <td>0.582568</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>ru</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>xlm-roberta-large-finetuned-ru-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.223035</td>\n",
              "      <td>0.224730</td>\n",
              "      <td>0.900575</td>\n",
              "      <td>0.223041</td>\n",
              "      <td>0.223041</td>\n",
              "      <td>0.003090</td>\n",
              "      <td>0.873057</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.561926</td>\n",
              "      <td>0.568527</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>en3</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>gpt2-medium-finetuned-en3-alpaca-lora-30b.csv.gz</td>\n",
              "      <td>0.211332</td>\n",
              "      <td>0.207221</td>\n",
              "      <td>0.890686</td>\n",
              "      <td>0.211367</td>\n",
              "      <td>0.211367</td>\n",
              "      <td>0.012979</td>\n",
              "      <td>0.884953</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.551034</td>\n",
              "      <td>0.629469</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>ru</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mdeberta-v3-base-finetuned-ru-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.209753</td>\n",
              "      <td>0.203637</td>\n",
              "      <td>0.897897</td>\n",
              "      <td>0.209831</td>\n",
              "      <td>0.209831</td>\n",
              "      <td>0.005253</td>\n",
              "      <td>0.887640</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.553554</td>\n",
              "      <td>0.591348</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>gpt2-medium-finetuned-ru-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.205929</td>\n",
              "      <td>0.199784</td>\n",
              "      <td>0.880748</td>\n",
              "      <td>0.206008</td>\n",
              "      <td>0.206008</td>\n",
              "      <td>0.022559</td>\n",
              "      <td>0.889789</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.543826</td>\n",
              "      <td>0.661491</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>mGPT</td>\n",
              "      <td>ru</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mGPT-finetuned-ru-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.203065</td>\n",
              "      <td>0.198572</td>\n",
              "      <td>0.856898</td>\n",
              "      <td>0.203106</td>\n",
              "      <td>0.203106</td>\n",
              "      <td>0.047899</td>\n",
              "      <td>0.889904</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.531099</td>\n",
              "      <td>0.563328</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>en</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>gpt2-medium-finetuned-en-gpt-4.csv.gz</td>\n",
              "      <td>0.197699</td>\n",
              "      <td>0.185879</td>\n",
              "      <td>0.883156</td>\n",
              "      <td>0.197986</td>\n",
              "      <td>0.197986</td>\n",
              "      <td>0.018232</td>\n",
              "      <td>0.899344</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.541212</td>\n",
              "      <td>0.623898</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>316</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>en</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>gpt2-medium-finetuned-en-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.197480</td>\n",
              "      <td>0.184696</td>\n",
              "      <td>0.889629</td>\n",
              "      <td>0.197815</td>\n",
              "      <td>0.197815</td>\n",
              "      <td>0.012052</td>\n",
              "      <td>0.900303</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.543822</td>\n",
              "      <td>0.613916</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>en</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>gpt2-medium-finetuned-en-text-davinci-003.csv.gz</td>\n",
              "      <td>0.193042</td>\n",
              "      <td>0.177290</td>\n",
              "      <td>0.890651</td>\n",
              "      <td>0.193548</td>\n",
              "      <td>0.193548</td>\n",
              "      <td>0.010507</td>\n",
              "      <td>0.905292</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.542101</td>\n",
              "      <td>0.599857</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>en3</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>gpt2-medium-finetuned-en3-text-davinci-003.csv.gz</td>\n",
              "      <td>0.187736</td>\n",
              "      <td>0.168382</td>\n",
              "      <td>0.892424</td>\n",
              "      <td>0.188496</td>\n",
              "      <td>0.188496</td>\n",
              "      <td>0.008344</td>\n",
              "      <td>0.911240</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.540208</td>\n",
              "      <td>0.604055</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>gpt2-medium-finetuned-en3-gpt-4.csv.gz</td>\n",
              "      <td>0.184907</td>\n",
              "      <td>0.163490</td>\n",
              "      <td>0.894758</td>\n",
              "      <td>0.185834</td>\n",
              "      <td>0.185834</td>\n",
              "      <td>0.006180</td>\n",
              "      <td>0.914502</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.539659</td>\n",
              "      <td>0.631604</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320</th>\n",
              "      <td>gpt2-medium</td>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>gpt2-medium-finetuned-en3-gpt-3.5-turbo.csv.gz</td>\n",
              "      <td>0.178861</td>\n",
              "      <td>0.153683</td>\n",
              "      <td>0.893620</td>\n",
              "      <td>0.180133</td>\n",
              "      <td>0.180133</td>\n",
              "      <td>0.006489</td>\n",
              "      <td>0.920872</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.536319</td>\n",
              "      <td>0.599225</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>ru</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>bert-base-multilingual-cased-finetuned-ru-opt-iml-max-1.3b.csv.gz</td>\n",
              "      <td>0.176563</td>\n",
              "      <td>0.149606</td>\n",
              "      <td>0.896816</td>\n",
              "      <td>0.178017</td>\n",
              "      <td>0.178017</td>\n",
              "      <td>0.004017</td>\n",
              "      <td>0.923558</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.536212</td>\n",
              "      <td>0.631759</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>electra-large-discriminator</td>\n",
              "      <td>en3</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>electra-large-discriminator-finetuned-en3-text-davinci-003.csv.gz</td>\n",
              "      <td>0.176044</td>\n",
              "      <td>0.150648</td>\n",
              "      <td>0.877163</td>\n",
              "      <td>0.177334</td>\n",
              "      <td>0.177334</td>\n",
              "      <td>0.018232</td>\n",
              "      <td>0.922560</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.529604</td>\n",
              "      <td>0.618568</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323</th>\n",
              "      <td>blackbox-GPTZero</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>blackbox-GPTZero</td>\n",
              "      <td>0.160532</td>\n",
              "      <td>0.125840</td>\n",
              "      <td>0.863584</td>\n",
              "      <td>0.162895</td>\n",
              "      <td>0.162895</td>\n",
              "      <td>0.022559</td>\n",
              "      <td>0.938255</td>\n",
              "      <td>3236</td>\n",
              "      <td>26059</td>\n",
              "      <td>0.519593</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f9e8c56-b23f-49b2-af38-928c18882334')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1f9e8c56-b23f-49b2-af38-928c18882334 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1f9e8c56-b23f-49b2-af38-928c18882334');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ec2016e9-3647-4cdc-a7ef-f266f39eb081\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ec2016e9-3647-4cdc-a7ef-f266f39eb081')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ec2016e9-3647-4cdc-a7ef-f266f39eb081 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "all = analyze(test_results)\n",
        "all"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#only best of each base model\n",
        "temp = all.drop_duplicates(subset=['Model'], keep='first')\n",
        "temp.loc[(~temp['Model'].str.contains('blackbox-') & ~temp['Model'].str.contains('statistical-')), 'Model'] = 'finetuned-' + temp['Model']\n",
        "#multilingual = ['mdeberta-v3-base', 'xlm-roberta-large', 'mGPT', 'bert-base-multilingual-cased']\n",
        "#temp['Model'] = [f'{x}*' if x in multilingual else x for x in temp['Model']]\n",
        "category = ['S' if 'statistical-' in x else 'B' if 'blackbox-' in x else 'F' for x in temp['Model']]\n",
        "temp.insert(1, 'Category', category)\n",
        "temp['Model'] = temp['Model'].str.replace('statistical-', '', regex=False).str.replace('blackbox-', '', regex=False).str.replace('finetuned-', '', regex=False)\n",
        "names_dic = { 'gptzero': 'GPTZero',\n",
        "              'ZeroGPT': 'ZeroGPT*',\n",
        "              'sapling': 'Sapling',\n",
        "              'gptkit': 'GPTKit',\n",
        "              'likelihood': 'Log-likelihood*',\n",
        "              'rank': 'Rank*',\n",
        "              'log_rank': 'Log-Rank*',\n",
        "              'entropy': 'Entropy*',\n",
        "              'entropy_RF-tuned': 'Entropy + RandomForest*',\n",
        "              'rank_GLTR': 'GLTR Test-2 (Rank)*',\n",
        "              'detectgpt': 'DetectGPT*',\n",
        "              'roberta-large-openai-detector': 'RoBERTa-large-OpenAI-detector',\n",
        "              'gpt2-medium': 'GPT-2 Medium',\n",
        "              'xlm-roberta-large' : 'XLM-RoBERTa-large*',\n",
        "              'bert-base-multilingual-cased': 'BERT-base-multilingual-cased*',\n",
        "              'mdeberta-v3-base': 'MDeBERTa-v3-base*',\n",
        "              'electra-large-discriminator': 'ELECTRA-large',\n",
        "              'mGPT': 'mGPT*'\n",
        "}\n",
        "temp['Model'] = temp['Model'].replace(dict(names_dic), regex=False)\n",
        "temp = temp.rename(columns={'Model': 'Detector Model'})\n",
        "temp = temp.set_index('Detector Model')\n",
        "temp.drop(columns=['Detector', 'Human samples', 'Machine samples'], inplace=True)\n",
        "if 'AUC_preds' in temp.columns:\n",
        "  temp.drop(columns=['threshold', 'AUC_preds'], inplace=True)\n",
        "print(temp.style.format(na_rep=\"N/A\", precision=4).applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True))\n",
        "temp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Tt2Hq_MqK-c1",
        "outputId": "0d8987fc-5001-48b8-a84b-a0ea29882b24"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-85-9ce49e425c53>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  temp['Model'] = temp['Model'].str.replace('statistical-', '', regex=False).str.replace('blackbox-', '', regex=False).str.replace('finetuned-', '', regex=False)\n",
            "<ipython-input-85-9ce49e425c53>:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  temp['Model'] = temp['Model'].replace(dict(names_dic), regex=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{llllrrrrrrrr}\n",
            " & \\bfseries Category & \\bfseries Train Language & \\bfseries Train LLM & \\bfseries Macro avg F1-score & \\bfseries Weighted avg F1-score & \\bfseries Weighted avg Precision & \\bfseries Weighted avg Recall & \\bfseries Accuracy & \\bfseries FPR & \\bfseries FNR & \\bfseries AUC_probs \\\\\n",
            "Detector Model &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
            "\\bfseries MDeBERTa-v3-base* & F & all & all & 0.8480 & 0.9400 & 0.9403 & 0.9396 & 0.9396 & 0.2614 & 0.0354 & 0.9607 \\\\\n",
            "\\bfseries XLM-RoBERTa-large* & F & all & all & 0.8240 & 0.9352 & 0.9357 & 0.9398 & 0.9398 & 0.4178 & 0.0158 & 0.9658 \\\\\n",
            "\\bfseries BERT-base-multilingual-cased* & F & all & all & 0.7563 & 0.9073 & 0.9051 & 0.9104 & 0.9104 & 0.4781 & 0.0414 & 0.9188 \\\\\n",
            "\\bfseries RoBERTa-large-OpenAI-detector & F & all & all & 0.7360 & 0.8933 & 0.8968 & 0.8904 & 0.8904 & 0.4308 & 0.0698 & 0.8645 \\\\\n",
            "\\bfseries mGPT* & F & ru & all & 0.7219 & 0.8976 & 0.8941 & 0.9048 & 0.9048 & 0.5751 & 0.0356 & 0.8780 \\\\\n",
            "\\bfseries GPT-2 Medium & F & all & all & 0.6646 & 0.8668 & 0.8682 & 0.8654 & 0.8654 & 0.5850 & 0.0787 & 0.7899 \\\\\n",
            "\\bfseries ELECTRA-large & F & en & all & 0.5559 & 0.7952 & 0.8310 & 0.7684 & 0.7684 & 0.6530 & 0.1793 & 0.6053 \\\\\n",
            "\\bfseries Entropy + RandomForest* & S & N/A & N/A & 0.4863 & 0.8335 & 0.8050 & 0.8729 & 0.8729 & 0.9756 & 0.0217 & N/A \\\\\n",
            "\\bfseries Rank* & S & N/A & N/A & 0.4708 & 0.8375 & 0.7913 & 0.8895 & 0.8895 & 1.0000 & 0.0000 & N/A \\\\\n",
            "\\bfseries DetectGPT* & S & N/A & N/A & 0.4708 & 0.8375 & 0.7913 & 0.8895 & 0.8895 & 1.0000 & 0.0000 & N/A \\\\\n",
            "\\bfseries Entropy* & S & N/A & N/A & 0.4708 & 0.8375 & 0.7913 & 0.8895 & 0.8895 & 1.0000 & 0.0000 & N/A \\\\\n",
            "\\bfseries Log-likelihood* & S & N/A & N/A & 0.4703 & 0.8368 & 0.7911 & 0.8880 & 0.8880 & 1.0000 & 0.0018 & N/A \\\\\n",
            "\\bfseries Log-Rank* & S & N/A & N/A & 0.4702 & 0.8364 & 0.7911 & 0.8874 & 0.8874 & 1.0000 & 0.0025 & N/A \\\\\n",
            "\\bfseries GLTR Test-2 (Rank)* & S & N/A & N/A & 0.4662 & 0.8282 & 0.7901 & 0.8707 & 0.8707 & 0.9991 & 0.0213 & N/A \\\\\n",
            "\\bfseries ZeroGPT* & B & N/A & N/A & 0.4259 & 0.5559 & 0.8653 & 0.4744 & 0.4744 & 0.1681 & 0.5700 & N/A \\\\\n",
            "\\bfseries GPTZero & B & N/A & N/A & 0.1605 & 0.1258 & 0.8636 & 0.1629 & 0.1629 & 0.0226 & 0.9383 & N/A \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              Category Train Language Train LLM  \\\n",
              "Detector Model                                                    \n",
              "MDeBERTa-v3-base*                    F            all       all   \n",
              "XLM-RoBERTa-large*                   F            all       all   \n",
              "BERT-base-multilingual-cased*        F            all       all   \n",
              "RoBERTa-large-OpenAI-detector        F            all       all   \n",
              "mGPT*                                F             ru       all   \n",
              "GPT-2 Medium                         F            all       all   \n",
              "ELECTRA-large                        F             en       all   \n",
              "Entropy + RandomForest*              S            N/A       N/A   \n",
              "Rank*                                S            N/A       N/A   \n",
              "DetectGPT*                           S            N/A       N/A   \n",
              "Entropy*                             S            N/A       N/A   \n",
              "Log-likelihood*                      S            N/A       N/A   \n",
              "Log-Rank*                            S            N/A       N/A   \n",
              "GLTR Test-2 (Rank)*                  S            N/A       N/A   \n",
              "ZeroGPT*                             B            N/A       N/A   \n",
              "GPTZero                              B            N/A       N/A   \n",
              "\n",
              "                               Macro avg F1-score  Weighted avg F1-score  \\\n",
              "Detector Model                                                             \n",
              "MDeBERTa-v3-base*                        0.848011               0.939955   \n",
              "XLM-RoBERTa-large*                       0.824012               0.935233   \n",
              "BERT-base-multilingual-cased*            0.756344               0.907265   \n",
              "RoBERTa-large-OpenAI-detector            0.736044               0.893278   \n",
              "mGPT*                                    0.721899               0.897587   \n",
              "GPT-2 Medium                             0.664612               0.866769   \n",
              "ELECTRA-large                            0.555896               0.795225   \n",
              "Entropy + RandomForest*                  0.486330               0.833501   \n",
              "Rank*                                    0.470770               0.837535   \n",
              "DetectGPT*                               0.470770               0.837535   \n",
              "Entropy*                                 0.470770               0.837535   \n",
              "Log-likelihood*                          0.470330               0.836752   \n",
              "Log-Rank*                                0.470157               0.836445   \n",
              "GLTR Test-2 (Rank)*                      0.466236               0.828237   \n",
              "ZeroGPT*                                 0.425926               0.555909   \n",
              "GPTZero                                  0.160532               0.125840   \n",
              "\n",
              "                               Weighted avg Precision  Weighted avg Recall  \\\n",
              "Detector Model                                                               \n",
              "MDeBERTa-v3-base*                            0.940286             0.939648   \n",
              "XLM-RoBERTa-large*                           0.935675             0.939819   \n",
              "BERT-base-multilingual-cased*                0.905064             0.910360   \n",
              "RoBERTa-large-OpenAI-detector                0.896759             0.890357   \n",
              "mGPT*                                        0.894128             0.904762   \n",
              "GPT-2 Medium                                 0.868235             0.865370   \n",
              "ELECTRA-large                                0.830961             0.768391   \n",
              "Entropy + RandomForest*                      0.805047             0.872913   \n",
              "Rank*                                        0.791277             0.889537   \n",
              "DetectGPT*                                   0.791277             0.889537   \n",
              "Entropy*                                     0.791277             0.889537   \n",
              "Log-likelihood*                              0.791122             0.887967   \n",
              "Log-Rank*                                    0.791062             0.887353   \n",
              "GLTR Test-2 (Rank)*                          0.790060             0.870729   \n",
              "ZeroGPT*                                     0.865302             0.474415   \n",
              "GPTZero                                      0.863584             0.162895   \n",
              "\n",
              "                               Accuracy       FPR       FNR  AUC_probs  \n",
              "Detector Model                                                          \n",
              "MDeBERTa-v3-base*              0.939648  0.261434  0.035381   0.960706  \n",
              "XLM-RoBERTa-large*             0.939819  0.417800  0.015772   0.965781  \n",
              "BERT-base-multilingual-cased*  0.910360  0.478059  0.041406   0.918754  \n",
              "RoBERTa-large-OpenAI-detector  0.890357  0.430779  0.069765   0.864470  \n",
              "mGPT*                          0.904762  0.575093  0.035650   0.878017  \n",
              "GPT-2 Medium                   0.865370  0.584981  0.078706   0.789929  \n",
              "ELECTRA-large                  0.768391  0.652967  0.179285   0.605279  \n",
              "Entropy + RandomForest*        0.872913  0.975587  0.021720        NaN  \n",
              "Rank*                          0.889537  1.000000  0.000000        NaN  \n",
              "DetectGPT*                     0.889537  1.000000  0.000000        NaN  \n",
              "Entropy*                       0.889537  1.000000  0.000000        NaN  \n",
              "Log-likelihood*                0.887967  1.000000  0.001765        NaN  \n",
              "Log-Rank*                      0.887353  1.000000  0.002456        NaN  \n",
              "GLTR Test-2 (Rank)*            0.870729  0.999073  0.021259        NaN  \n",
              "ZeroGPT*                       0.474415  0.168109  0.569976        NaN  \n",
              "GPTZero                        0.162895  0.022559  0.938255        NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-855e36b1-41af-4f01-9a91-eff09523c16a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Train Language</th>\n",
              "      <th>Train LLM</th>\n",
              "      <th>Macro avg F1-score</th>\n",
              "      <th>Weighted avg F1-score</th>\n",
              "      <th>Weighted avg Precision</th>\n",
              "      <th>Weighted avg Recall</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>FPR</th>\n",
              "      <th>FNR</th>\n",
              "      <th>AUC_probs</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Detector Model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MDeBERTa-v3-base*</th>\n",
              "      <td>F</td>\n",
              "      <td>all</td>\n",
              "      <td>all</td>\n",
              "      <td>0.848011</td>\n",
              "      <td>0.939955</td>\n",
              "      <td>0.940286</td>\n",
              "      <td>0.939648</td>\n",
              "      <td>0.939648</td>\n",
              "      <td>0.261434</td>\n",
              "      <td>0.035381</td>\n",
              "      <td>0.960706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XLM-RoBERTa-large*</th>\n",
              "      <td>F</td>\n",
              "      <td>all</td>\n",
              "      <td>all</td>\n",
              "      <td>0.824012</td>\n",
              "      <td>0.935233</td>\n",
              "      <td>0.935675</td>\n",
              "      <td>0.939819</td>\n",
              "      <td>0.939819</td>\n",
              "      <td>0.417800</td>\n",
              "      <td>0.015772</td>\n",
              "      <td>0.965781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BERT-base-multilingual-cased*</th>\n",
              "      <td>F</td>\n",
              "      <td>all</td>\n",
              "      <td>all</td>\n",
              "      <td>0.756344</td>\n",
              "      <td>0.907265</td>\n",
              "      <td>0.905064</td>\n",
              "      <td>0.910360</td>\n",
              "      <td>0.910360</td>\n",
              "      <td>0.478059</td>\n",
              "      <td>0.041406</td>\n",
              "      <td>0.918754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RoBERTa-large-OpenAI-detector</th>\n",
              "      <td>F</td>\n",
              "      <td>all</td>\n",
              "      <td>all</td>\n",
              "      <td>0.736044</td>\n",
              "      <td>0.893278</td>\n",
              "      <td>0.896759</td>\n",
              "      <td>0.890357</td>\n",
              "      <td>0.890357</td>\n",
              "      <td>0.430779</td>\n",
              "      <td>0.069765</td>\n",
              "      <td>0.864470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT*</th>\n",
              "      <td>F</td>\n",
              "      <td>ru</td>\n",
              "      <td>all</td>\n",
              "      <td>0.721899</td>\n",
              "      <td>0.897587</td>\n",
              "      <td>0.894128</td>\n",
              "      <td>0.904762</td>\n",
              "      <td>0.904762</td>\n",
              "      <td>0.575093</td>\n",
              "      <td>0.035650</td>\n",
              "      <td>0.878017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GPT-2 Medium</th>\n",
              "      <td>F</td>\n",
              "      <td>all</td>\n",
              "      <td>all</td>\n",
              "      <td>0.664612</td>\n",
              "      <td>0.866769</td>\n",
              "      <td>0.868235</td>\n",
              "      <td>0.865370</td>\n",
              "      <td>0.865370</td>\n",
              "      <td>0.584981</td>\n",
              "      <td>0.078706</td>\n",
              "      <td>0.789929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ELECTRA-large</th>\n",
              "      <td>F</td>\n",
              "      <td>en</td>\n",
              "      <td>all</td>\n",
              "      <td>0.555896</td>\n",
              "      <td>0.795225</td>\n",
              "      <td>0.830961</td>\n",
              "      <td>0.768391</td>\n",
              "      <td>0.768391</td>\n",
              "      <td>0.652967</td>\n",
              "      <td>0.179285</td>\n",
              "      <td>0.605279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Entropy + RandomForest*</th>\n",
              "      <td>S</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.486330</td>\n",
              "      <td>0.833501</td>\n",
              "      <td>0.805047</td>\n",
              "      <td>0.872913</td>\n",
              "      <td>0.872913</td>\n",
              "      <td>0.975587</td>\n",
              "      <td>0.021720</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Rank*</th>\n",
              "      <td>S</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.470770</td>\n",
              "      <td>0.837535</td>\n",
              "      <td>0.791277</td>\n",
              "      <td>0.889537</td>\n",
              "      <td>0.889537</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DetectGPT*</th>\n",
              "      <td>S</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.470770</td>\n",
              "      <td>0.837535</td>\n",
              "      <td>0.791277</td>\n",
              "      <td>0.889537</td>\n",
              "      <td>0.889537</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Entropy*</th>\n",
              "      <td>S</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.470770</td>\n",
              "      <td>0.837535</td>\n",
              "      <td>0.791277</td>\n",
              "      <td>0.889537</td>\n",
              "      <td>0.889537</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Log-likelihood*</th>\n",
              "      <td>S</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.470330</td>\n",
              "      <td>0.836752</td>\n",
              "      <td>0.791122</td>\n",
              "      <td>0.887967</td>\n",
              "      <td>0.887967</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.001765</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Log-Rank*</th>\n",
              "      <td>S</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.470157</td>\n",
              "      <td>0.836445</td>\n",
              "      <td>0.791062</td>\n",
              "      <td>0.887353</td>\n",
              "      <td>0.887353</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.002456</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GLTR Test-2 (Rank)*</th>\n",
              "      <td>S</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.466236</td>\n",
              "      <td>0.828237</td>\n",
              "      <td>0.790060</td>\n",
              "      <td>0.870729</td>\n",
              "      <td>0.870729</td>\n",
              "      <td>0.999073</td>\n",
              "      <td>0.021259</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ZeroGPT*</th>\n",
              "      <td>B</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.425926</td>\n",
              "      <td>0.555909</td>\n",
              "      <td>0.865302</td>\n",
              "      <td>0.474415</td>\n",
              "      <td>0.474415</td>\n",
              "      <td>0.168109</td>\n",
              "      <td>0.569976</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GPTZero</th>\n",
              "      <td>B</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.160532</td>\n",
              "      <td>0.125840</td>\n",
              "      <td>0.863584</td>\n",
              "      <td>0.162895</td>\n",
              "      <td>0.162895</td>\n",
              "      <td>0.022559</td>\n",
              "      <td>0.938255</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-855e36b1-41af-4f01-9a91-eff09523c16a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-855e36b1-41af-4f01-9a91-eff09523c16a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-855e36b1-41af-4f01-9a91-eff09523c16a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2c6d1e16-f4a5-4b04-b37c-860204380fe0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2c6d1e16-f4a5-4b04-b37c-860204380fe0')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2c6d1e16-f4a5-4b04-b37c-860204380fe0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RQ1 Zero Shot"
      ],
      "metadata": {
        "id": "ZKekSLLTK2T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#How do zero-shot detectors perform on specified language?\n",
        "def analyze_language(results_list, test_language):\n",
        "  results = pd.DataFrame()\n",
        "  for detector in tqdm(results_list, total=len(results_list)):\n",
        "    for detector_name, detector_data in detector.items():\n",
        "        #skip fine-tuned detectors\n",
        "        if 'statistical-' not in detector_name and 'blackbox-' not in detector_name: continue\n",
        "        temp = detector_data[detector_data.language.str.contains(test_language)]\n",
        "        if len(temp.label.unique()) < 2: continue\n",
        "        if optimize_threshold and 'prediction_probs' in temp.columns:\n",
        "          optimal_threshold = 0.5\n",
        "          labels = [label2id[x] for x in temp['label']]\n",
        "          predictions = [label2id[x] for x in temp['predictions']]\n",
        "          temp = temp.fillna(0.0)\n",
        "          temp['prediction_probs'] = temp['prediction_probs'].astype(float)\n",
        "          temp.loc[temp.predictions == 'human', 'prediction_probs'] = 1 - temp['prediction_probs']\n",
        "          if (optimize_threshold):\n",
        "            fpr, tpr, thresholds = roc_curve(labels, temp['prediction_probs'])\n",
        "            optimal_threshold = thresholds[np.argmax(tpr - fpr)]\n",
        "            #optimal_threshold = thresholds[fpr <= 0.05][-1] #get threshold for 5% FPR\n",
        "          preds = [\"machine\" if ((y > optimal_threshold)) else \"human\" for x,y in zip(temp['predictions'],temp['prediction_probs'])]\n",
        "        else:\n",
        "          preds = temp['predictions']\n",
        "        scores = cr2df(temp['label'], preds, detector_name)\n",
        "        results = pd.concat([results, scores], copy=False, ignore_index=True)\n",
        "  temp = results.sort_values(by=['Macro avg F1-score'], ascending=False).reset_index(drop=True)\n",
        "  return temp"
      ],
      "metadata": {
        "id": "n8P-YWmUi8-l"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "results_all = pd.DataFrame()\n",
        "for test_language in ['ar', 'ca', 'cs', 'de', 'en', 'es', 'nl', 'pt', 'ru', 'uk', 'zh']:\n",
        "    temp = analyze_language(test_results, test_language)\n",
        "    temp = temp[['Model', 'Macro avg F1-score']]\n",
        "    temp = temp.set_index(['Model'])\n",
        "    temp.rename(columns={'Macro avg F1-score': test_language}, inplace=True)\n",
        "    if len(results_all) > 0: temp = temp[test_language]\n",
        "    results_all = pd.concat([results_all, temp], copy=False, axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaK_cx6yhSNS",
        "outputId": "bcfdb623-1a62-4397-cd8d-a9a5178790f5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 324/324 [00:01<00:00, 297.11it/s] \n",
            "100%|██████████| 324/324 [00:01<00:00, 303.03it/s] \n",
            "100%|██████████| 324/324 [00:01<00:00, 305.85it/s] \n",
            "100%|██████████| 324/324 [00:01<00:00, 308.18it/s] \n",
            "100%|██████████| 324/324 [00:00<00:00, 333.91it/s] \n",
            "100%|██████████| 324/324 [00:01<00:00, 246.52it/s] \n",
            "100%|██████████| 324/324 [00:01<00:00, 163.14it/s] \n",
            "100%|██████████| 324/324 [00:01<00:00, 167.59it/s] \n",
            "100%|██████████| 324/324 [00:01<00:00, 287.85it/s] \n",
            "100%|██████████| 324/324 [00:01<00:00, 314.27it/s] \n",
            "100%|██████████| 324/324 [00:01<00:00, 315.37it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13.6 s, sys: 79.8 ms, total: 13.6 s\n",
            "Wall time: 13.8 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rename_models(temp):\n",
        "  temp = temp.reset_index()\n",
        "  temp['Model'] = temp['Model'].str.replace('statistical-', '', regex=False).str.replace('blackbox-', '', regex=False).str.replace('finetuned-', '', regex=False)\n",
        "  names_dic = { 'gptzero': 'GPTZero',\n",
        "              'ZeroGPT': 'ZeroGPT',\n",
        "              'sapling': 'Sapling',\n",
        "              'gptkit': 'GPTKit',\n",
        "              'likelihood': 'Log-likelihood',\n",
        "              'rank': 'Rank',\n",
        "              'log_rank': 'Log-Rank',\n",
        "              'entropy': 'Entropy',\n",
        "              'entropy_RF-tuned': 'Entropy + RandomForest',\n",
        "              'rank_GLTR': 'GLTR Test-2 (Rank)',\n",
        "              'detectgpt': 'DetectGPT',\n",
        "              'roberta-large-openai-detector': 'RoBERTa-large-OpenAI-detector',\n",
        "              'gpt2-medium': 'GPT-2 Medium',\n",
        "              'xlm-roberta-large' : 'XLM-RoBERTa-large',\n",
        "              'bert-base-multilingual-cased': 'BERT-base-multilingual-cased',\n",
        "              'mdeberta-v3-base': 'MDeBERTa-v3-base',\n",
        "              'electra-large-discriminator': 'ELECTRA-large',\n",
        "              'mGPT': 'mGPT'\n",
        "  }\n",
        "  temp['Model'] = temp['Model'].replace(dict(names_dic), regex=False)\n",
        "  return temp.set_index('Model')"
      ],
      "metadata": {
        "id": "iP1_SO8CPd89"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_all = rename_models(results_all)"
      ],
      "metadata": {
        "id": "YhXn9oP3P1g8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multilingual = ['mdeberta-v3-base', 'xlm-roberta-large', 'mGPT', 'bert-base-multilingual-cased']\n",
        "sorted_languages = ['en', 'de', 'nl', 'es', 'pt', 'ca', 'cs', 'ru', 'uk','ar', 'zh']\n",
        "bg_cmap = 'PuBu'\n",
        "bg_vmin = 0.0\n",
        "bg_vmax = 2.0\n",
        "bg_text_color_threshold = 0"
      ],
      "metadata": {
        "id": "W7fqktTkBUvl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = results_all.style.format(na_rep=0, precision=4)\n",
        "display(temp.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LlYZq2UWiQDB",
        "outputId": "32d0c519-f106-4b03-810f-719cf09ae9c3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd002014220>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_768d8_row0_col0 {\n",
              "  background-color: #d2d2e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_768d8_row0_col1, #T_768d8_row0_col2, #T_768d8_row0_col3, #T_768d8_row0_col4, #T_768d8_row0_col5, #T_768d8_row0_col6, #T_768d8_row0_col7, #T_768d8_row1_col0, #T_768d8_row1_col1, #T_768d8_row1_col2, #T_768d8_row1_col3, #T_768d8_row1_col4, #T_768d8_row1_col5, #T_768d8_row1_col6, #T_768d8_row1_col7, #T_768d8_row1_col8, #T_768d8_row1_col9, #T_768d8_row1_col10, #T_768d8_row2_col0, #T_768d8_row2_col1, #T_768d8_row2_col2, #T_768d8_row2_col3, #T_768d8_row2_col4, #T_768d8_row2_col5, #T_768d8_row2_col6, #T_768d8_row2_col7, #T_768d8_row2_col8, #T_768d8_row2_col9, #T_768d8_row2_col10, #T_768d8_row3_col0, #T_768d8_row3_col1, #T_768d8_row3_col2, #T_768d8_row3_col3, #T_768d8_row3_col4, #T_768d8_row3_col5, #T_768d8_row3_col6, #T_768d8_row3_col7, #T_768d8_row3_col8, #T_768d8_row3_col9, #T_768d8_row3_col10, #T_768d8_row4_col0, #T_768d8_row4_col1, #T_768d8_row4_col2, #T_768d8_row4_col3, #T_768d8_row4_col4, #T_768d8_row4_col5, #T_768d8_row4_col6, #T_768d8_row4_col7, #T_768d8_row4_col8, #T_768d8_row4_col9, #T_768d8_row5_col0, #T_768d8_row5_col1, #T_768d8_row5_col2, #T_768d8_row5_col3, #T_768d8_row5_col4, #T_768d8_row5_col5, #T_768d8_row5_col6, #T_768d8_row5_col7, #T_768d8_row5_col8, #T_768d8_row5_col9, #T_768d8_row6_col1, #T_768d8_row6_col2, #T_768d8_row6_col3, #T_768d8_row6_col4, #T_768d8_row6_col5, #T_768d8_row6_col6, #T_768d8_row6_col7, #T_768d8_row6_col8, #T_768d8_row6_col9, #T_768d8_row7_col5 {\n",
              "  background-color: #d3d4e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_768d8_row0_col8 {\n",
              "  background-color: #cdd0e5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_768d8_row0_col9 {\n",
              "  background-color: #d0d1e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_768d8_row0_col10, #T_768d8_row4_col10, #T_768d8_row5_col10, #T_768d8_row6_col10, #T_768d8_row7_col6 {\n",
              "  background-color: #d4d4e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_768d8_row6_col0, #T_768d8_row7_col8, #T_768d8_row7_col9 {\n",
              "  background-color: #d9d8ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_768d8_row7_col0 {\n",
              "  background-color: #e6e2ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_768d8_row7_col1 {\n",
              "  background-color: #d2d3e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_768d8_row7_col2, #T_768d8_row7_col7 {\n",
              "  background-color: #d6d6e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_768d8_row7_col3 {\n",
              "  background-color: #dbdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_768d8_row7_col4 {\n",
              "  background-color: #c0c9e2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_768d8_row7_col10 {\n",
              "  background-color: #f5eef6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_768d8_row8_col0, #T_768d8_row8_col1, #T_768d8_row8_col2, #T_768d8_row8_col6, #T_768d8_row8_col8 {\n",
              "  background-color: #f7f0f7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_768d8_row8_col3, #T_768d8_row8_col5, #T_768d8_row8_col7, #T_768d8_row8_col9 {\n",
              "  background-color: #f8f1f8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_768d8_row8_col4 {\n",
              "  background-color: #c5cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_768d8_row8_col10 {\n",
              "  background-color: #f6eff7;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_768d8\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_768d8_level0_col0\" class=\"col_heading level0 col0\" >ar</th>\n",
              "      <th id=\"T_768d8_level0_col1\" class=\"col_heading level0 col1\" >ca</th>\n",
              "      <th id=\"T_768d8_level0_col2\" class=\"col_heading level0 col2\" >cs</th>\n",
              "      <th id=\"T_768d8_level0_col3\" class=\"col_heading level0 col3\" >de</th>\n",
              "      <th id=\"T_768d8_level0_col4\" class=\"col_heading level0 col4\" >en</th>\n",
              "      <th id=\"T_768d8_level0_col5\" class=\"col_heading level0 col5\" >es</th>\n",
              "      <th id=\"T_768d8_level0_col6\" class=\"col_heading level0 col6\" >nl</th>\n",
              "      <th id=\"T_768d8_level0_col7\" class=\"col_heading level0 col7\" >pt</th>\n",
              "      <th id=\"T_768d8_level0_col8\" class=\"col_heading level0 col8\" >ru</th>\n",
              "      <th id=\"T_768d8_level0_col9\" class=\"col_heading level0 col9\" >uk</th>\n",
              "      <th id=\"T_768d8_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Model</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "      <th class=\"blank col7\" >&nbsp;</th>\n",
              "      <th class=\"blank col8\" >&nbsp;</th>\n",
              "      <th class=\"blank col9\" >&nbsp;</th>\n",
              "      <th class=\"blank col10\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_768d8_level0_row0\" class=\"row_heading level0 row0\" >Entropy + RandomForest</th>\n",
              "      <td id=\"T_768d8_row0_col0\" class=\"data row0 col0\" >0.4860</td>\n",
              "      <td id=\"T_768d8_row0_col1\" class=\"data row0 col1\" >0.4721</td>\n",
              "      <td id=\"T_768d8_row0_col2\" class=\"data row0 col2\" >0.4732</td>\n",
              "      <td id=\"T_768d8_row0_col3\" class=\"data row0 col3\" >0.4729</td>\n",
              "      <td id=\"T_768d8_row0_col4\" class=\"data row0 col4\" >0.4703</td>\n",
              "      <td id=\"T_768d8_row0_col5\" class=\"data row0 col5\" >0.4697</td>\n",
              "      <td id=\"T_768d8_row0_col6\" class=\"data row0 col6\" >0.4692</td>\n",
              "      <td id=\"T_768d8_row0_col7\" class=\"data row0 col7\" >0.4702</td>\n",
              "      <td id=\"T_768d8_row0_col8\" class=\"data row0 col8\" >0.5202</td>\n",
              "      <td id=\"T_768d8_row0_col9\" class=\"data row0 col9\" >0.5040</td>\n",
              "      <td id=\"T_768d8_row0_col10\" class=\"data row0 col10\" >0.4663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_768d8_level0_row1\" class=\"row_heading level0 row1\" >Rank</th>\n",
              "      <td id=\"T_768d8_row1_col0\" class=\"data row1 col0\" >0.4704</td>\n",
              "      <td id=\"T_768d8_row1_col1\" class=\"data row1 col1\" >0.4705</td>\n",
              "      <td id=\"T_768d8_row1_col2\" class=\"data row1 col2\" >0.4705</td>\n",
              "      <td id=\"T_768d8_row1_col3\" class=\"data row1 col3\" >0.4712</td>\n",
              "      <td id=\"T_768d8_row1_col4\" class=\"data row1 col4\" >0.4706</td>\n",
              "      <td id=\"T_768d8_row1_col5\" class=\"data row1 col5\" >0.4720</td>\n",
              "      <td id=\"T_768d8_row1_col6\" class=\"data row1 col6\" >0.4706</td>\n",
              "      <td id=\"T_768d8_row1_col7\" class=\"data row1 col7\" >0.4716</td>\n",
              "      <td id=\"T_768d8_row1_col8\" class=\"data row1 col8\" >0.4702</td>\n",
              "      <td id=\"T_768d8_row1_col9\" class=\"data row1 col9\" >0.4704</td>\n",
              "      <td id=\"T_768d8_row1_col10\" class=\"data row1 col10\" >0.4704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_768d8_level0_row2\" class=\"row_heading level0 row2\" >Entropy</th>\n",
              "      <td id=\"T_768d8_row2_col0\" class=\"data row2 col0\" >0.4704</td>\n",
              "      <td id=\"T_768d8_row2_col1\" class=\"data row2 col1\" >0.4705</td>\n",
              "      <td id=\"T_768d8_row2_col2\" class=\"data row2 col2\" >0.4705</td>\n",
              "      <td id=\"T_768d8_row2_col3\" class=\"data row2 col3\" >0.4712</td>\n",
              "      <td id=\"T_768d8_row2_col4\" class=\"data row2 col4\" >0.4706</td>\n",
              "      <td id=\"T_768d8_row2_col5\" class=\"data row2 col5\" >0.4720</td>\n",
              "      <td id=\"T_768d8_row2_col6\" class=\"data row2 col6\" >0.4706</td>\n",
              "      <td id=\"T_768d8_row2_col7\" class=\"data row2 col7\" >0.4716</td>\n",
              "      <td id=\"T_768d8_row2_col8\" class=\"data row2 col8\" >0.4702</td>\n",
              "      <td id=\"T_768d8_row2_col9\" class=\"data row2 col9\" >0.4704</td>\n",
              "      <td id=\"T_768d8_row2_col10\" class=\"data row2 col10\" >0.4704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_768d8_level0_row3\" class=\"row_heading level0 row3\" >DetectGPT</th>\n",
              "      <td id=\"T_768d8_row3_col0\" class=\"data row3 col0\" >0.4704</td>\n",
              "      <td id=\"T_768d8_row3_col1\" class=\"data row3 col1\" >0.4705</td>\n",
              "      <td id=\"T_768d8_row3_col2\" class=\"data row3 col2\" >0.4705</td>\n",
              "      <td id=\"T_768d8_row3_col3\" class=\"data row3 col3\" >0.4712</td>\n",
              "      <td id=\"T_768d8_row3_col4\" class=\"data row3 col4\" >0.4706</td>\n",
              "      <td id=\"T_768d8_row3_col5\" class=\"data row3 col5\" >0.4720</td>\n",
              "      <td id=\"T_768d8_row3_col6\" class=\"data row3 col6\" >0.4706</td>\n",
              "      <td id=\"T_768d8_row3_col7\" class=\"data row3 col7\" >0.4716</td>\n",
              "      <td id=\"T_768d8_row3_col8\" class=\"data row3 col8\" >0.4702</td>\n",
              "      <td id=\"T_768d8_row3_col9\" class=\"data row3 col9\" >0.4704</td>\n",
              "      <td id=\"T_768d8_row3_col10\" class=\"data row3 col10\" >0.4704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_768d8_level0_row4\" class=\"row_heading level0 row4\" >Log-Rank</th>\n",
              "      <td id=\"T_768d8_row4_col0\" class=\"data row4 col0\" >0.4702</td>\n",
              "      <td id=\"T_768d8_row4_col1\" class=\"data row4 col1\" >0.4705</td>\n",
              "      <td id=\"T_768d8_row4_col2\" class=\"data row4 col2\" >0.4705</td>\n",
              "      <td id=\"T_768d8_row4_col3\" class=\"data row4 col3\" >0.4712</td>\n",
              "      <td id=\"T_768d8_row4_col4\" class=\"data row4 col4\" >0.4706</td>\n",
              "      <td id=\"T_768d8_row4_col5\" class=\"data row4 col5\" >0.4720</td>\n",
              "      <td id=\"T_768d8_row4_col6\" class=\"data row4 col6\" >0.4706</td>\n",
              "      <td id=\"T_768d8_row4_col7\" class=\"data row4 col7\" >0.4716</td>\n",
              "      <td id=\"T_768d8_row4_col8\" class=\"data row4 col8\" >0.4698</td>\n",
              "      <td id=\"T_768d8_row4_col9\" class=\"data row4 col9\" >0.4703</td>\n",
              "      <td id=\"T_768d8_row4_col10\" class=\"data row4 col10\" >0.4644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_768d8_level0_row5\" class=\"row_heading level0 row5\" >Log-likelihood</th>\n",
              "      <td id=\"T_768d8_row5_col0\" class=\"data row5 col0\" >0.4702</td>\n",
              "      <td id=\"T_768d8_row5_col1\" class=\"data row5 col1\" >0.4705</td>\n",
              "      <td id=\"T_768d8_row5_col2\" class=\"data row5 col2\" >0.4705</td>\n",
              "      <td id=\"T_768d8_row5_col3\" class=\"data row5 col3\" >0.4712</td>\n",
              "      <td id=\"T_768d8_row5_col4\" class=\"data row5 col4\" >0.4706</td>\n",
              "      <td id=\"T_768d8_row5_col5\" class=\"data row5 col5\" >0.4720</td>\n",
              "      <td id=\"T_768d8_row5_col6\" class=\"data row5 col6\" >0.4706</td>\n",
              "      <td id=\"T_768d8_row5_col7\" class=\"data row5 col7\" >0.4716</td>\n",
              "      <td id=\"T_768d8_row5_col8\" class=\"data row5 col8\" >0.4699</td>\n",
              "      <td id=\"T_768d8_row5_col9\" class=\"data row5 col9\" >0.4703</td>\n",
              "      <td id=\"T_768d8_row5_col10\" class=\"data row5 col10\" >0.4662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_768d8_level0_row6\" class=\"row_heading level0 row6\" >GLTR Test-2 (Rank)</th>\n",
              "      <td id=\"T_768d8_row6_col0\" class=\"data row6 col0\" >0.4239</td>\n",
              "      <td id=\"T_768d8_row6_col1\" class=\"data row6 col1\" >0.4702</td>\n",
              "      <td id=\"T_768d8_row6_col2\" class=\"data row6 col2\" >0.4700</td>\n",
              "      <td id=\"T_768d8_row6_col3\" class=\"data row6 col3\" >0.4701</td>\n",
              "      <td id=\"T_768d8_row6_col4\" class=\"data row6 col4\" >0.4706</td>\n",
              "      <td id=\"T_768d8_row6_col5\" class=\"data row6 col5\" >0.4720</td>\n",
              "      <td id=\"T_768d8_row6_col6\" class=\"data row6 col6\" >0.4703</td>\n",
              "      <td id=\"T_768d8_row6_col7\" class=\"data row6 col7\" >0.4711</td>\n",
              "      <td id=\"T_768d8_row6_col8\" class=\"data row6 col8\" >0.4697</td>\n",
              "      <td id=\"T_768d8_row6_col9\" class=\"data row6 col9\" >0.4697</td>\n",
              "      <td id=\"T_768d8_row6_col10\" class=\"data row6 col10\" >0.4653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_768d8_level0_row7\" class=\"row_heading level0 row7\" >ZeroGPT</th>\n",
              "      <td id=\"T_768d8_row7_col0\" class=\"data row7 col0\" >0.3055</td>\n",
              "      <td id=\"T_768d8_row7_col1\" class=\"data row7 col1\" >0.4807</td>\n",
              "      <td id=\"T_768d8_row7_col2\" class=\"data row7 col2\" >0.4509</td>\n",
              "      <td id=\"T_768d8_row7_col3\" class=\"data row7 col3\" >0.4019</td>\n",
              "      <td id=\"T_768d8_row7_col4\" class=\"data row7 col4\" >0.5979</td>\n",
              "      <td id=\"T_768d8_row7_col5\" class=\"data row7 col5\" >0.4750</td>\n",
              "      <td id=\"T_768d8_row7_col6\" class=\"data row7 col6\" >0.4625</td>\n",
              "      <td id=\"T_768d8_row7_col7\" class=\"data row7 col7\" >0.4510</td>\n",
              "      <td id=\"T_768d8_row7_col8\" class=\"data row7 col8\" >0.4194</td>\n",
              "      <td id=\"T_768d8_row7_col9\" class=\"data row7 col9\" >0.4267</td>\n",
              "      <td id=\"T_768d8_row7_col10\" class=\"data row7 col10\" >0.1398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_768d8_level0_row8\" class=\"row_heading level0 row8\" >GPTZero</th>\n",
              "      <td id=\"T_768d8_row8_col0\" class=\"data row8 col0\" >0.1128</td>\n",
              "      <td id=\"T_768d8_row8_col1\" class=\"data row8 col1\" >0.1057</td>\n",
              "      <td id=\"T_768d8_row8_col2\" class=\"data row8 col2\" >0.1040</td>\n",
              "      <td id=\"T_768d8_row8_col3\" class=\"data row8 col3\" >0.0999</td>\n",
              "      <td id=\"T_768d8_row8_col4\" class=\"data row8 col4\" >0.5626</td>\n",
              "      <td id=\"T_768d8_row8_col5\" class=\"data row8 col5\" >0.0973</td>\n",
              "      <td id=\"T_768d8_row8_col6\" class=\"data row8 col6\" >0.1044</td>\n",
              "      <td id=\"T_768d8_row8_col7\" class=\"data row8 col7\" >0.1010</td>\n",
              "      <td id=\"T_768d8_row8_col8\" class=\"data row8 col8\" >0.1042</td>\n",
              "      <td id=\"T_768d8_row8_col9\" class=\"data row8 col9\" >0.1014</td>\n",
              "      <td id=\"T_768d8_row8_col10\" class=\"data row8 col10\" >0.1189</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(temp.highlight_max(props='font-weight: bold;', axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True).replace('_','\\_'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PPFdHQPmIES",
        "outputId": "f0fb848a-2535-4dad-e0b6-463e5357044d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{lrrrrrrrrrrr}\n",
            " & \\bfseries ar & \\bfseries ca & \\bfseries cs & \\bfseries de & \\bfseries en & \\bfseries es & \\bfseries nl & \\bfseries pt & \\bfseries ru & \\bfseries uk & \\bfseries zh \\\\\n",
            "Model &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
            "\\bfseries Entropy + RandomForest & {\\cellcolor[HTML]{D2D2E7}} \\color[HTML]{000000} \\bfseries 0.4860 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4721 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} \\bfseries 0.4732 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} \\bfseries 0.4729 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4703 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4697 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4692 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4702 & {\\cellcolor[HTML]{CDD0E5}} \\color[HTML]{000000} \\bfseries 0.5202 & {\\cellcolor[HTML]{D0D1E6}} \\color[HTML]{000000} \\bfseries 0.5040 & {\\cellcolor[HTML]{D4D4E8}} \\color[HTML]{000000} 0.4663 \\\\\n",
            "\\bfseries Rank & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4704 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4705 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4705 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4712 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4706 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4720 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} \\bfseries 0.4706 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} \\bfseries 0.4716 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4702 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4704 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} \\bfseries 0.4704 \\\\\n",
            "\\bfseries Entropy & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4704 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4705 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4705 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4712 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4706 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4720 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} \\bfseries 0.4706 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} \\bfseries 0.4716 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4702 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4704 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} \\bfseries 0.4704 \\\\\n",
            "\\bfseries DetectGPT & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4704 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4705 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4705 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4712 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4706 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4720 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} \\bfseries 0.4706 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} \\bfseries 0.4716 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4702 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4704 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} \\bfseries 0.4704 \\\\\n",
            "\\bfseries Log-Rank & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4702 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4705 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4705 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4712 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4706 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4720 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} \\bfseries 0.4706 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} \\bfseries 0.4716 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4698 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4703 & {\\cellcolor[HTML]{D4D4E8}} \\color[HTML]{000000} 0.4644 \\\\\n",
            "\\bfseries Log-likelihood & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4702 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4705 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4705 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4712 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4706 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4720 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} \\bfseries 0.4706 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} \\bfseries 0.4716 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4699 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4703 & {\\cellcolor[HTML]{D4D4E8}} \\color[HTML]{000000} 0.4662 \\\\\n",
            "\\bfseries GLTR Test-2 (Rank) & {\\cellcolor[HTML]{D9D8EA}} \\color[HTML]{000000} 0.4239 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4702 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4700 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4701 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4706 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4720 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4703 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4711 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4697 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4697 & {\\cellcolor[HTML]{D4D4E8}} \\color[HTML]{000000} 0.4653 \\\\\n",
            "\\bfseries ZeroGPT & {\\cellcolor[HTML]{E6E2EF}} \\color[HTML]{000000} 0.3055 & {\\cellcolor[HTML]{D2D3E7}} \\color[HTML]{000000} \\bfseries 0.4807 & {\\cellcolor[HTML]{D6D6E9}} \\color[HTML]{000000} 0.4509 & {\\cellcolor[HTML]{DBDAEB}} \\color[HTML]{000000} 0.4019 & {\\cellcolor[HTML]{C0C9E2}} \\color[HTML]{000000} \\bfseries 0.5979 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} \\bfseries 0.4750 & {\\cellcolor[HTML]{D4D4E8}} \\color[HTML]{000000} 0.4625 & {\\cellcolor[HTML]{D6D6E9}} \\color[HTML]{000000} 0.4510 & {\\cellcolor[HTML]{D9D8EA}} \\color[HTML]{000000} 0.4194 & {\\cellcolor[HTML]{D9D8EA}} \\color[HTML]{000000} 0.4267 & {\\cellcolor[HTML]{F5EEF6}} \\color[HTML]{000000} 0.1398 \\\\\n",
            "\\bfseries GPTZero & {\\cellcolor[HTML]{F7F0F7}} \\color[HTML]{000000} 0.1128 & {\\cellcolor[HTML]{F7F0F7}} \\color[HTML]{000000} 0.1057 & {\\cellcolor[HTML]{F7F0F7}} \\color[HTML]{000000} 0.1040 & {\\cellcolor[HTML]{F8F1F8}} \\color[HTML]{000000} 0.0999 & {\\cellcolor[HTML]{C5CCE3}} \\color[HTML]{000000} 0.5626 & {\\cellcolor[HTML]{F8F1F8}} \\color[HTML]{000000} 0.0973 & {\\cellcolor[HTML]{F7F0F7}} \\color[HTML]{000000} 0.1044 & {\\cellcolor[HTML]{F8F1F8}} \\color[HTML]{000000} 0.1010 & {\\cellcolor[HTML]{F7F0F7}} \\color[HTML]{000000} 0.1042 & {\\cellcolor[HTML]{F8F1F8}} \\color[HTML]{000000} 0.1014 & {\\cellcolor[HTML]{F6EFF7}} \\color[HTML]{000000} 0.1189 \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RQ2 Monolingual Generalization"
      ],
      "metadata": {
        "id": "iouhTOsXAECh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#How do detectors perform on individual language and LLM when trained on specified language?\n",
        "def analyze_language_for_train_language_per_llm(results_list, train_language, test_language):\n",
        "  results = pd.DataFrame()\n",
        "  for detector in tqdm(results_list, total=len(results_list)):\n",
        "    for detector_name, detector_data in detector.items():\n",
        "      for llm in ['text-davinci-003', 'gpt-3.5-turbo', 'gpt-4', 'alpaca-lora-30b', 'vicuna-13b', 'llama-65b', 'opt-66b', 'opt-iml-max-1.3b']:\n",
        "        if f'-{train_language}-' not in detector_name: continue\n",
        "        if f'-{llm}.' not in detector_name: continue\n",
        "        temp = detector_data[(detector_data.language.str.contains(test_language)) & (detector_data.multi_label.str.contains(llm) | detector_data.multi_label.str.contains('human'))]\n",
        "        if len(temp.label.unique()) < 2: continue\n",
        "        if optimize_threshold and 'prediction_probs' in temp.columns:\n",
        "          optimal_threshold = 0.5\n",
        "          labels = [label2id[x] for x in temp['label']]\n",
        "          predictions = [label2id[x] for x in temp['predictions']]\n",
        "          temp = temp.fillna(0.0)\n",
        "          temp['prediction_probs'] = temp['prediction_probs'].astype(float)\n",
        "          temp.loc[temp.predictions == 'human', 'prediction_probs'] = 1 - temp['prediction_probs']\n",
        "          if (optimize_threshold):\n",
        "            fpr, tpr, thresholds = roc_curve(labels, temp['prediction_probs'])\n",
        "            optimal_threshold = thresholds[np.argmax(tpr - fpr)]\n",
        "            #optimal_threshold = thresholds[fpr <= 0.05][-1] #get threshold for 5% FPR\n",
        "          preds = [\"machine\" if ((y > optimal_threshold)) else \"human\" for x,y in zip(temp['predictions'],temp['prediction_probs'])]\n",
        "        else:\n",
        "          preds = temp['predictions']\n",
        "        scores = cr2df(temp['label'], preds, detector_name)\n",
        "        results = pd.concat([results, scores], copy=False, ignore_index=True)\n",
        "  temp = results.sort_values(by=['Macro avg F1-score'], ascending=False).reset_index(drop=True)\n",
        "  return temp"
      ],
      "metadata": {
        "id": "SSvfeax0BHsq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#How do detectors perform on individual language (only corresponding LLM machine data) when trained on specified language and LLM?\n",
        "results_all = pd.DataFrame()\n",
        "for train_language in ['en', 'es', 'ru']:\n",
        "  results = pd.DataFrame()\n",
        "  for test_language in ['ar', 'ca', 'cs', 'de', 'en', 'es', 'nl', 'pt', 'ru', 'uk', 'zh']:\n",
        "    temp = analyze_language_for_train_language_per_llm(test_results, train_language, test_language)\n",
        "    temp = temp[~temp['Train LLM'].str.contains('all')]\n",
        "    temp = temp[['Train Language', 'Train LLM', 'Model', 'Macro avg F1-score']]\n",
        "    temp = temp.sort_values(by=['Train Language', 'Train LLM', 'Model'])\n",
        "    temp = temp.set_index(['Train Language', 'Train LLM', 'Model'])\n",
        "    temp.rename(columns={'Macro avg F1-score': test_language}, inplace=True)\n",
        "    if len(results) > 0: temp = temp[test_language]\n",
        "    results = pd.concat([results, temp], copy=False, axis=1)\n",
        "  results_all = pd.concat([results_all, results], copy=False)"
      ],
      "metadata": {
        "id": "Te0M1cB2ASGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09cb7966-6b6e-41c4-af39-6778dbbed374"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 324/324 [00:03<00:00, 87.93it/s]\n",
            "100%|██████████| 324/324 [00:04<00:00, 79.08it/s]\n",
            "100%|██████████| 324/324 [00:05<00:00, 62.25it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 90.94it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 91.02it/s]\n",
            "100%|██████████| 324/324 [00:05<00:00, 58.38it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 88.12it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 92.48it/s]\n",
            "100%|██████████| 324/324 [00:04<00:00, 80.17it/s]\n",
            "100%|██████████| 324/324 [00:05<00:00, 61.72it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 90.96it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 90.22it/s] \n",
            "100%|██████████| 324/324 [00:05<00:00, 57.54it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 87.47it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 90.56it/s] \n",
            "100%|██████████| 324/324 [00:04<00:00, 80.64it/s]\n",
            "100%|██████████| 324/324 [00:05<00:00, 62.67it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 90.85it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 91.02it/s] \n",
            "100%|██████████| 324/324 [00:05<00:00, 57.16it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 89.60it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 90.80it/s] \n",
            "100%|██████████| 324/324 [00:04<00:00, 76.71it/s]\n",
            "100%|██████████| 324/324 [00:05<00:00, 63.74it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 90.80it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 92.27it/s] \n",
            "100%|██████████| 324/324 [00:05<00:00, 59.14it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 88.58it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 91.57it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 81.60it/s]\n",
            "100%|██████████| 324/324 [00:05<00:00, 60.49it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 90.71it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 91.05it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 14s, sys: 921 ms, total: 2min 15s\n",
            "Wall time: 2min 17s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#results_all.style.background_gradient(axis=1).format(na_rep=0, precision=4)\n",
        "temp = results_all.style.format(na_rep=0, precision=4)\n",
        "display(temp.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None))"
      ],
      "metadata": {
        "id": "vs1gqhFhBc_3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5b86a1a0-3b73-4896-8a01-810ac313c6e7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001e23e80>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_d5c64_row0_col0, #T_d5c64_row2_col9, #T_d5c64_row12_col5, #T_d5c64_row23_col3, #T_d5c64_row23_col6, #T_d5c64_row34_col10, #T_d5c64_row35_col9, #T_d5c64_row54_col3, #T_d5c64_row64_col8, #T_d5c64_row79_col10, #T_d5c64_row90_col10, #T_d5c64_row110_col0, #T_d5c64_row114_col7, #T_d5c64_row115_col6, #T_d5c64_row121_col0, #T_d5c64_row136_col0, #T_d5c64_row137_col7, #T_d5c64_row147_col6, #T_d5c64_row155_col4 {\n",
              "  background-color: #cacee5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row0_col1, #T_d5c64_row3_col9, #T_d5c64_row8_col5, #T_d5c64_row14_col5, #T_d5c64_row18_col4, #T_d5c64_row46_col2, #T_d5c64_row48_col7, #T_d5c64_row58_col3, #T_d5c64_row60_col0, #T_d5c64_row60_col5, #T_d5c64_row62_col9, #T_d5c64_row77_col8, #T_d5c64_row84_col10, #T_d5c64_row105_col8, #T_d5c64_row111_col6, #T_d5c64_row114_col9, #T_d5c64_row116_col10, #T_d5c64_row133_col5, #T_d5c64_row147_col10 {\n",
              "  background-color: #97b7d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row0_col2, #T_d5c64_row0_col6, #T_d5c64_row4_col1, #T_d5c64_row4_col7, #T_d5c64_row7_col5, #T_d5c64_row20_col5, #T_d5c64_row28_col4, #T_d5c64_row32_col4, #T_d5c64_row41_col1, #T_d5c64_row42_col2, #T_d5c64_row42_col3, #T_d5c64_row46_col3, #T_d5c64_row49_col7, #T_d5c64_row69_col3, #T_d5c64_row74_col8, #T_d5c64_row88_col6, #T_d5c64_row89_col2, #T_d5c64_row89_col6, #T_d5c64_row97_col2, #T_d5c64_row101_col8, #T_d5c64_row109_col9, #T_d5c64_row116_col4, #T_d5c64_row118_col10, #T_d5c64_row119_col3, #T_d5c64_row129_col9, #T_d5c64_row131_col9, #T_d5c64_row137_col3, #T_d5c64_row146_col1, #T_d5c64_row146_col10, #T_d5c64_row148_col9, #T_d5c64_row156_col8, #T_d5c64_row164_col3, #T_d5c64_row167_col4 {\n",
              "  background-color: #91b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row0_col3, #T_d5c64_row4_col6, #T_d5c64_row7_col9, #T_d5c64_row7_col10, #T_d5c64_row18_col3, #T_d5c64_row20_col3, #T_d5c64_row31_col4, #T_d5c64_row45_col3, #T_d5c64_row48_col3, #T_d5c64_row48_col6, #T_d5c64_row56_col1, #T_d5c64_row56_col6, #T_d5c64_row56_col7, #T_d5c64_row58_col5, #T_d5c64_row63_col8, #T_d5c64_row65_col5, #T_d5c64_row66_col0, #T_d5c64_row66_col6, #T_d5c64_row66_col8, #T_d5c64_row70_col8, #T_d5c64_row82_col1, #T_d5c64_row92_col3, #T_d5c64_row94_col8, #T_d5c64_row99_col7, #T_d5c64_row104_col8, #T_d5c64_row109_col7, #T_d5c64_row111_col7, #T_d5c64_row118_col1, #T_d5c64_row129_col0, #T_d5c64_row129_col1, #T_d5c64_row130_col9, #T_d5c64_row164_col9 {\n",
              "  background-color: #89b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row0_col4, #T_d5c64_row38_col4, #T_d5c64_row39_col3, #T_d5c64_row39_col9, #T_d5c64_row62_col6, #T_d5c64_row66_col7, #T_d5c64_row67_col3, #T_d5c64_row67_col5, #T_d5c64_row69_col1, #T_d5c64_row71_col5, #T_d5c64_row74_col2, #T_d5c64_row76_col1, #T_d5c64_row81_col3, #T_d5c64_row83_col3, #T_d5c64_row91_col0, #T_d5c64_row96_col5, #T_d5c64_row99_col5, #T_d5c64_row119_col8, #T_d5c64_row122_col8, #T_d5c64_row123_col4, #T_d5c64_row129_col8, #T_d5c64_row132_col1, #T_d5c64_row132_col8, #T_d5c64_row146_col0, #T_d5c64_row147_col0, #T_d5c64_row158_col9, #T_d5c64_row164_col8, #T_d5c64_row165_col8, #T_d5c64_row165_col9 {\n",
              "  background-color: #7dacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row0_col5, #T_d5c64_row4_col9, #T_d5c64_row47_col6, #T_d5c64_row56_col4, #T_d5c64_row61_col10, #T_d5c64_row80_col3, #T_d5c64_row82_col2, #T_d5c64_row84_col9, #T_d5c64_row97_col8, #T_d5c64_row100_col3, #T_d5c64_row100_col8, #T_d5c64_row105_col4, #T_d5c64_row126_col3, #T_d5c64_row126_col6, #T_d5c64_row128_col8, #T_d5c64_row153_col3, #T_d5c64_row167_col1 {\n",
              "  background-color: #a5bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row0_col7, #T_d5c64_row0_col9, #T_d5c64_row3_col1, #T_d5c64_row11_col3, #T_d5c64_row52_col7, #T_d5c64_row56_col2, #T_d5c64_row58_col1, #T_d5c64_row71_col1, #T_d5c64_row75_col2, #T_d5c64_row90_col3, #T_d5c64_row90_col5, #T_d5c64_row90_col8, #T_d5c64_row102_col5, #T_d5c64_row112_col2, #T_d5c64_row116_col1, #T_d5c64_row121_col9, #T_d5c64_row126_col10, #T_d5c64_row154_col3 {\n",
              "  background-color: #9ab8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row0_col8, #T_d5c64_row8_col1, #T_d5c64_row14_col0, #T_d5c64_row14_col10, #T_d5c64_row17_col3, #T_d5c64_row18_col8, #T_d5c64_row30_col4, #T_d5c64_row41_col3, #T_d5c64_row42_col7, #T_d5c64_row42_col10, #T_d5c64_row45_col7, #T_d5c64_row55_col6, #T_d5c64_row59_col8, #T_d5c64_row61_col3, #T_d5c64_row66_col4, #T_d5c64_row68_col7, #T_d5c64_row90_col9, #T_d5c64_row92_col2, #T_d5c64_row94_col6, #T_d5c64_row98_col10, #T_d5c64_row109_col2, #T_d5c64_row119_col7, #T_d5c64_row123_col1, #T_d5c64_row148_col8, #T_d5c64_row150_col6, #T_d5c64_row156_col9, #T_d5c64_row161_col9, #T_d5c64_row167_col7 {\n",
              "  background-color: #94b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row0_col10, #T_d5c64_row12_col3, #T_d5c64_row34_col2, #T_d5c64_row35_col8, #T_d5c64_row36_col1, #T_d5c64_row99_col8, #T_d5c64_row106_col9, #T_d5c64_row113_col8, #T_d5c64_row117_col10, #T_d5c64_row141_col0, #T_d5c64_row146_col3, #T_d5c64_row146_col5, #T_d5c64_row148_col2, #T_d5c64_row148_col5, #T_d5c64_row162_col8, #T_d5c64_row162_col10, #T_d5c64_row164_col7 {\n",
              "  background-color: #c8cde4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row1_col0, #T_d5c64_row24_col5, #T_d5c64_row33_col1, #T_d5c64_row37_col2, #T_d5c64_row37_col3, #T_d5c64_row37_col9, #T_d5c64_row38_col5, #T_d5c64_row45_col10, #T_d5c64_row47_col2, #T_d5c64_row54_col9, #T_d5c64_row57_col6, #T_d5c64_row64_col0, #T_d5c64_row107_col9, #T_d5c64_row115_col4, #T_d5c64_row133_col1, #T_d5c64_row149_col10, #T_d5c64_row162_col9 {\n",
              "  background-color: #cccfe5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row1_col1, #T_d5c64_row19_col5, #T_d5c64_row19_col10, #T_d5c64_row20_col2, #T_d5c64_row31_col6, #T_d5c64_row32_col6, #T_d5c64_row38_col9, #T_d5c64_row65_col8, #T_d5c64_row113_col0, #T_d5c64_row126_col4, #T_d5c64_row135_col7, #T_d5c64_row136_col5 {\n",
              "  background-color: #d1d2e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row1_col2, #T_d5c64_row5_col5, #T_d5c64_row5_col9, #T_d5c64_row19_col2, #T_d5c64_row24_col7, #T_d5c64_row24_col9, #T_d5c64_row27_col0, #T_d5c64_row38_col8, #T_d5c64_row40_col10, #T_d5c64_row43_col9, #T_d5c64_row47_col10, #T_d5c64_row50_col6, #T_d5c64_row79_col8, #T_d5c64_row106_col0, #T_d5c64_row106_col6, #T_d5c64_row114_col1, #T_d5c64_row163_col2 {\n",
              "  background-color: #d2d3e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row1_col3, #T_d5c64_row2_col3, #T_d5c64_row16_col8, #T_d5c64_row23_col7, #T_d5c64_row31_col2, #T_d5c64_row50_col0, #T_d5c64_row51_col5, #T_d5c64_row51_col6, #T_d5c64_row93_col10, #T_d5c64_row114_col10, #T_d5c64_row138_col10, #T_d5c64_row142_col5, #T_d5c64_row149_col3, #T_d5c64_row151_col5, #T_d5c64_row151_col7, #T_d5c64_row152_col2, #T_d5c64_row153_col5, #T_d5c64_row155_col10, #T_d5c64_row163_col6 {\n",
              "  background-color: #dcdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row1_col4, #T_d5c64_row6_col4, #T_d5c64_row7_col4, #T_d5c64_row9_col4, #T_d5c64_row13_col4, #T_d5c64_row36_col4, #T_d5c64_row43_col4, #T_d5c64_row45_col4, #T_d5c64_row50_col4, #T_d5c64_row51_col4, #T_d5c64_row55_col4, #T_d5c64_row69_col4, #T_d5c64_row73_col5, #T_d5c64_row73_col7, #T_d5c64_row76_col5, #T_d5c64_row77_col7, #T_d5c64_row81_col7, #T_d5c64_row83_col2, #T_d5c64_row94_col7, #T_d5c64_row95_col5, #T_d5c64_row95_col7, #T_d5c64_row97_col0, #T_d5c64_row97_col5, #T_d5c64_row97_col6, #T_d5c64_row97_col7, #T_d5c64_row104_col5, #T_d5c64_row123_col2, #T_d5c64_row125_col8, #T_d5c64_row136_col8, #T_d5c64_row137_col9, #T_d5c64_row143_col9, #T_d5c64_row144_col2, #T_d5c64_row147_col8, #T_d5c64_row147_col9, #T_d5c64_row150_col9 {\n",
              "  background-color: #78abd0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row1_col5, #T_d5c64_row29_col3, #T_d5c64_row32_col8, #T_d5c64_row35_col2, #T_d5c64_row37_col5, #T_d5c64_row38_col10, #T_d5c64_row41_col0, #T_d5c64_row94_col10, #T_d5c64_row99_col1, #T_d5c64_row106_col1, #T_d5c64_row106_col8, #T_d5c64_row124_col9, #T_d5c64_row139_col7, #T_d5c64_row158_col3, #T_d5c64_row163_col0 {\n",
              "  background-color: #c2cbe2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row1_col6, #T_d5c64_row5_col2, #T_d5c64_row12_col8, #T_d5c64_row19_col3, #T_d5c64_row25_col7, #T_d5c64_row28_col5, #T_d5c64_row37_col7, #T_d5c64_row45_col0, #T_d5c64_row46_col0, #T_d5c64_row71_col6, #T_d5c64_row80_col8, #T_d5c64_row86_col6, #T_d5c64_row96_col8, #T_d5c64_row108_col2, #T_d5c64_row147_col1, #T_d5c64_row148_col7, #T_d5c64_row152_col10, #T_d5c64_row161_col0, #T_d5c64_row164_col10 {\n",
              "  background-color: #c9cee4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row1_col7, #T_d5c64_row21_col10, #T_d5c64_row24_col10, #T_d5c64_row29_col0, #T_d5c64_row29_col1, #T_d5c64_row30_col0, #T_d5c64_row32_col1, #T_d5c64_row35_col1, #T_d5c64_row38_col7, #T_d5c64_row50_col7, #T_d5c64_row54_col8, #T_d5c64_row85_col9, #T_d5c64_row96_col10, #T_d5c64_row113_col5, #T_d5c64_row128_col0, #T_d5c64_row133_col6, #T_d5c64_row158_col5 {\n",
              "  background-color: #cdd0e5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row1_col8, #T_d5c64_row5_col3, #T_d5c64_row13_col10, #T_d5c64_row25_col0, #T_d5c64_row35_col5, #T_d5c64_row35_col6, #T_d5c64_row38_col1, #T_d5c64_row47_col3, #T_d5c64_row53_col7, #T_d5c64_row57_col0, #T_d5c64_row76_col0, #T_d5c64_row93_col3, #T_d5c64_row113_col9, #T_d5c64_row143_col1, #T_d5c64_row149_col1, #T_d5c64_row151_col1, #T_d5c64_row155_col8, #T_d5c64_row158_col7, #T_d5c64_row162_col0 {\n",
              "  background-color: #ced0e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row1_col9, #T_d5c64_row2_col1, #T_d5c64_row2_col6, #T_d5c64_row12_col0, #T_d5c64_row15_col6, #T_d5c64_row23_col1, #T_d5c64_row28_col6, #T_d5c64_row30_col5, #T_d5c64_row31_col10, #T_d5c64_row34_col6, #T_d5c64_row43_col2, #T_d5c64_row51_col3, #T_d5c64_row54_col0, #T_d5c64_row64_col6, #T_d5c64_row72_col0, #T_d5c64_row107_col10, #T_d5c64_row114_col6, #T_d5c64_row135_col1, #T_d5c64_row135_col4, #T_d5c64_row137_col1, #T_d5c64_row137_col6, #T_d5c64_row143_col5, #T_d5c64_row143_col7, #T_d5c64_row145_col0, #T_d5c64_row147_col7, #T_d5c64_row159_col2, #T_d5c64_row159_col6 {\n",
              "  background-color: #d9d8ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row1_col10, #T_d5c64_row3_col10, #T_d5c64_row24_col8, #T_d5c64_row31_col9, #T_d5c64_row33_col2, #T_d5c64_row34_col1, #T_d5c64_row43_col0, #T_d5c64_row51_col9, #T_d5c64_row53_col9, #T_d5c64_row89_col0, #T_d5c64_row112_col4, #T_d5c64_row114_col2, #T_d5c64_row148_col6 {\n",
              "  background-color: #d4d4e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row2_col0, #T_d5c64_row38_col3, #T_d5c64_row41_col8, #T_d5c64_row43_col5, #T_d5c64_row82_col3, #T_d5c64_row90_col1, #T_d5c64_row102_col9, #T_d5c64_row135_col5, #T_d5c64_row144_col1, #T_d5c64_row154_col0, #T_d5c64_row160_col2 {\n",
              "  background-color: #a7bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row2_col2, #T_d5c64_row5_col10, #T_d5c64_row6_col0, #T_d5c64_row6_col10, #T_d5c64_row30_col10, #T_d5c64_row65_col9, #T_d5c64_row86_col0, #T_d5c64_row86_col4, #T_d5c64_row99_col6, #T_d5c64_row113_col10, #T_d5c64_row148_col3, #T_d5c64_row163_col10 {\n",
              "  background-color: #d6d6e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row2_col4, #T_d5c64_row3_col4, #T_d5c64_row39_col6, #T_d5c64_row66_col1, #T_d5c64_row67_col2, #T_d5c64_row69_col6, #T_d5c64_row69_col7, #T_d5c64_row76_col6, #T_d5c64_row76_col7, #T_d5c64_row77_col2, #T_d5c64_row79_col5, #T_d5c64_row80_col5, #T_d5c64_row83_col9, #T_d5c64_row88_col9, #T_d5c64_row91_col2, #T_d5c64_row92_col1, #T_d5c64_row94_col1, #T_d5c64_row96_col1, #T_d5c64_row101_col5, #T_d5c64_row108_col5, #T_d5c64_row116_col8, #T_d5c64_row116_col9, #T_d5c64_row125_col4, #T_d5c64_row125_col9, #T_d5c64_row132_col6, #T_d5c64_row138_col8, #T_d5c64_row143_col8, #T_d5c64_row150_col0, #T_d5c64_row167_col8 {\n",
              "  background-color: #7bacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row2_col5, #T_d5c64_row19_col7, #T_d5c64_row24_col6, #T_d5c64_row30_col1, #T_d5c64_row30_col7, #T_d5c64_row47_col0, #T_d5c64_row52_col0, #T_d5c64_row53_col8, #T_d5c64_row96_col0, #T_d5c64_row106_col10, #T_d5c64_row113_col6, #T_d5c64_row114_col3, #T_d5c64_row115_col1, #T_d5c64_row149_col5 {\n",
              "  background-color: #dad9ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row2_col7, #T_d5c64_row2_col10, #T_d5c64_row9_col0, #T_d5c64_row9_col8, #T_d5c64_row9_col9, #T_d5c64_row15_col9, #T_d5c64_row16_col9, #T_d5c64_row23_col0, #T_d5c64_row30_col2, #T_d5c64_row33_col7, #T_d5c64_row33_col9, #T_d5c64_row79_col0, #T_d5c64_row80_col10, #T_d5c64_row87_col4, #T_d5c64_row113_col1, #T_d5c64_row124_col2, #T_d5c64_row127_col4, #T_d5c64_row127_col10, #T_d5c64_row134_col10, #T_d5c64_row139_col4, #T_d5c64_row139_col6, #T_d5c64_row148_col1, #T_d5c64_row150_col4, #T_d5c64_row161_col7, #T_d5c64_row164_col4 {\n",
              "  background-color: #dfddec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row2_col8, #T_d5c64_row10_col10, #T_d5c64_row11_col1, #T_d5c64_row12_col2, #T_d5c64_row15_col1, #T_d5c64_row23_col5, #T_d5c64_row28_col9, #T_d5c64_row29_col6, #T_d5c64_row32_col10, #T_d5c64_row34_col5, #T_d5c64_row37_col6, #T_d5c64_row43_col6, #T_d5c64_row47_col1, #T_d5c64_row79_col9, #T_d5c64_row89_col8, #T_d5c64_row89_col10, #T_d5c64_row120_col9, #T_d5c64_row130_col1, #T_d5c64_row137_col10, #T_d5c64_row150_col3 {\n",
              "  background-color: #c6cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row3_col0, #T_d5c64_row5_col8, #T_d5c64_row8_col0, #T_d5c64_row15_col8, #T_d5c64_row25_col1, #T_d5c64_row33_col3, #T_d5c64_row43_col3, #T_d5c64_row57_col10, #T_d5c64_row72_col9, #T_d5c64_row76_col9, #T_d5c64_row76_col10, #T_d5c64_row79_col4, #T_d5c64_row94_col4, #T_d5c64_row136_col10, #T_d5c64_row143_col3, #T_d5c64_row149_col4, #T_d5c64_row149_col6 {\n",
              "  background-color: #dbdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row3_col2, #T_d5c64_row28_col2, #T_d5c64_row62_col10, #T_d5c64_row85_col2, #T_d5c64_row89_col1, #T_d5c64_row100_col0, #T_d5c64_row134_col8, #T_d5c64_row136_col2, #T_d5c64_row146_col7, #T_d5c64_row159_col10, #T_d5c64_row163_col9, #T_d5c64_row166_col10 {\n",
              "  background-color: #bdc8e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row3_col3, #T_d5c64_row3_col6, #T_d5c64_row34_col4, #T_d5c64_row56_col3, #T_d5c64_row60_col4, #T_d5c64_row63_col10, #T_d5c64_row67_col1, #T_d5c64_row70_col0, #T_d5c64_row70_col3, #T_d5c64_row70_col9, #T_d5c64_row70_col10, #T_d5c64_row74_col4, #T_d5c64_row75_col3, #T_d5c64_row75_col7, #T_d5c64_row82_col7, #T_d5c64_row88_col5, #T_d5c64_row90_col2, #T_d5c64_row91_col7, #T_d5c64_row92_col6, #T_d5c64_row104_col10, #T_d5c64_row105_col7, #T_d5c64_row109_col6, #T_d5c64_row111_col1, #T_d5c64_row119_col5, #T_d5c64_row119_col6, #T_d5c64_row122_col4, #T_d5c64_row130_col2, #T_d5c64_row133_col2, #T_d5c64_row142_col9, #T_d5c64_row157_col0, #T_d5c64_row158_col0, #T_d5c64_row159_col8, #T_d5c64_row165_col6 {\n",
              "  background-color: #8cb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row3_col5, #T_d5c64_row8_col6, #T_d5c64_row19_col6, #T_d5c64_row64_col1, #T_d5c64_row69_col9, #T_d5c64_row84_col0, #T_d5c64_row98_col9, #T_d5c64_row103_col10, #T_d5c64_row112_col3, #T_d5c64_row158_col10 {\n",
              "  background-color: #a8bedc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row3_col7, #T_d5c64_row6_col9, #T_d5c64_row17_col6, #T_d5c64_row25_col2, #T_d5c64_row36_col7, #T_d5c64_row49_col8, #T_d5c64_row58_col6, #T_d5c64_row61_col6, #T_d5c64_row62_col1, #T_d5c64_row65_col7, #T_d5c64_row80_col9, #T_d5c64_row83_col1, #T_d5c64_row100_col7, #T_d5c64_row105_col3, #T_d5c64_row107_col7, #T_d5c64_row109_col10, #T_d5c64_row123_col10, #T_d5c64_row130_col8, #T_d5c64_row149_col0, #T_d5c64_row151_col10 {\n",
              "  background-color: #96b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row3_col8, #T_d5c64_row12_col6, #T_d5c64_row14_col3, #T_d5c64_row49_col9, #T_d5c64_row54_col6, #T_d5c64_row56_col9, #T_d5c64_row82_col6, #T_d5c64_row89_col5, #T_d5c64_row97_col10, #T_d5c64_row116_col7, #T_d5c64_row154_col5, #T_d5c64_row157_col9, #T_d5c64_row159_col9, #T_d5c64_row165_col4 {\n",
              "  background-color: #99b8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row4_col0, #T_d5c64_row68_col4 {\n",
              "  background-color: #f0eaf4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row4_col2, #T_d5c64_row6_col6, #T_d5c64_row15_col5, #T_d5c64_row18_col9, #T_d5c64_row20_col8, #T_d5c64_row41_col6, #T_d5c64_row45_col5, #T_d5c64_row45_col8, #T_d5c64_row48_col5, #T_d5c64_row56_col10, #T_d5c64_row69_col0, #T_d5c64_row77_col3, #T_d5c64_row84_col5, #T_d5c64_row84_col7, #T_d5c64_row85_col5, #T_d5c64_row86_col5, #T_d5c64_row101_col10, #T_d5c64_row103_col2, #T_d5c64_row105_col2, #T_d5c64_row107_col0, #T_d5c64_row115_col5, #T_d5c64_row140_col2, #T_d5c64_row146_col6, #T_d5c64_row150_col10 {\n",
              "  background-color: #a2bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row4_col3, #T_d5c64_row7_col3, #T_d5c64_row10_col8, #T_d5c64_row21_col4, #T_d5c64_row24_col4, #T_d5c64_row25_col4, #T_d5c64_row35_col4, #T_d5c64_row39_col5, #T_d5c64_row39_col7, #T_d5c64_row42_col1, #T_d5c64_row42_col6, #T_d5c64_row45_col6, #T_d5c64_row59_col3, #T_d5c64_row62_col2, #T_d5c64_row63_col2, #T_d5c64_row73_col8, #T_d5c64_row74_col7, #T_d5c64_row74_col10, #T_d5c64_row78_col1, #T_d5c64_row91_col8, #T_d5c64_row93_col1, #T_d5c64_row98_col1, #T_d5c64_row100_col5, #T_d5c64_row101_col4, #T_d5c64_row103_col7, #T_d5c64_row105_col6, #T_d5c64_row109_col1, #T_d5c64_row109_col3, #T_d5c64_row110_col7, #T_d5c64_row111_col4, #T_d5c64_row118_col5, #T_d5c64_row118_col7, #T_d5c64_row119_col2, #T_d5c64_row122_col7, #T_d5c64_row123_col7, #T_d5c64_row125_col10, #T_d5c64_row145_col8, #T_d5c64_row165_col3, #T_d5c64_row165_col5, #T_d5c64_row167_col10 {\n",
              "  background-color: #88b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row4_col4, #T_d5c64_row10_col6, #T_d5c64_row13_col3, #T_d5c64_row18_col5, #T_d5c64_row41_col4, #T_d5c64_row55_col1, #T_d5c64_row59_col5, #T_d5c64_row59_col7, #T_d5c64_row67_col10, #T_d5c64_row79_col7, #T_d5c64_row83_col8, #T_d5c64_row94_col0, #T_d5c64_row96_col6, #T_d5c64_row101_col6, #T_d5c64_row102_col4, #T_d5c64_row103_col5, #T_d5c64_row104_col4, #T_d5c64_row110_col5, #T_d5c64_row111_col5, #T_d5c64_row116_col3, #T_d5c64_row118_col3, #T_d5c64_row118_col6, #T_d5c64_row118_col8, #T_d5c64_row118_col9, #T_d5c64_row119_col0, #T_d5c64_row125_col1, #T_d5c64_row125_col3, #T_d5c64_row125_col5, #T_d5c64_row129_col5, #T_d5c64_row132_col3, #T_d5c64_row132_col9, #T_d5c64_row135_col8, #T_d5c64_row135_col9, #T_d5c64_row144_col0, #T_d5c64_row151_col2, #T_d5c64_row158_col8, #T_d5c64_row160_col8, #T_d5c64_row165_col2, #T_d5c64_row167_col3 {\n",
              "  background-color: #80aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row4_col5, #T_d5c64_row5_col6, #T_d5c64_row8_col7, #T_d5c64_row11_col2, #T_d5c64_row11_col6, #T_d5c64_row42_col5, #T_d5c64_row46_col10, #T_d5c64_row53_col4, #T_d5c64_row55_col8, #T_d5c64_row60_col7, #T_d5c64_row100_col6, #T_d5c64_row103_col6, #T_d5c64_row111_col8, #T_d5c64_row119_col4, #T_d5c64_row128_col9, #T_d5c64_row130_col10, #T_d5c64_row160_col10 {\n",
              "  background-color: #a1bbda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row4_col8, #T_d5c64_row17_col8, #T_d5c64_row21_col5, #T_d5c64_row27_col3, #T_d5c64_row49_col2, #T_d5c64_row60_col6, #T_d5c64_row76_col3, #T_d5c64_row87_col5, #T_d5c64_row98_col8, #T_d5c64_row101_col1, #T_d5c64_row104_col0, #T_d5c64_row131_col8, #T_d5c64_row157_col6, #T_d5c64_row157_col7, #T_d5c64_row165_col1 {\n",
              "  background-color: #9cb9d9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row4_col10 {\n",
              "  background-color: #e6e2ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row5_col0, #T_d5c64_row9_col2, #T_d5c64_row16_col7, #T_d5c64_row24_col1, #T_d5c64_row26_col2, #T_d5c64_row26_col5, #T_d5c64_row26_col9, #T_d5c64_row51_col10, #T_d5c64_row80_col4, #T_d5c64_row113_col2, #T_d5c64_row120_col4, #T_d5c64_row121_col1, #T_d5c64_row124_col0, #T_d5c64_row124_col1, #T_d5c64_row124_col3, #T_d5c64_row124_col5, #T_d5c64_row128_col3, #T_d5c64_row128_col7, #T_d5c64_row128_col10, #T_d5c64_row131_col2, #T_d5c64_row131_col3, #T_d5c64_row143_col4, #T_d5c64_row146_col4, #T_d5c64_row158_col4, #T_d5c64_row159_col3, #T_d5c64_row159_col5, #T_d5c64_row161_col10 {\n",
              "  background-color: #e1dfed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row5_col1, #T_d5c64_row6_col5, #T_d5c64_row13_col9, #T_d5c64_row27_col9, #T_d5c64_row32_col2, #T_d5c64_row51_col0, #T_d5c64_row53_col3, #T_d5c64_row75_col10, #T_d5c64_row81_col4, #T_d5c64_row100_col2, #T_d5c64_row112_col1, #T_d5c64_row122_col10, #T_d5c64_row135_col2, #T_d5c64_row139_col10, #T_d5c64_row144_col6 {\n",
              "  background-color: #bbc7e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row5_col4, #T_d5c64_row10_col1, #T_d5c64_row13_col6, #T_d5c64_row13_col7, #T_d5c64_row14_col1, #T_d5c64_row14_col6, #T_d5c64_row20_col7, #T_d5c64_row22_col4, #T_d5c64_row39_col4, #T_d5c64_row47_col4, #T_d5c64_row61_col5, #T_d5c64_row62_col4, #T_d5c64_row62_col5, #T_d5c64_row63_col5, #T_d5c64_row63_col6, #T_d5c64_row67_col0, #T_d5c64_row70_col5, #T_d5c64_row73_col6, #T_d5c64_row75_col5, #T_d5c64_row77_col9, #T_d5c64_row78_col2, #T_d5c64_row81_col1, #T_d5c64_row100_col1, #T_d5c64_row101_col3, #T_d5c64_row102_col2, #T_d5c64_row102_col3, #T_d5c64_row104_col1, #T_d5c64_row105_col5, #T_d5c64_row107_col1, #T_d5c64_row122_col5, #T_d5c64_row122_col6, #T_d5c64_row122_col9, #T_d5c64_row123_col3, #T_d5c64_row123_col5, #T_d5c64_row123_col8, #T_d5c64_row129_col6, #T_d5c64_row129_col7, #T_d5c64_row130_col3, #T_d5c64_row130_col4, #T_d5c64_row140_col9, #T_d5c64_row151_col8, #T_d5c64_row152_col9, #T_d5c64_row161_col8, #T_d5c64_row167_col2, #T_d5c64_row167_col9 {\n",
              "  background-color: #83afd3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row5_col7, #T_d5c64_row23_col10, #T_d5c64_row27_col6, #T_d5c64_row29_col8, #T_d5c64_row40_col9, #T_d5c64_row47_col5, #T_d5c64_row51_col8, #T_d5c64_row54_col7, #T_d5c64_row55_col10, #T_d5c64_row65_col0, #T_d5c64_row72_col2, #T_d5c64_row72_col3, #T_d5c64_row85_col8, #T_d5c64_row110_col8, #T_d5c64_row142_col10, #T_d5c64_row163_col7 {\n",
              "  background-color: #d8d7e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row6_col1, #T_d5c64_row10_col5, #T_d5c64_row27_col2, #T_d5c64_row40_col1, #T_d5c64_row49_col3, #T_d5c64_row52_col3, #T_d5c64_row68_col2, #T_d5c64_row69_col10, #T_d5c64_row73_col0, #T_d5c64_row73_col9, #T_d5c64_row75_col6, #T_d5c64_row78_col6, #T_d5c64_row83_col0, #T_d5c64_row88_col1, #T_d5c64_row102_col7, #T_d5c64_row105_col10, #T_d5c64_row110_col6, #T_d5c64_row111_col0, #T_d5c64_row117_col9, #T_d5c64_row165_col7 {\n",
              "  background-color: #93b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row6_col2, #T_d5c64_row6_col8, #T_d5c64_row14_col7, #T_d5c64_row33_col4, #T_d5c64_row36_col5, #T_d5c64_row63_col9, #T_d5c64_row68_col3, #T_d5c64_row68_col6, #T_d5c64_row70_col7, #T_d5c64_row73_col1, #T_d5c64_row79_col2, #T_d5c64_row79_col6, #T_d5c64_row91_col3, #T_d5c64_row95_col3, #T_d5c64_row95_col10, #T_d5c64_row103_col3, #T_d5c64_row106_col7, #T_d5c64_row108_col1, #T_d5c64_row110_col3, #T_d5c64_row115_col3, #T_d5c64_row119_col1, #T_d5c64_row121_col8, #T_d5c64_row124_col8, #T_d5c64_row129_col4, #T_d5c64_row147_col2, #T_d5c64_row157_col5, #T_d5c64_row163_col8 {\n",
              "  background-color: #8eb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row6_col3, #T_d5c64_row7_col6, #T_d5c64_row18_col6, #T_d5c64_row18_col7, #T_d5c64_row20_col1, #T_d5c64_row37_col4, #T_d5c64_row49_col6, #T_d5c64_row54_col4, #T_d5c64_row57_col5, #T_d5c64_row60_col8, #T_d5c64_row62_col7, #T_d5c64_row63_col1, #T_d5c64_row66_col3, #T_d5c64_row67_col4, #T_d5c64_row70_col2, #T_d5c64_row74_col0, #T_d5c64_row74_col3, #T_d5c64_row74_col6, #T_d5c64_row79_col1, #T_d5c64_row95_col6, #T_d5c64_row98_col5, #T_d5c64_row104_col7, #T_d5c64_row115_col8, #T_d5c64_row119_col9, #T_d5c64_row123_col0, #T_d5c64_row125_col0, #T_d5c64_row126_col8, #T_d5c64_row130_col0, #T_d5c64_row132_col5, #T_d5c64_row132_col7, #T_d5c64_row136_col9, #T_d5c64_row138_col9, #T_d5c64_row140_col8, #T_d5c64_row149_col8, #T_d5c64_row152_col8, #T_d5c64_row153_col10, #T_d5c64_row157_col8, #T_d5c64_row167_col5 {\n",
              "  background-color: #81aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row6_col7, #T_d5c64_row11_col4, #T_d5c64_row24_col3, #T_d5c64_row25_col3, #T_d5c64_row37_col1, #T_d5c64_row42_col8, #T_d5c64_row55_col7, #T_d5c64_row101_col9, #T_d5c64_row134_col0, #T_d5c64_row154_col6, #T_d5c64_row160_col3 {\n",
              "  background-color: #acc0dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row7_col0, #T_d5c64_row7_col2, #T_d5c64_row7_col8, #T_d5c64_row10_col7, #T_d5c64_row12_col4, #T_d5c64_row27_col4, #T_d5c64_row40_col6, #T_d5c64_row45_col1, #T_d5c64_row48_col1, #T_d5c64_row60_col3, #T_d5c64_row63_col4, #T_d5c64_row64_col7, #T_d5c64_row66_col9, #T_d5c64_row68_col5, #T_d5c64_row72_col5, #T_d5c64_row73_col3, #T_d5c64_row74_col5, #T_d5c64_row77_col1, #T_d5c64_row78_col3, #T_d5c64_row92_col7, #T_d5c64_row94_col2, #T_d5c64_row101_col7, #T_d5c64_row102_col10, #T_d5c64_row105_col1, #T_d5c64_row107_col5, #T_d5c64_row108_col3, #T_d5c64_row108_col7, #T_d5c64_row109_col4, #T_d5c64_row118_col0, #T_d5c64_row118_col4, #T_d5c64_row123_col6, #T_d5c64_row125_col7, #T_d5c64_row126_col9, #T_d5c64_row129_col3, #T_d5c64_row132_col0, #T_d5c64_row132_col10, #T_d5c64_row139_col3, #T_d5c64_row149_col9, #T_d5c64_row154_col8 {\n",
              "  background-color: #84b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row7_col1, #T_d5c64_row13_col8, #T_d5c64_row23_col4, #T_d5c64_row26_col4, #T_d5c64_row29_col4, #T_d5c64_row49_col1, #T_d5c64_row52_col6, #T_d5c64_row59_col9, #T_d5c64_row62_col8, #T_d5c64_row63_col0, #T_d5c64_row63_col3, #T_d5c64_row63_col7, #T_d5c64_row65_col1, #T_d5c64_row67_col8, #T_d5c64_row70_col1, #T_d5c64_row74_col1, #T_d5c64_row98_col2, #T_d5c64_row98_col6, #T_d5c64_row110_col1, #T_d5c64_row112_col9, #T_d5c64_row116_col5, #T_d5c64_row116_col6, #T_d5c64_row122_col1, #T_d5c64_row139_col0, #T_d5c64_row145_col9, #T_d5c64_row151_col0, #T_d5c64_row166_col8 {\n",
              "  background-color: #8bb2d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row7_col7, #T_d5c64_row10_col3, #T_d5c64_row10_col9, #T_d5c64_row55_col3, #T_d5c64_row56_col5, #T_d5c64_row60_col2, #T_d5c64_row60_col9, #T_d5c64_row67_col6, #T_d5c64_row67_col7, #T_d5c64_row67_col9, #T_d5c64_row69_col8, #T_d5c64_row70_col6, #T_d5c64_row74_col9, #T_d5c64_row75_col1, #T_d5c64_row79_col3, #T_d5c64_row81_col0, #T_d5c64_row88_col0, #T_d5c64_row93_col5, #T_d5c64_row96_col7, #T_d5c64_row102_col6, #T_d5c64_row112_col8, #T_d5c64_row115_col9, #T_d5c64_row122_col3, #T_d5c64_row142_col8, #T_d5c64_row154_col9 {\n",
              "  background-color: #86b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row8_col2, #T_d5c64_row15_col2, #T_d5c64_row15_col3, #T_d5c64_row15_col10, #T_d5c64_row16_col10, #T_d5c64_row22_col1, #T_d5c64_row22_col2, #T_d5c64_row22_col6, #T_d5c64_row26_col0, #T_d5c64_row26_col3, #T_d5c64_row26_col8, #T_d5c64_row26_col10, #T_d5c64_row33_col0, #T_d5c64_row33_col8, #T_d5c64_row36_col8, #T_d5c64_row36_col10, #T_d5c64_row44_col0, #T_d5c64_row44_col1, #T_d5c64_row44_col2, #T_d5c64_row44_col3, #T_d5c64_row44_col10, #T_d5c64_row50_col9, #T_d5c64_row57_col4, #T_d5c64_row64_col2, #T_d5c64_row64_col3, #T_d5c64_row64_col4, #T_d5c64_row64_col10, #T_d5c64_row65_col10, #T_d5c64_row68_col0, #T_d5c64_row68_col8, #T_d5c64_row68_col9, #T_d5c64_row71_col2, #T_d5c64_row71_col3, #T_d5c64_row75_col0, #T_d5c64_row75_col9, #T_d5c64_row78_col4, #T_d5c64_row78_col10, #T_d5c64_row82_col0, #T_d5c64_row82_col4, #T_d5c64_row82_col10, #T_d5c64_row85_col4, #T_d5c64_row85_col10, #T_d5c64_row89_col4, #T_d5c64_row92_col4, #T_d5c64_row92_col9, #T_d5c64_row92_col10, #T_d5c64_row99_col2, #T_d5c64_row99_col3, #T_d5c64_row100_col10, #T_d5c64_row103_col0, #T_d5c64_row103_col8, #T_d5c64_row106_col4, #T_d5c64_row117_col1, #T_d5c64_row120_col1, #T_d5c64_row120_col2, #T_d5c64_row120_col3, #T_d5c64_row120_col6, #T_d5c64_row120_col10, #T_d5c64_row121_col7, #T_d5c64_row121_col10, #T_d5c64_row127_col1, #T_d5c64_row127_col2, #T_d5c64_row127_col3, #T_d5c64_row127_col6, #T_d5c64_row127_col7, #T_d5c64_row133_col4, #T_d5c64_row134_col1, #T_d5c64_row134_col2, #T_d5c64_row134_col3, #T_d5c64_row134_col4, #T_d5c64_row134_col6, #T_d5c64_row134_col7, #T_d5c64_row136_col1, #T_d5c64_row136_col4, #T_d5c64_row138_col1, #T_d5c64_row138_col3, #T_d5c64_row138_col6, #T_d5c64_row138_col7, #T_d5c64_row141_col1, #T_d5c64_row141_col2, #T_d5c64_row141_col3, #T_d5c64_row141_col4, #T_d5c64_row141_col6, #T_d5c64_row145_col2, #T_d5c64_row145_col3, #T_d5c64_row145_col6, #T_d5c64_row147_col4, #T_d5c64_row151_col4, #T_d5c64_row155_col1, #T_d5c64_row155_col2, #T_d5c64_row155_col3, #T_d5c64_row155_col6, #T_d5c64_row155_col7, #T_d5c64_row156_col1, #T_d5c64_row156_col2, #T_d5c64_row156_col3, #T_d5c64_row156_col10, #T_d5c64_row161_col4, #T_d5c64_row162_col1, #T_d5c64_row162_col3, #T_d5c64_row162_col4, #T_d5c64_row162_col6, #T_d5c64_row166_col1, #T_d5c64_row166_col2, #T_d5c64_row166_col3, #T_d5c64_row166_col6 {\n",
              "  background-color: #e3e0ee;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row8_col3, #T_d5c64_row8_col9, #T_d5c64_row16_col2, #T_d5c64_row16_col3, #T_d5c64_row20_col10, #T_d5c64_row22_col7, #T_d5c64_row26_col1, #T_d5c64_row33_col10, #T_d5c64_row50_col10, #T_d5c64_row61_col0, #T_d5c64_row61_col8, #T_d5c64_row66_col2, #T_d5c64_row81_col10, #T_d5c64_row92_col0, #T_d5c64_row113_col3, #T_d5c64_row122_col2, #T_d5c64_row128_col1, #T_d5c64_row129_col2, #T_d5c64_row142_col3, #T_d5c64_row152_col6, #T_d5c64_row157_col2, #T_d5c64_row161_col6 {\n",
              "  background-color: #e0deed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row8_col4, #T_d5c64_row13_col1, #T_d5c64_row14_col4, #T_d5c64_row18_col1, #T_d5c64_row19_col4, #T_d5c64_row39_col1, #T_d5c64_row40_col4, #T_d5c64_row42_col4, #T_d5c64_row49_col4, #T_d5c64_row52_col4, #T_d5c64_row64_col5, #T_d5c64_row66_col5, #T_d5c64_row69_col5, #T_d5c64_row76_col4, #T_d5c64_row78_col5, #T_d5c64_row78_col7, #T_d5c64_row81_col8, #T_d5c64_row83_col7, #T_d5c64_row88_col8, #T_d5c64_row91_col1, #T_d5c64_row91_col6, #T_d5c64_row91_col9, #T_d5c64_row104_col6, #T_d5c64_row106_col5, #T_d5c64_row116_col0, #T_d5c64_row118_col2, #T_d5c64_row132_col4, #T_d5c64_row133_col9, #T_d5c64_row143_col0, #T_d5c64_row151_col9, #T_d5c64_row167_col0 {\n",
              "  background-color: #79abd0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row8_col8, #T_d5c64_row16_col0, #T_d5c64_row16_col6, #T_d5c64_row17_col2, #T_d5c64_row24_col0, #T_d5c64_row32_col0, #T_d5c64_row37_col0, #T_d5c64_row47_col7, #T_d5c64_row50_col1, #T_d5c64_row55_col0, #T_d5c64_row57_col8, #T_d5c64_row65_col2, #T_d5c64_row71_col0, #T_d5c64_row87_col0, #T_d5c64_row87_col6, #T_d5c64_row101_col2, #T_d5c64_row113_col4, #T_d5c64_row113_col7, #T_d5c64_row149_col7, #T_d5c64_row150_col5 {\n",
              "  background-color: #dddbec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row8_col10, #T_d5c64_row9_col1, #T_d5c64_row9_col3, #T_d5c64_row9_col6, #T_d5c64_row9_col7, #T_d5c64_row9_col10, #T_d5c64_row10_col2, #T_d5c64_row22_col3, #T_d5c64_row26_col7, #T_d5c64_row36_col9, #T_d5c64_row38_col0, #T_d5c64_row40_col0, #T_d5c64_row44_col6, #T_d5c64_row44_col9, #T_d5c64_row50_col8, #T_d5c64_row57_col2, #T_d5c64_row57_col3, #T_d5c64_row71_col4, #T_d5c64_row71_col10, #T_d5c64_row72_col4, #T_d5c64_row72_col10, #T_d5c64_row73_col2, #T_d5c64_row75_col8, #T_d5c64_row82_col8, #T_d5c64_row99_col4, #T_d5c64_row99_col10, #T_d5c64_row103_col9, #T_d5c64_row117_col2, #T_d5c64_row117_col3, #T_d5c64_row117_col5, #T_d5c64_row117_col6, #T_d5c64_row117_col7, #T_d5c64_row121_col2, #T_d5c64_row121_col3, #T_d5c64_row121_col4, #T_d5c64_row121_col6, #T_d5c64_row124_col7, #T_d5c64_row127_col5, #T_d5c64_row128_col4, #T_d5c64_row131_col1, #T_d5c64_row131_col5, #T_d5c64_row131_col7, #T_d5c64_row134_col5, #T_d5c64_row136_col6, #T_d5c64_row138_col4, #T_d5c64_row138_col5, #T_d5c64_row141_col5, #T_d5c64_row141_col7, #T_d5c64_row141_col10, #T_d5c64_row142_col6, #T_d5c64_row144_col4, #T_d5c64_row144_col5, #T_d5c64_row145_col5, #T_d5c64_row145_col7, #T_d5c64_row153_col4, #T_d5c64_row155_col5, #T_d5c64_row156_col6, #T_d5c64_row159_col7, #T_d5c64_row162_col2, #T_d5c64_row162_col5, #T_d5c64_row162_col7, #T_d5c64_row164_col1, #T_d5c64_row166_col5, #T_d5c64_row166_col7 {\n",
              "  background-color: #e2dfee;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row9_col5, #T_d5c64_row22_col10, #T_d5c64_row29_col10, #T_d5c64_row36_col0, #T_d5c64_row44_col5, #T_d5c64_row44_col7, #T_d5c64_row44_col8, #T_d5c64_row92_col8, #T_d5c64_row120_col5, #T_d5c64_row120_col7, #T_d5c64_row121_col5, #T_d5c64_row138_col2, #T_d5c64_row145_col1, #T_d5c64_row152_col3, #T_d5c64_row152_col5, #T_d5c64_row152_col7, #T_d5c64_row156_col4, #T_d5c64_row156_col5, #T_d5c64_row156_col7 {\n",
              "  background-color: #e4e1ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row10_col0, #T_d5c64_row21_col2, #T_d5c64_row21_col9, #T_d5c64_row29_col7, #T_d5c64_row34_col9, #T_d5c64_row38_col6, #T_d5c64_row39_col10, #T_d5c64_row41_col7, #T_d5c64_row46_col8, #T_d5c64_row48_col2, #T_d5c64_row59_col0, #T_d5c64_row59_col2, #T_d5c64_row62_col0, #T_d5c64_row85_col3, #T_d5c64_row87_col3, #T_d5c64_row88_col10, #T_d5c64_row154_col1, #T_d5c64_row167_col6 {\n",
              "  background-color: #afc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row10_col4, #T_d5c64_row17_col4, #T_d5c64_row20_col4, #T_d5c64_row44_col4, #T_d5c64_row48_col4, #T_d5c64_row77_col5, #T_d5c64_row81_col2, #T_d5c64_row81_col5, #T_d5c64_row81_col9, #T_d5c64_row83_col5, #T_d5c64_row88_col2, #T_d5c64_row94_col5, #T_d5c64_row94_col9, #T_d5c64_row95_col1, #T_d5c64_row97_col1, #T_d5c64_row97_col3, #T_d5c64_row116_col2, #T_d5c64_row125_col2, #T_d5c64_row133_col8, #T_d5c64_row137_col2, #T_d5c64_row137_col8, #T_d5c64_row139_col2, #T_d5c64_row139_col9, #T_d5c64_row144_col8, #T_d5c64_row146_col2, #T_d5c64_row146_col8, #T_d5c64_row146_col9, #T_d5c64_row153_col2, #T_d5c64_row165_col0 {\n",
              "  background-color: #76aad0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row11_col0, #T_d5c64_row13_col5, #T_d5c64_row17_col1, #T_d5c64_row17_col9, #T_d5c64_row40_col2, #T_d5c64_row49_col5, #T_d5c64_row52_col1, #T_d5c64_row72_col7, #T_d5c64_row88_col7, #T_d5c64_row105_col9, #T_d5c64_row122_col0, #T_d5c64_row126_col2 {\n",
              "  background-color: #9fbad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row11_col5, #T_d5c64_row21_col1, #T_d5c64_row25_col5, #T_d5c64_row27_col7, #T_d5c64_row31_col7, #T_d5c64_row32_col5, #T_d5c64_row32_col9, #T_d5c64_row50_col2, #T_d5c64_row56_col0, #T_d5c64_row90_col0, #T_d5c64_row93_col0, #T_d5c64_row112_col0, #T_d5c64_row133_col7, #T_d5c64_row134_col9, #T_d5c64_row140_col1, #T_d5c64_row140_col3, #T_d5c64_row143_col6 {\n",
              "  background-color: #bfc9e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row11_col7, #T_d5c64_row11_col10, #T_d5c64_row21_col7, #T_d5c64_row22_col0, #T_d5c64_row34_col3, #T_d5c64_row61_col2, #T_d5c64_row66_col10, #T_d5c64_row84_col8, #T_d5c64_row108_col4, #T_d5c64_row109_col0, #T_d5c64_row110_col2, #T_d5c64_row115_col2, #T_d5c64_row140_col7, #T_d5c64_row148_col0 {\n",
              "  background-color: #b3c3de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row11_col8, #T_d5c64_row29_col2, #T_d5c64_row32_col7, #T_d5c64_row38_col2, #T_d5c64_row54_col1, #T_d5c64_row84_col2, #T_d5c64_row86_col9, #T_d5c64_row87_col9, #T_d5c64_row102_col0, #T_d5c64_row111_col10, #T_d5c64_row133_col3, #T_d5c64_row136_col3, #T_d5c64_row138_col0, #T_d5c64_row140_col5, #T_d5c64_row140_col6, #T_d5c64_row140_col10, #T_d5c64_row154_col2, #T_d5c64_row157_col10 {\n",
              "  background-color: #b8c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row11_col9, #T_d5c64_row14_col2, #T_d5c64_row36_col6, #T_d5c64_row41_col9, #T_d5c64_row42_col0, #T_d5c64_row46_col6, #T_d5c64_row68_col10, #T_d5c64_row84_col1, #T_d5c64_row93_col2, #T_d5c64_row98_col4, #T_d5c64_row102_col1, #T_d5c64_row130_col5, #T_d5c64_row130_col6, #T_d5c64_row137_col0, #T_d5c64_row140_col0, #T_d5c64_row143_col10, #T_d5c64_row144_col10, #T_d5c64_row154_col7 {\n",
              "  background-color: #a4bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row12_col1, #T_d5c64_row25_col9, #T_d5c64_row34_col8, #T_d5c64_row42_col9, #T_d5c64_row49_col10, #T_d5c64_row73_col10, #T_d5c64_row77_col6, #T_d5c64_row83_col4, #T_d5c64_row93_col7, #T_d5c64_row111_col2, #T_d5c64_row112_col6, #T_d5c64_row112_col10, #T_d5c64_row131_col10 {\n",
              "  background-color: #b4c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row12_col7, #T_d5c64_row15_col0, #T_d5c64_row25_col10, #T_d5c64_row31_col8, #T_d5c64_row35_col10, #T_d5c64_row48_col10, #T_d5c64_row51_col2, #T_d5c64_row58_col10, #T_d5c64_row64_col9, #T_d5c64_row72_col8, #T_d5c64_row90_col4, #T_d5c64_row147_col5, #T_d5c64_row149_col2, #T_d5c64_row150_col7, #T_d5c64_row153_col7, #T_d5c64_row155_col0, #T_d5c64_row158_col1, #T_d5c64_row160_col1 {\n",
              "  background-color: #d7d6e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row12_col9, #T_d5c64_row30_col9, #T_d5c64_row41_col10, #T_d5c64_row47_col9, #T_d5c64_row53_col6, #T_d5c64_row88_col4, #T_d5c64_row97_col4, #T_d5c64_row108_col10, #T_d5c64_row124_col10, #T_d5c64_row127_col9, #T_d5c64_row151_col6, #T_d5c64_row164_col5 {\n",
              "  background-color: #bcc7e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row12_col10, #T_d5c64_row17_col10, #T_d5c64_row35_col7, #T_d5c64_row37_col8, #T_d5c64_row37_col10, #T_d5c64_row47_col8, #T_d5c64_row48_col9, #T_d5c64_row53_col5, #T_d5c64_row54_col10, #T_d5c64_row57_col1, #T_d5c64_row84_col4, #T_d5c64_row86_col2, #T_d5c64_row86_col10, #T_d5c64_row90_col6, #T_d5c64_row93_col4, #T_d5c64_row133_col10, #T_d5c64_row140_col4, #T_d5c64_row142_col4, #T_d5c64_row155_col9, #T_d5c64_row161_col5, #T_d5c64_row163_col3 {\n",
              "  background-color: #d3d4e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row13_col0, #T_d5c64_row13_col2, #T_d5c64_row18_col0, #T_d5c64_row27_col5, #T_d5c64_row85_col1, #T_d5c64_row85_col7, #T_d5c64_row96_col3, #T_d5c64_row101_col0, #T_d5c64_row107_col4, #T_d5c64_row108_col0, #T_d5c64_row108_col8, #T_d5c64_row152_col0, #T_d5c64_row154_col4, #T_d5c64_row156_col0 {\n",
              "  background-color: #abbfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row14_col8, #T_d5c64_row17_col5, #T_d5c64_row39_col0, #T_d5c64_row49_col0, #T_d5c64_row60_col10, #T_d5c64_row72_col1, #T_d5c64_row77_col0, #T_d5c64_row104_col9, #T_d5c64_row108_col9, #T_d5c64_row115_col7, #T_d5c64_row154_col10 {\n",
              "  background-color: #9ebad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row14_col9, #T_d5c64_row21_col8, #T_d5c64_row25_col8, #T_d5c64_row52_col9, #T_d5c64_row59_col6, #T_d5c64_row61_col1, #T_d5c64_row84_col6, #T_d5c64_row93_col9, #T_d5c64_row97_col9, #T_d5c64_row117_col0, #T_d5c64_row126_col1, #T_d5c64_row129_col10, #T_d5c64_row133_col0, #T_d5c64_row148_col10 {\n",
              "  background-color: #adc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row15_col4, #T_d5c64_row16_col4, #T_d5c64_row94_col3, #T_d5c64_row95_col0, #T_d5c64_row95_col2, #T_d5c64_row95_col8, #T_d5c64_row95_col9, #T_d5c64_row132_col2, #T_d5c64_row139_col8, #T_d5c64_row144_col9, #T_d5c64_row150_col8, #T_d5c64_row153_col0, #T_d5c64_row153_col8, #T_d5c64_row153_col9 {\n",
              "  background-color: #75a9cf;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row15_col7, #T_d5c64_row55_col5, #T_d5c64_row83_col10, #T_d5c64_row85_col6, #T_d5c64_row91_col4, #T_d5c64_row96_col9, #T_d5c64_row126_col5, #T_d5c64_row127_col8, #T_d5c64_row142_col0, #T_d5c64_row157_col4, #T_d5c64_row164_col0 {\n",
              "  background-color: #b1c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row16_col1, #T_d5c64_row16_col5, #T_d5c64_row22_col5, #T_d5c64_row26_col6, #T_d5c64_row30_col6, #T_d5c64_row40_col8, #T_d5c64_row45_col2, #T_d5c64_row51_col1, #T_d5c64_row61_col9, #T_d5c64_row65_col4, #T_d5c64_row71_col9, #T_d5c64_row75_col4, #T_d5c64_row77_col4, #T_d5c64_row78_col8, #T_d5c64_row80_col6, #T_d5c64_row82_col9, #T_d5c64_row124_col6, #T_d5c64_row128_col2, #T_d5c64_row128_col5, #T_d5c64_row128_col6, #T_d5c64_row131_col6, #T_d5c64_row142_col7, #T_d5c64_row144_col7, #T_d5c64_row148_col4, #T_d5c64_row150_col2, #T_d5c64_row159_col1, #T_d5c64_row161_col1, #T_d5c64_row164_col6 {\n",
              "  background-color: #e0dded;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row17_col0, #T_d5c64_row21_col3, #T_d5c64_row24_col2, #T_d5c64_row27_col8, #T_d5c64_row40_col5, #T_d5c64_row45_col9, #T_d5c64_row58_col0, #T_d5c64_row80_col1, #T_d5c64_row100_col9, #T_d5c64_row107_col3, #T_d5c64_row108_col6, #T_d5c64_row126_col7, #T_d5c64_row130_col7, #T_d5c64_row141_col9, #T_d5c64_row158_col2, #T_d5c64_row159_col0 {\n",
              "  background-color: #b0c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row17_col7, #T_d5c64_row18_col2, #T_d5c64_row41_col2, #T_d5c64_row56_col8, #T_d5c64_row57_col7, #T_d5c64_row58_col7, #T_d5c64_row59_col1, #T_d5c64_row61_col7, #T_d5c64_row68_col1, #T_d5c64_row69_col2, #T_d5c64_row80_col2, #T_d5c64_row98_col3, #T_d5c64_row98_col7, #T_d5c64_row102_col8, #T_d5c64_row105_col0, #T_d5c64_row109_col8, #T_d5c64_row111_col3, #T_d5c64_row114_col8, #T_d5c64_row117_col8, #T_d5c64_row119_col10, #T_d5c64_row126_col0, #T_d5c64_row153_col6, #T_d5c64_row157_col3, #T_d5c64_row160_col0, #T_d5c64_row164_col2, #T_d5c64_row165_col10 {\n",
              "  background-color: #8fb4d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row18_col10, #T_d5c64_row19_col0, #T_d5c64_row21_col6, #T_d5c64_row30_col8, #T_d5c64_row33_col6, #T_d5c64_row43_col1, #T_d5c64_row46_col5, #T_d5c64_row48_col8, #T_d5c64_row58_col8, #T_d5c64_row87_col8, #T_d5c64_row139_col1, #T_d5c64_row143_col2, #T_d5c64_row151_col3, #T_d5c64_row161_col2 {\n",
              "  background-color: #c4cbe3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row19_col1, #T_d5c64_row19_col9, #T_d5c64_row20_col0, #T_d5c64_row22_col9, #T_d5c64_row28_col7, #T_d5c64_row54_col2, #T_d5c64_row58_col9, #T_d5c64_row59_col10, #T_d5c64_row76_col2, #T_d5c64_row89_col9, #T_d5c64_row99_col9, #T_d5c64_row115_col0, #T_d5c64_row135_col10, #T_d5c64_row160_col5, #T_d5c64_row160_col7 {\n",
              "  background-color: #c5cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row19_col8, #T_d5c64_row31_col3, #T_d5c64_row46_col1, #T_d5c64_row46_col7, #T_d5c64_row52_col8, #T_d5c64_row77_col10, #T_d5c64_row90_col7, #T_d5c64_row111_col9, #T_d5c64_row120_col8, #T_d5c64_row157_col1 {\n",
              "  background-color: #b9c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row20_col6, #T_d5c64_row39_col8, #T_d5c64_row46_col4, #T_d5c64_row62_col3, #T_d5c64_row71_col7, #T_d5c64_row73_col4, #T_d5c64_row80_col7, #T_d5c64_row82_col5, #T_d5c64_row88_col3, #T_d5c64_row91_col5, #T_d5c64_row91_col10, #T_d5c64_row92_col5, #T_d5c64_row96_col2, #T_d5c64_row104_col2, #T_d5c64_row104_col3, #T_d5c64_row109_col5, #T_d5c64_row123_col9, #T_d5c64_row125_col6, #T_d5c64_row160_col9 {\n",
              "  background-color: #7eadd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row20_col9, #T_d5c64_row23_col8, #T_d5c64_row25_col6, #T_d5c64_row28_col0, #T_d5c64_row28_col1, #T_d5c64_row35_col0, #T_d5c64_row72_col6, #T_d5c64_row78_col9, #T_d5c64_row83_col6, #T_d5c64_row85_col0, #T_d5c64_row87_col2, #T_d5c64_row87_col10, #T_d5c64_row161_col3 {\n",
              "  background-color: #d5d5e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row21_col0, #T_d5c64_row23_col2, #T_d5c64_row28_col3, #T_d5c64_row28_col8, #T_d5c64_row34_col7, #T_d5c64_row35_col3, #T_d5c64_row53_col10, #T_d5c64_row80_col0, #T_d5c64_row84_col3, #T_d5c64_row86_col3, #T_d5c64_row95_col4, #T_d5c64_row100_col4, #T_d5c64_row107_col2, #T_d5c64_row131_col0, #T_d5c64_row160_col6 {\n",
              "  background-color: #c1cae2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row22_col8, #T_d5c64_row23_col9, #T_d5c64_row27_col1, #T_d5c64_row27_col10, #T_d5c64_row34_col0, #T_d5c64_row43_col10, #T_d5c64_row55_col2, #T_d5c64_row55_col9, #T_d5c64_row65_col6, #T_d5c64_row78_col0, #T_d5c64_row99_col0, #T_d5c64_row115_col10, #T_d5c64_row136_col7, #T_d5c64_row144_col3, #T_d5c64_row158_col6 {\n",
              "  background-color: #d2d2e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row28_col10, #T_d5c64_row29_col9, #T_d5c64_row31_col1, #T_d5c64_row31_col5, #T_d5c64_row36_col2, #T_d5c64_row43_col8, #T_d5c64_row48_col0, #T_d5c64_row50_col3, #T_d5c64_row52_col2, #T_d5c64_row53_col1, #T_d5c64_row53_col2, #T_d5c64_row54_col5, #T_d5c64_row107_col8, #T_d5c64_row127_col0, #T_d5c64_row135_col6, #T_d5c64_row139_col5, #T_d5c64_row147_col3, #T_d5c64_row163_col4, #T_d5c64_row166_col0 {\n",
              "  background-color: #d0d1e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row29_col5, #T_d5c64_row32_col3, #T_d5c64_row36_col3, #T_d5c64_row40_col7, #T_d5c64_row50_col5, #T_d5c64_row52_col5, #T_d5c64_row58_col2, #T_d5c64_row59_col4, #T_d5c64_row81_col6, #T_d5c64_row93_col6, #T_d5c64_row112_col5, #T_d5c64_row112_col7, #T_d5c64_row114_col4, #T_d5c64_row120_col0, #T_d5c64_row135_col3, #T_d5c64_row150_col1 {\n",
              "  background-color: #b5c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row30_col3, #T_d5c64_row33_col5, #T_d5c64_row51_col7, #T_d5c64_row57_col9, #T_d5c64_row65_col3, #T_d5c64_row71_col8, #T_d5c64_row106_col2, #T_d5c64_row106_col3, #T_d5c64_row110_col9, #T_d5c64_row137_col4, #T_d5c64_row142_col1, #T_d5c64_row142_col2, #T_d5c64_row145_col10, #T_d5c64_row160_col4, #T_d5c64_row163_col1, #T_d5c64_row163_col5 {\n",
              "  background-color: #dedcec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row31_col0, #T_d5c64_row131_col4, #T_d5c64_row145_col4 {\n",
              "  background-color: #e7e3f0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row39_col2 {\n",
              "  background-color: #73a9cf;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row40_col3, #T_d5c64_row89_col3, #T_d5c64_row93_col8, #T_d5c64_row114_col5 {\n",
              "  background-color: #b7c5df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row41_col5, #T_d5c64_row58_col4, #T_d5c64_row60_col1, #T_d5c64_row70_col4, #T_d5c64_row86_col1, #T_d5c64_row87_col1, #T_d5c64_row89_col7, #T_d5c64_row98_col0, #T_d5c64_row107_col6, #T_d5c64_row114_col0, #T_d5c64_row137_col5, #T_d5c64_row141_col8, #T_d5c64_row153_col1 {\n",
              "  background-color: #a9bfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row43_col7, #T_d5c64_row46_col9, #T_d5c64_row52_col10, #T_d5c64_row76_col8, #T_d5c64_row86_col7, #T_d5c64_row86_col8, #T_d5c64_row87_col7, #T_d5c64_row103_col1, #T_d5c64_row110_col10, #T_d5c64_row135_col0, #T_d5c64_row166_col9 {\n",
              "  background-color: #c0c9e2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row53_col0, #T_d5c64_row124_col4, #T_d5c64_row159_col4 {\n",
              "  background-color: #f2ecf5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row61_col4 {\n",
              "  background-color: #eae6f1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row96_col4, #T_d5c64_row103_col4 {\n",
              "  background-color: #efe9f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row110_col4, #T_d5c64_row152_col1, #T_d5c64_row166_col4 {\n",
              "  background-color: #e5e1ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row117_col4 {\n",
              "  background-color: #ede8f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d5c64_row152_col4 {\n",
              "  background-color: #f5eff6;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_d5c64\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank\" >&nbsp;</th>\n",
              "      <th class=\"blank\" >&nbsp;</th>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_d5c64_level0_col0\" class=\"col_heading level0 col0\" >ar</th>\n",
              "      <th id=\"T_d5c64_level0_col1\" class=\"col_heading level0 col1\" >ca</th>\n",
              "      <th id=\"T_d5c64_level0_col2\" class=\"col_heading level0 col2\" >cs</th>\n",
              "      <th id=\"T_d5c64_level0_col3\" class=\"col_heading level0 col3\" >de</th>\n",
              "      <th id=\"T_d5c64_level0_col4\" class=\"col_heading level0 col4\" >en</th>\n",
              "      <th id=\"T_d5c64_level0_col5\" class=\"col_heading level0 col5\" >es</th>\n",
              "      <th id=\"T_d5c64_level0_col6\" class=\"col_heading level0 col6\" >nl</th>\n",
              "      <th id=\"T_d5c64_level0_col7\" class=\"col_heading level0 col7\" >pt</th>\n",
              "      <th id=\"T_d5c64_level0_col8\" class=\"col_heading level0 col8\" >ru</th>\n",
              "      <th id=\"T_d5c64_level0_col9\" class=\"col_heading level0 col9\" >uk</th>\n",
              "      <th id=\"T_d5c64_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Train Language</th>\n",
              "      <th class=\"index_name level1\" >Train LLM</th>\n",
              "      <th class=\"index_name level2\" >Model</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "      <th class=\"blank col7\" >&nbsp;</th>\n",
              "      <th class=\"blank col8\" >&nbsp;</th>\n",
              "      <th class=\"blank col9\" >&nbsp;</th>\n",
              "      <th class=\"blank col10\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"56\">en</th>\n",
              "      <th id=\"T_d5c64_level1_row0\" class=\"row_heading level1 row0\" rowspan=\"7\">alpaca-lora-30b</th>\n",
              "      <th id=\"T_d5c64_level2_row0\" class=\"row_heading level2 row0\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row0_col0\" class=\"data row0 col0\" >0.5375</td>\n",
              "      <td id=\"T_d5c64_row0_col1\" class=\"data row0 col1\" >0.8271</td>\n",
              "      <td id=\"T_d5c64_row0_col2\" class=\"data row0 col2\" >0.8546</td>\n",
              "      <td id=\"T_d5c64_row0_col3\" class=\"data row0 col3\" >0.8917</td>\n",
              "      <td id=\"T_d5c64_row0_col4\" class=\"data row0 col4\" >0.9567</td>\n",
              "      <td id=\"T_d5c64_row0_col5\" class=\"data row0 col5\" >0.7517</td>\n",
              "      <td id=\"T_d5c64_row0_col6\" class=\"data row0 col6\" >0.8563</td>\n",
              "      <td id=\"T_d5c64_row0_col7\" class=\"data row0 col7\" >0.8055</td>\n",
              "      <td id=\"T_d5c64_row0_col8\" class=\"data row0 col8\" >0.8374</td>\n",
              "      <td id=\"T_d5c64_row0_col9\" class=\"data row0 col9\" >0.8091</td>\n",
              "      <td id=\"T_d5c64_row0_col10\" class=\"data row0 col10\" >0.5537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row1\" class=\"row_heading level2 row1\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row1_col0\" class=\"data row1 col0\" >0.5270</td>\n",
              "      <td id=\"T_d5c64_row1_col1\" class=\"data row1 col1\" >0.4929</td>\n",
              "      <td id=\"T_d5c64_row1_col2\" class=\"data row1 col2\" >0.4819</td>\n",
              "      <td id=\"T_d5c64_row1_col3\" class=\"data row1 col3\" >0.3956</td>\n",
              "      <td id=\"T_d5c64_row1_col4\" class=\"data row1 col4\" >0.9783</td>\n",
              "      <td id=\"T_d5c64_row1_col5\" class=\"data row1 col5\" >0.5856</td>\n",
              "      <td id=\"T_d5c64_row1_col6\" class=\"data row1 col6\" >0.5448</td>\n",
              "      <td id=\"T_d5c64_row1_col7\" class=\"data row1 col7\" >0.5196</td>\n",
              "      <td id=\"T_d5c64_row1_col8\" class=\"data row1 col8\" >0.5153</td>\n",
              "      <td id=\"T_d5c64_row1_col9\" class=\"data row1 col9\" >0.4215</td>\n",
              "      <td id=\"T_d5c64_row1_col10\" class=\"data row1 col10\" >0.4642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row2\" class=\"row_heading level2 row2\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row2_col0\" class=\"data row2 col0\" >0.7438</td>\n",
              "      <td id=\"T_d5c64_row2_col1\" class=\"data row2 col1\" >0.4252</td>\n",
              "      <td id=\"T_d5c64_row2_col2\" class=\"data row2 col2\" >0.4513</td>\n",
              "      <td id=\"T_d5c64_row2_col3\" class=\"data row2 col3\" >0.3928</td>\n",
              "      <td id=\"T_d5c64_row2_col4\" class=\"data row2 col4\" >0.9657</td>\n",
              "      <td id=\"T_d5c64_row2_col5\" class=\"data row2 col5\" >0.4063</td>\n",
              "      <td id=\"T_d5c64_row2_col6\" class=\"data row2 col6\" >0.4233</td>\n",
              "      <td id=\"T_d5c64_row2_col7\" class=\"data row2 col7\" >0.3719</td>\n",
              "      <td id=\"T_d5c64_row2_col8\" class=\"data row2 col8\" >0.5547</td>\n",
              "      <td id=\"T_d5c64_row2_col9\" class=\"data row2 col9\" >0.5326</td>\n",
              "      <td id=\"T_d5c64_row2_col10\" class=\"data row2 col10\" >0.3742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row3\" class=\"row_heading level2 row3\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row3_col0\" class=\"data row3 col0\" >0.4024</td>\n",
              "      <td id=\"T_d5c64_row3_col1\" class=\"data row3 col1\" >0.8089</td>\n",
              "      <td id=\"T_d5c64_row3_col2\" class=\"data row3 col2\" >0.6132</td>\n",
              "      <td id=\"T_d5c64_row3_col3\" class=\"data row3 col3\" >0.8763</td>\n",
              "      <td id=\"T_d5c64_row3_col4\" class=\"data row3 col4\" >0.9639</td>\n",
              "      <td id=\"T_d5c64_row3_col5\" class=\"data row3 col5\" >0.7390</td>\n",
              "      <td id=\"T_d5c64_row3_col6\" class=\"data row3 col6\" >0.8791</td>\n",
              "      <td id=\"T_d5c64_row3_col7\" class=\"data row3 col7\" >0.8333</td>\n",
              "      <td id=\"T_d5c64_row3_col8\" class=\"data row3 col8\" >0.8162</td>\n",
              "      <td id=\"T_d5c64_row3_col9\" class=\"data row3 col9\" >0.8210</td>\n",
              "      <td id=\"T_d5c64_row3_col10\" class=\"data row3 col10\" >0.4626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row4\" class=\"row_heading level2 row4\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row4_col0\" class=\"data row4 col0\" >0.2080</td>\n",
              "      <td id=\"T_d5c64_row4_col1\" class=\"data row4 col1\" >0.8592</td>\n",
              "      <td id=\"T_d5c64_row4_col2\" class=\"data row4 col2\" >0.7691</td>\n",
              "      <td id=\"T_d5c64_row4_col3\" class=\"data row4 col3\" >0.9003</td>\n",
              "      <td id=\"T_d5c64_row4_col4\" class=\"data row4 col4\" >0.9439</td>\n",
              "      <td id=\"T_d5c64_row4_col5\" class=\"data row4 col5\" >0.7744</td>\n",
              "      <td id=\"T_d5c64_row4_col6\" class=\"data row4 col6\" >0.8977</td>\n",
              "      <td id=\"T_d5c64_row4_col7\" class=\"data row4 col7\" >0.8570</td>\n",
              "      <td id=\"T_d5c64_row4_col8\" class=\"data row4 col8\" >0.7988</td>\n",
              "      <td id=\"T_d5c64_row4_col9\" class=\"data row4 col9\" >0.7563</td>\n",
              "      <td id=\"T_d5c64_row4_col10\" class=\"data row4 col10\" >0.3099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row5\" class=\"row_heading level2 row5\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row5_col0\" class=\"data row5 col0\" >0.3484</td>\n",
              "      <td id=\"T_d5c64_row5_col1\" class=\"data row5 col1\" >0.6258</td>\n",
              "      <td id=\"T_d5c64_row5_col2\" class=\"data row5 col2\" >0.5430</td>\n",
              "      <td id=\"T_d5c64_row5_col3\" class=\"data row5 col3\" >0.5133</td>\n",
              "      <td id=\"T_d5c64_row5_col4\" class=\"data row5 col4\" >0.9238</td>\n",
              "      <td id=\"T_d5c64_row5_col5\" class=\"data row5 col5\" >0.4809</td>\n",
              "      <td id=\"T_d5c64_row5_col6\" class=\"data row5 col6\" >0.7787</td>\n",
              "      <td id=\"T_d5c64_row5_col7\" class=\"data row5 col7\" >0.4348</td>\n",
              "      <td id=\"T_d5c64_row5_col8\" class=\"data row5 col8\" >0.4029</td>\n",
              "      <td id=\"T_d5c64_row5_col9\" class=\"data row5 col9\" >0.4838</td>\n",
              "      <td id=\"T_d5c64_row5_col10\" class=\"data row5 col10\" >0.4487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row6\" class=\"row_heading level2 row6\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row6_col0\" class=\"data row6 col0\" >0.4474</td>\n",
              "      <td id=\"T_d5c64_row6_col1\" class=\"data row6 col1\" >0.8488</td>\n",
              "      <td id=\"T_d5c64_row6_col2\" class=\"data row6 col2\" >0.8713</td>\n",
              "      <td id=\"T_d5c64_row6_col3\" class=\"data row6 col3\" >0.9324</td>\n",
              "      <td id=\"T_d5c64_row6_col4\" class=\"data row6 col4\" >0.9801</td>\n",
              "      <td id=\"T_d5c64_row6_col5\" class=\"data row6 col5\" >0.6319</td>\n",
              "      <td id=\"T_d5c64_row6_col6\" class=\"data row6 col6\" >0.7706</td>\n",
              "      <td id=\"T_d5c64_row6_col7\" class=\"data row6 col7\" >0.7130</td>\n",
              "      <td id=\"T_d5c64_row6_col8\" class=\"data row6 col8\" >0.8733</td>\n",
              "      <td id=\"T_d5c64_row6_col9\" class=\"data row6 col9\" >0.8319</td>\n",
              "      <td id=\"T_d5c64_row6_col10\" class=\"data row6 col10\" >0.4474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row7\" class=\"row_heading level1 row7\" rowspan=\"7\">gpt-3.5-turbo</th>\n",
              "      <th id=\"T_d5c64_level2_row7\" class=\"row_heading level2 row7\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row7_col0\" class=\"data row7 col0\" >0.9215</td>\n",
              "      <td id=\"T_d5c64_row7_col1\" class=\"data row7 col1\" >0.8904</td>\n",
              "      <td id=\"T_d5c64_row7_col2\" class=\"data row7 col2\" >0.9150</td>\n",
              "      <td id=\"T_d5c64_row7_col3\" class=\"data row7 col3\" >0.9020</td>\n",
              "      <td id=\"T_d5c64_row7_col4\" class=\"data row7 col4\" >0.9783</td>\n",
              "      <td id=\"T_d5c64_row7_col5\" class=\"data row7 col5\" >0.8545</td>\n",
              "      <td id=\"T_d5c64_row7_col6\" class=\"data row7 col6\" >0.9348</td>\n",
              "      <td id=\"T_d5c64_row7_col7\" class=\"data row7 col7\" >0.9124</td>\n",
              "      <td id=\"T_d5c64_row7_col8\" class=\"data row7 col8\" >0.9183</td>\n",
              "      <td id=\"T_d5c64_row7_col9\" class=\"data row7 col9\" >0.8962</td>\n",
              "      <td id=\"T_d5c64_row7_col10\" class=\"data row7 col10\" >0.8933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row8\" class=\"row_heading level2 row8\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row8_col0\" class=\"data row8 col0\" >0.3985</td>\n",
              "      <td id=\"T_d5c64_row8_col1\" class=\"data row8 col1\" >0.8409</td>\n",
              "      <td id=\"T_d5c64_row8_col2\" class=\"data row8 col2\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row8_col3\" class=\"data row8 col3\" >0.3576</td>\n",
              "      <td id=\"T_d5c64_row8_col4\" class=\"data row8 col4\" >0.9765</td>\n",
              "      <td id=\"T_d5c64_row8_col5\" class=\"data row8 col5\" >0.8243</td>\n",
              "      <td id=\"T_d5c64_row8_col6\" class=\"data row8 col6\" >0.7355</td>\n",
              "      <td id=\"T_d5c64_row8_col7\" class=\"data row8 col7\" >0.7810</td>\n",
              "      <td id=\"T_d5c64_row8_col8\" class=\"data row8 col8\" >0.3849</td>\n",
              "      <td id=\"T_d5c64_row8_col9\" class=\"data row8 col9\" >0.3526</td>\n",
              "      <td id=\"T_d5c64_row8_col10\" class=\"data row8 col10\" >0.3399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row9\" class=\"row_heading level2 row9\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row9_col0\" class=\"data row9 col0\" >0.3673</td>\n",
              "      <td id=\"T_d5c64_row9_col1\" class=\"data row9 col1\" >0.3407</td>\n",
              "      <td id=\"T_d5c64_row9_col2\" class=\"data row9 col2\" >0.3443</td>\n",
              "      <td id=\"T_d5c64_row9_col3\" class=\"data row9 col3\" >0.3369</td>\n",
              "      <td id=\"T_d5c64_row9_col4\" class=\"data row9 col4\" >0.9838</td>\n",
              "      <td id=\"T_d5c64_row9_col5\" class=\"data row9 col5\" >0.3264</td>\n",
              "      <td id=\"T_d5c64_row9_col6\" class=\"data row9 col6\" >0.3432</td>\n",
              "      <td id=\"T_d5c64_row9_col7\" class=\"data row9 col7\" >0.3378</td>\n",
              "      <td id=\"T_d5c64_row9_col8\" class=\"data row9 col8\" >0.3689</td>\n",
              "      <td id=\"T_d5c64_row9_col9\" class=\"data row9 col9\" >0.3710</td>\n",
              "      <td id=\"T_d5c64_row9_col10\" class=\"data row9 col10\" >0.3428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row10\" class=\"row_heading level2 row10\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row10_col0\" class=\"data row10 col0\" >0.7009</td>\n",
              "      <td id=\"T_d5c64_row10_col1\" class=\"data row10 col1\" >0.9266</td>\n",
              "      <td id=\"T_d5c64_row10_col2\" class=\"data row10 col2\" >0.3370</td>\n",
              "      <td id=\"T_d5c64_row10_col3\" class=\"data row10 col3\" >0.9071</td>\n",
              "      <td id=\"T_d5c64_row10_col4\" class=\"data row10 col4\" >0.9892</td>\n",
              "      <td id=\"T_d5c64_row10_col5\" class=\"data row10 col5\" >0.8506</td>\n",
              "      <td id=\"T_d5c64_row10_col6\" class=\"data row10 col6\" >0.9432</td>\n",
              "      <td id=\"T_d5c64_row10_col7\" class=\"data row10 col7\" >0.9147</td>\n",
              "      <td id=\"T_d5c64_row10_col8\" class=\"data row10 col8\" >0.9013</td>\n",
              "      <td id=\"T_d5c64_row10_col9\" class=\"data row10 col9\" >0.9080</td>\n",
              "      <td id=\"T_d5c64_row10_col10\" class=\"data row10 col10\" >0.5610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row11\" class=\"row_heading level2 row11\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row11_col0\" class=\"data row11 col0\" >0.7876</td>\n",
              "      <td id=\"T_d5c64_row11_col1\" class=\"data row11 col1\" >0.5599</td>\n",
              "      <td id=\"T_d5c64_row11_col2\" class=\"data row11 col2\" >0.7803</td>\n",
              "      <td id=\"T_d5c64_row11_col3\" class=\"data row11 col3\" >0.8052</td>\n",
              "      <td id=\"T_d5c64_row11_col4\" class=\"data row11 col4\" >0.7178</td>\n",
              "      <td id=\"T_d5c64_row11_col5\" class=\"data row11 col5\" >0.6023</td>\n",
              "      <td id=\"T_d5c64_row11_col6\" class=\"data row11 col6\" >0.7790</td>\n",
              "      <td id=\"T_d5c64_row11_col7\" class=\"data row11 col7\" >0.6782</td>\n",
              "      <td id=\"T_d5c64_row11_col8\" class=\"data row11 col8\" >0.6484</td>\n",
              "      <td id=\"T_d5c64_row11_col9\" class=\"data row11 col9\" >0.7649</td>\n",
              "      <td id=\"T_d5c64_row11_col10\" class=\"data row11 col10\" >0.6733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row12\" class=\"row_heading level2 row12\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row12_col0\" class=\"data row12 col0\" >0.4174</td>\n",
              "      <td id=\"T_d5c64_row12_col1\" class=\"data row12 col1\" >0.6710</td>\n",
              "      <td id=\"T_d5c64_row12_col2\" class=\"data row12 col2\" >0.5623</td>\n",
              "      <td id=\"T_d5c64_row12_col3\" class=\"data row12 col3\" >0.5476</td>\n",
              "      <td id=\"T_d5c64_row12_col4\" class=\"data row12 col4\" >0.9166</td>\n",
              "      <td id=\"T_d5c64_row12_col5\" class=\"data row12 col5\" >0.5356</td>\n",
              "      <td id=\"T_d5c64_row12_col6\" class=\"data row12 col6\" >0.8203</td>\n",
              "      <td id=\"T_d5c64_row12_col7\" class=\"data row12 col7\" >0.4381</td>\n",
              "      <td id=\"T_d5c64_row12_col8\" class=\"data row12 col8\" >0.5442</td>\n",
              "      <td id=\"T_d5c64_row12_col9\" class=\"data row12 col9\" >0.6236</td>\n",
              "      <td id=\"T_d5c64_row12_col10\" class=\"data row12 col10\" >0.4727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row13\" class=\"row_heading level2 row13\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row13_col0\" class=\"data row13 col0\" >0.7218</td>\n",
              "      <td id=\"T_d5c64_row13_col1\" class=\"data row13 col1\" >0.9733</td>\n",
              "      <td id=\"T_d5c64_row13_col2\" class=\"data row13 col2\" >0.7211</td>\n",
              "      <td id=\"T_d5c64_row13_col3\" class=\"data row13 col3\" >0.9392</td>\n",
              "      <td id=\"T_d5c64_row13_col4\" class=\"data row13 col4\" >0.9838</td>\n",
              "      <td id=\"T_d5c64_row13_col5\" class=\"data row13 col5\" >0.7861</td>\n",
              "      <td id=\"T_d5c64_row13_col6\" class=\"data row13 col6\" >0.9229</td>\n",
              "      <td id=\"T_d5c64_row13_col7\" class=\"data row13 col7\" >0.9281</td>\n",
              "      <td id=\"T_d5c64_row13_col8\" class=\"data row13 col8\" >0.8839</td>\n",
              "      <td id=\"T_d5c64_row13_col9\" class=\"data row13 col9\" >0.6290</td>\n",
              "      <td id=\"T_d5c64_row13_col10\" class=\"data row13 col10\" >0.5155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row14\" class=\"row_heading level1 row14\" rowspan=\"7\">gpt-4</th>\n",
              "      <th id=\"T_d5c64_level2_row14\" class=\"row_heading level2 row14\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row14_col0\" class=\"data row14 col0\" >0.8419</td>\n",
              "      <td id=\"T_d5c64_row14_col1\" class=\"data row14 col1\" >0.9265</td>\n",
              "      <td id=\"T_d5c64_row14_col2\" class=\"data row14 col2\" >0.7592</td>\n",
              "      <td id=\"T_d5c64_row14_col3\" class=\"data row14 col3\" >0.8163</td>\n",
              "      <td id=\"T_d5c64_row14_col4\" class=\"data row14 col4\" >0.9765</td>\n",
              "      <td id=\"T_d5c64_row14_col5\" class=\"data row14 col5\" >0.8261</td>\n",
              "      <td id=\"T_d5c64_row14_col6\" class=\"data row14 col6\" >0.9265</td>\n",
              "      <td id=\"T_d5c64_row14_col7\" class=\"data row14 col7\" >0.8704</td>\n",
              "      <td id=\"T_d5c64_row14_col8\" class=\"data row14 col8\" >0.7929</td>\n",
              "      <td id=\"T_d5c64_row14_col9\" class=\"data row14 col9\" >0.7104</td>\n",
              "      <td id=\"T_d5c64_row14_col10\" class=\"data row14 col10\" >0.8360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row15\" class=\"row_heading level2 row15\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row15_col0\" class=\"data row15 col0\" >0.4448</td>\n",
              "      <td id=\"T_d5c64_row15_col1\" class=\"data row15 col1\" >0.5611</td>\n",
              "      <td id=\"T_d5c64_row15_col2\" class=\"data row15 col2\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row15_col3\" class=\"data row15 col3\" >0.3303</td>\n",
              "      <td id=\"T_d5c64_row15_col4\" class=\"data row15 col4\" >0.9928</td>\n",
              "      <td id=\"T_d5c64_row15_col5\" class=\"data row15 col5\" >0.7723</td>\n",
              "      <td id=\"T_d5c64_row15_col6\" class=\"data row15 col6\" >0.4206</td>\n",
              "      <td id=\"T_d5c64_row15_col7\" class=\"data row15 col7\" >0.6814</td>\n",
              "      <td id=\"T_d5c64_row15_col8\" class=\"data row15 col8\" >0.4044</td>\n",
              "      <td id=\"T_d5c64_row15_col9\" class=\"data row15 col9\" >0.3697</td>\n",
              "      <td id=\"T_d5c64_row15_col10\" class=\"data row15 col10\" >0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row16\" class=\"row_heading level2 row16\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row16_col0\" class=\"data row16 col0\" >0.3853</td>\n",
              "      <td id=\"T_d5c64_row16_col1\" class=\"data row16 col1\" >0.3658</td>\n",
              "      <td id=\"T_d5c64_row16_col2\" class=\"data row16 col2\" >0.3516</td>\n",
              "      <td id=\"T_d5c64_row16_col3\" class=\"data row16 col3\" >0.3593</td>\n",
              "      <td id=\"T_d5c64_row16_col4\" class=\"data row16 col4\" >0.9928</td>\n",
              "      <td id=\"T_d5c64_row16_col5\" class=\"data row16 col5\" >0.3659</td>\n",
              "      <td id=\"T_d5c64_row16_col6\" class=\"data row16 col6\" >0.3887</td>\n",
              "      <td id=\"T_d5c64_row16_col7\" class=\"data row16 col7\" >0.3486</td>\n",
              "      <td id=\"T_d5c64_row16_col8\" class=\"data row16 col8\" >0.3969</td>\n",
              "      <td id=\"T_d5c64_row16_col9\" class=\"data row16 col9\" >0.3732</td>\n",
              "      <td id=\"T_d5c64_row16_col10\" class=\"data row16 col10\" >0.3318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row17\" class=\"row_heading level2 row17\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row17_col0\" class=\"data row17 col0\" >0.6945</td>\n",
              "      <td id=\"T_d5c64_row17_col1\" class=\"data row17 col1\" >0.7888</td>\n",
              "      <td id=\"T_d5c64_row17_col2\" class=\"data row17 col2\" >0.3866</td>\n",
              "      <td id=\"T_d5c64_row17_col3\" class=\"data row17 col3\" >0.8365</td>\n",
              "      <td id=\"T_d5c64_row17_col4\" class=\"data row17 col4\" >0.9910</td>\n",
              "      <td id=\"T_d5c64_row17_col5\" class=\"data row17 col5\" >0.7954</td>\n",
              "      <td id=\"T_d5c64_row17_col6\" class=\"data row17 col6\" >0.8334</td>\n",
              "      <td id=\"T_d5c64_row17_col7\" class=\"data row17 col7\" >0.8658</td>\n",
              "      <td id=\"T_d5c64_row17_col8\" class=\"data row17 col8\" >0.8008</td>\n",
              "      <td id=\"T_d5c64_row17_col9\" class=\"data row17 col9\" >0.7849</td>\n",
              "      <td id=\"T_d5c64_row17_col10\" class=\"data row17 col10\" >0.4734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row18\" class=\"row_heading level2 row18\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row18_col0\" class=\"data row18 col0\" >0.7199</td>\n",
              "      <td id=\"T_d5c64_row18_col1\" class=\"data row18 col1\" >0.9733</td>\n",
              "      <td id=\"T_d5c64_row18_col2\" class=\"data row18 col2\" >0.8626</td>\n",
              "      <td id=\"T_d5c64_row18_col3\" class=\"data row18 col3\" >0.8944</td>\n",
              "      <td id=\"T_d5c64_row18_col4\" class=\"data row18 col4\" >0.8255</td>\n",
              "      <td id=\"T_d5c64_row18_col5\" class=\"data row18 col5\" >0.9435</td>\n",
              "      <td id=\"T_d5c64_row18_col6\" class=\"data row18 col6\" >0.9331</td>\n",
              "      <td id=\"T_d5c64_row18_col7\" class=\"data row18 col7\" >0.9319</td>\n",
              "      <td id=\"T_d5c64_row18_col8\" class=\"data row18 col8\" >0.8395</td>\n",
              "      <td id=\"T_d5c64_row18_col9\" class=\"data row18 col9\" >0.7668</td>\n",
              "      <td id=\"T_d5c64_row18_col10\" class=\"data row18 col10\" >0.5706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row19\" class=\"row_heading level2 row19\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row19_col0\" class=\"data row19 col0\" >0.5765</td>\n",
              "      <td id=\"T_d5c64_row19_col1\" class=\"data row19 col1\" >0.5661</td>\n",
              "      <td id=\"T_d5c64_row19_col2\" class=\"data row19 col2\" >0.4808</td>\n",
              "      <td id=\"T_d5c64_row19_col3\" class=\"data row19 col3\" >0.5395</td>\n",
              "      <td id=\"T_d5c64_row19_col4\" class=\"data row19 col4\" >0.9765</td>\n",
              "      <td id=\"T_d5c64_row19_col5\" class=\"data row19 col5\" >0.4974</td>\n",
              "      <td id=\"T_d5c64_row19_col6\" class=\"data row19 col6\" >0.7401</td>\n",
              "      <td id=\"T_d5c64_row19_col7\" class=\"data row19 col7\" >0.4077</td>\n",
              "      <td id=\"T_d5c64_row19_col8\" class=\"data row19 col8\" >0.6384</td>\n",
              "      <td id=\"T_d5c64_row19_col9\" class=\"data row19 col9\" >0.5689</td>\n",
              "      <td id=\"T_d5c64_row19_col10\" class=\"data row19 col10\" >0.4995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row20\" class=\"row_heading level2 row20\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row20_col0\" class=\"data row20 col0\" >0.5659</td>\n",
              "      <td id=\"T_d5c64_row20_col1\" class=\"data row20 col1\" >0.9348</td>\n",
              "      <td id=\"T_d5c64_row20_col2\" class=\"data row20 col2\" >0.4987</td>\n",
              "      <td id=\"T_d5c64_row20_col3\" class=\"data row20 col3\" >0.8927</td>\n",
              "      <td id=\"T_d5c64_row20_col4\" class=\"data row20 col4\" >0.9892</td>\n",
              "      <td id=\"T_d5c64_row20_col5\" class=\"data row20 col5\" >0.8592</td>\n",
              "      <td id=\"T_d5c64_row20_col6\" class=\"data row20 col6\" >0.9499</td>\n",
              "      <td id=\"T_d5c64_row20_col7\" class=\"data row20 col7\" >0.9267</td>\n",
              "      <td id=\"T_d5c64_row20_col8\" class=\"data row20 col8\" >0.7676</td>\n",
              "      <td id=\"T_d5c64_row20_col9\" class=\"data row20 col9\" >0.4567</td>\n",
              "      <td id=\"T_d5c64_row20_col10\" class=\"data row20 col10\" >0.3587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row21\" class=\"row_heading level1 row21\" rowspan=\"7\">llama-65b</th>\n",
              "      <th id=\"T_d5c64_level2_row21\" class=\"row_heading level2 row21\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row21_col0\" class=\"data row21 col0\" >0.5896</td>\n",
              "      <td id=\"T_d5c64_row21_col1\" class=\"data row21 col1\" >0.6070</td>\n",
              "      <td id=\"T_d5c64_row21_col2\" class=\"data row21 col2\" >0.6955</td>\n",
              "      <td id=\"T_d5c64_row21_col3\" class=\"data row21 col3\" >0.6895</td>\n",
              "      <td id=\"T_d5c64_row21_col4\" class=\"data row21 col4\" >0.9017</td>\n",
              "      <td id=\"T_d5c64_row21_col5\" class=\"data row21 col5\" >0.7998</td>\n",
              "      <td id=\"T_d5c64_row21_col6\" class=\"data row21 col6\" >0.5771</td>\n",
              "      <td id=\"T_d5c64_row21_col7\" class=\"data row21 col7\" >0.6752</td>\n",
              "      <td id=\"T_d5c64_row21_col8\" class=\"data row21 col8\" >0.7032</td>\n",
              "      <td id=\"T_d5c64_row21_col9\" class=\"data row21 col9\" >0.7020</td>\n",
              "      <td id=\"T_d5c64_row21_col10\" class=\"data row21 col10\" >0.5194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row22\" class=\"row_heading level2 row22\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row22_col0\" class=\"data row22 col0\" >0.6770</td>\n",
              "      <td id=\"T_d5c64_row22_col1\" class=\"data row22 col1\" >0.3311</td>\n",
              "      <td id=\"T_d5c64_row22_col2\" class=\"data row22 col2\" >0.3299</td>\n",
              "      <td id=\"T_d5c64_row22_col3\" class=\"data row22 col3\" >0.3386</td>\n",
              "      <td id=\"T_d5c64_row22_col4\" class=\"data row22 col4\" >0.9220</td>\n",
              "      <td id=\"T_d5c64_row22_col5\" class=\"data row22 col5\" >0.3613</td>\n",
              "      <td id=\"T_d5c64_row22_col6\" class=\"data row22 col6\" >0.3322</td>\n",
              "      <td id=\"T_d5c64_row22_col7\" class=\"data row22 col7\" >0.3538</td>\n",
              "      <td id=\"T_d5c64_row22_col8\" class=\"data row22 col8\" >0.4911</td>\n",
              "      <td id=\"T_d5c64_row22_col9\" class=\"data row22 col9\" >0.5700</td>\n",
              "      <td id=\"T_d5c64_row22_col10\" class=\"data row22 col10\" >0.3281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row23\" class=\"row_heading level2 row23\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row23_col0\" class=\"data row23 col0\" >0.3735</td>\n",
              "      <td id=\"T_d5c64_row23_col1\" class=\"data row23 col1\" >0.4296</td>\n",
              "      <td id=\"T_d5c64_row23_col2\" class=\"data row23 col2\" >0.5929</td>\n",
              "      <td id=\"T_d5c64_row23_col3\" class=\"data row23 col3\" >0.5370</td>\n",
              "      <td id=\"T_d5c64_row23_col4\" class=\"data row23 col4\" >0.8854</td>\n",
              "      <td id=\"T_d5c64_row23_col5\" class=\"data row23 col5\" >0.5623</td>\n",
              "      <td id=\"T_d5c64_row23_col6\" class=\"data row23 col6\" >0.5372</td>\n",
              "      <td id=\"T_d5c64_row23_col7\" class=\"data row23 col7\" >0.3950</td>\n",
              "      <td id=\"T_d5c64_row23_col8\" class=\"data row23 col8\" >0.4543</td>\n",
              "      <td id=\"T_d5c64_row23_col9\" class=\"data row23 col9\" >0.4893</td>\n",
              "      <td id=\"T_d5c64_row23_col10\" class=\"data row23 col10\" >0.4342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row24\" class=\"row_heading level2 row24\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row24_col0\" class=\"data row24 col0\" >0.3848</td>\n",
              "      <td id=\"T_d5c64_row24_col1\" class=\"data row24 col1\" >0.3508</td>\n",
              "      <td id=\"T_d5c64_row24_col2\" class=\"data row24 col2\" >0.6898</td>\n",
              "      <td id=\"T_d5c64_row24_col3\" class=\"data row24 col3\" >0.7115</td>\n",
              "      <td id=\"T_d5c64_row24_col4\" class=\"data row24 col4\" >0.9037</td>\n",
              "      <td id=\"T_d5c64_row24_col5\" class=\"data row24 col5\" >0.5248</td>\n",
              "      <td id=\"T_d5c64_row24_col6\" class=\"data row24 col6\" >0.4102</td>\n",
              "      <td id=\"T_d5c64_row24_col7\" class=\"data row24 col7\" >0.4835</td>\n",
              "      <td id=\"T_d5c64_row24_col8\" class=\"data row24 col8\" >0.4669</td>\n",
              "      <td id=\"T_d5c64_row24_col9\" class=\"data row24 col9\" >0.4779</td>\n",
              "      <td id=\"T_d5c64_row24_col10\" class=\"data row24 col10\" >0.5233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row25\" class=\"row_heading level2 row25\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row25_col0\" class=\"data row25 col0\" >0.5115</td>\n",
              "      <td id=\"T_d5c64_row25_col1\" class=\"data row25 col1\" >0.4060</td>\n",
              "      <td id=\"T_d5c64_row25_col2\" class=\"data row25 col2\" >0.8307</td>\n",
              "      <td id=\"T_d5c64_row25_col3\" class=\"data row25 col3\" >0.7162</td>\n",
              "      <td id=\"T_d5c64_row25_col4\" class=\"data row25 col4\" >0.9003</td>\n",
              "      <td id=\"T_d5c64_row25_col5\" class=\"data row25 col5\" >0.6074</td>\n",
              "      <td id=\"T_d5c64_row25_col6\" class=\"data row25 col6\" >0.4591</td>\n",
              "      <td id=\"T_d5c64_row25_col7\" class=\"data row25 col7\" >0.5413</td>\n",
              "      <td id=\"T_d5c64_row25_col8\" class=\"data row25 col8\" >0.7103</td>\n",
              "      <td id=\"T_d5c64_row25_col9\" class=\"data row25 col9\" >0.6693</td>\n",
              "      <td id=\"T_d5c64_row25_col10\" class=\"data row25 col10\" >0.4384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row26\" class=\"row_heading level2 row26\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row26_col0\" class=\"data row26 col0\" >0.3303</td>\n",
              "      <td id=\"T_d5c64_row26_col1\" class=\"data row26 col1\" >0.3528</td>\n",
              "      <td id=\"T_d5c64_row26_col2\" class=\"data row26 col2\" >0.3446</td>\n",
              "      <td id=\"T_d5c64_row26_col3\" class=\"data row26 col3\" >0.3341</td>\n",
              "      <td id=\"T_d5c64_row26_col4\" class=\"data row26 col4\" >0.8890</td>\n",
              "      <td id=\"T_d5c64_row26_col5\" class=\"data row26 col5\" >0.3460</td>\n",
              "      <td id=\"T_d5c64_row26_col6\" class=\"data row26 col6\" >0.3612</td>\n",
              "      <td id=\"T_d5c64_row26_col7\" class=\"data row26 col7\" >0.3371</td>\n",
              "      <td id=\"T_d5c64_row26_col8\" class=\"data row26 col8\" >0.3311</td>\n",
              "      <td id=\"T_d5c64_row26_col9\" class=\"data row26 col9\" >0.3449</td>\n",
              "      <td id=\"T_d5c64_row26_col10\" class=\"data row26 col10\" >0.3332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row27\" class=\"row_heading level2 row27\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row27_col0\" class=\"data row27 col0\" >0.4779</td>\n",
              "      <td id=\"T_d5c64_row27_col1\" class=\"data row27 col1\" >0.4892</td>\n",
              "      <td id=\"T_d5c64_row27_col2\" class=\"data row27 col2\" >0.8507</td>\n",
              "      <td id=\"T_d5c64_row27_col3\" class=\"data row27 col3\" >0.7983</td>\n",
              "      <td id=\"T_d5c64_row27_col4\" class=\"data row27 col4\" >0.9165</td>\n",
              "      <td id=\"T_d5c64_row27_col5\" class=\"data row27 col5\" >0.7241</td>\n",
              "      <td id=\"T_d5c64_row27_col6\" class=\"data row27 col6\" >0.4368</td>\n",
              "      <td id=\"T_d5c64_row27_col7\" class=\"data row27 col7\" >0.6082</td>\n",
              "      <td id=\"T_d5c64_row27_col8\" class=\"data row27 col8\" >0.6899</td>\n",
              "      <td id=\"T_d5c64_row27_col9\" class=\"data row27 col9\" >0.6290</td>\n",
              "      <td id=\"T_d5c64_row27_col10\" class=\"data row27 col10\" >0.4892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row28\" class=\"row_heading level1 row28\" rowspan=\"7\">opt-66b</th>\n",
              "      <th id=\"T_d5c64_level2_row28\" class=\"row_heading level2 row28\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row28_col0\" class=\"data row28 col0\" >0.4557</td>\n",
              "      <td id=\"T_d5c64_row28_col1\" class=\"data row28 col1\" >0.4586</td>\n",
              "      <td id=\"T_d5c64_row28_col2\" class=\"data row28 col2\" >0.6107</td>\n",
              "      <td id=\"T_d5c64_row28_col3\" class=\"data row28 col3\" >0.5890</td>\n",
              "      <td id=\"T_d5c64_row28_col4\" class=\"data row28 col4\" >0.8592</td>\n",
              "      <td id=\"T_d5c64_row28_col5\" class=\"data row28 col5\" >0.5394</td>\n",
              "      <td id=\"T_d5c64_row28_col6\" class=\"data row28 col6\" >0.4268</td>\n",
              "      <td id=\"T_d5c64_row28_col7\" class=\"data row28 col7\" >0.5635</td>\n",
              "      <td id=\"T_d5c64_row28_col8\" class=\"data row28 col8\" >0.5876</td>\n",
              "      <td id=\"T_d5c64_row28_col9\" class=\"data row28 col9\" >0.5594</td>\n",
              "      <td id=\"T_d5c64_row28_col10\" class=\"data row28 col10\" >0.5041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row29\" class=\"row_heading level2 row29\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row29_col0\" class=\"data row29 col0\" >0.5167</td>\n",
              "      <td id=\"T_d5c64_row29_col1\" class=\"data row29 col1\" >0.5157</td>\n",
              "      <td id=\"T_d5c64_row29_col2\" class=\"data row29 col2\" >0.6473</td>\n",
              "      <td id=\"T_d5c64_row29_col3\" class=\"data row29 col3\" >0.5853</td>\n",
              "      <td id=\"T_d5c64_row29_col4\" class=\"data row29 col4\" >0.8899</td>\n",
              "      <td id=\"T_d5c64_row29_col5\" class=\"data row29 col5\" >0.6591</td>\n",
              "      <td id=\"T_d5c64_row29_col6\" class=\"data row29 col6\" >0.5600</td>\n",
              "      <td id=\"T_d5c64_row29_col7\" class=\"data row29 col7\" >0.6993</td>\n",
              "      <td id=\"T_d5c64_row29_col8\" class=\"data row29 col8\" >0.4347</td>\n",
              "      <td id=\"T_d5c64_row29_col9\" class=\"data row29 col9\" >0.5061</td>\n",
              "      <td id=\"T_d5c64_row29_col10\" class=\"data row29 col10\" >0.3273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row30\" class=\"row_heading level2 row30\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row30_col0\" class=\"data row30 col0\" >0.5226</td>\n",
              "      <td id=\"T_d5c64_row30_col1\" class=\"data row30 col1\" >0.4138</td>\n",
              "      <td id=\"T_d5c64_row30_col2\" class=\"data row30 col2\" >0.3720</td>\n",
              "      <td id=\"T_d5c64_row30_col3\" class=\"data row30 col3\" >0.3827</td>\n",
              "      <td id=\"T_d5c64_row30_col4\" class=\"data row30 col4\" >0.8427</td>\n",
              "      <td id=\"T_d5c64_row30_col5\" class=\"data row30 col5\" >0.4202</td>\n",
              "      <td id=\"T_d5c64_row30_col6\" class=\"data row30 col6\" >0.3647</td>\n",
              "      <td id=\"T_d5c64_row30_col7\" class=\"data row30 col7\" >0.4089</td>\n",
              "      <td id=\"T_d5c64_row30_col8\" class=\"data row30 col8\" >0.5713</td>\n",
              "      <td id=\"T_d5c64_row30_col9\" class=\"data row30 col9\" >0.6213</td>\n",
              "      <td id=\"T_d5c64_row30_col10\" class=\"data row30 col10\" >0.4473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row31\" class=\"row_heading level2 row31\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row31_col0\" class=\"data row31 col0\" >0.2959</td>\n",
              "      <td id=\"T_d5c64_row31_col1\" class=\"data row31 col1\" >0.5049</td>\n",
              "      <td id=\"T_d5c64_row31_col2\" class=\"data row31 col2\" >0.3954</td>\n",
              "      <td id=\"T_d5c64_row31_col3\" class=\"data row31 col3\" >0.6351</td>\n",
              "      <td id=\"T_d5c64_row31_col4\" class=\"data row31 col4\" >0.8931</td>\n",
              "      <td id=\"T_d5c64_row31_col5\" class=\"data row31 col5\" >0.5064</td>\n",
              "      <td id=\"T_d5c64_row31_col6\" class=\"data row31 col6\" >0.4942</td>\n",
              "      <td id=\"T_d5c64_row31_col7\" class=\"data row31 col7\" >0.6017</td>\n",
              "      <td id=\"T_d5c64_row31_col8\" class=\"data row31 col8\" >0.4416</td>\n",
              "      <td id=\"T_d5c64_row31_col9\" class=\"data row31 col9\" >0.4645</td>\n",
              "      <td id=\"T_d5c64_row31_col10\" class=\"data row31 col10\" >0.4161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row32\" class=\"row_heading level2 row32\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row32_col0\" class=\"data row32 col0\" >0.3903</td>\n",
              "      <td id=\"T_d5c64_row32_col1\" class=\"data row32 col1\" >0.5181</td>\n",
              "      <td id=\"T_d5c64_row32_col2\" class=\"data row32 col2\" >0.6278</td>\n",
              "      <td id=\"T_d5c64_row32_col3\" class=\"data row32 col3\" >0.6568</td>\n",
              "      <td id=\"T_d5c64_row32_col4\" class=\"data row32 col4\" >0.8538</td>\n",
              "      <td id=\"T_d5c64_row32_col5\" class=\"data row32 col5\" >0.6053</td>\n",
              "      <td id=\"T_d5c64_row32_col6\" class=\"data row32 col6\" >0.4927</td>\n",
              "      <td id=\"T_d5c64_row32_col7\" class=\"data row32 col7\" >0.6431</td>\n",
              "      <td id=\"T_d5c64_row32_col8\" class=\"data row32 col8\" >0.5799</td>\n",
              "      <td id=\"T_d5c64_row32_col9\" class=\"data row32 col9\" >0.6016</td>\n",
              "      <td id=\"T_d5c64_row32_col10\" class=\"data row32 col10\" >0.5617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row33\" class=\"row_heading level2 row33\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row33_col0\" class=\"data row33 col0\" >0.3315</td>\n",
              "      <td id=\"T_d5c64_row33_col1\" class=\"data row33 col1\" >0.5268</td>\n",
              "      <td id=\"T_d5c64_row33_col2\" class=\"data row33 col2\" >0.4635</td>\n",
              "      <td id=\"T_d5c64_row33_col3\" class=\"data row33 col3\" >0.4038</td>\n",
              "      <td id=\"T_d5c64_row33_col4\" class=\"data row33 col4\" >0.8711</td>\n",
              "      <td id=\"T_d5c64_row33_col5\" class=\"data row33 col5\" >0.3792</td>\n",
              "      <td id=\"T_d5c64_row33_col6\" class=\"data row33 col6\" >0.5761</td>\n",
              "      <td id=\"T_d5c64_row33_col7\" class=\"data row33 col7\" >0.3714</td>\n",
              "      <td id=\"T_d5c64_row33_col8\" class=\"data row33 col8\" >0.3293</td>\n",
              "      <td id=\"T_d5c64_row33_col9\" class=\"data row33 col9\" >0.3733</td>\n",
              "      <td id=\"T_d5c64_row33_col10\" class=\"data row33 col10\" >0.3591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row34\" class=\"row_heading level2 row34\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row34_col0\" class=\"data row34 col0\" >0.4910</td>\n",
              "      <td id=\"T_d5c64_row34_col1\" class=\"data row34 col1\" >0.4613</td>\n",
              "      <td id=\"T_d5c64_row34_col2\" class=\"data row34 col2\" >0.5495</td>\n",
              "      <td id=\"T_d5c64_row34_col3\" class=\"data row34 col3\" >0.6735</td>\n",
              "      <td id=\"T_d5c64_row34_col4\" class=\"data row34 col4\" >0.8827</td>\n",
              "      <td id=\"T_d5c64_row34_col5\" class=\"data row34 col5\" >0.5613</td>\n",
              "      <td id=\"T_d5c64_row34_col6\" class=\"data row34 col6\" >0.4187</td>\n",
              "      <td id=\"T_d5c64_row34_col7\" class=\"data row34 col7\" >0.5897</td>\n",
              "      <td id=\"T_d5c64_row34_col8\" class=\"data row34 col8\" >0.6706</td>\n",
              "      <td id=\"T_d5c64_row34_col9\" class=\"data row34 col9\" >0.6961</td>\n",
              "      <td id=\"T_d5c64_row34_col10\" class=\"data row34 col10\" >0.5358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row35\" class=\"row_heading level1 row35\" rowspan=\"7\">opt-iml-max-1.3b</th>\n",
              "      <th id=\"T_d5c64_level2_row35\" class=\"row_heading level2 row35\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row35_col0\" class=\"data row35 col0\" >0.4592</td>\n",
              "      <td id=\"T_d5c64_row35_col1\" class=\"data row35 col1\" >0.5171</td>\n",
              "      <td id=\"T_d5c64_row35_col2\" class=\"data row35 col2\" >0.5792</td>\n",
              "      <td id=\"T_d5c64_row35_col3\" class=\"data row35 col3\" >0.5916</td>\n",
              "      <td id=\"T_d5c64_row35_col4\" class=\"data row35 col4\" >0.9061</td>\n",
              "      <td id=\"T_d5c64_row35_col5\" class=\"data row35 col5\" >0.5143</td>\n",
              "      <td id=\"T_d5c64_row35_col6\" class=\"data row35 col6\" >0.5136</td>\n",
              "      <td id=\"T_d5c64_row35_col7\" class=\"data row35 col7\" >0.4721</td>\n",
              "      <td id=\"T_d5c64_row35_col8\" class=\"data row35 col8\" >0.5494</td>\n",
              "      <td id=\"T_d5c64_row35_col9\" class=\"data row35 col9\" >0.5333</td>\n",
              "      <td id=\"T_d5c64_row35_col10\" class=\"data row35 col10\" >0.4390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row36\" class=\"row_heading level2 row36\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row36_col0\" class=\"data row36 col0\" >0.3264</td>\n",
              "      <td id=\"T_d5c64_row36_col1\" class=\"data row36 col1\" >0.5542</td>\n",
              "      <td id=\"T_d5c64_row36_col2\" class=\"data row36 col2\" >0.5036</td>\n",
              "      <td id=\"T_d5c64_row36_col3\" class=\"data row36 col3\" >0.6618</td>\n",
              "      <td id=\"T_d5c64_row36_col4\" class=\"data row36 col4\" >0.9819</td>\n",
              "      <td id=\"T_d5c64_row36_col5\" class=\"data row36 col5\" >0.8709</td>\n",
              "      <td id=\"T_d5c64_row36_col6\" class=\"data row36 col6\" >0.7625</td>\n",
              "      <td id=\"T_d5c64_row36_col7\" class=\"data row36 col7\" >0.8334</td>\n",
              "      <td id=\"T_d5c64_row36_col8\" class=\"data row36 col8\" >0.3309</td>\n",
              "      <td id=\"T_d5c64_row36_col9\" class=\"data row36 col9\" >0.3390</td>\n",
              "      <td id=\"T_d5c64_row36_col10\" class=\"data row36 col10\" >0.3299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row37\" class=\"row_heading level2 row37\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row37_col0\" class=\"data row37 col0\" >0.3869</td>\n",
              "      <td id=\"T_d5c64_row37_col1\" class=\"data row37 col1\" >0.7116</td>\n",
              "      <td id=\"T_d5c64_row37_col2\" class=\"data row37 col2\" >0.5300</td>\n",
              "      <td id=\"T_d5c64_row37_col3\" class=\"data row37 col3\" >0.5259</td>\n",
              "      <td id=\"T_d5c64_row37_col4\" class=\"data row37 col4\" >0.9368</td>\n",
              "      <td id=\"T_d5c64_row37_col5\" class=\"data row37 col5\" >0.5789</td>\n",
              "      <td id=\"T_d5c64_row37_col6\" class=\"data row37 col6\" >0.5600</td>\n",
              "      <td id=\"T_d5c64_row37_col7\" class=\"data row37 col7\" >0.5453</td>\n",
              "      <td id=\"T_d5c64_row37_col8\" class=\"data row37 col8\" >0.4733</td>\n",
              "      <td id=\"T_d5c64_row37_col9\" class=\"data row37 col9\" >0.5295</td>\n",
              "      <td id=\"T_d5c64_row37_col10\" class=\"data row37 col10\" >0.4720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row38\" class=\"row_heading level2 row38\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row38_col0\" class=\"data row38 col0\" >0.3380</td>\n",
              "      <td id=\"T_d5c64_row38_col1\" class=\"data row38 col1\" >0.5134</td>\n",
              "      <td id=\"T_d5c64_row38_col2\" class=\"data row38 col2\" >0.6410</td>\n",
              "      <td id=\"T_d5c64_row38_col3\" class=\"data row38 col3\" >0.7450</td>\n",
              "      <td id=\"T_d5c64_row38_col4\" class=\"data row38 col4\" >0.9585</td>\n",
              "      <td id=\"T_d5c64_row38_col5\" class=\"data row38 col5\" >0.5284</td>\n",
              "      <td id=\"T_d5c64_row38_col6\" class=\"data row38 col6\" >0.6955</td>\n",
              "      <td id=\"T_d5c64_row38_col7\" class=\"data row38 col7\" >0.5212</td>\n",
              "      <td id=\"T_d5c64_row38_col8\" class=\"data row38 col8\" >0.4829</td>\n",
              "      <td id=\"T_d5c64_row38_col9\" class=\"data row38 col9\" >0.4925</td>\n",
              "      <td id=\"T_d5c64_row38_col10\" class=\"data row38 col10\" >0.5788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row39\" class=\"row_heading level2 row39\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row39_col0\" class=\"data row39 col0\" >0.7932</td>\n",
              "      <td id=\"T_d5c64_row39_col1\" class=\"data row39 col1\" >0.9731</td>\n",
              "      <td id=\"T_d5c64_row39_col2\" class=\"data row39 col2\" >1.0000</td>\n",
              "      <td id=\"T_d5c64_row39_col3\" class=\"data row39 col3\" >0.9609</td>\n",
              "      <td id=\"T_d5c64_row39_col4\" class=\"data row39 col4\" >0.9257</td>\n",
              "      <td id=\"T_d5c64_row39_col5\" class=\"data row39 col5\" >0.9023</td>\n",
              "      <td id=\"T_d5c64_row39_col6\" class=\"data row39 col6\" >0.9615</td>\n",
              "      <td id=\"T_d5c64_row39_col7\" class=\"data row39 col7\" >0.9025</td>\n",
              "      <td id=\"T_d5c64_row39_col8\" class=\"data row39 col8\" >0.9471</td>\n",
              "      <td id=\"T_d5c64_row39_col9\" class=\"data row39 col9\" >0.9570</td>\n",
              "      <td id=\"T_d5c64_row39_col10\" class=\"data row39 col10\" >0.7016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row40\" class=\"row_heading level2 row40\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row40_col0\" class=\"data row40 col0\" >0.3427</td>\n",
              "      <td id=\"T_d5c64_row40_col1\" class=\"data row40 col1\" >0.8511</td>\n",
              "      <td id=\"T_d5c64_row40_col2\" class=\"data row40 col2\" >0.7842</td>\n",
              "      <td id=\"T_d5c64_row40_col3\" class=\"data row40 col3\" >0.6490</td>\n",
              "      <td id=\"T_d5c64_row40_col4\" class=\"data row40 col4\" >0.9729</td>\n",
              "      <td id=\"T_d5c64_row40_col5\" class=\"data row40 col5\" >0.6895</td>\n",
              "      <td id=\"T_d5c64_row40_col6\" class=\"data row40 col6\" >0.9211</td>\n",
              "      <td id=\"T_d5c64_row40_col7\" class=\"data row40 col7\" >0.6590</td>\n",
              "      <td id=\"T_d5c64_row40_col8\" class=\"data row40 col8\" >0.3614</td>\n",
              "      <td id=\"T_d5c64_row40_col9\" class=\"data row40 col9\" >0.4318</td>\n",
              "      <td id=\"T_d5c64_row40_col10\" class=\"data row40 col10\" >0.4777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row41\" class=\"row_heading level2 row41\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row41_col0\" class=\"data row41 col0\" >0.5810</td>\n",
              "      <td id=\"T_d5c64_row41_col1\" class=\"data row41 col1\" >0.8567</td>\n",
              "      <td id=\"T_d5c64_row41_col2\" class=\"data row41 col2\" >0.8657</td>\n",
              "      <td id=\"T_d5c64_row41_col3\" class=\"data row41 col3\" >0.8416</td>\n",
              "      <td id=\"T_d5c64_row41_col4\" class=\"data row41 col4\" >0.9439</td>\n",
              "      <td id=\"T_d5c64_row41_col5\" class=\"data row41 col5\" >0.7268</td>\n",
              "      <td id=\"T_d5c64_row41_col6\" class=\"data row41 col6\" >0.7684</td>\n",
              "      <td id=\"T_d5c64_row41_col7\" class=\"data row41 col7\" >0.6991</td>\n",
              "      <td id=\"T_d5c64_row41_col8\" class=\"data row41 col8\" >0.7483</td>\n",
              "      <td id=\"T_d5c64_row41_col9\" class=\"data row41 col9\" >0.7587</td>\n",
              "      <td id=\"T_d5c64_row41_col10\" class=\"data row41 col10\" >0.6237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row42\" class=\"row_heading level1 row42\" rowspan=\"7\">text-davinci-003</th>\n",
              "      <th id=\"T_d5c64_level2_row42\" class=\"row_heading level2 row42\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row42_col0\" class=\"data row42 col0\" >0.7647</td>\n",
              "      <td id=\"T_d5c64_row42_col1\" class=\"data row42 col1\" >0.9014</td>\n",
              "      <td id=\"T_d5c64_row42_col2\" class=\"data row42 col2\" >0.8537</td>\n",
              "      <td id=\"T_d5c64_row42_col3\" class=\"data row42 col3\" >0.8554</td>\n",
              "      <td id=\"T_d5c64_row42_col4\" class=\"data row42 col4\" >0.9693</td>\n",
              "      <td id=\"T_d5c64_row42_col5\" class=\"data row42 col5\" >0.7750</td>\n",
              "      <td id=\"T_d5c64_row42_col6\" class=\"data row42 col6\" >0.9032</td>\n",
              "      <td id=\"T_d5c64_row42_col7\" class=\"data row42 col7\" >0.8408</td>\n",
              "      <td id=\"T_d5c64_row42_col8\" class=\"data row42 col8\" >0.7161</td>\n",
              "      <td id=\"T_d5c64_row42_col9\" class=\"data row42 col9\" >0.6657</td>\n",
              "      <td id=\"T_d5c64_row42_col10\" class=\"data row42 col10\" >0.8409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row43\" class=\"row_heading level2 row43\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row43_col0\" class=\"data row43 col0\" >0.4628</td>\n",
              "      <td id=\"T_d5c64_row43_col1\" class=\"data row43 col1\" >0.5751</td>\n",
              "      <td id=\"T_d5c64_row43_col2\" class=\"data row43 col2\" >0.4275</td>\n",
              "      <td id=\"T_d5c64_row43_col3\" class=\"data row43 col3\" >0.4024</td>\n",
              "      <td id=\"T_d5c64_row43_col4\" class=\"data row43 col4\" >0.9783</td>\n",
              "      <td id=\"T_d5c64_row43_col5\" class=\"data row43 col5\" >0.7471</td>\n",
              "      <td id=\"T_d5c64_row43_col6\" class=\"data row43 col6\" >0.5568</td>\n",
              "      <td id=\"T_d5c64_row43_col7\" class=\"data row43 col7\" >0.6000</td>\n",
              "      <td id=\"T_d5c64_row43_col8\" class=\"data row43 col8\" >0.5075</td>\n",
              "      <td id=\"T_d5c64_row43_col9\" class=\"data row43 col9\" >0.4826</td>\n",
              "      <td id=\"T_d5c64_row43_col10\" class=\"data row43 col10\" >0.4880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row44\" class=\"row_heading level2 row44\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row44_col0\" class=\"data row44 col0\" >0.3322</td>\n",
              "      <td id=\"T_d5c64_row44_col1\" class=\"data row44 col1\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row44_col2\" class=\"data row44 col2\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row44_col3\" class=\"data row44 col3\" >0.3303</td>\n",
              "      <td id=\"T_d5c64_row44_col4\" class=\"data row44 col4\" >0.9856</td>\n",
              "      <td id=\"T_d5c64_row44_col5\" class=\"data row44 col5\" >0.3264</td>\n",
              "      <td id=\"T_d5c64_row44_col6\" class=\"data row44 col6\" >0.3367</td>\n",
              "      <td id=\"T_d5c64_row44_col7\" class=\"data row44 col7\" >0.3268</td>\n",
              "      <td id=\"T_d5c64_row44_col8\" class=\"data row44 col8\" >0.3267</td>\n",
              "      <td id=\"T_d5c64_row44_col9\" class=\"data row44 col9\" >0.3361</td>\n",
              "      <td id=\"T_d5c64_row44_col10\" class=\"data row44 col10\" >0.3326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row45\" class=\"row_heading level2 row45\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row45_col0\" class=\"data row45 col0\" >0.5393</td>\n",
              "      <td id=\"T_d5c64_row45_col1\" class=\"data row45 col1\" >0.9199</td>\n",
              "      <td id=\"T_d5c64_row45_col2\" class=\"data row45 col2\" >0.3623</td>\n",
              "      <td id=\"T_d5c64_row45_col3\" class=\"data row45 col3\" >0.8936</td>\n",
              "      <td id=\"T_d5c64_row45_col4\" class=\"data row45 col4\" >0.9783</td>\n",
              "      <td id=\"T_d5c64_row45_col5\" class=\"data row45 col5\" >0.7678</td>\n",
              "      <td id=\"T_d5c64_row45_col6\" class=\"data row45 col6\" >0.9027</td>\n",
              "      <td id=\"T_d5c64_row45_col7\" class=\"data row45 col7\" >0.8389</td>\n",
              "      <td id=\"T_d5c64_row45_col8\" class=\"data row45 col8\" >0.7713</td>\n",
              "      <td id=\"T_d5c64_row45_col9\" class=\"data row45 col9\" >0.6932</td>\n",
              "      <td id=\"T_d5c64_row45_col10\" class=\"data row45 col10\" >0.5246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row46\" class=\"row_heading level2 row46\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row46_col0\" class=\"data row46 col0\" >0.5432</td>\n",
              "      <td id=\"T_d5c64_row46_col1\" class=\"data row46 col1\" >0.6398</td>\n",
              "      <td id=\"T_d5c64_row46_col2\" class=\"data row46 col2\" >0.8239</td>\n",
              "      <td id=\"T_d5c64_row46_col3\" class=\"data row46 col3\" >0.8563</td>\n",
              "      <td id=\"T_d5c64_row46_col4\" class=\"data row46 col4\" >0.9513</td>\n",
              "      <td id=\"T_d5c64_row46_col5\" class=\"data row46 col5\" >0.5774</td>\n",
              "      <td id=\"T_d5c64_row46_col6\" class=\"data row46 col6\" >0.7596</td>\n",
              "      <td id=\"T_d5c64_row46_col7\" class=\"data row46 col7\" >0.6335</td>\n",
              "      <td id=\"T_d5c64_row46_col8\" class=\"data row46 col8\" >0.6969</td>\n",
              "      <td id=\"T_d5c64_row46_col9\" class=\"data row46 col9\" >0.6015</td>\n",
              "      <td id=\"T_d5c64_row46_col10\" class=\"data row46 col10\" >0.7779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row47\" class=\"row_heading level2 row47\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row47_col0\" class=\"data row47 col0\" >0.4087</td>\n",
              "      <td id=\"T_d5c64_row47_col1\" class=\"data row47 col1\" >0.5610</td>\n",
              "      <td id=\"T_d5c64_row47_col2\" class=\"data row47 col2\" >0.5307</td>\n",
              "      <td id=\"T_d5c64_row47_col3\" class=\"data row47 col3\" >0.5112</td>\n",
              "      <td id=\"T_d5c64_row47_col4\" class=\"data row47 col4\" >0.9293</td>\n",
              "      <td id=\"T_d5c64_row47_col5\" class=\"data row47 col5\" >0.4369</td>\n",
              "      <td id=\"T_d5c64_row47_col6\" class=\"data row47 col6\" >0.7561</td>\n",
              "      <td id=\"T_d5c64_row47_col7\" class=\"data row47 col7\" >0.3902</td>\n",
              "      <td id=\"T_d5c64_row47_col8\" class=\"data row47 col8\" >0.4746</td>\n",
              "      <td id=\"T_d5c64_row47_col9\" class=\"data row47 col9\" >0.6221</td>\n",
              "      <td id=\"T_d5c64_row47_col10\" class=\"data row47 col10\" >0.4820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row48\" class=\"row_heading level2 row48\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row48_col0\" class=\"data row48 col0\" >0.5035</td>\n",
              "      <td id=\"T_d5c64_row48_col1\" class=\"data row48 col1\" >0.9213</td>\n",
              "      <td id=\"T_d5c64_row48_col2\" class=\"data row48 col2\" >0.6965</td>\n",
              "      <td id=\"T_d5c64_row48_col3\" class=\"data row48 col3\" >0.8913</td>\n",
              "      <td id=\"T_d5c64_row48_col4\" class=\"data row48 col4\" >0.9856</td>\n",
              "      <td id=\"T_d5c64_row48_col5\" class=\"data row48 col5\" >0.7715</td>\n",
              "      <td id=\"T_d5c64_row48_col6\" class=\"data row48 col6\" >0.8973</td>\n",
              "      <td id=\"T_d5c64_row48_col7\" class=\"data row48 col7\" >0.8232</td>\n",
              "      <td id=\"T_d5c64_row48_col8\" class=\"data row48 col8\" >0.5769</td>\n",
              "      <td id=\"T_d5c64_row48_col9\" class=\"data row48 col9\" >0.4739</td>\n",
              "      <td id=\"T_d5c64_row48_col10\" class=\"data row48 col10\" >0.4377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row49\" class=\"row_heading level1 row49\" rowspan=\"7\">vicuna-13b</th>\n",
              "      <th id=\"T_d5c64_level2_row49\" class=\"row_heading level2 row49\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row49_col0\" class=\"data row49 col0\" >0.7913</td>\n",
              "      <td id=\"T_d5c64_row49_col1\" class=\"data row49 col1\" >0.8856</td>\n",
              "      <td id=\"T_d5c64_row49_col2\" class=\"data row49 col2\" >0.8012</td>\n",
              "      <td id=\"T_d5c64_row49_col3\" class=\"data row49 col3\" >0.8446</td>\n",
              "      <td id=\"T_d5c64_row49_col4\" class=\"data row49 col4\" >0.9747</td>\n",
              "      <td id=\"T_d5c64_row49_col5\" class=\"data row49 col5\" >0.7859</td>\n",
              "      <td id=\"T_d5c64_row49_col6\" class=\"data row49 col6\" >0.9365</td>\n",
              "      <td id=\"T_d5c64_row49_col7\" class=\"data row49 col7\" >0.8520</td>\n",
              "      <td id=\"T_d5c64_row49_col8\" class=\"data row49 col8\" >0.8309</td>\n",
              "      <td id=\"T_d5c64_row49_col9\" class=\"data row49 col9\" >0.8137</td>\n",
              "      <td id=\"T_d5c64_row49_col10\" class=\"data row49 col10\" >0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row50\" class=\"row_heading level2 row50\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row50_col0\" class=\"data row50 col0\" >0.3912</td>\n",
              "      <td id=\"T_d5c64_row50_col1\" class=\"data row50 col1\" >0.3848</td>\n",
              "      <td id=\"T_d5c64_row50_col2\" class=\"data row50 col2\" >0.6025</td>\n",
              "      <td id=\"T_d5c64_row50_col3\" class=\"data row50 col3\" >0.5077</td>\n",
              "      <td id=\"T_d5c64_row50_col4\" class=\"data row50 col4\" >0.9838</td>\n",
              "      <td id=\"T_d5c64_row50_col5\" class=\"data row50 col5\" >0.6577</td>\n",
              "      <td id=\"T_d5c64_row50_col6\" class=\"data row50 col6\" >0.4839</td>\n",
              "      <td id=\"T_d5c64_row50_col7\" class=\"data row50 col7\" >0.5162</td>\n",
              "      <td id=\"T_d5c64_row50_col8\" class=\"data row50 col8\" >0.3420</td>\n",
              "      <td id=\"T_d5c64_row50_col9\" class=\"data row50 col9\" >0.3296</td>\n",
              "      <td id=\"T_d5c64_row50_col10\" class=\"data row50 col10\" >0.3551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row51\" class=\"row_heading level2 row51\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row51_col0\" class=\"data row51 col0\" >0.6275</td>\n",
              "      <td id=\"T_d5c64_row51_col1\" class=\"data row51 col1\" >0.3658</td>\n",
              "      <td id=\"T_d5c64_row51_col2\" class=\"data row51 col2\" >0.4420</td>\n",
              "      <td id=\"T_d5c64_row51_col3\" class=\"data row51 col3\" >0.4286</td>\n",
              "      <td id=\"T_d5c64_row51_col4\" class=\"data row51 col4\" >0.9783</td>\n",
              "      <td id=\"T_d5c64_row51_col5\" class=\"data row51 col5\" >0.3931</td>\n",
              "      <td id=\"T_d5c64_row51_col6\" class=\"data row51 col6\" >0.3921</td>\n",
              "      <td id=\"T_d5c64_row51_col7\" class=\"data row51 col7\" >0.3814</td>\n",
              "      <td id=\"T_d5c64_row51_col8\" class=\"data row51 col8\" >0.4329</td>\n",
              "      <td id=\"T_d5c64_row51_col9\" class=\"data row51 col9\" >0.4687</td>\n",
              "      <td id=\"T_d5c64_row51_col10\" class=\"data row51 col10\" >0.3502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row52\" class=\"row_heading level2 row52\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row52_col0\" class=\"data row52 col0\" >0.4086</td>\n",
              "      <td id=\"T_d5c64_row52_col1\" class=\"data row52 col1\" >0.7888</td>\n",
              "      <td id=\"T_d5c64_row52_col2\" class=\"data row52 col2\" >0.5043</td>\n",
              "      <td id=\"T_d5c64_row52_col3\" class=\"data row52 col3\" >0.8511</td>\n",
              "      <td id=\"T_d5c64_row52_col4\" class=\"data row52 col4\" >0.9765</td>\n",
              "      <td id=\"T_d5c64_row52_col5\" class=\"data row52 col5\" >0.6633</td>\n",
              "      <td id=\"T_d5c64_row52_col6\" class=\"data row52 col6\" >0.8876</td>\n",
              "      <td id=\"T_d5c64_row52_col7\" class=\"data row52 col7\" >0.8071</td>\n",
              "      <td id=\"T_d5c64_row52_col8\" class=\"data row52 col8\" >0.6347</td>\n",
              "      <td id=\"T_d5c64_row52_col9\" class=\"data row52 col9\" >0.7088</td>\n",
              "      <td id=\"T_d5c64_row52_col10\" class=\"data row52 col10\" >0.5941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row53\" class=\"row_heading level2 row53\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row53_col0\" class=\"data row53 col0\" >0.1784</td>\n",
              "      <td id=\"T_d5c64_row53_col1\" class=\"data row53 col1\" >0.5000</td>\n",
              "      <td id=\"T_d5c64_row53_col2\" class=\"data row53 col2\" >0.5018</td>\n",
              "      <td id=\"T_d5c64_row53_col3\" class=\"data row53 col3\" >0.6295</td>\n",
              "      <td id=\"T_d5c64_row53_col4\" class=\"data row53 col4\" >0.7775</td>\n",
              "      <td id=\"T_d5c64_row53_col5\" class=\"data row53 col5\" >0.4754</td>\n",
              "      <td id=\"T_d5c64_row53_col6\" class=\"data row53 col6\" >0.6191</td>\n",
              "      <td id=\"T_d5c64_row53_col7\" class=\"data row53 col7\" >0.5078</td>\n",
              "      <td id=\"T_d5c64_row53_col8\" class=\"data row53 col8\" >0.4121</td>\n",
              "      <td id=\"T_d5c64_row53_col9\" class=\"data row53 col9\" >0.4635</td>\n",
              "      <td id=\"T_d5c64_row53_col10\" class=\"data row53 col10\" >0.5888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row54\" class=\"row_heading level2 row54\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row54_col0\" class=\"data row54 col0\" >0.4179</td>\n",
              "      <td id=\"T_d5c64_row54_col1\" class=\"data row54 col1\" >0.6409</td>\n",
              "      <td id=\"T_d5c64_row54_col2\" class=\"data row54 col2\" >0.5661</td>\n",
              "      <td id=\"T_d5c64_row54_col3\" class=\"data row54 col3\" >0.5341</td>\n",
              "      <td id=\"T_d5c64_row54_col4\" class=\"data row54 col4\" >0.9348</td>\n",
              "      <td id=\"T_d5c64_row54_col5\" class=\"data row54 col5\" >0.5070</td>\n",
              "      <td id=\"T_d5c64_row54_col6\" class=\"data row54 col6\" >0.8146</td>\n",
              "      <td id=\"T_d5c64_row54_col7\" class=\"data row54 col7\" >0.4350</td>\n",
              "      <td id=\"T_d5c64_row54_col8\" class=\"data row54 col8\" >0.5180</td>\n",
              "      <td id=\"T_d5c64_row54_col9\" class=\"data row54 col9\" >0.5303</td>\n",
              "      <td id=\"T_d5c64_row54_col10\" class=\"data row54 col10\" >0.4740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row55\" class=\"row_heading level2 row55\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row55_col0\" class=\"data row55 col0\" >0.3864</td>\n",
              "      <td id=\"T_d5c64_row55_col1\" class=\"data row55 col1\" >0.9417</td>\n",
              "      <td id=\"T_d5c64_row55_col2\" class=\"data row55 col2\" >0.4894</td>\n",
              "      <td id=\"T_d5c64_row55_col3\" class=\"data row55 col3\" >0.9086</td>\n",
              "      <td id=\"T_d5c64_row55_col4\" class=\"data row55 col4\" >0.9801</td>\n",
              "      <td id=\"T_d5c64_row55_col5\" class=\"data row55 col5\" >0.6866</td>\n",
              "      <td id=\"T_d5c64_row55_col6\" class=\"data row55 col6\" >0.8400</td>\n",
              "      <td id=\"T_d5c64_row55_col7\" class=\"data row55 col7\" >0.7163</td>\n",
              "      <td id=\"T_d5c64_row55_col8\" class=\"data row55 col8\" >0.7787</td>\n",
              "      <td id=\"T_d5c64_row55_col9\" class=\"data row55 col9\" >0.4897</td>\n",
              "      <td id=\"T_d5c64_row55_col10\" class=\"data row55 col10\" >0.4370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level0_row56\" class=\"row_heading level0 row56\" rowspan=\"56\">es</th>\n",
              "      <th id=\"T_d5c64_level1_row56\" class=\"row_heading level1 row56\" rowspan=\"7\">alpaca-lora-30b</th>\n",
              "      <th id=\"T_d5c64_level2_row56\" class=\"row_heading level2 row56\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row56_col0\" class=\"data row56 col0\" >0.6019</td>\n",
              "      <td id=\"T_d5c64_row56_col1\" class=\"data row56 col1\" >0.8950</td>\n",
              "      <td id=\"T_d5c64_row56_col2\" class=\"data row56 col2\" >0.8108</td>\n",
              "      <td id=\"T_d5c64_row56_col3\" class=\"data row56 col3\" >0.8765</td>\n",
              "      <td id=\"T_d5c64_row56_col4\" class=\"data row56 col4\" >0.7517</td>\n",
              "      <td id=\"T_d5c64_row56_col5\" class=\"data row56 col5\" >0.9092</td>\n",
              "      <td id=\"T_d5c64_row56_col6\" class=\"data row56 col6\" >0.8941</td>\n",
              "      <td id=\"T_d5c64_row56_col7\" class=\"data row56 col7\" >0.8943</td>\n",
              "      <td id=\"T_d5c64_row56_col8\" class=\"data row56 col8\" >0.8600</td>\n",
              "      <td id=\"T_d5c64_row56_col9\" class=\"data row56 col9\" >0.8182</td>\n",
              "      <td id=\"T_d5c64_row56_col10\" class=\"data row56 col10\" >0.7713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row57\" class=\"row_heading level2 row57\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row57_col0\" class=\"data row57 col0\" >0.5116</td>\n",
              "      <td id=\"T_d5c64_row57_col1\" class=\"data row57 col1\" >0.4754</td>\n",
              "      <td id=\"T_d5c64_row57_col2\" class=\"data row57 col2\" >0.3370</td>\n",
              "      <td id=\"T_d5c64_row57_col3\" class=\"data row57 col3\" >0.3361</td>\n",
              "      <td id=\"T_d5c64_row57_col4\" class=\"data row57 col4\" >0.3325</td>\n",
              "      <td id=\"T_d5c64_row57_col5\" class=\"data row57 col5\" >0.9348</td>\n",
              "      <td id=\"T_d5c64_row57_col6\" class=\"data row57 col6\" >0.5286</td>\n",
              "      <td id=\"T_d5c64_row57_col7\" class=\"data row57 col7\" >0.8665</td>\n",
              "      <td id=\"T_d5c64_row57_col8\" class=\"data row57 col8\" >0.3891</td>\n",
              "      <td id=\"T_d5c64_row57_col9\" class=\"data row57 col9\" >0.3820</td>\n",
              "      <td id=\"T_d5c64_row57_col10\" class=\"data row57 col10\" >0.3986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row58\" class=\"row_heading level2 row58\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row58_col0\" class=\"data row58 col0\" >0.6937</td>\n",
              "      <td id=\"T_d5c64_row58_col1\" class=\"data row58 col1\" >0.8090</td>\n",
              "      <td id=\"T_d5c64_row58_col2\" class=\"data row58 col2\" >0.6564</td>\n",
              "      <td id=\"T_d5c64_row58_col3\" class=\"data row58 col3\" >0.8226</td>\n",
              "      <td id=\"T_d5c64_row58_col4\" class=\"data row58 col4\" >0.7332</td>\n",
              "      <td id=\"T_d5c64_row58_col5\" class=\"data row58 col5\" >0.8935</td>\n",
              "      <td id=\"T_d5c64_row58_col6\" class=\"data row58 col6\" >0.8291</td>\n",
              "      <td id=\"T_d5c64_row58_col7\" class=\"data row58 col7\" >0.8602</td>\n",
              "      <td id=\"T_d5c64_row58_col8\" class=\"data row58 col8\" >0.5735</td>\n",
              "      <td id=\"T_d5c64_row58_col9\" class=\"data row58 col9\" >0.5679</td>\n",
              "      <td id=\"T_d5c64_row58_col10\" class=\"data row58 col10\" >0.4391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row59\" class=\"row_heading level2 row59\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row59_col0\" class=\"data row59 col0\" >0.6980</td>\n",
              "      <td id=\"T_d5c64_row59_col1\" class=\"data row59 col1\" >0.8639</td>\n",
              "      <td id=\"T_d5c64_row59_col2\" class=\"data row59 col2\" >0.6989</td>\n",
              "      <td id=\"T_d5c64_row59_col3\" class=\"data row59 col3\" >0.9001</td>\n",
              "      <td id=\"T_d5c64_row59_col4\" class=\"data row59 col4\" >0.6589</td>\n",
              "      <td id=\"T_d5c64_row59_col5\" class=\"data row59 col5\" >0.9452</td>\n",
              "      <td id=\"T_d5c64_row59_col6\" class=\"data row59 col6\" >0.7088</td>\n",
              "      <td id=\"T_d5c64_row59_col7\" class=\"data row59 col7\" >0.9403</td>\n",
              "      <td id=\"T_d5c64_row59_col8\" class=\"data row59 col8\" >0.8366</td>\n",
              "      <td id=\"T_d5c64_row59_col9\" class=\"data row59 col9\" >0.8880</td>\n",
              "      <td id=\"T_d5c64_row59_col10\" class=\"data row59 col10\" >0.5691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row60\" class=\"row_heading level2 row60\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row60_col0\" class=\"data row60 col0\" >0.8255</td>\n",
              "      <td id=\"T_d5c64_row60_col1\" class=\"data row60 col1\" >0.7338</td>\n",
              "      <td id=\"T_d5c64_row60_col2\" class=\"data row60 col2\" >0.9110</td>\n",
              "      <td id=\"T_d5c64_row60_col3\" class=\"data row60 col3\" >0.9171</td>\n",
              "      <td id=\"T_d5c64_row60_col4\" class=\"data row60 col4\" >0.8771</td>\n",
              "      <td id=\"T_d5c64_row60_col5\" class=\"data row60 col5\" >0.8256</td>\n",
              "      <td id=\"T_d5c64_row60_col6\" class=\"data row60 col6\" >0.8038</td>\n",
              "      <td id=\"T_d5c64_row60_col7\" class=\"data row60 col7\" >0.7777</td>\n",
              "      <td id=\"T_d5c64_row60_col8\" class=\"data row60 col8\" >0.9317</td>\n",
              "      <td id=\"T_d5c64_row60_col9\" class=\"data row60 col9\" >0.9096</td>\n",
              "      <td id=\"T_d5c64_row60_col10\" class=\"data row60 col10\" >0.7954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row61\" class=\"row_heading level2 row61\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row61_col0\" class=\"data row61 col0\" >0.3578</td>\n",
              "      <td id=\"T_d5c64_row61_col1\" class=\"data row61 col1\" >0.7052</td>\n",
              "      <td id=\"T_d5c64_row61_col2\" class=\"data row61 col2\" >0.6789</td>\n",
              "      <td id=\"T_d5c64_row61_col3\" class=\"data row61 col3\" >0.8414</td>\n",
              "      <td id=\"T_d5c64_row61_col4\" class=\"data row61 col4\" >0.2717</td>\n",
              "      <td id=\"T_d5c64_row61_col5\" class=\"data row61 col5\" >0.9295</td>\n",
              "      <td id=\"T_d5c64_row61_col6\" class=\"data row61 col6\" >0.8296</td>\n",
              "      <td id=\"T_d5c64_row61_col7\" class=\"data row61 col7\" >0.8671</td>\n",
              "      <td id=\"T_d5c64_row61_col8\" class=\"data row61 col8\" >0.3587</td>\n",
              "      <td id=\"T_d5c64_row61_col9\" class=\"data row61 col9\" >0.3634</td>\n",
              "      <td id=\"T_d5c64_row61_col10\" class=\"data row61 col10\" >0.7537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row62\" class=\"row_heading level2 row62\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row62_col0\" class=\"data row62 col0\" >0.6999</td>\n",
              "      <td id=\"T_d5c64_row62_col1\" class=\"data row62 col1\" >0.8349</td>\n",
              "      <td id=\"T_d5c64_row62_col2\" class=\"data row62 col2\" >0.9008</td>\n",
              "      <td id=\"T_d5c64_row62_col3\" class=\"data row62 col3\" >0.9493</td>\n",
              "      <td id=\"T_d5c64_row62_col4\" class=\"data row62 col4\" >0.9295</td>\n",
              "      <td id=\"T_d5c64_row62_col5\" class=\"data row62 col5\" >0.9281</td>\n",
              "      <td id=\"T_d5c64_row62_col6\" class=\"data row62 col6\" >0.9532</td>\n",
              "      <td id=\"T_d5c64_row62_col7\" class=\"data row62 col7\" >0.9301</td>\n",
              "      <td id=\"T_d5c64_row62_col8\" class=\"data row62 col8\" >0.8845</td>\n",
              "      <td id=\"T_d5c64_row62_col9\" class=\"data row62 col9\" >0.8278</td>\n",
              "      <td id=\"T_d5c64_row62_col10\" class=\"data row62 col10\" >0.6164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row63\" class=\"row_heading level1 row63\" rowspan=\"7\">gpt-3.5-turbo</th>\n",
              "      <th id=\"T_d5c64_level2_row63\" class=\"row_heading level2 row63\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row63_col0\" class=\"data row63 col0\" >0.8896</td>\n",
              "      <td id=\"T_d5c64_row63_col1\" class=\"data row63 col1\" >0.9365</td>\n",
              "      <td id=\"T_d5c64_row63_col2\" class=\"data row63 col2\" >0.8997</td>\n",
              "      <td id=\"T_d5c64_row63_col3\" class=\"data row63 col3\" >0.8900</td>\n",
              "      <td id=\"T_d5c64_row63_col4\" class=\"data row63 col4\" >0.9169</td>\n",
              "      <td id=\"T_d5c64_row63_col5\" class=\"data row63 col5\" >0.9263</td>\n",
              "      <td id=\"T_d5c64_row63_col6\" class=\"data row63 col6\" >0.9232</td>\n",
              "      <td id=\"T_d5c64_row63_col7\" class=\"data row63 col7\" >0.8858</td>\n",
              "      <td id=\"T_d5c64_row63_col8\" class=\"data row63 col8\" >0.8949</td>\n",
              "      <td id=\"T_d5c64_row63_col9\" class=\"data row63 col9\" >0.8722</td>\n",
              "      <td id=\"T_d5c64_row63_col10\" class=\"data row63 col10\" >0.8765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row64\" class=\"row_heading level2 row64\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row64_col0\" class=\"data row64 col0\" >0.5268</td>\n",
              "      <td id=\"T_d5c64_row64_col1\" class=\"data row64 col1\" >0.7374</td>\n",
              "      <td id=\"T_d5c64_row64_col2\" class=\"data row64 col2\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row64_col3\" class=\"data row64 col3\" >0.3340</td>\n",
              "      <td id=\"T_d5c64_row64_col4\" class=\"data row64 col4\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row64_col5\" class=\"data row64 col5\" >0.9743</td>\n",
              "      <td id=\"T_d5c64_row64_col6\" class=\"data row64 col6\" >0.4290</td>\n",
              "      <td id=\"T_d5c64_row64_col7\" class=\"data row64 col7\" >0.9215</td>\n",
              "      <td id=\"T_d5c64_row64_col8\" class=\"data row64 col8\" >0.5390</td>\n",
              "      <td id=\"T_d5c64_row64_col9\" class=\"data row64 col9\" >0.4383</td>\n",
              "      <td id=\"T_d5c64_row64_col10\" class=\"data row64 col10\" >0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row65\" class=\"row_heading level2 row65\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row65_col0\" class=\"data row65 col0\" >0.4313</td>\n",
              "      <td id=\"T_d5c64_row65_col1\" class=\"data row65 col1\" >0.8889</td>\n",
              "      <td id=\"T_d5c64_row65_col2\" class=\"data row65 col2\" >0.3900</td>\n",
              "      <td id=\"T_d5c64_row65_col3\" class=\"data row65 col3\" >0.3827</td>\n",
              "      <td id=\"T_d5c64_row65_col4\" class=\"data row65 col4\" >0.3669</td>\n",
              "      <td id=\"T_d5c64_row65_col5\" class=\"data row65 col5\" >0.8970</td>\n",
              "      <td id=\"T_d5c64_row65_col6\" class=\"data row65 col6\" >0.4874</td>\n",
              "      <td id=\"T_d5c64_row65_col7\" class=\"data row65 col7\" >0.8320</td>\n",
              "      <td id=\"T_d5c64_row65_col8\" class=\"data row65 col8\" >0.4949</td>\n",
              "      <td id=\"T_d5c64_row65_col9\" class=\"data row65 col9\" >0.4526</td>\n",
              "      <td id=\"T_d5c64_row65_col10\" class=\"data row65 col10\" >0.3355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row66\" class=\"row_heading level2 row66\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row66_col0\" class=\"data row66 col0\" >0.8941</td>\n",
              "      <td id=\"T_d5c64_row66_col1\" class=\"data row66 col1\" >0.9650</td>\n",
              "      <td id=\"T_d5c64_row66_col2\" class=\"data row66 col2\" >0.3552</td>\n",
              "      <td id=\"T_d5c64_row66_col3\" class=\"data row66 col3\" >0.9307</td>\n",
              "      <td id=\"T_d5c64_row66_col4\" class=\"data row66 col4\" >0.8397</td>\n",
              "      <td id=\"T_d5c64_row66_col5\" class=\"data row66 col5\" >0.9692</td>\n",
              "      <td id=\"T_d5c64_row66_col6\" class=\"data row66 col6\" >0.8974</td>\n",
              "      <td id=\"T_d5c64_row66_col7\" class=\"data row66 col7\" >0.9556</td>\n",
              "      <td id=\"T_d5c64_row66_col8\" class=\"data row66 col8\" >0.8942</td>\n",
              "      <td id=\"T_d5c64_row66_col9\" class=\"data row66 col9\" >0.9196</td>\n",
              "      <td id=\"T_d5c64_row66_col10\" class=\"data row66 col10\" >0.6781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row67\" class=\"row_heading level2 row67\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row67_col0\" class=\"data row67 col0\" >0.9281</td>\n",
              "      <td id=\"T_d5c64_row67_col1\" class=\"data row67 col1\" >0.8817</td>\n",
              "      <td id=\"T_d5c64_row67_col2\" class=\"data row67 col2\" >0.9666</td>\n",
              "      <td id=\"T_d5c64_row67_col3\" class=\"data row67 col3\" >0.9577</td>\n",
              "      <td id=\"T_d5c64_row67_col4\" class=\"data row67 col4\" >0.9368</td>\n",
              "      <td id=\"T_d5c64_row67_col5\" class=\"data row67 col5\" >0.9570</td>\n",
              "      <td id=\"T_d5c64_row67_col6\" class=\"data row67 col6\" >0.9109</td>\n",
              "      <td id=\"T_d5c64_row67_col7\" class=\"data row67 col7\" >0.9089</td>\n",
              "      <td id=\"T_d5c64_row67_col8\" class=\"data row67 col8\" >0.8890</td>\n",
              "      <td id=\"T_d5c64_row67_col9\" class=\"data row67 col9\" >0.9128</td>\n",
              "      <td id=\"T_d5c64_row67_col10\" class=\"data row67 col10\" >0.9416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row68\" class=\"row_heading level2 row68\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row68_col0\" class=\"data row68 col0\" >0.3307</td>\n",
              "      <td id=\"T_d5c64_row68_col1\" class=\"data row68 col1\" >0.8610</td>\n",
              "      <td id=\"T_d5c64_row68_col2\" class=\"data row68 col2\" >0.8449</td>\n",
              "      <td id=\"T_d5c64_row68_col3\" class=\"data row68 col3\" >0.8700</td>\n",
              "      <td id=\"T_d5c64_row68_col4\" class=\"data row68 col4\" >0.2073</td>\n",
              "      <td id=\"T_d5c64_row68_col5\" class=\"data row68 col5\" >0.9153</td>\n",
              "      <td id=\"T_d5c64_row68_col6\" class=\"data row68 col6\" >0.8743</td>\n",
              "      <td id=\"T_d5c64_row68_col7\" class=\"data row68 col7\" >0.8392</td>\n",
              "      <td id=\"T_d5c64_row68_col8\" class=\"data row68 col8\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row68_col9\" class=\"data row68 col9\" >0.3311</td>\n",
              "      <td id=\"T_d5c64_row68_col10\" class=\"data row68 col10\" >0.7601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row69\" class=\"row_heading level2 row69\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row69_col0\" class=\"data row69 col0\" >0.7671</td>\n",
              "      <td id=\"T_d5c64_row69_col1\" class=\"data row69 col1\" >0.9550</td>\n",
              "      <td id=\"T_d5c64_row69_col2\" class=\"data row69 col2\" >0.8609</td>\n",
              "      <td id=\"T_d5c64_row69_col3\" class=\"data row69 col3\" >0.8539</td>\n",
              "      <td id=\"T_d5c64_row69_col4\" class=\"data row69 col4\" >0.9801</td>\n",
              "      <td id=\"T_d5c64_row69_col5\" class=\"data row69 col5\" >0.9743</td>\n",
              "      <td id=\"T_d5c64_row69_col6\" class=\"data row69 col6\" >0.9616</td>\n",
              "      <td id=\"T_d5c64_row69_col7\" class=\"data row69 col7\" >0.9676</td>\n",
              "      <td id=\"T_d5c64_row69_col8\" class=\"data row69 col8\" >0.9093</td>\n",
              "      <td id=\"T_d5c64_row69_col9\" class=\"data row69 col9\" >0.7368</td>\n",
              "      <td id=\"T_d5c64_row69_col10\" class=\"data row69 col10\" >0.8504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row70\" class=\"row_heading level1 row70\" rowspan=\"7\">gpt-4</th>\n",
              "      <th id=\"T_d5c64_level2_row70\" class=\"row_heading level2 row70\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row70_col0\" class=\"data row70 col0\" >0.8764</td>\n",
              "      <td id=\"T_d5c64_row70_col1\" class=\"data row70 col1\" >0.8852</td>\n",
              "      <td id=\"T_d5c64_row70_col2\" class=\"data row70 col2\" >0.9300</td>\n",
              "      <td id=\"T_d5c64_row70_col3\" class=\"data row70 col3\" >0.8808</td>\n",
              "      <td id=\"T_d5c64_row70_col4\" class=\"data row70 col4\" >0.7306</td>\n",
              "      <td id=\"T_d5c64_row70_col5\" class=\"data row70 col5\" >0.9277</td>\n",
              "      <td id=\"T_d5c64_row70_col6\" class=\"data row70 col6\" >0.9075</td>\n",
              "      <td id=\"T_d5c64_row70_col7\" class=\"data row70 col7\" >0.8679</td>\n",
              "      <td id=\"T_d5c64_row70_col8\" class=\"data row70 col8\" >0.8916</td>\n",
              "      <td id=\"T_d5c64_row70_col9\" class=\"data row70 col9\" >0.8792</td>\n",
              "      <td id=\"T_d5c64_row70_col10\" class=\"data row70 col10\" >0.8799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row71\" class=\"row_heading level2 row71\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row71_col0\" class=\"data row71 col0\" >0.3860</td>\n",
              "      <td id=\"T_d5c64_row71_col1\" class=\"data row71 col1\" >0.8103</td>\n",
              "      <td id=\"T_d5c64_row71_col2\" class=\"data row71 col2\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row71_col3\" class=\"data row71 col3\" >0.3303</td>\n",
              "      <td id=\"T_d5c64_row71_col4\" class=\"data row71 col4\" >0.3373</td>\n",
              "      <td id=\"T_d5c64_row71_col5\" class=\"data row71 col5\" >0.9571</td>\n",
              "      <td id=\"T_d5c64_row71_col6\" class=\"data row71 col6\" >0.5424</td>\n",
              "      <td id=\"T_d5c64_row71_col7\" class=\"data row71 col7\" >0.9506</td>\n",
              "      <td id=\"T_d5c64_row71_col8\" class=\"data row71 col8\" >0.3798</td>\n",
              "      <td id=\"T_d5c64_row71_col9\" class=\"data row71 col9\" >0.3670</td>\n",
              "      <td id=\"T_d5c64_row71_col10\" class=\"data row71 col10\" >0.3407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row72\" class=\"row_heading level2 row72\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row72_col0\" class=\"data row72 col0\" >0.4260</td>\n",
              "      <td id=\"T_d5c64_row72_col1\" class=\"data row72 col1\" >0.7906</td>\n",
              "      <td id=\"T_d5c64_row72_col2\" class=\"data row72 col2\" >0.4326</td>\n",
              "      <td id=\"T_d5c64_row72_col3\" class=\"data row72 col3\" >0.4318</td>\n",
              "      <td id=\"T_d5c64_row72_col4\" class=\"data row72 col4\" >0.3362</td>\n",
              "      <td id=\"T_d5c64_row72_col5\" class=\"data row72 col5\" >0.9143</td>\n",
              "      <td id=\"T_d5c64_row72_col6\" class=\"data row72 col6\" >0.4571</td>\n",
              "      <td id=\"T_d5c64_row72_col7\" class=\"data row72 col7\" >0.7844</td>\n",
              "      <td id=\"T_d5c64_row72_col8\" class=\"data row72 col8\" >0.4378</td>\n",
              "      <td id=\"T_d5c64_row72_col9\" class=\"data row72 col9\" >0.4042</td>\n",
              "      <td id=\"T_d5c64_row72_col10\" class=\"data row72 col10\" >0.3399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row73\" class=\"row_heading level2 row73\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row73_col0\" class=\"data row73 col0\" >0.8491</td>\n",
              "      <td id=\"T_d5c64_row73_col1\" class=\"data row73 col1\" >0.8705</td>\n",
              "      <td id=\"T_d5c64_row73_col2\" class=\"data row73 col2\" >0.3370</td>\n",
              "      <td id=\"T_d5c64_row73_col3\" class=\"data row73 col3\" >0.9187</td>\n",
              "      <td id=\"T_d5c64_row73_col4\" class=\"data row73 col4\" >0.9476</td>\n",
              "      <td id=\"T_d5c64_row73_col5\" class=\"data row73 col5\" >0.9829</td>\n",
              "      <td id=\"T_d5c64_row73_col6\" class=\"data row73 col6\" >0.9245</td>\n",
              "      <td id=\"T_d5c64_row73_col7\" class=\"data row73 col7\" >0.9830</td>\n",
              "      <td id=\"T_d5c64_row73_col8\" class=\"data row73 col8\" >0.9016</td>\n",
              "      <td id=\"T_d5c64_row73_col9\" class=\"data row73 col9\" >0.8454</td>\n",
              "      <td id=\"T_d5c64_row73_col10\" class=\"data row73 col10\" >0.6649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row74\" class=\"row_heading level2 row74\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row74_col0\" class=\"data row74 col0\" >0.9331</td>\n",
              "      <td id=\"T_d5c64_row74_col1\" class=\"data row74 col1\" >0.8897</td>\n",
              "      <td id=\"T_d5c64_row74_col2\" class=\"data row74 col2\" >0.9549</td>\n",
              "      <td id=\"T_d5c64_row74_col3\" class=\"data row74 col3\" >0.9356</td>\n",
              "      <td id=\"T_d5c64_row74_col4\" class=\"data row74 col4\" >0.8826</td>\n",
              "      <td id=\"T_d5c64_row74_col5\" class=\"data row74 col5\" >0.9195</td>\n",
              "      <td id=\"T_d5c64_row74_col6\" class=\"data row74 col6\" >0.9331</td>\n",
              "      <td id=\"T_d5c64_row74_col7\" class=\"data row74 col7\" >0.9027</td>\n",
              "      <td id=\"T_d5c64_row74_col8\" class=\"data row74 col8\" >0.8545</td>\n",
              "      <td id=\"T_d5c64_row74_col9\" class=\"data row74 col9\" >0.9096</td>\n",
              "      <td id=\"T_d5c64_row74_col10\" class=\"data row74 col10\" >0.9045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row75\" class=\"row_heading level2 row75\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row75_col0\" class=\"data row75 col0\" >0.3322</td>\n",
              "      <td id=\"T_d5c64_row75_col1\" class=\"data row75 col1\" >0.9128</td>\n",
              "      <td id=\"T_d5c64_row75_col2\" class=\"data row75 col2\" >0.8091</td>\n",
              "      <td id=\"T_d5c64_row75_col3\" class=\"data row75 col3\" >0.8765</td>\n",
              "      <td id=\"T_d5c64_row75_col4\" class=\"data row75 col4\" >0.3602</td>\n",
              "      <td id=\"T_d5c64_row75_col5\" class=\"data row75 col5\" >0.9293</td>\n",
              "      <td id=\"T_d5c64_row75_col6\" class=\"data row75 col6\" >0.8449</td>\n",
              "      <td id=\"T_d5c64_row75_col7\" class=\"data row75 col7\" >0.8768</td>\n",
              "      <td id=\"T_d5c64_row75_col8\" class=\"data row75 col8\" >0.3370</td>\n",
              "      <td id=\"T_d5c64_row75_col9\" class=\"data row75 col9\" >0.3318</td>\n",
              "      <td id=\"T_d5c64_row75_col10\" class=\"data row75 col10\" >0.6283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row76\" class=\"row_heading level2 row76\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row76_col0\" class=\"data row76 col0\" >0.5124</td>\n",
              "      <td id=\"T_d5c64_row76_col1\" class=\"data row76 col1\" >0.9599</td>\n",
              "      <td id=\"T_d5c64_row76_col2\" class=\"data row76 col2\" >0.5688</td>\n",
              "      <td id=\"T_d5c64_row76_col3\" class=\"data row76 col3\" >0.8011</td>\n",
              "      <td id=\"T_d5c64_row76_col4\" class=\"data row76 col4\" >0.9729</td>\n",
              "      <td id=\"T_d5c64_row76_col5\" class=\"data row76 col5\" >0.9812</td>\n",
              "      <td id=\"T_d5c64_row76_col6\" class=\"data row76 col6\" >0.9616</td>\n",
              "      <td id=\"T_d5c64_row76_col7\" class=\"data row76 col7\" >0.9676</td>\n",
              "      <td id=\"T_d5c64_row76_col8\" class=\"data row76 col8\" >0.5987</td>\n",
              "      <td id=\"T_d5c64_row76_col9\" class=\"data row76 col9\" >0.3994</td>\n",
              "      <td id=\"T_d5c64_row76_col10\" class=\"data row76 col10\" >0.4001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row77\" class=\"row_heading level1 row77\" rowspan=\"7\">llama-65b</th>\n",
              "      <th id=\"T_d5c64_level2_row77\" class=\"row_heading level2 row77\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row77_col0\" class=\"data row77 col0\" >0.7956</td>\n",
              "      <td id=\"T_d5c64_row77_col1\" class=\"data row77 col1\" >0.9177</td>\n",
              "      <td id=\"T_d5c64_row77_col2\" class=\"data row77 col2\" >0.9645</td>\n",
              "      <td id=\"T_d5c64_row77_col3\" class=\"data row77 col3\" >0.7711</td>\n",
              "      <td id=\"T_d5c64_row77_col4\" class=\"data row77 col4\" >0.3611</td>\n",
              "      <td id=\"T_d5c64_row77_col5\" class=\"data row77 col5\" >0.9914</td>\n",
              "      <td id=\"T_d5c64_row77_col6\" class=\"data row77 col6\" >0.6665</td>\n",
              "      <td id=\"T_d5c64_row77_col7\" class=\"data row77 col7\" >0.9794</td>\n",
              "      <td id=\"T_d5c64_row77_col8\" class=\"data row77 col8\" >0.8227</td>\n",
              "      <td id=\"T_d5c64_row77_col9\" class=\"data row77 col9\" >0.9290</td>\n",
              "      <td id=\"T_d5c64_row77_col10\" class=\"data row77 col10\" >0.6396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row78\" class=\"row_heading level2 row78\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row78_col0\" class=\"data row78 col0\" >0.4887</td>\n",
              "      <td id=\"T_d5c64_row78_col1\" class=\"data row78 col1\" >0.9026</td>\n",
              "      <td id=\"T_d5c64_row78_col2\" class=\"data row78 col2\" >0.9272</td>\n",
              "      <td id=\"T_d5c64_row78_col3\" class=\"data row78 col3\" >0.9214</td>\n",
              "      <td id=\"T_d5c64_row78_col4\" class=\"data row78 col4\" >0.3325</td>\n",
              "      <td id=\"T_d5c64_row78_col5\" class=\"data row78 col5\" >0.9741</td>\n",
              "      <td id=\"T_d5c64_row78_col6\" class=\"data row78 col6\" >0.8515</td>\n",
              "      <td id=\"T_d5c64_row78_col7\" class=\"data row78 col7\" >0.9691</td>\n",
              "      <td id=\"T_d5c64_row78_col8\" class=\"data row78 col8\" >0.3620</td>\n",
              "      <td id=\"T_d5c64_row78_col9\" class=\"data row78 col9\" >0.4606</td>\n",
              "      <td id=\"T_d5c64_row78_col10\" class=\"data row78 col10\" >0.3318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row79\" class=\"row_heading level2 row79\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row79_col0\" class=\"data row79 col0\" >0.3680</td>\n",
              "      <td id=\"T_d5c64_row79_col1\" class=\"data row79 col1\" >0.9365</td>\n",
              "      <td id=\"T_d5c64_row79_col2\" class=\"data row79 col2\" >0.8738</td>\n",
              "      <td id=\"T_d5c64_row79_col3\" class=\"data row79 col3\" >0.9079</td>\n",
              "      <td id=\"T_d5c64_row79_col4\" class=\"data row79 col4\" >0.3990</td>\n",
              "      <td id=\"T_d5c64_row79_col5\" class=\"data row79 col5\" >0.9621</td>\n",
              "      <td id=\"T_d5c64_row79_col6\" class=\"data row79 col6\" >0.8744</td>\n",
              "      <td id=\"T_d5c64_row79_col7\" class=\"data row79 col7\" >0.9398</td>\n",
              "      <td id=\"T_d5c64_row79_col8\" class=\"data row79 col8\" >0.4793</td>\n",
              "      <td id=\"T_d5c64_row79_col9\" class=\"data row79 col9\" >0.5608</td>\n",
              "      <td id=\"T_d5c64_row79_col10\" class=\"data row79 col10\" >0.5344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row80\" class=\"row_heading level2 row80\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row80_col0\" class=\"data row80 col0\" >0.5912</td>\n",
              "      <td id=\"T_d5c64_row80_col1\" class=\"data row80 col1\" >0.6899</td>\n",
              "      <td id=\"T_d5c64_row80_col2\" class=\"data row80 col2\" >0.8606</td>\n",
              "      <td id=\"T_d5c64_row80_col3\" class=\"data row80 col3\" >0.7540</td>\n",
              "      <td id=\"T_d5c64_row80_col4\" class=\"data row80 col4\" >0.3505</td>\n",
              "      <td id=\"T_d5c64_row80_col5\" class=\"data row80 col5\" >0.9673</td>\n",
              "      <td id=\"T_d5c64_row80_col6\" class=\"data row80 col6\" >0.3604</td>\n",
              "      <td id=\"T_d5c64_row80_col7\" class=\"data row80 col7\" >0.9501</td>\n",
              "      <td id=\"T_d5c64_row80_col8\" class=\"data row80 col8\" >0.5401</td>\n",
              "      <td id=\"T_d5c64_row80_col9\" class=\"data row80 col9\" >0.8348</td>\n",
              "      <td id=\"T_d5c64_row80_col10\" class=\"data row80 col10\" >0.3731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row81\" class=\"row_heading level2 row81\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row81_col0\" class=\"data row81 col0\" >0.9135</td>\n",
              "      <td id=\"T_d5c64_row81_col1\" class=\"data row81 col1\" >0.9262</td>\n",
              "      <td id=\"T_d5c64_row81_col2\" class=\"data row81 col2\" >0.9882</td>\n",
              "      <td id=\"T_d5c64_row81_col3\" class=\"data row81 col3\" >0.9608</td>\n",
              "      <td id=\"T_d5c64_row81_col4\" class=\"data row81 col4\" >0.6260</td>\n",
              "      <td id=\"T_d5c64_row81_col5\" class=\"data row81 col5\" >0.9862</td>\n",
              "      <td id=\"T_d5c64_row81_col6\" class=\"data row81 col6\" >0.6591</td>\n",
              "      <td id=\"T_d5c64_row81_col7\" class=\"data row81 col7\" >0.9777</td>\n",
              "      <td id=\"T_d5c64_row81_col8\" class=\"data row81 col8\" >0.9765</td>\n",
              "      <td id=\"T_d5c64_row81_col9\" class=\"data row81 col9\" >0.9848</td>\n",
              "      <td id=\"T_d5c64_row81_col10\" class=\"data row81 col10\" >0.3539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row82\" class=\"row_heading level2 row82\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row82_col0\" class=\"data row82 col0\" >0.3296</td>\n",
              "      <td id=\"T_d5c64_row82_col1\" class=\"data row82 col1\" >0.8944</td>\n",
              "      <td id=\"T_d5c64_row82_col2\" class=\"data row82 col2\" >0.7558</td>\n",
              "      <td id=\"T_d5c64_row82_col3\" class=\"data row82 col3\" >0.7449</td>\n",
              "      <td id=\"T_d5c64_row82_col4\" class=\"data row82 col4\" >0.3357</td>\n",
              "      <td id=\"T_d5c64_row82_col5\" class=\"data row82 col5\" >0.9517</td>\n",
              "      <td id=\"T_d5c64_row82_col6\" class=\"data row82 col6\" >0.8188</td>\n",
              "      <td id=\"T_d5c64_row82_col7\" class=\"data row82 col7\" >0.8808</td>\n",
              "      <td id=\"T_d5c64_row82_col8\" class=\"data row82 col8\" >0.3392</td>\n",
              "      <td id=\"T_d5c64_row82_col9\" class=\"data row82 col9\" >0.3617</td>\n",
              "      <td id=\"T_d5c64_row82_col10\" class=\"data row82 col10\" >0.3296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row83\" class=\"row_heading level2 row83\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row83_col0\" class=\"data row83 col0\" >0.8461</td>\n",
              "      <td id=\"T_d5c64_row83_col1\" class=\"data row83 col1\" >0.8307</td>\n",
              "      <td id=\"T_d5c64_row83_col2\" class=\"data row83 col2\" >0.9831</td>\n",
              "      <td id=\"T_d5c64_row83_col3\" class=\"data row83 col3\" >0.9557</td>\n",
              "      <td id=\"T_d5c64_row83_col4\" class=\"data row83 col4\" >0.6680</td>\n",
              "      <td id=\"T_d5c64_row83_col5\" class=\"data row83 col5\" >0.9879</td>\n",
              "      <td id=\"T_d5c64_row83_col6\" class=\"data row83 col6\" >0.4583</td>\n",
              "      <td id=\"T_d5c64_row83_col7\" class=\"data row83 col7\" >0.9725</td>\n",
              "      <td id=\"T_d5c64_row83_col8\" class=\"data row83 col8\" >0.9412</td>\n",
              "      <td id=\"T_d5c64_row83_col9\" class=\"data row83 col9\" >0.9680</td>\n",
              "      <td id=\"T_d5c64_row83_col10\" class=\"data row83 col10\" >0.6842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row84\" class=\"row_heading level1 row84\" rowspan=\"7\">opt-66b</th>\n",
              "      <th id=\"T_d5c64_level2_row84\" class=\"row_heading level2 row84\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row84_col0\" class=\"data row84 col0\" >0.7378</td>\n",
              "      <td id=\"T_d5c64_row84_col1\" class=\"data row84 col1\" >0.7608</td>\n",
              "      <td id=\"T_d5c64_row84_col2\" class=\"data row84 col2\" >0.6443</td>\n",
              "      <td id=\"T_d5c64_row84_col3\" class=\"data row84 col3\" >0.5872</td>\n",
              "      <td id=\"T_d5c64_row84_col4\" class=\"data row84 col4\" >0.4705</td>\n",
              "      <td id=\"T_d5c64_row84_col5\" class=\"data row84 col5\" >0.7658</td>\n",
              "      <td id=\"T_d5c64_row84_col6\" class=\"data row84 col6\" >0.7038</td>\n",
              "      <td id=\"T_d5c64_row84_col7\" class=\"data row84 col7\" >0.7662</td>\n",
              "      <td id=\"T_d5c64_row84_col8\" class=\"data row84 col8\" >0.6748</td>\n",
              "      <td id=\"T_d5c64_row84_col9\" class=\"data row84 col9\" >0.7543</td>\n",
              "      <td id=\"T_d5c64_row84_col10\" class=\"data row84 col10\" >0.8225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row85\" class=\"row_heading level2 row85\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row85_col0\" class=\"data row85 col0\" >0.4594</td>\n",
              "      <td id=\"T_d5c64_row85_col1\" class=\"data row85 col1\" >0.7202</td>\n",
              "      <td id=\"T_d5c64_row85_col2\" class=\"data row85 col2\" >0.6132</td>\n",
              "      <td id=\"T_d5c64_row85_col3\" class=\"data row85 col3\" >0.6954</td>\n",
              "      <td id=\"T_d5c64_row85_col4\" class=\"data row85 col4\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row85_col5\" class=\"data row85 col5\" >0.7692</td>\n",
              "      <td id=\"T_d5c64_row85_col6\" class=\"data row85 col6\" >0.6844</td>\n",
              "      <td id=\"T_d5c64_row85_col7\" class=\"data row85 col7\" >0.7265</td>\n",
              "      <td id=\"T_d5c64_row85_col8\" class=\"data row85 col8\" >0.4311</td>\n",
              "      <td id=\"T_d5c64_row85_col9\" class=\"data row85 col9\" >0.5203</td>\n",
              "      <td id=\"T_d5c64_row85_col10\" class=\"data row85 col10\" >0.3311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row86\" class=\"row_heading level2 row86\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row86_col0\" class=\"data row86 col0\" >0.4455</td>\n",
              "      <td id=\"T_d5c64_row86_col1\" class=\"data row86 col1\" >0.7310</td>\n",
              "      <td id=\"T_d5c64_row86_col2\" class=\"data row86 col2\" >0.4703</td>\n",
              "      <td id=\"T_d5c64_row86_col3\" class=\"data row86 col3\" >0.5913</td>\n",
              "      <td id=\"T_d5c64_row86_col4\" class=\"data row86 col4\" >0.4524</td>\n",
              "      <td id=\"T_d5c64_row86_col5\" class=\"data row86 col5\" >0.7703</td>\n",
              "      <td id=\"T_d5c64_row86_col6\" class=\"data row86 col6\" >0.5466</td>\n",
              "      <td id=\"T_d5c64_row86_col7\" class=\"data row86 col7\" >0.5972</td>\n",
              "      <td id=\"T_d5c64_row86_col8\" class=\"data row86 col8\" >0.5993</td>\n",
              "      <td id=\"T_d5c64_row86_col9\" class=\"data row86 col9\" >0.6464</td>\n",
              "      <td id=\"T_d5c64_row86_col10\" class=\"data row86 col10\" >0.4690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row87\" class=\"row_heading level2 row87\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row87_col0\" class=\"data row87 col0\" >0.3906</td>\n",
              "      <td id=\"T_d5c64_row87_col1\" class=\"data row87 col1\" >0.7297</td>\n",
              "      <td id=\"T_d5c64_row87_col2\" class=\"data row87 col2\" >0.4550</td>\n",
              "      <td id=\"T_d5c64_row87_col3\" class=\"data row87 col3\" >0.6957</td>\n",
              "      <td id=\"T_d5c64_row87_col4\" class=\"data row87 col4\" >0.3722</td>\n",
              "      <td id=\"T_d5c64_row87_col5\" class=\"data row87 col5\" >0.7992</td>\n",
              "      <td id=\"T_d5c64_row87_col6\" class=\"data row87 col6\" >0.3866</td>\n",
              "      <td id=\"T_d5c64_row87_col7\" class=\"data row87 col7\" >0.5949</td>\n",
              "      <td id=\"T_d5c64_row87_col8\" class=\"data row87 col8\" >0.5713</td>\n",
              "      <td id=\"T_d5c64_row87_col9\" class=\"data row87 col9\" >0.6426</td>\n",
              "      <td id=\"T_d5c64_row87_col10\" class=\"data row87 col10\" >0.4554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row88\" class=\"row_heading level2 row88\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row88_col0\" class=\"data row88 col0\" >0.9139</td>\n",
              "      <td id=\"T_d5c64_row88_col1\" class=\"data row88 col1\" >0.8500</td>\n",
              "      <td id=\"T_d5c64_row88_col2\" class=\"data row88 col2\" >0.9883</td>\n",
              "      <td id=\"T_d5c64_row88_col3\" class=\"data row88 col3\" >0.9493</td>\n",
              "      <td id=\"T_d5c64_row88_col4\" class=\"data row88 col4\" >0.6240</td>\n",
              "      <td id=\"T_d5c64_row88_col5\" class=\"data row88 col5\" >0.8762</td>\n",
              "      <td id=\"T_d5c64_row88_col6\" class=\"data row88 col6\" >0.8538</td>\n",
              "      <td id=\"T_d5c64_row88_col7\" class=\"data row88 col7\" >0.7872</td>\n",
              "      <td id=\"T_d5c64_row88_col8\" class=\"data row88 col8\" >0.9696</td>\n",
              "      <td id=\"T_d5c64_row88_col9\" class=\"data row88 col9\" >0.9627</td>\n",
              "      <td id=\"T_d5c64_row88_col10\" class=\"data row88 col10\" >0.6997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row89\" class=\"row_heading level2 row89\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row89_col0\" class=\"data row89 col0\" >0.4668</td>\n",
              "      <td id=\"T_d5c64_row89_col1\" class=\"data row89 col1\" >0.6129</td>\n",
              "      <td id=\"T_d5c64_row89_col2\" class=\"data row89 col2\" >0.8550</td>\n",
              "      <td id=\"T_d5c64_row89_col3\" class=\"data row89 col3\" >0.6502</td>\n",
              "      <td id=\"T_d5c64_row89_col4\" class=\"data row89 col4\" >0.3355</td>\n",
              "      <td id=\"T_d5c64_row89_col5\" class=\"data row89 col5\" >0.8141</td>\n",
              "      <td id=\"T_d5c64_row89_col6\" class=\"data row89 col6\" >0.8564</td>\n",
              "      <td id=\"T_d5c64_row89_col7\" class=\"data row89 col7\" >0.7337</td>\n",
              "      <td id=\"T_d5c64_row89_col8\" class=\"data row89 col8\" >0.5548</td>\n",
              "      <td id=\"T_d5c64_row89_col9\" class=\"data row89 col9\" >0.5646</td>\n",
              "      <td id=\"T_d5c64_row89_col10\" class=\"data row89 col10\" >0.5596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row90\" class=\"row_heading level2 row90\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row90_col0\" class=\"data row90 col0\" >0.6058</td>\n",
              "      <td id=\"T_d5c64_row90_col1\" class=\"data row90 col1\" >0.7452</td>\n",
              "      <td id=\"T_d5c64_row90_col2\" class=\"data row90 col2\" >0.8765</td>\n",
              "      <td id=\"T_d5c64_row90_col3\" class=\"data row90 col3\" >0.8104</td>\n",
              "      <td id=\"T_d5c64_row90_col4\" class=\"data row90 col4\" >0.4443</td>\n",
              "      <td id=\"T_d5c64_row90_col5\" class=\"data row90 col5\" >0.8123</td>\n",
              "      <td id=\"T_d5c64_row90_col6\" class=\"data row90 col6\" >0.4702</td>\n",
              "      <td id=\"T_d5c64_row90_col7\" class=\"data row90 col7\" >0.6335</td>\n",
              "      <td id=\"T_d5c64_row90_col8\" class=\"data row90 col8\" >0.8106</td>\n",
              "      <td id=\"T_d5c64_row90_col9\" class=\"data row90 col9\" >0.8408</td>\n",
              "      <td id=\"T_d5c64_row90_col10\" class=\"data row90 col10\" >0.5381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row91\" class=\"row_heading level1 row91\" rowspan=\"7\">opt-iml-max-1.3b</th>\n",
              "      <th id=\"T_d5c64_level2_row91\" class=\"row_heading level2 row91\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row91_col0\" class=\"data row91 col0\" >0.9537</td>\n",
              "      <td id=\"T_d5c64_row91_col1\" class=\"data row91 col1\" >0.9697</td>\n",
              "      <td id=\"T_d5c64_row91_col2\" class=\"data row91 col2\" >0.9632</td>\n",
              "      <td id=\"T_d5c64_row91_col3\" class=\"data row91 col3\" >0.8676</td>\n",
              "      <td id=\"T_d5c64_row91_col4\" class=\"data row91 col4\" >0.6840</td>\n",
              "      <td id=\"T_d5c64_row91_col5\" class=\"data row91 col5\" >0.9482</td>\n",
              "      <td id=\"T_d5c64_row91_col6\" class=\"data row91 col6\" >0.9699</td>\n",
              "      <td id=\"T_d5c64_row91_col7\" class=\"data row91 col7\" >0.8820</td>\n",
              "      <td id=\"T_d5c64_row91_col8\" class=\"data row91 col8\" >0.9027</td>\n",
              "      <td id=\"T_d5c64_row91_col9\" class=\"data row91 col9\" >0.9707</td>\n",
              "      <td id=\"T_d5c64_row91_col10\" class=\"data row91 col10\" >0.9528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row92\" class=\"row_heading level2 row92\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row92_col0\" class=\"data row92 col0\" >0.3527</td>\n",
              "      <td id=\"T_d5c64_row92_col1\" class=\"data row92 col1\" >0.9681</td>\n",
              "      <td id=\"T_d5c64_row92_col2\" class=\"data row92 col2\" >0.8392</td>\n",
              "      <td id=\"T_d5c64_row92_col3\" class=\"data row92 col3\" >0.8930</td>\n",
              "      <td id=\"T_d5c64_row92_col4\" class=\"data row92 col4\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row92_col5\" class=\"data row92 col5\" >0.9515</td>\n",
              "      <td id=\"T_d5c64_row92_col6\" class=\"data row92 col6\" >0.8798</td>\n",
              "      <td id=\"T_d5c64_row92_col7\" class=\"data row92 col7\" >0.9189</td>\n",
              "      <td id=\"T_d5c64_row92_col8\" class=\"data row92 col8\" >0.3280</td>\n",
              "      <td id=\"T_d5c64_row92_col9\" class=\"data row92 col9\" >0.3305</td>\n",
              "      <td id=\"T_d5c64_row92_col10\" class=\"data row92 col10\" >0.3307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row93\" class=\"row_heading level2 row93\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row93_col0\" class=\"data row93 col0\" >0.6084</td>\n",
              "      <td id=\"T_d5c64_row93_col1\" class=\"data row93 col1\" >0.9049</td>\n",
              "      <td id=\"T_d5c64_row93_col2\" class=\"data row93 col2\" >0.7598</td>\n",
              "      <td id=\"T_d5c64_row93_col3\" class=\"data row93 col3\" >0.5085</td>\n",
              "      <td id=\"T_d5c64_row93_col4\" class=\"data row93 col4\" >0.4730</td>\n",
              "      <td id=\"T_d5c64_row93_col5\" class=\"data row93 col5\" >0.9100</td>\n",
              "      <td id=\"T_d5c64_row93_col6\" class=\"data row93 col6\" >0.6565</td>\n",
              "      <td id=\"T_d5c64_row93_col7\" class=\"data row93 col7\" >0.6696</td>\n",
              "      <td id=\"T_d5c64_row93_col8\" class=\"data row93 col8\" >0.6559</td>\n",
              "      <td id=\"T_d5c64_row93_col9\" class=\"data row93 col9\" >0.7088</td>\n",
              "      <td id=\"T_d5c64_row93_col10\" class=\"data row93 col10\" >0.3941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row94\" class=\"row_heading level2 row94\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row94_col0\" class=\"data row94 col0\" >0.9434</td>\n",
              "      <td id=\"T_d5c64_row94_col1\" class=\"data row94 col1\" >0.9613</td>\n",
              "      <td id=\"T_d5c64_row94_col2\" class=\"data row94 col2\" >0.9210</td>\n",
              "      <td id=\"T_d5c64_row94_col3\" class=\"data row94 col3\" >0.9932</td>\n",
              "      <td id=\"T_d5c64_row94_col4\" class=\"data row94 col4\" >0.4045</td>\n",
              "      <td id=\"T_d5c64_row94_col5\" class=\"data row94 col5\" >0.9879</td>\n",
              "      <td id=\"T_d5c64_row94_col6\" class=\"data row94 col6\" >0.8410</td>\n",
              "      <td id=\"T_d5c64_row94_col7\" class=\"data row94 col7\" >0.9810</td>\n",
              "      <td id=\"T_d5c64_row94_col8\" class=\"data row94 col8\" >0.8970</td>\n",
              "      <td id=\"T_d5c64_row94_col9\" class=\"data row94 col9\" >0.9845</td>\n",
              "      <td id=\"T_d5c64_row94_col10\" class=\"data row94 col10\" >0.5838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row95\" class=\"row_heading level2 row95\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row95_col0\" class=\"data row95 col0\" >0.9931</td>\n",
              "      <td id=\"T_d5c64_row95_col1\" class=\"data row95 col1\" >0.9866</td>\n",
              "      <td id=\"T_d5c64_row95_col2\" class=\"data row95 col2\" >0.9933</td>\n",
              "      <td id=\"T_d5c64_row95_col3\" class=\"data row95 col3\" >0.8710</td>\n",
              "      <td id=\"T_d5c64_row95_col4\" class=\"data row95 col4\" >0.5865</td>\n",
              "      <td id=\"T_d5c64_row95_col5\" class=\"data row95 col5\" >0.9827</td>\n",
              "      <td id=\"T_d5c64_row95_col6\" class=\"data row95 col6\" >0.9329</td>\n",
              "      <td id=\"T_d5c64_row95_col7\" class=\"data row95 col7\" >0.9828</td>\n",
              "      <td id=\"T_d5c64_row95_col8\" class=\"data row95 col8\" >0.9949</td>\n",
              "      <td id=\"T_d5c64_row95_col9\" class=\"data row95 col9\" >0.9931</td>\n",
              "      <td id=\"T_d5c64_row95_col10\" class=\"data row95 col10\" >0.8744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row96\" class=\"row_heading level2 row96\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row96_col0\" class=\"data row96 col0\" >0.4128</td>\n",
              "      <td id=\"T_d5c64_row96_col1\" class=\"data row96 col1\" >0.9646</td>\n",
              "      <td id=\"T_d5c64_row96_col2\" class=\"data row96 col2\" >0.9498</td>\n",
              "      <td id=\"T_d5c64_row96_col3\" class=\"data row96 col3\" >0.7265</td>\n",
              "      <td id=\"T_d5c64_row96_col4\" class=\"data row96 col4\" >0.2160</td>\n",
              "      <td id=\"T_d5c64_row96_col5\" class=\"data row96 col5\" >0.9602</td>\n",
              "      <td id=\"T_d5c64_row96_col6\" class=\"data row96 col6\" >0.9381</td>\n",
              "      <td id=\"T_d5c64_row96_col7\" class=\"data row96 col7\" >0.9103</td>\n",
              "      <td id=\"T_d5c64_row96_col8\" class=\"data row96 col8\" >0.5456</td>\n",
              "      <td id=\"T_d5c64_row96_col9\" class=\"data row96 col9\" >0.6839</td>\n",
              "      <td id=\"T_d5c64_row96_col10\" class=\"data row96 col10\" >0.5212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row97\" class=\"row_heading level2 row97\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row97_col0\" class=\"data row97 col0\" >0.9794</td>\n",
              "      <td id=\"T_d5c64_row97_col1\" class=\"data row97 col1\" >0.9916</td>\n",
              "      <td id=\"T_d5c64_row97_col2\" class=\"data row97 col2\" >0.8583</td>\n",
              "      <td id=\"T_d5c64_row97_col3\" class=\"data row97 col3\" >0.9881</td>\n",
              "      <td id=\"T_d5c64_row97_col4\" class=\"data row97 col4\" >0.6192</td>\n",
              "      <td id=\"T_d5c64_row97_col5\" class=\"data row97 col5\" >0.9810</td>\n",
              "      <td id=\"T_d5c64_row97_col6\" class=\"data row97 col6\" >0.9799</td>\n",
              "      <td id=\"T_d5c64_row97_col7\" class=\"data row97 col7\" >0.9793</td>\n",
              "      <td id=\"T_d5c64_row97_col8\" class=\"data row97 col8\" >0.7578</td>\n",
              "      <td id=\"T_d5c64_row97_col9\" class=\"data row97 col9\" >0.7060</td>\n",
              "      <td id=\"T_d5c64_row97_col10\" class=\"data row97 col10\" >0.8187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row98\" class=\"row_heading level1 row98\" rowspan=\"7\">text-davinci-003</th>\n",
              "      <th id=\"T_d5c64_level2_row98\" class=\"row_heading level2 row98\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row98_col0\" class=\"data row98 col0\" >0.7319</td>\n",
              "      <td id=\"T_d5c64_row98_col1\" class=\"data row98 col1\" >0.9047</td>\n",
              "      <td id=\"T_d5c64_row98_col2\" class=\"data row98 col2\" >0.8833</td>\n",
              "      <td id=\"T_d5c64_row98_col3\" class=\"data row98 col3\" >0.8602</td>\n",
              "      <td id=\"T_d5c64_row98_col4\" class=\"data row98 col4\" >0.7605</td>\n",
              "      <td id=\"T_d5c64_row98_col5\" class=\"data row98 col5\" >0.9312</td>\n",
              "      <td id=\"T_d5c64_row98_col6\" class=\"data row98 col6\" >0.8892</td>\n",
              "      <td id=\"T_d5c64_row98_col7\" class=\"data row98 col7\" >0.8631</td>\n",
              "      <td id=\"T_d5c64_row98_col8\" class=\"data row98 col8\" >0.8027</td>\n",
              "      <td id=\"T_d5c64_row98_col9\" class=\"data row98 col9\" >0.7405</td>\n",
              "      <td id=\"T_d5c64_row98_col10\" class=\"data row98 col10\" >0.8400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row99\" class=\"row_heading level2 row99\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row99_col0\" class=\"data row99 col0\" >0.4855</td>\n",
              "      <td id=\"T_d5c64_row99_col1\" class=\"data row99 col1\" >0.5814</td>\n",
              "      <td id=\"T_d5c64_row99_col2\" class=\"data row99 col2\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row99_col3\" class=\"data row99 col3\" >0.3296</td>\n",
              "      <td id=\"T_d5c64_row99_col4\" class=\"data row99 col4\" >0.3373</td>\n",
              "      <td id=\"T_d5c64_row99_col5\" class=\"data row99 col5\" >0.9605</td>\n",
              "      <td id=\"T_d5c64_row99_col6\" class=\"data row99 col6\" >0.4529</td>\n",
              "      <td id=\"T_d5c64_row99_col7\" class=\"data row99 col7\" >0.8925</td>\n",
              "      <td id=\"T_d5c64_row99_col8\" class=\"data row99 col8\" >0.5528</td>\n",
              "      <td id=\"T_d5c64_row99_col9\" class=\"data row99 col9\" >0.5678</td>\n",
              "      <td id=\"T_d5c64_row99_col10\" class=\"data row99 col10\" >0.3370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row100\" class=\"row_heading level2 row100\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row100_col0\" class=\"data row100 col0\" >0.6145</td>\n",
              "      <td id=\"T_d5c64_row100_col1\" class=\"data row100 col1\" >0.9250</td>\n",
              "      <td id=\"T_d5c64_row100_col2\" class=\"data row100 col2\" >0.6295</td>\n",
              "      <td id=\"T_d5c64_row100_col3\" class=\"data row100 col3\" >0.7554</td>\n",
              "      <td id=\"T_d5c64_row100_col4\" class=\"data row100 col4\" >0.5875</td>\n",
              "      <td id=\"T_d5c64_row100_col5\" class=\"data row100 col5\" >0.9047</td>\n",
              "      <td id=\"T_d5c64_row100_col6\" class=\"data row100 col6\" >0.7750</td>\n",
              "      <td id=\"T_d5c64_row100_col7\" class=\"data row100 col7\" >0.8313</td>\n",
              "      <td id=\"T_d5c64_row100_col8\" class=\"data row100 col8\" >0.7541</td>\n",
              "      <td id=\"T_d5c64_row100_col9\" class=\"data row100 col9\" >0.6885</td>\n",
              "      <td id=\"T_d5c64_row100_col10\" class=\"data row100 col10\" >0.3355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row101\" class=\"row_heading level2 row101\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row101_col0\" class=\"data row101 col0\" >0.7188</td>\n",
              "      <td id=\"T_d5c64_row101_col1\" class=\"data row101 col1\" >0.8020</td>\n",
              "      <td id=\"T_d5c64_row101_col2\" class=\"data row101 col2\" >0.3832</td>\n",
              "      <td id=\"T_d5c64_row101_col3\" class=\"data row101 col3\" >0.9274</td>\n",
              "      <td id=\"T_d5c64_row101_col4\" class=\"data row101 col4\" >0.9039</td>\n",
              "      <td id=\"T_d5c64_row101_col5\" class=\"data row101 col5\" >0.9657</td>\n",
              "      <td id=\"T_d5c64_row101_col6\" class=\"data row101 col6\" >0.9382</td>\n",
              "      <td id=\"T_d5c64_row101_col7\" class=\"data row101 col7\" >0.9165</td>\n",
              "      <td id=\"T_d5c64_row101_col8\" class=\"data row101 col8\" >0.8578</td>\n",
              "      <td id=\"T_d5c64_row101_col9\" class=\"data row101 col9\" >0.7165</td>\n",
              "      <td id=\"T_d5c64_row101_col10\" class=\"data row101 col10\" >0.7663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row102\" class=\"row_heading level2 row102\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row102_col0\" class=\"data row102 col0\" >0.6436</td>\n",
              "      <td id=\"T_d5c64_row102_col1\" class=\"data row102 col1\" >0.7615</td>\n",
              "      <td id=\"T_d5c64_row102_col2\" class=\"data row102 col2\" >0.9248</td>\n",
              "      <td id=\"T_d5c64_row102_col3\" class=\"data row102 col3\" >0.9239</td>\n",
              "      <td id=\"T_d5c64_row102_col4\" class=\"data row102 col4\" >0.9404</td>\n",
              "      <td id=\"T_d5c64_row102_col5\" class=\"data row102 col5\" >0.8059</td>\n",
              "      <td id=\"T_d5c64_row102_col6\" class=\"data row102 col6\" >0.9065</td>\n",
              "      <td id=\"T_d5c64_row102_col7\" class=\"data row102 col7\" >0.8484</td>\n",
              "      <td id=\"T_d5c64_row102_col8\" class=\"data row102 col8\" >0.8597</td>\n",
              "      <td id=\"T_d5c64_row102_col9\" class=\"data row102 col9\" >0.7446</td>\n",
              "      <td id=\"T_d5c64_row102_col10\" class=\"data row102 col10\" >0.9166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row103\" class=\"row_heading level2 row103\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row103_col0\" class=\"data row103 col0\" >0.3329</td>\n",
              "      <td id=\"T_d5c64_row103_col1\" class=\"data row103 col1\" >0.5973</td>\n",
              "      <td id=\"T_d5c64_row103_col2\" class=\"data row103 col2\" >0.7689</td>\n",
              "      <td id=\"T_d5c64_row103_col3\" class=\"data row103 col3\" >0.8681</td>\n",
              "      <td id=\"T_d5c64_row103_col4\" class=\"data row103 col4\" >0.2113</td>\n",
              "      <td id=\"T_d5c64_row103_col5\" class=\"data row103 col5\" >0.9415</td>\n",
              "      <td id=\"T_d5c64_row103_col6\" class=\"data row103 col6\" >0.7787</td>\n",
              "      <td id=\"T_d5c64_row103_col7\" class=\"data row103 col7\" >0.9003</td>\n",
              "      <td id=\"T_d5c64_row103_col8\" class=\"data row103 col8\" >0.3337</td>\n",
              "      <td id=\"T_d5c64_row103_col9\" class=\"data row103 col9\" >0.3388</td>\n",
              "      <td id=\"T_d5c64_row103_col10\" class=\"data row103 col10\" >0.7381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row104\" class=\"row_heading level2 row104\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row104_col0\" class=\"data row104 col0\" >0.7993</td>\n",
              "      <td id=\"T_d5c64_row104_col1\" class=\"data row104 col1\" >0.9264</td>\n",
              "      <td id=\"T_d5c64_row104_col2\" class=\"data row104 col2\" >0.9482</td>\n",
              "      <td id=\"T_d5c64_row104_col3\" class=\"data row104 col3\" >0.9510</td>\n",
              "      <td id=\"T_d5c64_row104_col4\" class=\"data row104 col4\" >0.9422</td>\n",
              "      <td id=\"T_d5c64_row104_col5\" class=\"data row104 col5\" >0.9777</td>\n",
              "      <td id=\"T_d5c64_row104_col6\" class=\"data row104 col6\" >0.9733</td>\n",
              "      <td id=\"T_d5c64_row104_col7\" class=\"data row104 col7\" >0.9352</td>\n",
              "      <td id=\"T_d5c64_row104_col8\" class=\"data row104 col8\" >0.8944</td>\n",
              "      <td id=\"T_d5c64_row104_col9\" class=\"data row104 col9\" >0.7921</td>\n",
              "      <td id=\"T_d5c64_row104_col10\" class=\"data row104 col10\" >0.8768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row105\" class=\"row_heading level1 row105\" rowspan=\"7\">vicuna-13b</th>\n",
              "      <th id=\"T_d5c64_level2_row105\" class=\"row_heading level2 row105\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row105_col0\" class=\"data row105 col0\" >0.8628</td>\n",
              "      <td id=\"T_d5c64_row105_col1\" class=\"data row105 col1\" >0.9149</td>\n",
              "      <td id=\"T_d5c64_row105_col2\" class=\"data row105 col2\" >0.7705</td>\n",
              "      <td id=\"T_d5c64_row105_col3\" class=\"data row105 col3\" >0.8341</td>\n",
              "      <td id=\"T_d5c64_row105_col4\" class=\"data row105 col4\" >0.7503</td>\n",
              "      <td id=\"T_d5c64_row105_col5\" class=\"data row105 col5\" >0.9281</td>\n",
              "      <td id=\"T_d5c64_row105_col6\" class=\"data row105 col6\" >0.8996</td>\n",
              "      <td id=\"T_d5c64_row105_col7\" class=\"data row105 col7\" >0.8823</td>\n",
              "      <td id=\"T_d5c64_row105_col8\" class=\"data row105 col8\" >0.8226</td>\n",
              "      <td id=\"T_d5c64_row105_col9\" class=\"data row105 col9\" >0.7869</td>\n",
              "      <td id=\"T_d5c64_row105_col10\" class=\"data row105 col10\" >0.8514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row106\" class=\"row_heading level2 row106\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row106_col0\" class=\"data row106 col0\" >0.4812</td>\n",
              "      <td id=\"T_d5c64_row106_col1\" class=\"data row106 col1\" >0.5839</td>\n",
              "      <td id=\"T_d5c64_row106_col2\" class=\"data row106 col2\" >0.3798</td>\n",
              "      <td id=\"T_d5c64_row106_col3\" class=\"data row106 col3\" >0.3827</td>\n",
              "      <td id=\"T_d5c64_row106_col4\" class=\"data row106 col4\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row106_col5\" class=\"data row106 col5\" >0.9743</td>\n",
              "      <td id=\"T_d5c64_row106_col6\" class=\"data row106 col6\" >0.4827</td>\n",
              "      <td id=\"T_d5c64_row106_col7\" class=\"data row106 col7\" >0.8677</td>\n",
              "      <td id=\"T_d5c64_row106_col8\" class=\"data row106 col8\" >0.5833</td>\n",
              "      <td id=\"T_d5c64_row106_col9\" class=\"data row106 col9\" >0.5508</td>\n",
              "      <td id=\"T_d5c64_row106_col10\" class=\"data row106 col10\" >0.4081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row107\" class=\"row_heading level2 row107\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row107_col0\" class=\"data row107 col0\" >0.7679</td>\n",
              "      <td id=\"T_d5c64_row107_col1\" class=\"data row107 col1\" >0.9232</td>\n",
              "      <td id=\"T_d5c64_row107_col2\" class=\"data row107 col2\" >0.5873</td>\n",
              "      <td id=\"T_d5c64_row107_col3\" class=\"data row107 col3\" >0.6943</td>\n",
              "      <td id=\"T_d5c64_row107_col4\" class=\"data row107 col4\" >0.7214</td>\n",
              "      <td id=\"T_d5c64_row107_col5\" class=\"data row107 col5\" >0.9159</td>\n",
              "      <td id=\"T_d5c64_row107_col6\" class=\"data row107 col6\" >0.7333</td>\n",
              "      <td id=\"T_d5c64_row107_col7\" class=\"data row107 col7\" >0.8299</td>\n",
              "      <td id=\"T_d5c64_row107_col8\" class=\"data row107 col8\" >0.5051</td>\n",
              "      <td id=\"T_d5c64_row107_col9\" class=\"data row107 col9\" >0.5253</td>\n",
              "      <td id=\"T_d5c64_row107_col10\" class=\"data row107 col10\" >0.4208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row108\" class=\"row_heading level2 row108\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row108_col0\" class=\"data row108 col0\" >0.7234</td>\n",
              "      <td id=\"T_d5c64_row108_col1\" class=\"data row108 col1\" >0.8716</td>\n",
              "      <td id=\"T_d5c64_row108_col2\" class=\"data row108 col2\" >0.5427</td>\n",
              "      <td id=\"T_d5c64_row108_col3\" class=\"data row108 col3\" >0.9206</td>\n",
              "      <td id=\"T_d5c64_row108_col4\" class=\"data row108 col4\" >0.6739</td>\n",
              "      <td id=\"T_d5c64_row108_col5\" class=\"data row108 col5\" >0.9674</td>\n",
              "      <td id=\"T_d5c64_row108_col6\" class=\"data row108 col6\" >0.6880</td>\n",
              "      <td id=\"T_d5c64_row108_col7\" class=\"data row108 col7\" >0.9164</td>\n",
              "      <td id=\"T_d5c64_row108_col8\" class=\"data row108 col8\" >0.7205</td>\n",
              "      <td id=\"T_d5c64_row108_col9\" class=\"data row108 col9\" >0.7893</td>\n",
              "      <td id=\"T_d5c64_row108_col10\" class=\"data row108 col10\" >0.6191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row109\" class=\"row_heading level2 row109\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row109_col0\" class=\"data row109 col0\" >0.6784</td>\n",
              "      <td id=\"T_d5c64_row109_col1\" class=\"data row109 col1\" >0.8994</td>\n",
              "      <td id=\"T_d5c64_row109_col2\" class=\"data row109 col2\" >0.8387</td>\n",
              "      <td id=\"T_d5c64_row109_col3\" class=\"data row109 col3\" >0.9052</td>\n",
              "      <td id=\"T_d5c64_row109_col4\" class=\"data row109 col4\" >0.9169</td>\n",
              "      <td id=\"T_d5c64_row109_col5\" class=\"data row109 col5\" >0.9520</td>\n",
              "      <td id=\"T_d5c64_row109_col6\" class=\"data row109 col6\" >0.8753</td>\n",
              "      <td id=\"T_d5c64_row109_col7\" class=\"data row109 col7\" >0.8956</td>\n",
              "      <td id=\"T_d5c64_row109_col8\" class=\"data row109 col8\" >0.8595</td>\n",
              "      <td id=\"T_d5c64_row109_col9\" class=\"data row109 col9\" >0.8562</td>\n",
              "      <td id=\"T_d5c64_row109_col10\" class=\"data row109 col10\" >0.8291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row110\" class=\"row_heading level2 row110\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row110_col0\" class=\"data row110 col0\" >0.5358</td>\n",
              "      <td id=\"T_d5c64_row110_col1\" class=\"data row110 col1\" >0.8853</td>\n",
              "      <td id=\"T_d5c64_row110_col2\" class=\"data row110 col2\" >0.6725</td>\n",
              "      <td id=\"T_d5c64_row110_col3\" class=\"data row110 col3\" >0.8681</td>\n",
              "      <td id=\"T_d5c64_row110_col4\" class=\"data row110 col4\" >0.3147</td>\n",
              "      <td id=\"T_d5c64_row110_col5\" class=\"data row110 col5\" >0.9432</td>\n",
              "      <td id=\"T_d5c64_row110_col6\" class=\"data row110 col6\" >0.8460</td>\n",
              "      <td id=\"T_d5c64_row110_col7\" class=\"data row110 col7\" >0.9058</td>\n",
              "      <td id=\"T_d5c64_row110_col8\" class=\"data row110 col8\" >0.4371</td>\n",
              "      <td id=\"T_d5c64_row110_col9\" class=\"data row110 col9\" >0.3755</td>\n",
              "      <td id=\"T_d5c64_row110_col10\" class=\"data row110 col10\" >0.5963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row111\" class=\"row_heading level2 row111\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row111_col0\" class=\"data row111 col0\" >0.8447</td>\n",
              "      <td id=\"T_d5c64_row111_col1\" class=\"data row111 col1\" >0.8789</td>\n",
              "      <td id=\"T_d5c64_row111_col2\" class=\"data row111 col2\" >0.6681</td>\n",
              "      <td id=\"T_d5c64_row111_col3\" class=\"data row111 col3\" >0.8614</td>\n",
              "      <td id=\"T_d5c64_row111_col4\" class=\"data row111 col4\" >0.9061</td>\n",
              "      <td id=\"T_d5c64_row111_col5\" class=\"data row111 col5\" >0.9434</td>\n",
              "      <td id=\"T_d5c64_row111_col6\" class=\"data row111 col6\" >0.8267</td>\n",
              "      <td id=\"T_d5c64_row111_col7\" class=\"data row111 col7\" >0.8959</td>\n",
              "      <td id=\"T_d5c64_row111_col8\" class=\"data row111 col8\" >0.7773</td>\n",
              "      <td id=\"T_d5c64_row111_col9\" class=\"data row111 col9\" >0.6400</td>\n",
              "      <td id=\"T_d5c64_row111_col10\" class=\"data row111 col10\" >0.6473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level0_row112\" class=\"row_heading level0 row112\" rowspan=\"56\">ru</th>\n",
              "      <th id=\"T_d5c64_level1_row112\" class=\"row_heading level1 row112\" rowspan=\"7\">alpaca-lora-30b</th>\n",
              "      <th id=\"T_d5c64_level2_row112\" class=\"row_heading level2 row112\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row112_col0\" class=\"data row112 col0\" >0.6026</td>\n",
              "      <td id=\"T_d5c64_row112_col1\" class=\"data row112 col1\" >0.6327</td>\n",
              "      <td id=\"T_d5c64_row112_col2\" class=\"data row112 col2\" >0.8067</td>\n",
              "      <td id=\"T_d5c64_row112_col3\" class=\"data row112 col3\" >0.7409</td>\n",
              "      <td id=\"T_d5c64_row112_col4\" class=\"data row112 col4\" >0.4634</td>\n",
              "      <td id=\"T_d5c64_row112_col5\" class=\"data row112 col5\" >0.6572</td>\n",
              "      <td id=\"T_d5c64_row112_col6\" class=\"data row112 col6\" >0.6705</td>\n",
              "      <td id=\"T_d5c64_row112_col7\" class=\"data row112 col7\" >0.6567</td>\n",
              "      <td id=\"T_d5c64_row112_col8\" class=\"data row112 col8\" >0.9099</td>\n",
              "      <td id=\"T_d5c64_row112_col9\" class=\"data row112 col9\" >0.8829</td>\n",
              "      <td id=\"T_d5c64_row112_col10\" class=\"data row112 col10\" >0.6696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row113\" class=\"row_heading level2 row113\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row113_col0\" class=\"data row113 col0\" >0.4945</td>\n",
              "      <td id=\"T_d5c64_row113_col1\" class=\"data row113 col1\" >0.3680</td>\n",
              "      <td id=\"T_d5c64_row113_col2\" class=\"data row113 col2\" >0.3443</td>\n",
              "      <td id=\"T_d5c64_row113_col3\" class=\"data row113 col3\" >0.3516</td>\n",
              "      <td id=\"T_d5c64_row113_col4\" class=\"data row113 col4\" >0.3829</td>\n",
              "      <td id=\"T_d5c64_row113_col5\" class=\"data row113 col5\" >0.5214</td>\n",
              "      <td id=\"T_d5c64_row113_col6\" class=\"data row113 col6\" >0.4081</td>\n",
              "      <td id=\"T_d5c64_row113_col7\" class=\"data row113 col7\" >0.3884</td>\n",
              "      <td id=\"T_d5c64_row113_col8\" class=\"data row113 col8\" >0.5506</td>\n",
              "      <td id=\"T_d5c64_row113_col9\" class=\"data row113 col9\" >0.5100</td>\n",
              "      <td id=\"T_d5c64_row113_col10\" class=\"data row113 col10\" >0.4514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row114\" class=\"row_heading level2 row114\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row114_col0\" class=\"data row114 col0\" >0.7291</td>\n",
              "      <td id=\"T_d5c64_row114_col1\" class=\"data row114 col1\" >0.4784</td>\n",
              "      <td id=\"T_d5c64_row114_col2\" class=\"data row114 col2\" >0.4665</td>\n",
              "      <td id=\"T_d5c64_row114_col3\" class=\"data row114 col3\" >0.4094</td>\n",
              "      <td id=\"T_d5c64_row114_col4\" class=\"data row114 col4\" >0.6639</td>\n",
              "      <td id=\"T_d5c64_row114_col5\" class=\"data row114 col5\" >0.6505</td>\n",
              "      <td id=\"T_d5c64_row114_col6\" class=\"data row114 col6\" >0.4265</td>\n",
              "      <td id=\"T_d5c64_row114_col7\" class=\"data row114 col7\" >0.5331</td>\n",
              "      <td id=\"T_d5c64_row114_col8\" class=\"data row114 col8\" >0.8599</td>\n",
              "      <td id=\"T_d5c64_row114_col9\" class=\"data row114 col9\" >0.8249</td>\n",
              "      <td id=\"T_d5c64_row114_col10\" class=\"data row114 col10\" >0.3950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row115\" class=\"row_heading level2 row115\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row115_col0\" class=\"data row115 col0\" >0.5635</td>\n",
              "      <td id=\"T_d5c64_row115_col1\" class=\"data row115 col1\" >0.4123</td>\n",
              "      <td id=\"T_d5c64_row115_col2\" class=\"data row115 col2\" >0.6725</td>\n",
              "      <td id=\"T_d5c64_row115_col3\" class=\"data row115 col3\" >0.8716</td>\n",
              "      <td id=\"T_d5c64_row115_col4\" class=\"data row115 col4\" >0.5272</td>\n",
              "      <td id=\"T_d5c64_row115_col5\" class=\"data row115 col5\" >0.7714</td>\n",
              "      <td id=\"T_d5c64_row115_col6\" class=\"data row115 col6\" >0.5325</td>\n",
              "      <td id=\"T_d5c64_row115_col7\" class=\"data row115 col7\" >0.7920</td>\n",
              "      <td id=\"T_d5c64_row115_col8\" class=\"data row115 col8\" >0.9350</td>\n",
              "      <td id=\"T_d5c64_row115_col9\" class=\"data row115 col9\" >0.9129</td>\n",
              "      <td id=\"T_d5c64_row115_col10\" class=\"data row115 col10\" >0.4906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row116\" class=\"row_heading level2 row116\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row116_col0\" class=\"data row116 col0\" >0.9716</td>\n",
              "      <td id=\"T_d5c64_row116_col1\" class=\"data row116 col1\" >0.8123</td>\n",
              "      <td id=\"T_d5c64_row116_col2\" class=\"data row116 col2\" >0.9850</td>\n",
              "      <td id=\"T_d5c64_row116_col3\" class=\"data row116 col3\" >0.9409</td>\n",
              "      <td id=\"T_d5c64_row116_col4\" class=\"data row116 col4\" >0.8550</td>\n",
              "      <td id=\"T_d5c64_row116_col5\" class=\"data row116 col5\" >0.8885</td>\n",
              "      <td id=\"T_d5c64_row116_col6\" class=\"data row116 col6\" >0.8868</td>\n",
              "      <td id=\"T_d5c64_row116_col7\" class=\"data row116 col7\" >0.8199</td>\n",
              "      <td id=\"T_d5c64_row116_col8\" class=\"data row116 col8\" >0.9617</td>\n",
              "      <td id=\"T_d5c64_row116_col9\" class=\"data row116 col9\" >0.9615</td>\n",
              "      <td id=\"T_d5c64_row116_col10\" class=\"data row116 col10\" >0.8237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row117\" class=\"row_heading level2 row117\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row117_col0\" class=\"data row117 col0\" >0.7062</td>\n",
              "      <td id=\"T_d5c64_row117_col1\" class=\"data row117 col1\" >0.3326</td>\n",
              "      <td id=\"T_d5c64_row117_col2\" class=\"data row117 col2\" >0.3407</td>\n",
              "      <td id=\"T_d5c64_row117_col3\" class=\"data row117 col3\" >0.3363</td>\n",
              "      <td id=\"T_d5c64_row117_col4\" class=\"data row117 col4\" >0.2381</td>\n",
              "      <td id=\"T_d5c64_row117_col5\" class=\"data row117 col5\" >0.3394</td>\n",
              "      <td id=\"T_d5c64_row117_col6\" class=\"data row117 col6\" >0.3370</td>\n",
              "      <td id=\"T_d5c64_row117_col7\" class=\"data row117 col7\" >0.3421</td>\n",
              "      <td id=\"T_d5c64_row117_col8\" class=\"data row117 col8\" >0.8599</td>\n",
              "      <td id=\"T_d5c64_row117_col9\" class=\"data row117 col9\" >0.8464</td>\n",
              "      <td id=\"T_d5c64_row117_col10\" class=\"data row117 col10\" >0.5544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row118\" class=\"row_heading level2 row118\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row118_col0\" class=\"data row118 col0\" >0.9180</td>\n",
              "      <td id=\"T_d5c64_row118_col1\" class=\"data row118 col1\" >0.8983</td>\n",
              "      <td id=\"T_d5c64_row118_col2\" class=\"data row118 col2\" >0.9700</td>\n",
              "      <td id=\"T_d5c64_row118_col3\" class=\"data row118 col3\" >0.9442</td>\n",
              "      <td id=\"T_d5c64_row118_col4\" class=\"data row118 col4\" >0.9169</td>\n",
              "      <td id=\"T_d5c64_row118_col5\" class=\"data row118 col5\" >0.9058</td>\n",
              "      <td id=\"T_d5c64_row118_col6\" class=\"data row118 col6\" >0.9431</td>\n",
              "      <td id=\"T_d5c64_row118_col7\" class=\"data row118 col7\" >0.8993</td>\n",
              "      <td id=\"T_d5c64_row118_col8\" class=\"data row118 col8\" >0.9433</td>\n",
              "      <td id=\"T_d5c64_row118_col9\" class=\"data row118 col9\" >0.9397</td>\n",
              "      <td id=\"T_d5c64_row118_col10\" class=\"data row118 col10\" >0.8522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row119\" class=\"row_heading level1 row119\" rowspan=\"7\">gpt-3.5-turbo</th>\n",
              "      <th id=\"T_d5c64_level2_row119\" class=\"row_heading level2 row119\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row119_col0\" class=\"data row119 col0\" >0.9416</td>\n",
              "      <td id=\"T_d5c64_row119_col1\" class=\"data row119 col1\" >0.8680</td>\n",
              "      <td id=\"T_d5c64_row119_col2\" class=\"data row119 col2\" >0.9045</td>\n",
              "      <td id=\"T_d5c64_row119_col3\" class=\"data row119 col3\" >0.8538</td>\n",
              "      <td id=\"T_d5c64_row119_col4\" class=\"data row119 col4\" >0.7810</td>\n",
              "      <td id=\"T_d5c64_row119_col5\" class=\"data row119 col5\" >0.8754</td>\n",
              "      <td id=\"T_d5c64_row119_col6\" class=\"data row119 col6\" >0.8783</td>\n",
              "      <td id=\"T_d5c64_row119_col7\" class=\"data row119 col7\" >0.8378</td>\n",
              "      <td id=\"T_d5c64_row119_col8\" class=\"data row119 col8\" >0.9567</td>\n",
              "      <td id=\"T_d5c64_row119_col9\" class=\"data row119 col9\" >0.9331</td>\n",
              "      <td id=\"T_d5c64_row119_col10\" class=\"data row119 col10\" >0.8666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row120\" class=\"row_heading level2 row120\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row120_col0\" class=\"data row120 col0\" >0.6636</td>\n",
              "      <td id=\"T_d5c64_row120_col1\" class=\"data row120 col1\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row120_col2\" class=\"data row120 col2\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row120_col3\" class=\"data row120 col3\" >0.3303</td>\n",
              "      <td id=\"T_d5c64_row120_col4\" class=\"data row120 col4\" >0.3457</td>\n",
              "      <td id=\"T_d5c64_row120_col5\" class=\"data row120 col5\" >0.3272</td>\n",
              "      <td id=\"T_d5c64_row120_col6\" class=\"data row120 col6\" >0.3330</td>\n",
              "      <td id=\"T_d5c64_row120_col7\" class=\"data row120 col7\" >0.3276</td>\n",
              "      <td id=\"T_d5c64_row120_col8\" class=\"data row120 col8\" >0.6336</td>\n",
              "      <td id=\"T_d5c64_row120_col9\" class=\"data row120 col9\" >0.5592</td>\n",
              "      <td id=\"T_d5c64_row120_col10\" class=\"data row120 col10\" >0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row121\" class=\"row_heading level2 row121\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row121_col0\" class=\"data row121 col0\" >0.5384</td>\n",
              "      <td id=\"T_d5c64_row121_col1\" class=\"data row121 col1\" >0.3443</td>\n",
              "      <td id=\"T_d5c64_row121_col2\" class=\"data row121 col2\" >0.3407</td>\n",
              "      <td id=\"T_d5c64_row121_col3\" class=\"data row121 col3\" >0.3369</td>\n",
              "      <td id=\"T_d5c64_row121_col4\" class=\"data row121 col4\" >0.3423</td>\n",
              "      <td id=\"T_d5c64_row121_col5\" class=\"data row121 col5\" >0.3264</td>\n",
              "      <td id=\"T_d5c64_row121_col6\" class=\"data row121 col6\" >0.3403</td>\n",
              "      <td id=\"T_d5c64_row121_col7\" class=\"data row121 col7\" >0.3350</td>\n",
              "      <td id=\"T_d5c64_row121_col8\" class=\"data row121 col8\" >0.8716</td>\n",
              "      <td id=\"T_d5c64_row121_col9\" class=\"data row121 col9\" >0.8061</td>\n",
              "      <td id=\"T_d5c64_row121_col10\" class=\"data row121 col10\" >0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row122\" class=\"row_heading level2 row122\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row122_col0\" class=\"data row122 col0\" >0.7846</td>\n",
              "      <td id=\"T_d5c64_row122_col1\" class=\"data row122 col1\" >0.8857</td>\n",
              "      <td id=\"T_d5c64_row122_col2\" class=\"data row122 col2\" >0.3552</td>\n",
              "      <td id=\"T_d5c64_row122_col3\" class=\"data row122 col3\" >0.9070</td>\n",
              "      <td id=\"T_d5c64_row122_col4\" class=\"data row122 col4\" >0.8817</td>\n",
              "      <td id=\"T_d5c64_row122_col5\" class=\"data row122 col5\" >0.9228</td>\n",
              "      <td id=\"T_d5c64_row122_col6\" class=\"data row122 col6\" >0.9230</td>\n",
              "      <td id=\"T_d5c64_row122_col7\" class=\"data row122 col7\" >0.9043</td>\n",
              "      <td id=\"T_d5c64_row122_col8\" class=\"data row122 col8\" >0.9533</td>\n",
              "      <td id=\"T_d5c64_row122_col9\" class=\"data row122 col9\" >0.9244</td>\n",
              "      <td id=\"T_d5c64_row122_col10\" class=\"data row122 col10\" >0.6258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row123\" class=\"row_heading level2 row123\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row123_col0\" class=\"data row123 col0\" >0.9365</td>\n",
              "      <td id=\"T_d5c64_row123_col1\" class=\"data row123 col1\" >0.8387</td>\n",
              "      <td id=\"T_d5c64_row123_col2\" class=\"data row123 col2\" >0.9817</td>\n",
              "      <td id=\"T_d5c64_row123_col3\" class=\"data row123 col3\" >0.9256</td>\n",
              "      <td id=\"T_d5c64_row123_col4\" class=\"data row123 col4\" >0.9603</td>\n",
              "      <td id=\"T_d5c64_row123_col5\" class=\"data row123 col5\" >0.9296</td>\n",
              "      <td id=\"T_d5c64_row123_col6\" class=\"data row123 col6\" >0.9146</td>\n",
              "      <td id=\"T_d5c64_row123_col7\" class=\"data row123 col7\" >0.9023</td>\n",
              "      <td id=\"T_d5c64_row123_col8\" class=\"data row123 col8\" >0.9283</td>\n",
              "      <td id=\"T_d5c64_row123_col9\" class=\"data row123 col9\" >0.9482</td>\n",
              "      <td id=\"T_d5c64_row123_col10\" class=\"data row123 col10\" >0.8329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row124\" class=\"row_heading level2 row124\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row124_col0\" class=\"data row124 col0\" >0.3496</td>\n",
              "      <td id=\"T_d5c64_row124_col1\" class=\"data row124 col1\" >0.3472</td>\n",
              "      <td id=\"T_d5c64_row124_col2\" class=\"data row124 col2\" >0.3720</td>\n",
              "      <td id=\"T_d5c64_row124_col3\" class=\"data row124 col3\" >0.3476</td>\n",
              "      <td id=\"T_d5c64_row124_col4\" class=\"data row124 col4\" >0.1697</td>\n",
              "      <td id=\"T_d5c64_row124_col5\" class=\"data row124 col5\" >0.3471</td>\n",
              "      <td id=\"T_d5c64_row124_col6\" class=\"data row124 col6\" >0.3663</td>\n",
              "      <td id=\"T_d5c64_row124_col7\" class=\"data row124 col7\" >0.3421</td>\n",
              "      <td id=\"T_d5c64_row124_col8\" class=\"data row124 col8\" >0.8695</td>\n",
              "      <td id=\"T_d5c64_row124_col9\" class=\"data row124 col9\" >0.5855</td>\n",
              "      <td id=\"T_d5c64_row124_col10\" class=\"data row124 col10\" >0.6240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row125\" class=\"row_heading level2 row125\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row125_col0\" class=\"data row125 col0\" >0.9332</td>\n",
              "      <td id=\"T_d5c64_row125_col1\" class=\"data row125 col1\" >0.9432</td>\n",
              "      <td id=\"T_d5c64_row125_col2\" class=\"data row125 col2\" >0.9883</td>\n",
              "      <td id=\"T_d5c64_row125_col3\" class=\"data row125 col3\" >0.9391</td>\n",
              "      <td id=\"T_d5c64_row125_col4\" class=\"data row125 col4\" >0.9675</td>\n",
              "      <td id=\"T_d5c64_row125_col5\" class=\"data row125 col5\" >0.9435</td>\n",
              "      <td id=\"T_d5c64_row125_col6\" class=\"data row125 col6\" >0.9466</td>\n",
              "      <td id=\"T_d5c64_row125_col7\" class=\"data row125 col7\" >0.9143</td>\n",
              "      <td id=\"T_d5c64_row125_col8\" class=\"data row125 col8\" >0.9767</td>\n",
              "      <td id=\"T_d5c64_row125_col9\" class=\"data row125 col9\" >0.9615</td>\n",
              "      <td id=\"T_d5c64_row125_col10\" class=\"data row125 col10\" >0.9048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row126\" class=\"row_heading level1 row126\" rowspan=\"7\">gpt-4</th>\n",
              "      <th id=\"T_d5c64_level2_row126\" class=\"row_heading level2 row126\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row126_col0\" class=\"data row126 col0\" >0.8667</td>\n",
              "      <td id=\"T_d5c64_row126_col1\" class=\"data row126 col1\" >0.7067</td>\n",
              "      <td id=\"T_d5c64_row126_col2\" class=\"data row126 col2\" >0.7828</td>\n",
              "      <td id=\"T_d5c64_row126_col3\" class=\"data row126 col3\" >0.7535</td>\n",
              "      <td id=\"T_d5c64_row126_col4\" class=\"data row126 col4\" >0.4958</td>\n",
              "      <td id=\"T_d5c64_row126_col5\" class=\"data row126 col5\" >0.6805</td>\n",
              "      <td id=\"T_d5c64_row126_col6\" class=\"data row126 col6\" >0.7514</td>\n",
              "      <td id=\"T_d5c64_row126_col7\" class=\"data row126 col7\" >0.6918</td>\n",
              "      <td id=\"T_d5c64_row126_col8\" class=\"data row126 col8\" >0.9333</td>\n",
              "      <td id=\"T_d5c64_row126_col9\" class=\"data row126 col9\" >0.9214</td>\n",
              "      <td id=\"T_d5c64_row126_col10\" class=\"data row126 col10\" >0.8122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row127\" class=\"row_heading level2 row127\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row127_col0\" class=\"data row127 col0\" >0.5003</td>\n",
              "      <td id=\"T_d5c64_row127_col1\" class=\"data row127 col1\" >0.3311</td>\n",
              "      <td id=\"T_d5c64_row127_col2\" class=\"data row127 col2\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row127_col3\" class=\"data row127 col3\" >0.3303</td>\n",
              "      <td id=\"T_d5c64_row127_col4\" class=\"data row127 col4\" >0.3718</td>\n",
              "      <td id=\"T_d5c64_row127_col5\" class=\"data row127 col5\" >0.3374</td>\n",
              "      <td id=\"T_d5c64_row127_col6\" class=\"data row127 col6\" >0.3292</td>\n",
              "      <td id=\"T_d5c64_row127_col7\" class=\"data row127 col7\" >0.3326</td>\n",
              "      <td id=\"T_d5c64_row127_col8\" class=\"data row127 col8\" >0.6837</td>\n",
              "      <td id=\"T_d5c64_row127_col9\" class=\"data row127 col9\" >0.6241</td>\n",
              "      <td id=\"T_d5c64_row127_col10\" class=\"data row127 col10\" >0.3677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row128\" class=\"row_heading level2 row128\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row128_col0\" class=\"data row128 col0\" >0.5234</td>\n",
              "      <td id=\"T_d5c64_row128_col1\" class=\"data row128 col1\" >0.3587</td>\n",
              "      <td id=\"T_d5c64_row128_col2\" class=\"data row128 col2\" >0.3623</td>\n",
              "      <td id=\"T_d5c64_row128_col3\" class=\"data row128 col3\" >0.3514</td>\n",
              "      <td id=\"T_d5c64_row128_col4\" class=\"data row128 col4\" >0.3368</td>\n",
              "      <td id=\"T_d5c64_row128_col5\" class=\"data row128 col5\" >0.3659</td>\n",
              "      <td id=\"T_d5c64_row128_col6\" class=\"data row128 col6\" >0.3655</td>\n",
              "      <td id=\"T_d5c64_row128_col7\" class=\"data row128 col7\" >0.3458</td>\n",
              "      <td id=\"T_d5c64_row128_col8\" class=\"data row128 col8\" >0.7535</td>\n",
              "      <td id=\"T_d5c64_row128_col9\" class=\"data row128 col9\" >0.7794</td>\n",
              "      <td id=\"T_d5c64_row128_col10\" class=\"data row128 col10\" >0.3508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row129\" class=\"row_heading level2 row129\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row129_col0\" class=\"data row129 col0\" >0.8978</td>\n",
              "      <td id=\"T_d5c64_row129_col1\" class=\"data row129 col1\" >0.8978</td>\n",
              "      <td id=\"T_d5c64_row129_col2\" class=\"data row129 col2\" >0.3587</td>\n",
              "      <td id=\"T_d5c64_row129_col3\" class=\"data row129 col3\" >0.9188</td>\n",
              "      <td id=\"T_d5c64_row129_col4\" class=\"data row129 col4\" >0.8713</td>\n",
              "      <td id=\"T_d5c64_row129_col5\" class=\"data row129 col5\" >0.9383</td>\n",
              "      <td id=\"T_d5c64_row129_col6\" class=\"data row129 col6\" >0.9246</td>\n",
              "      <td id=\"T_d5c64_row129_col7\" class=\"data row129 col7\" >0.9264</td>\n",
              "      <td id=\"T_d5c64_row129_col8\" class=\"data row129 col8\" >0.9566</td>\n",
              "      <td id=\"T_d5c64_row129_col9\" class=\"data row129 col9\" >0.8570</td>\n",
              "      <td id=\"T_d5c64_row129_col10\" class=\"data row129 col10\" >0.7078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row130\" class=\"row_heading level2 row130\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row130_col0\" class=\"data row130 col0\" >0.9349</td>\n",
              "      <td id=\"T_d5c64_row130_col1\" class=\"data row130 col1\" >0.5556</td>\n",
              "      <td id=\"T_d5c64_row130_col2\" class=\"data row130 col2\" >0.8788</td>\n",
              "      <td id=\"T_d5c64_row130_col3\" class=\"data row130 col3\" >0.9221</td>\n",
              "      <td id=\"T_d5c64_row130_col4\" class=\"data row130 col4\" >0.9259</td>\n",
              "      <td id=\"T_d5c64_row130_col5\" class=\"data row130 col5\" >0.7611</td>\n",
              "      <td id=\"T_d5c64_row130_col6\" class=\"data row130 col6\" >0.7609</td>\n",
              "      <td id=\"T_d5c64_row130_col7\" class=\"data row130 col7\" >0.6932</td>\n",
              "      <td id=\"T_d5c64_row130_col8\" class=\"data row130 col8\" >0.8305</td>\n",
              "      <td id=\"T_d5c64_row130_col9\" class=\"data row130 col9\" >0.8940</td>\n",
              "      <td id=\"T_d5c64_row130_col10\" class=\"data row130 col10\" >0.7797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row131\" class=\"row_heading level2 row131\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row131_col0\" class=\"data row131 col0\" >0.5936</td>\n",
              "      <td id=\"T_d5c64_row131_col1\" class=\"data row131 col1\" >0.3407</td>\n",
              "      <td id=\"T_d5c64_row131_col2\" class=\"data row131 col2\" >0.3480</td>\n",
              "      <td id=\"T_d5c64_row131_col3\" class=\"data row131 col3\" >0.3439</td>\n",
              "      <td id=\"T_d5c64_row131_col4\" class=\"data row131 col4\" >0.2994</td>\n",
              "      <td id=\"T_d5c64_row131_col5\" class=\"data row131 col5\" >0.3433</td>\n",
              "      <td id=\"T_d5c64_row131_col6\" class=\"data row131 col6\" >0.3628</td>\n",
              "      <td id=\"T_d5c64_row131_col7\" class=\"data row131 col7\" >0.3421</td>\n",
              "      <td id=\"T_d5c64_row131_col8\" class=\"data row131 col8\" >0.7991</td>\n",
              "      <td id=\"T_d5c64_row131_col9\" class=\"data row131 col9\" >0.8526</td>\n",
              "      <td id=\"T_d5c64_row131_col10\" class=\"data row131 col10\" >0.6649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row132\" class=\"row_heading level2 row132\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row132_col0\" class=\"data row132 col0\" >0.9215</td>\n",
              "      <td id=\"T_d5c64_row132_col1\" class=\"data row132 col1\" >0.9600</td>\n",
              "      <td id=\"T_d5c64_row132_col2\" class=\"data row132 col2\" >0.9950</td>\n",
              "      <td id=\"T_d5c64_row132_col3\" class=\"data row132 col3\" >0.9391</td>\n",
              "      <td id=\"T_d5c64_row132_col4\" class=\"data row132 col4\" >0.9765</td>\n",
              "      <td id=\"T_d5c64_row132_col5\" class=\"data row132 col5\" >0.9349</td>\n",
              "      <td id=\"T_d5c64_row132_col6\" class=\"data row132 col6\" >0.9633</td>\n",
              "      <td id=\"T_d5c64_row132_col7\" class=\"data row132 col7\" >0.9368</td>\n",
              "      <td id=\"T_d5c64_row132_col8\" class=\"data row132 col8\" >0.9567</td>\n",
              "      <td id=\"T_d5c64_row132_col9\" class=\"data row132 col9\" >0.9447</td>\n",
              "      <td id=\"T_d5c64_row132_col10\" class=\"data row132 col10\" >0.9214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row133\" class=\"row_heading level1 row133\" rowspan=\"7\">llama-65b</th>\n",
              "      <th id=\"T_d5c64_level2_row133\" class=\"row_heading level2 row133\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row133_col0\" class=\"data row133 col0\" >0.7078</td>\n",
              "      <td id=\"T_d5c64_row133_col1\" class=\"data row133 col1\" >0.5300</td>\n",
              "      <td id=\"T_d5c64_row133_col2\" class=\"data row133 col2\" >0.8750</td>\n",
              "      <td id=\"T_d5c64_row133_col3\" class=\"data row133 col3\" >0.6477</td>\n",
              "      <td id=\"T_d5c64_row133_col4\" class=\"data row133 col4\" >0.3325</td>\n",
              "      <td id=\"T_d5c64_row133_col5\" class=\"data row133 col5\" >0.8211</td>\n",
              "      <td id=\"T_d5c64_row133_col6\" class=\"data row133 col6\" >0.5191</td>\n",
              "      <td id=\"T_d5c64_row133_col7\" class=\"data row133 col7\" >0.6043</td>\n",
              "      <td id=\"T_d5c64_row133_col8\" class=\"data row133 col8\" >0.9849</td>\n",
              "      <td id=\"T_d5c64_row133_col9\" class=\"data row133 col9\" >0.9764</td>\n",
              "      <td id=\"T_d5c64_row133_col10\" class=\"data row133 col10\" >0.4765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row134\" class=\"row_heading level2 row134\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row134_col0\" class=\"data row134 col0\" >0.7140</td>\n",
              "      <td id=\"T_d5c64_row134_col1\" class=\"data row134 col1\" >0.3303</td>\n",
              "      <td id=\"T_d5c64_row134_col2\" class=\"data row134 col2\" >0.3299</td>\n",
              "      <td id=\"T_d5c64_row134_col3\" class=\"data row134 col3\" >0.3348</td>\n",
              "      <td id=\"T_d5c64_row134_col4\" class=\"data row134 col4\" >0.3325</td>\n",
              "      <td id=\"T_d5c64_row134_col5\" class=\"data row134 col5\" >0.3383</td>\n",
              "      <td id=\"T_d5c64_row134_col6\" class=\"data row134 col6\" >0.3330</td>\n",
              "      <td id=\"T_d5c64_row134_col7\" class=\"data row134 col7\" >0.3356</td>\n",
              "      <td id=\"T_d5c64_row134_col8\" class=\"data row134 col8\" >0.6109</td>\n",
              "      <td id=\"T_d5c64_row134_col9\" class=\"data row134 col9\" >0.6025</td>\n",
              "      <td id=\"T_d5c64_row134_col10\" class=\"data row134 col10\" >0.3676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row135\" class=\"row_heading level2 row135\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row135_col0\" class=\"data row135 col0\" >0.6015</td>\n",
              "      <td id=\"T_d5c64_row135_col1\" class=\"data row135 col1\" >0.4216</td>\n",
              "      <td id=\"T_d5c64_row135_col2\" class=\"data row135 col2\" >0.6322</td>\n",
              "      <td id=\"T_d5c64_row135_col3\" class=\"data row135 col3\" >0.6629</td>\n",
              "      <td id=\"T_d5c64_row135_col4\" class=\"data row135 col4\" >0.4170</td>\n",
              "      <td id=\"T_d5c64_row135_col5\" class=\"data row135 col5\" >0.7424</td>\n",
              "      <td id=\"T_d5c64_row135_col6\" class=\"data row135 col6\" >0.5012</td>\n",
              "      <td id=\"T_d5c64_row135_col7\" class=\"data row135 col7\" >0.4982</td>\n",
              "      <td id=\"T_d5c64_row135_col8\" class=\"data row135 col8\" >0.9395</td>\n",
              "      <td id=\"T_d5c64_row135_col9\" class=\"data row135 col9\" >0.9394</td>\n",
              "      <td id=\"T_d5c64_row135_col10\" class=\"data row135 col10\" >0.5690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row136\" class=\"row_heading level2 row136\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row136_col0\" class=\"data row136 col0\" >0.5347</td>\n",
              "      <td id=\"T_d5c64_row136_col1\" class=\"data row136 col1\" >0.3326</td>\n",
              "      <td id=\"T_d5c64_row136_col2\" class=\"data row136 col2\" >0.6154</td>\n",
              "      <td id=\"T_d5c64_row136_col3\" class=\"data row136 col3\" >0.6470</td>\n",
              "      <td id=\"T_d5c64_row136_col4\" class=\"data row136 col4\" >0.3325</td>\n",
              "      <td id=\"T_d5c64_row136_col5\" class=\"data row136 col5\" >0.4964</td>\n",
              "      <td id=\"T_d5c64_row136_col6\" class=\"data row136 col6\" >0.3404</td>\n",
              "      <td id=\"T_d5c64_row136_col7\" class=\"data row136 col7\" >0.4847</td>\n",
              "      <td id=\"T_d5c64_row136_col8\" class=\"data row136 col8\" >0.9782</td>\n",
              "      <td id=\"T_d5c64_row136_col9\" class=\"data row136 col9\" >0.9359</td>\n",
              "      <td id=\"T_d5c64_row136_col10\" class=\"data row136 col10\" >0.4056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row137\" class=\"row_heading level2 row137\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row137_col0\" class=\"data row137 col0\" >0.7629</td>\n",
              "      <td id=\"T_d5c64_row137_col1\" class=\"data row137 col1\" >0.4277</td>\n",
              "      <td id=\"T_d5c64_row137_col2\" class=\"data row137 col2\" >0.9848</td>\n",
              "      <td id=\"T_d5c64_row137_col3\" class=\"data row137 col3\" >0.8540</td>\n",
              "      <td id=\"T_d5c64_row137_col4\" class=\"data row137 col4\" >0.3824</td>\n",
              "      <td id=\"T_d5c64_row137_col5\" class=\"data row137 col5\" >0.7305</td>\n",
              "      <td id=\"T_d5c64_row137_col6\" class=\"data row137 col6\" >0.4145</td>\n",
              "      <td id=\"T_d5c64_row137_col7\" class=\"data row137 col7\" >0.5383</td>\n",
              "      <td id=\"T_d5c64_row137_col8\" class=\"data row137 col8\" >0.9883</td>\n",
              "      <td id=\"T_d5c64_row137_col9\" class=\"data row137 col9\" >0.9815</td>\n",
              "      <td id=\"T_d5c64_row137_col10\" class=\"data row137 col10\" >0.5622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row138\" class=\"row_heading level2 row138\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row138_col0\" class=\"data row138 col0\" >0.6454</td>\n",
              "      <td id=\"T_d5c64_row138_col1\" class=\"data row138 col1\" >0.3311</td>\n",
              "      <td id=\"T_d5c64_row138_col2\" class=\"data row138 col2\" >0.3269</td>\n",
              "      <td id=\"T_d5c64_row138_col3\" class=\"data row138 col3\" >0.3341</td>\n",
              "      <td id=\"T_d5c64_row138_col4\" class=\"data row138 col4\" >0.3388</td>\n",
              "      <td id=\"T_d5c64_row138_col5\" class=\"data row138 col5\" >0.3375</td>\n",
              "      <td id=\"T_d5c64_row138_col6\" class=\"data row138 col6\" >0.3307</td>\n",
              "      <td id=\"T_d5c64_row138_col7\" class=\"data row138 col7\" >0.3356</td>\n",
              "      <td id=\"T_d5c64_row138_col8\" class=\"data row138 col8\" >0.9631</td>\n",
              "      <td id=\"T_d5c64_row138_col9\" class=\"data row138 col9\" >0.9309</td>\n",
              "      <td id=\"T_d5c64_row138_col10\" class=\"data row138 col10\" >0.3969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row139\" class=\"row_heading level2 row139\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row139_col0\" class=\"data row139 col0\" >0.8858</td>\n",
              "      <td id=\"T_d5c64_row139_col1\" class=\"data row139 col1\" >0.5705</td>\n",
              "      <td id=\"T_d5c64_row139_col2\" class=\"data row139 col2\" >0.9865</td>\n",
              "      <td id=\"T_d5c64_row139_col3\" class=\"data row139 col3\" >0.9160</td>\n",
              "      <td id=\"T_d5c64_row139_col4\" class=\"data row139 col4\" >0.3705</td>\n",
              "      <td id=\"T_d5c64_row139_col5\" class=\"data row139 col5\" >0.5024</td>\n",
              "      <td id=\"T_d5c64_row139_col6\" class=\"data row139 col6\" >0.3674</td>\n",
              "      <td id=\"T_d5c64_row139_col7\" class=\"data row139 col7\" >0.5826</td>\n",
              "      <td id=\"T_d5c64_row139_col8\" class=\"data row139 col8\" >0.9933</td>\n",
              "      <td id=\"T_d5c64_row139_col9\" class=\"data row139 col9\" >0.9882</td>\n",
              "      <td id=\"T_d5c64_row139_col10\" class=\"data row139 col10\" >0.6314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row140\" class=\"row_heading level1 row140\" rowspan=\"7\">opt-66b</th>\n",
              "      <th id=\"T_d5c64_level2_row140\" class=\"row_heading level2 row140\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row140_col0\" class=\"data row140 col0\" >0.7593</td>\n",
              "      <td id=\"T_d5c64_row140_col1\" class=\"data row140 col1\" >0.6061</td>\n",
              "      <td id=\"T_d5c64_row140_col2\" class=\"data row140 col2\" >0.7673</td>\n",
              "      <td id=\"T_d5c64_row140_col3\" class=\"data row140 col3\" >0.6078</td>\n",
              "      <td id=\"T_d5c64_row140_col4\" class=\"data row140 col4\" >0.4720</td>\n",
              "      <td id=\"T_d5c64_row140_col5\" class=\"data row140 col5\" >0.6423</td>\n",
              "      <td id=\"T_d5c64_row140_col6\" class=\"data row140 col6\" >0.6428</td>\n",
              "      <td id=\"T_d5c64_row140_col7\" class=\"data row140 col7\" >0.6734</td>\n",
              "      <td id=\"T_d5c64_row140_col8\" class=\"data row140 col8\" >0.9340</td>\n",
              "      <td id=\"T_d5c64_row140_col9\" class=\"data row140 col9\" >0.9254</td>\n",
              "      <td id=\"T_d5c64_row140_col10\" class=\"data row140 col10\" >0.6408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row141\" class=\"row_heading level2 row141\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row141_col0\" class=\"data row141 col0\" >0.5543</td>\n",
              "      <td id=\"T_d5c64_row141_col1\" class=\"data row141 col1\" >0.3326</td>\n",
              "      <td id=\"T_d5c64_row141_col2\" class=\"data row141 col2\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row141_col3\" class=\"data row141 col3\" >0.3356</td>\n",
              "      <td id=\"T_d5c64_row141_col4\" class=\"data row141 col4\" >0.3325</td>\n",
              "      <td id=\"T_d5c64_row141_col5\" class=\"data row141 col5\" >0.3394</td>\n",
              "      <td id=\"T_d5c64_row141_col6\" class=\"data row141 col6\" >0.3337</td>\n",
              "      <td id=\"T_d5c64_row141_col7\" class=\"data row141 col7\" >0.3375</td>\n",
              "      <td id=\"T_d5c64_row141_col8\" class=\"data row141 col8\" >0.7304</td>\n",
              "      <td id=\"T_d5c64_row141_col9\" class=\"data row141 col9\" >0.6932</td>\n",
              "      <td id=\"T_d5c64_row141_col10\" class=\"data row141 col10\" >0.3425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row142\" class=\"row_heading level2 row142\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row142_col0\" class=\"data row142 col0\" >0.6817</td>\n",
              "      <td id=\"T_d5c64_row142_col1\" class=\"data row142 col1\" >0.3755</td>\n",
              "      <td id=\"T_d5c64_row142_col2\" class=\"data row142 col2\" >0.3814</td>\n",
              "      <td id=\"T_d5c64_row142_col3\" class=\"data row142 col3\" >0.3576</td>\n",
              "      <td id=\"T_d5c64_row142_col4\" class=\"data row142 col4\" >0.4726</td>\n",
              "      <td id=\"T_d5c64_row142_col5\" class=\"data row142 col5\" >0.3972</td>\n",
              "      <td id=\"T_d5c64_row142_col6\" class=\"data row142 col6\" >0.3388</td>\n",
              "      <td id=\"T_d5c64_row142_col7\" class=\"data row142 col7\" >0.3598</td>\n",
              "      <td id=\"T_d5c64_row142_col8\" class=\"data row142 col8\" >0.9085</td>\n",
              "      <td id=\"T_d5c64_row142_col9\" class=\"data row142 col9\" >0.8797</td>\n",
              "      <td id=\"T_d5c64_row142_col10\" class=\"data row142 col10\" >0.4361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row143\" class=\"row_heading level2 row143\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row143_col0\" class=\"data row143 col0\" >0.9714</td>\n",
              "      <td id=\"T_d5c64_row143_col1\" class=\"data row143 col1\" >0.5123</td>\n",
              "      <td id=\"T_d5c64_row143_col2\" class=\"data row143 col2\" >0.5776</td>\n",
              "      <td id=\"T_d5c64_row143_col3\" class=\"data row143 col3\" >0.3999</td>\n",
              "      <td id=\"T_d5c64_row143_col4\" class=\"data row143 col4\" >0.3473</td>\n",
              "      <td id=\"T_d5c64_row143_col5\" class=\"data row143 col5\" >0.4262</td>\n",
              "      <td id=\"T_d5c64_row143_col6\" class=\"data row143 col6\" >0.6050</td>\n",
              "      <td id=\"T_d5c64_row143_col7\" class=\"data row143 col7\" >0.4165</td>\n",
              "      <td id=\"T_d5c64_row143_col8\" class=\"data row143 col8\" >0.9662</td>\n",
              "      <td id=\"T_d5c64_row143_col9\" class=\"data row143 col9\" >0.9814</td>\n",
              "      <td id=\"T_d5c64_row143_col10\" class=\"data row143 col10\" >0.7649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row144\" class=\"row_heading level2 row144\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row144_col0\" class=\"data row144 col0\" >0.9393</td>\n",
              "      <td id=\"T_d5c64_row144_col1\" class=\"data row144 col1\" >0.7428</td>\n",
              "      <td id=\"T_d5c64_row144_col2\" class=\"data row144 col2\" >0.9833</td>\n",
              "      <td id=\"T_d5c64_row144_col3\" class=\"data row144 col3\" >0.4860</td>\n",
              "      <td id=\"T_d5c64_row144_col4\" class=\"data row144 col4\" >0.3413</td>\n",
              "      <td id=\"T_d5c64_row144_col5\" class=\"data row144 col5\" >0.3394</td>\n",
              "      <td id=\"T_d5c64_row144_col6\" class=\"data row144 col6\" >0.6255</td>\n",
              "      <td id=\"T_d5c64_row144_col7\" class=\"data row144 col7\" >0.3610</td>\n",
              "      <td id=\"T_d5c64_row144_col8\" class=\"data row144 col8\" >0.9848</td>\n",
              "      <td id=\"T_d5c64_row144_col9\" class=\"data row144 col9\" >0.9932</td>\n",
              "      <td id=\"T_d5c64_row144_col10\" class=\"data row144 col10\" >0.7645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row145\" class=\"row_heading level2 row145\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row145_col0\" class=\"data row145 col0\" >0.4195</td>\n",
              "      <td id=\"T_d5c64_row145_col1\" class=\"data row145 col1\" >0.3266</td>\n",
              "      <td id=\"T_d5c64_row145_col2\" class=\"data row145 col2\" >0.3326</td>\n",
              "      <td id=\"T_d5c64_row145_col3\" class=\"data row145 col3\" >0.3348</td>\n",
              "      <td id=\"T_d5c64_row145_col4\" class=\"data row145 col4\" >0.3046</td>\n",
              "      <td id=\"T_d5c64_row145_col5\" class=\"data row145 col5\" >0.3386</td>\n",
              "      <td id=\"T_d5c64_row145_col6\" class=\"data row145 col6\" >0.3322</td>\n",
              "      <td id=\"T_d5c64_row145_col7\" class=\"data row145 col7\" >0.3375</td>\n",
              "      <td id=\"T_d5c64_row145_col8\" class=\"data row145 col8\" >0.9003</td>\n",
              "      <td id=\"T_d5c64_row145_col9\" class=\"data row145 col9\" >0.8830</td>\n",
              "      <td id=\"T_d5c64_row145_col10\" class=\"data row145 col10\" >0.3759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row146\" class=\"row_heading level2 row146\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row146_col0\" class=\"data row146 col0\" >0.9546</td>\n",
              "      <td id=\"T_d5c64_row146_col1\" class=\"data row146 col1\" >0.8552</td>\n",
              "      <td id=\"T_d5c64_row146_col2\" class=\"data row146 col2\" >0.9917</td>\n",
              "      <td id=\"T_d5c64_row146_col3\" class=\"data row146 col3\" >0.5544</td>\n",
              "      <td id=\"T_d5c64_row146_col4\" class=\"data row146 col4\" >0.3453</td>\n",
              "      <td id=\"T_d5c64_row146_col5\" class=\"data row146 col5\" >0.5474</td>\n",
              "      <td id=\"T_d5c64_row146_col6\" class=\"data row146 col6\" >0.7734</td>\n",
              "      <td id=\"T_d5c64_row146_col7\" class=\"data row146 col7\" >0.6168</td>\n",
              "      <td id=\"T_d5c64_row146_col8\" class=\"data row146 col8\" >0.9848</td>\n",
              "      <td id=\"T_d5c64_row146_col9\" class=\"data row146 col9\" >0.9864</td>\n",
              "      <td id=\"T_d5c64_row146_col10\" class=\"data row146 col10\" >0.8530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row147\" class=\"row_heading level1 row147\" rowspan=\"7\">opt-iml-max-1.3b</th>\n",
              "      <th id=\"T_d5c64_level2_row147\" class=\"row_heading level2 row147\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row147_col0\" class=\"data row147 col0\" >0.9588</td>\n",
              "      <td id=\"T_d5c64_row147_col1\" class=\"data row147 col1\" >0.5420</td>\n",
              "      <td id=\"T_d5c64_row147_col2\" class=\"data row147 col2\" >0.8707</td>\n",
              "      <td id=\"T_d5c64_row147_col3\" class=\"data row147 col3\" >0.5073</td>\n",
              "      <td id=\"T_d5c64_row147_col4\" class=\"data row147 col4\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row147_col5\" class=\"data row147 col5\" >0.4417</td>\n",
              "      <td id=\"T_d5c64_row147_col6\" class=\"data row147 col6\" >0.5379</td>\n",
              "      <td id=\"T_d5c64_row147_col7\" class=\"data row147 col7\" >0.4153</td>\n",
              "      <td id=\"T_d5c64_row147_col8\" class=\"data row147 col8\" >0.9795</td>\n",
              "      <td id=\"T_d5c64_row147_col9\" class=\"data row147 col9\" >0.9828</td>\n",
              "      <td id=\"T_d5c64_row147_col10\" class=\"data row147 col10\" >0.8243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row148\" class=\"row_heading level2 row148\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row148_col0\" class=\"data row148 col0\" >0.6770</td>\n",
              "      <td id=\"T_d5c64_row148_col1\" class=\"data row148 col1\" >0.3709</td>\n",
              "      <td id=\"T_d5c64_row148_col2\" class=\"data row148 col2\" >0.5542</td>\n",
              "      <td id=\"T_d5c64_row148_col3\" class=\"data row148 col3\" >0.4481</td>\n",
              "      <td id=\"T_d5c64_row148_col4\" class=\"data row148 col4\" >0.3669</td>\n",
              "      <td id=\"T_d5c64_row148_col5\" class=\"data row148 col5\" >0.5480</td>\n",
              "      <td id=\"T_d5c64_row148_col6\" class=\"data row148 col6\" >0.4653</td>\n",
              "      <td id=\"T_d5c64_row148_col7\" class=\"data row148 col7\" >0.5424</td>\n",
              "      <td id=\"T_d5c64_row148_col8\" class=\"data row148 col8\" >0.8379</td>\n",
              "      <td id=\"T_d5c64_row148_col9\" class=\"data row148 col9\" >0.8536</td>\n",
              "      <td id=\"T_d5c64_row148_col10\" class=\"data row148 col10\" >0.7043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row149\" class=\"row_heading level2 row149\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row149_col0\" class=\"data row149 col0\" >0.8338</td>\n",
              "      <td id=\"T_d5c64_row149_col1\" class=\"data row149 col1\" >0.5143</td>\n",
              "      <td id=\"T_d5c64_row149_col2\" class=\"data row149 col2\" >0.4434</td>\n",
              "      <td id=\"T_d5c64_row149_col3\" class=\"data row149 col3\" >0.3979</td>\n",
              "      <td id=\"T_d5c64_row149_col4\" class=\"data row149 col4\" >0.3991</td>\n",
              "      <td id=\"T_d5c64_row149_col5\" class=\"data row149 col5\" >0.4127</td>\n",
              "      <td id=\"T_d5c64_row149_col6\" class=\"data row149 col6\" >0.4003</td>\n",
              "      <td id=\"T_d5c64_row149_col7\" class=\"data row149 col7\" >0.3891</td>\n",
              "      <td id=\"T_d5c64_row149_col8\" class=\"data row149 col8\" >0.9368</td>\n",
              "      <td id=\"T_d5c64_row149_col9\" class=\"data row149 col9\" >0.9188</td>\n",
              "      <td id=\"T_d5c64_row149_col10\" class=\"data row149 col10\" >0.5273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row150\" class=\"row_heading level2 row150\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row150_col0\" class=\"data row150 col0\" >0.9640</td>\n",
              "      <td id=\"T_d5c64_row150_col1\" class=\"data row150 col1\" >0.6583</td>\n",
              "      <td id=\"T_d5c64_row150_col2\" class=\"data row150 col2\" >0.3597</td>\n",
              "      <td id=\"T_d5c64_row150_col3\" class=\"data row150 col3\" >0.5613</td>\n",
              "      <td id=\"T_d5c64_row150_col4\" class=\"data row150 col4\" >0.3750</td>\n",
              "      <td id=\"T_d5c64_row150_col5\" class=\"data row150 col5\" >0.3873</td>\n",
              "      <td id=\"T_d5c64_row150_col6\" class=\"data row150 col6\" >0.8374</td>\n",
              "      <td id=\"T_d5c64_row150_col7\" class=\"data row150 col7\" >0.4378</td>\n",
              "      <td id=\"T_d5c64_row150_col8\" class=\"data row150 col8\" >0.9949</td>\n",
              "      <td id=\"T_d5c64_row150_col9\" class=\"data row150 col9\" >0.9810</td>\n",
              "      <td id=\"T_d5c64_row150_col10\" class=\"data row150 col10\" >0.7678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row151\" class=\"row_heading level2 row151\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row151_col0\" class=\"data row151 col0\" >0.8902</td>\n",
              "      <td id=\"T_d5c64_row151_col1\" class=\"data row151 col1\" >0.5114</td>\n",
              "      <td id=\"T_d5c64_row151_col2\" class=\"data row151 col2\" >0.9379</td>\n",
              "      <td id=\"T_d5c64_row151_col3\" class=\"data row151 col3\" >0.5767</td>\n",
              "      <td id=\"T_d5c64_row151_col4\" class=\"data row151 col4\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row151_col5\" class=\"data row151 col5\" >0.3926</td>\n",
              "      <td id=\"T_d5c64_row151_col6\" class=\"data row151 col6\" >0.6230</td>\n",
              "      <td id=\"T_d5c64_row151_col7\" class=\"data row151 col7\" >0.3984</td>\n",
              "      <td id=\"T_d5c64_row151_col8\" class=\"data row151 col8\" >0.9224</td>\n",
              "      <td id=\"T_d5c64_row151_col9\" class=\"data row151 col9\" >0.9707</td>\n",
              "      <td id=\"T_d5c64_row151_col10\" class=\"data row151 col10\" >0.8335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row152\" class=\"row_heading level2 row152\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row152_col0\" class=\"data row152 col0\" >0.7226</td>\n",
              "      <td id=\"T_d5c64_row152_col1\" class=\"data row152 col1\" >0.3149</td>\n",
              "      <td id=\"T_d5c64_row152_col2\" class=\"data row152 col2\" >0.3979</td>\n",
              "      <td id=\"T_d5c64_row152_col3\" class=\"data row152 col3\" >0.3242</td>\n",
              "      <td id=\"T_d5c64_row152_col4\" class=\"data row152 col4\" >0.1294</td>\n",
              "      <td id=\"T_d5c64_row152_col5\" class=\"data row152 col5\" >0.3216</td>\n",
              "      <td id=\"T_d5c64_row152_col6\" class=\"data row152 col6\" >0.3521</td>\n",
              "      <td id=\"T_d5c64_row152_col7\" class=\"data row152 col7\" >0.3245</td>\n",
              "      <td id=\"T_d5c64_row152_col8\" class=\"data row152 col8\" >0.9333</td>\n",
              "      <td id=\"T_d5c64_row152_col9\" class=\"data row152 col9\" >0.9293</td>\n",
              "      <td id=\"T_d5c64_row152_col10\" class=\"data row152 col10\" >0.5404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row153\" class=\"row_heading level2 row153\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row153_col0\" class=\"data row153 col0\" >0.9931</td>\n",
              "      <td id=\"T_d5c64_row153_col1\" class=\"data row153 col1\" >0.7275</td>\n",
              "      <td id=\"T_d5c64_row153_col2\" class=\"data row153 col2\" >0.9900</td>\n",
              "      <td id=\"T_d5c64_row153_col3\" class=\"data row153 col3\" >0.7551</td>\n",
              "      <td id=\"T_d5c64_row153_col4\" class=\"data row153 col4\" >0.3373</td>\n",
              "      <td id=\"T_d5c64_row153_col5\" class=\"data row153 col5\" >0.3944</td>\n",
              "      <td id=\"T_d5c64_row153_col6\" class=\"data row153 col6\" >0.8638</td>\n",
              "      <td id=\"T_d5c64_row153_col7\" class=\"data row153 col7\" >0.4432</td>\n",
              "      <td id=\"T_d5c64_row153_col8\" class=\"data row153 col8\" >0.9966</td>\n",
              "      <td id=\"T_d5c64_row153_col9\" class=\"data row153 col9\" >0.9948</td>\n",
              "      <td id=\"T_d5c64_row153_col10\" class=\"data row153 col10\" >0.9338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row154\" class=\"row_heading level1 row154\" rowspan=\"7\">text-davinci-003</th>\n",
              "      <th id=\"T_d5c64_level2_row154\" class=\"row_heading level2 row154\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row154_col0\" class=\"data row154 col0\" >0.7480</td>\n",
              "      <td id=\"T_d5c64_row154_col1\" class=\"data row154 col1\" >0.6998</td>\n",
              "      <td id=\"T_d5c64_row154_col2\" class=\"data row154 col2\" >0.6476</td>\n",
              "      <td id=\"T_d5c64_row154_col3\" class=\"data row154 col3\" >0.8087</td>\n",
              "      <td id=\"T_d5c64_row154_col4\" class=\"data row154 col4\" >0.7249</td>\n",
              "      <td id=\"T_d5c64_row154_col5\" class=\"data row154 col5\" >0.8167</td>\n",
              "      <td id=\"T_d5c64_row154_col6\" class=\"data row154 col6\" >0.7157</td>\n",
              "      <td id=\"T_d5c64_row154_col7\" class=\"data row154 col7\" >0.7645</td>\n",
              "      <td id=\"T_d5c64_row154_col8\" class=\"data row154 col8\" >0.9146</td>\n",
              "      <td id=\"T_d5c64_row154_col9\" class=\"data row154 col9\" >0.9079</td>\n",
              "      <td id=\"T_d5c64_row154_col10\" class=\"data row154 col10\" >0.7911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row155\" class=\"row_heading level2 row155\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row155_col0\" class=\"data row155 col0\" >0.4395</td>\n",
              "      <td id=\"T_d5c64_row155_col1\" class=\"data row155 col1\" >0.3348</td>\n",
              "      <td id=\"T_d5c64_row155_col2\" class=\"data row155 col2\" >0.3326</td>\n",
              "      <td id=\"T_d5c64_row155_col3\" class=\"data row155 col3\" >0.3332</td>\n",
              "      <td id=\"T_d5c64_row155_col4\" class=\"data row155 col4\" >0.5357</td>\n",
              "      <td id=\"T_d5c64_row155_col5\" class=\"data row155 col5\" >0.3413</td>\n",
              "      <td id=\"T_d5c64_row155_col6\" class=\"data row155 col6\" >0.3306</td>\n",
              "      <td id=\"T_d5c64_row155_col7\" class=\"data row155 col7\" >0.3318</td>\n",
              "      <td id=\"T_d5c64_row155_col8\" class=\"data row155 col8\" >0.5136</td>\n",
              "      <td id=\"T_d5c64_row155_col9\" class=\"data row155 col9\" >0.4716</td>\n",
              "      <td id=\"T_d5c64_row155_col10\" class=\"data row155 col10\" >0.3935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row156\" class=\"row_heading level2 row156\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row156_col0\" class=\"data row156 col0\" >0.7234</td>\n",
              "      <td id=\"T_d5c64_row156_col1\" class=\"data row156 col1\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row156_col2\" class=\"data row156 col2\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row156_col3\" class=\"data row156 col3\" >0.3296</td>\n",
              "      <td id=\"T_d5c64_row156_col4\" class=\"data row156 col4\" >0.3244</td>\n",
              "      <td id=\"T_d5c64_row156_col5\" class=\"data row156 col5\" >0.3264</td>\n",
              "      <td id=\"T_d5c64_row156_col6\" class=\"data row156 col6\" >0.3367</td>\n",
              "      <td id=\"T_d5c64_row156_col7\" class=\"data row156 col7\" >0.3245</td>\n",
              "      <td id=\"T_d5c64_row156_col8\" class=\"data row156 col8\" >0.8528</td>\n",
              "      <td id=\"T_d5c64_row156_col9\" class=\"data row156 col9\" >0.8408</td>\n",
              "      <td id=\"T_d5c64_row156_col10\" class=\"data row156 col10\" >0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row157\" class=\"row_heading level2 row157\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row157_col0\" class=\"data row157 col0\" >0.8798</td>\n",
              "      <td id=\"T_d5c64_row157_col1\" class=\"data row157 col1\" >0.6376</td>\n",
              "      <td id=\"T_d5c64_row157_col2\" class=\"data row157 col2\" >0.3552</td>\n",
              "      <td id=\"T_d5c64_row157_col3\" class=\"data row157 col3\" >0.8629</td>\n",
              "      <td id=\"T_d5c64_row157_col4\" class=\"data row157 col4\" >0.6831</td>\n",
              "      <td id=\"T_d5c64_row157_col5\" class=\"data row157 col5\" >0.8696</td>\n",
              "      <td id=\"T_d5c64_row157_col6\" class=\"data row157 col6\" >0.7987</td>\n",
              "      <td id=\"T_d5c64_row157_col7\" class=\"data row157 col7\" >0.7996</td>\n",
              "      <td id=\"T_d5c64_row157_col8\" class=\"data row157 col8\" >0.9315</td>\n",
              "      <td id=\"T_d5c64_row157_col9\" class=\"data row157 col9\" >0.8193</td>\n",
              "      <td id=\"T_d5c64_row157_col10\" class=\"data row157 col10\" >0.6410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row158\" class=\"row_heading level2 row158\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row158_col0\" class=\"data row158 col0\" >0.8800</td>\n",
              "      <td id=\"T_d5c64_row158_col1\" class=\"data row158 col1\" >0.4399</td>\n",
              "      <td id=\"T_d5c64_row158_col2\" class=\"data row158 col2\" >0.6877</td>\n",
              "      <td id=\"T_d5c64_row158_col3\" class=\"data row158 col3\" >0.5837</td>\n",
              "      <td id=\"T_d5c64_row158_col4\" class=\"data row158 col4\" >0.3453</td>\n",
              "      <td id=\"T_d5c64_row158_col5\" class=\"data row158 col5\" >0.5184</td>\n",
              "      <td id=\"T_d5c64_row158_col6\" class=\"data row158 col6\" >0.4845</td>\n",
              "      <td id=\"T_d5c64_row158_col7\" class=\"data row158 col7\" >0.5132</td>\n",
              "      <td id=\"T_d5c64_row158_col8\" class=\"data row158 col8\" >0.9381</td>\n",
              "      <td id=\"T_d5c64_row158_col9\" class=\"data row158 col9\" >0.9598</td>\n",
              "      <td id=\"T_d5c64_row158_col10\" class=\"data row158 col10\" >0.7358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row159\" class=\"row_heading level2 row159\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row159_col0\" class=\"data row159 col0\" >0.6924</td>\n",
              "      <td id=\"T_d5c64_row159_col1\" class=\"data row159 col1\" >0.3623</td>\n",
              "      <td id=\"T_d5c64_row159_col2\" class=\"data row159 col2\" >0.4165</td>\n",
              "      <td id=\"T_d5c64_row159_col3\" class=\"data row159 col3\" >0.3514</td>\n",
              "      <td id=\"T_d5c64_row159_col4\" class=\"data row159 col4\" >0.1657</td>\n",
              "      <td id=\"T_d5c64_row159_col5\" class=\"data row159 col5\" >0.3471</td>\n",
              "      <td id=\"T_d5c64_row159_col6\" class=\"data row159 col6\" >0.4236</td>\n",
              "      <td id=\"T_d5c64_row159_col7\" class=\"data row159 col7\" >0.3421</td>\n",
              "      <td id=\"T_d5c64_row159_col8\" class=\"data row159 col8\" >0.8814</td>\n",
              "      <td id=\"T_d5c64_row159_col9\" class=\"data row159 col9\" >0.8155</td>\n",
              "      <td id=\"T_d5c64_row159_col10\" class=\"data row159 col10\" >0.6138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row160\" class=\"row_heading level2 row160\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row160_col0\" class=\"data row160 col0\" >0.8664</td>\n",
              "      <td id=\"T_d5c64_row160_col1\" class=\"data row160 col1\" >0.4440</td>\n",
              "      <td id=\"T_d5c64_row160_col2\" class=\"data row160 col2\" >0.7422</td>\n",
              "      <td id=\"T_d5c64_row160_col3\" class=\"data row160 col3\" >0.7117</td>\n",
              "      <td id=\"T_d5c64_row160_col4\" class=\"data row160 col4\" >0.3778</td>\n",
              "      <td id=\"T_d5c64_row160_col5\" class=\"data row160 col5\" >0.5679</td>\n",
              "      <td id=\"T_d5c64_row160_col6\" class=\"data row160 col6\" >0.5897</td>\n",
              "      <td id=\"T_d5c64_row160_col7\" class=\"data row160 col7\" >0.5700</td>\n",
              "      <td id=\"T_d5c64_row160_col8\" class=\"data row160 col8\" >0.9382</td>\n",
              "      <td id=\"T_d5c64_row160_col9\" class=\"data row160 col9\" >0.9497</td>\n",
              "      <td id=\"T_d5c64_row160_col10\" class=\"data row160 col10\" >0.7750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level1_row161\" class=\"row_heading level1 row161\" rowspan=\"7\">vicuna-13b</th>\n",
              "      <th id=\"T_d5c64_level2_row161\" class=\"row_heading level2 row161\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_d5c64_row161_col0\" class=\"data row161 col0\" >0.5450</td>\n",
              "      <td id=\"T_d5c64_row161_col1\" class=\"data row161 col1\" >0.3623</td>\n",
              "      <td id=\"T_d5c64_row161_col2\" class=\"data row161 col2\" >0.5724</td>\n",
              "      <td id=\"T_d5c64_row161_col3\" class=\"data row161 col3\" >0.4549</td>\n",
              "      <td id=\"T_d5c64_row161_col4\" class=\"data row161 col4\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row161_col5\" class=\"data row161 col5\" >0.4751</td>\n",
              "      <td id=\"T_d5c64_row161_col6\" class=\"data row161 col6\" >0.3548</td>\n",
              "      <td id=\"T_d5c64_row161_col7\" class=\"data row161 col7\" >0.3721</td>\n",
              "      <td id=\"T_d5c64_row161_col8\" class=\"data row161 col8\" >0.9264</td>\n",
              "      <td id=\"T_d5c64_row161_col9\" class=\"data row161 col9\" >0.8401</td>\n",
              "      <td id=\"T_d5c64_row161_col10\" class=\"data row161 col10\" >0.3472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row162\" class=\"row_heading level2 row162\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_d5c64_row162_col0\" class=\"data row162 col0\" >0.5084</td>\n",
              "      <td id=\"T_d5c64_row162_col1\" class=\"data row162 col1\" >0.3355</td>\n",
              "      <td id=\"T_d5c64_row162_col2\" class=\"data row162 col2\" >0.3376</td>\n",
              "      <td id=\"T_d5c64_row162_col3\" class=\"data row162 col3\" >0.3318</td>\n",
              "      <td id=\"T_d5c64_row162_col4\" class=\"data row162 col4\" >0.3333</td>\n",
              "      <td id=\"T_d5c64_row162_col5\" class=\"data row162 col5\" >0.3386</td>\n",
              "      <td id=\"T_d5c64_row162_col6\" class=\"data row162 col6\" >0.3306</td>\n",
              "      <td id=\"T_d5c64_row162_col7\" class=\"data row162 col7\" >0.3421</td>\n",
              "      <td id=\"T_d5c64_row162_col8\" class=\"data row162 col8\" >0.5535</td>\n",
              "      <td id=\"T_d5c64_row162_col9\" class=\"data row162 col9\" >0.5256</td>\n",
              "      <td id=\"T_d5c64_row162_col10\" class=\"data row162 col10\" >0.5498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row163\" class=\"row_heading level2 row163\" >gpt2-medium</th>\n",
              "      <td id=\"T_d5c64_row163_col0\" class=\"data row163 col0\" >0.5858</td>\n",
              "      <td id=\"T_d5c64_row163_col1\" class=\"data row163 col1\" >0.3787</td>\n",
              "      <td id=\"T_d5c64_row163_col2\" class=\"data row163 col2\" >0.4784</td>\n",
              "      <td id=\"T_d5c64_row163_col3\" class=\"data row163 col3\" >0.4744</td>\n",
              "      <td id=\"T_d5c64_row163_col4\" class=\"data row163 col4\" >0.5022</td>\n",
              "      <td id=\"T_d5c64_row163_col5\" class=\"data row163 col5\" >0.3811</td>\n",
              "      <td id=\"T_d5c64_row163_col6\" class=\"data row163 col6\" >0.3954</td>\n",
              "      <td id=\"T_d5c64_row163_col7\" class=\"data row163 col7\" >0.4326</td>\n",
              "      <td id=\"T_d5c64_row163_col8\" class=\"data row163 col8\" >0.8712</td>\n",
              "      <td id=\"T_d5c64_row163_col9\" class=\"data row163 col9\" >0.6147</td>\n",
              "      <td id=\"T_d5c64_row163_col10\" class=\"data row163 col10\" >0.4456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row164\" class=\"row_heading level2 row164\" >mGPT</th>\n",
              "      <td id=\"T_d5c64_row164_col0\" class=\"data row164 col0\" >0.6846</td>\n",
              "      <td id=\"T_d5c64_row164_col1\" class=\"data row164 col1\" >0.3407</td>\n",
              "      <td id=\"T_d5c64_row164_col2\" class=\"data row164 col2\" >0.8659</td>\n",
              "      <td id=\"T_d5c64_row164_col3\" class=\"data row164 col3\" >0.8576</td>\n",
              "      <td id=\"T_d5c64_row164_col4\" class=\"data row164 col4\" >0.3722</td>\n",
              "      <td id=\"T_d5c64_row164_col5\" class=\"data row164 col5\" >0.6188</td>\n",
              "      <td id=\"T_d5c64_row164_col6\" class=\"data row164 col6\" >0.3663</td>\n",
              "      <td id=\"T_d5c64_row164_col7\" class=\"data row164 col7\" >0.5499</td>\n",
              "      <td id=\"T_d5c64_row164_col8\" class=\"data row164 col8\" >0.9548</td>\n",
              "      <td id=\"T_d5c64_row164_col9\" class=\"data row164 col9\" >0.8978</td>\n",
              "      <td id=\"T_d5c64_row164_col10\" class=\"data row164 col10\" >0.5443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row165\" class=\"row_heading level2 row165\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_d5c64_row165_col0\" class=\"data row165 col0\" >0.9883</td>\n",
              "      <td id=\"T_d5c64_row165_col1\" class=\"data row165 col1\" >0.7980</td>\n",
              "      <td id=\"T_d5c64_row165_col2\" class=\"data row165 col2\" >0.9381</td>\n",
              "      <td id=\"T_d5c64_row165_col3\" class=\"data row165 col3\" >0.9033</td>\n",
              "      <td id=\"T_d5c64_row165_col4\" class=\"data row165 col4\" >0.8136</td>\n",
              "      <td id=\"T_d5c64_row165_col5\" class=\"data row165 col5\" >0.9023</td>\n",
              "      <td id=\"T_d5c64_row165_col6\" class=\"data row165 col6\" >0.8760</td>\n",
              "      <td id=\"T_d5c64_row165_col7\" class=\"data row165 col7\" >0.8464</td>\n",
              "      <td id=\"T_d5c64_row165_col8\" class=\"data row165 col8\" >0.9565</td>\n",
              "      <td id=\"T_d5c64_row165_col9\" class=\"data row165 col9\" >0.9582</td>\n",
              "      <td id=\"T_d5c64_row165_col10\" class=\"data row165 col10\" >0.8594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row166\" class=\"row_heading level2 row166\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_d5c64_row166_col0\" class=\"data row166 col0\" >0.5037</td>\n",
              "      <td id=\"T_d5c64_row166_col1\" class=\"data row166 col1\" >0.3326</td>\n",
              "      <td id=\"T_d5c64_row166_col2\" class=\"data row166 col2\" >0.3318</td>\n",
              "      <td id=\"T_d5c64_row166_col3\" class=\"data row166 col3\" >0.3356</td>\n",
              "      <td id=\"T_d5c64_row166_col4\" class=\"data row166 col4\" >0.3138</td>\n",
              "      <td id=\"T_d5c64_row166_col5\" class=\"data row166 col5\" >0.3394</td>\n",
              "      <td id=\"T_d5c64_row166_col6\" class=\"data row166 col6\" >0.3330</td>\n",
              "      <td id=\"T_d5c64_row166_col7\" class=\"data row166 col7\" >0.3382</td>\n",
              "      <td id=\"T_d5c64_row166_col8\" class=\"data row166 col8\" >0.8845</td>\n",
              "      <td id=\"T_d5c64_row166_col9\" class=\"data row166 col9\" >0.5946</td>\n",
              "      <td id=\"T_d5c64_row166_col10\" class=\"data row166 col10\" >0.6119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d5c64_level2_row167\" class=\"row_heading level2 row167\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_d5c64_row167_col0\" class=\"data row167 col0\" >0.9716</td>\n",
              "      <td id=\"T_d5c64_row167_col1\" class=\"data row167 col1\" >0.7526</td>\n",
              "      <td id=\"T_d5c64_row167_col2\" class=\"data row167 col2\" >0.9280</td>\n",
              "      <td id=\"T_d5c64_row167_col3\" class=\"data row167 col3\" >0.9391</td>\n",
              "      <td id=\"T_d5c64_row167_col4\" class=\"data row167 col4\" >0.8572</td>\n",
              "      <td id=\"T_d5c64_row167_col5\" class=\"data row167 col5\" >0.9298</td>\n",
              "      <td id=\"T_d5c64_row167_col6\" class=\"data row167 col6\" >0.6979</td>\n",
              "      <td id=\"T_d5c64_row167_col7\" class=\"data row167 col7\" >0.8416</td>\n",
              "      <td id=\"T_d5c64_row167_col8\" class=\"data row167 col8\" >0.9615</td>\n",
              "      <td id=\"T_d5c64_row167_col9\" class=\"data row167 col9\" >0.9278</td>\n",
              "      <td id=\"T_d5c64_row167_col10\" class=\"data row167 col10\" >0.8998</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#display(results_all.reset_index().drop(columns=['Train LLM', 'Model']).groupby(['Train Language']).agg(['mean','std']).style.background_gradient(axis=1).format(na_rep=0, precision=4))\n",
        "temp = results_all.reset_index().drop(columns=['Train LLM', 'Model']).groupby(['Train Language']).agg(['mean','std']).style.format(na_rep=0, precision=4)\n",
        "display(temp.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None))"
      ],
      "metadata": {
        "id": "O3EwTeuqCJh4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "617b33a4-4cc9-4ab9-a05b-551fe279d59b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd004ed7190>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_50834_row0_col0 {\n",
              "  background-color: #d0d1e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row0_col1, #T_50834_row0_col11, #T_50834_row0_col19, #T_50834_row2_col1 {\n",
              "  background-color: #f2ecf5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row0_col2, #T_50834_row1_col0 {\n",
              "  background-color: #b8c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row0_col3, #T_50834_row0_col13, #T_50834_row1_col17, #T_50834_row1_col19 {\n",
              "  background-color: #efe9f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row0_col4, #T_50834_row2_col6 {\n",
              "  background-color: #c1cae2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row0_col5, #T_50834_row0_col17, #T_50834_row1_col13 {\n",
              "  background-color: #f1ebf5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row0_col6 {\n",
              "  background-color: #b5c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row0_col7, #T_50834_row0_col15, #T_50834_row1_col1, #T_50834_row1_col21, #T_50834_row2_col3, #T_50834_row2_col15 {\n",
              "  background-color: #f0eaf4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row0_col8 {\n",
              "  background-color: #81aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row0_col9, #T_50834_row1_col11 {\n",
              "  background-color: #fbf3f9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row0_col10, #T_50834_row0_col14 {\n",
              "  background-color: #bbc7e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row0_col12 {\n",
              "  background-color: #b4c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row0_col16 {\n",
              "  background-color: #c0c9e2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row0_col18, #T_50834_row1_col8 {\n",
              "  background-color: #c2cbe2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row0_col20 {\n",
              "  background-color: #d2d2e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row0_col21 {\n",
              "  background-color: #f5eef6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row1_col2 {\n",
              "  background-color: #94b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row1_col3, #T_50834_row2_col17 {\n",
              "  background-color: #f6eff7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row1_col4 {\n",
              "  background-color: #a9bfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row1_col5, #T_50834_row2_col11, #T_50834_row2_col13 {\n",
              "  background-color: #eee9f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row1_col6 {\n",
              "  background-color: #9fbad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row1_col7, #T_50834_row2_col21 {\n",
              "  background-color: #f1ebf4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row1_col9 {\n",
              "  background-color: #ede7f2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row1_col10 {\n",
              "  background-color: #83afd3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row1_col12 {\n",
              "  background-color: #a2bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row1_col14 {\n",
              "  background-color: #8eb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row1_col15 {\n",
              "  background-color: #f8f1f8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row1_col16, #T_50834_row1_col18 {\n",
              "  background-color: #b1c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row1_col20 {\n",
              "  background-color: #bfc9e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row2_col0 {\n",
              "  background-color: #a8bedc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row2_col2 {\n",
              "  background-color: #cccfe5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row2_col4 {\n",
              "  background-color: #bdc8e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row2_col5 {\n",
              "  background-color: #ebe6f2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row2_col7 {\n",
              "  background-color: #ede8f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row2_col8 {\n",
              "  background-color: #d2d3e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row2_col9 {\n",
              "  background-color: #eee8f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row2_col10 {\n",
              "  background-color: #c6cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row2_col12 {\n",
              "  background-color: #c8cde4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row2_col14 {\n",
              "  background-color: #cacee5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row2_col16 {\n",
              "  background-color: #8bb2d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row2_col18 {\n",
              "  background-color: #91b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row2_col19 {\n",
              "  background-color: #f4eef6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_50834_row2_col20 {\n",
              "  background-color: #bcc7e1;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_50834\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_50834_level0_col0\" class=\"col_heading level0 col0\" colspan=\"2\">ar</th>\n",
              "      <th id=\"T_50834_level0_col2\" class=\"col_heading level0 col2\" colspan=\"2\">ca</th>\n",
              "      <th id=\"T_50834_level0_col4\" class=\"col_heading level0 col4\" colspan=\"2\">cs</th>\n",
              "      <th id=\"T_50834_level0_col6\" class=\"col_heading level0 col6\" colspan=\"2\">de</th>\n",
              "      <th id=\"T_50834_level0_col8\" class=\"col_heading level0 col8\" colspan=\"2\">en</th>\n",
              "      <th id=\"T_50834_level0_col10\" class=\"col_heading level0 col10\" colspan=\"2\">es</th>\n",
              "      <th id=\"T_50834_level0_col12\" class=\"col_heading level0 col12\" colspan=\"2\">nl</th>\n",
              "      <th id=\"T_50834_level0_col14\" class=\"col_heading level0 col14\" colspan=\"2\">pt</th>\n",
              "      <th id=\"T_50834_level0_col16\" class=\"col_heading level0 col16\" colspan=\"2\">ru</th>\n",
              "      <th id=\"T_50834_level0_col18\" class=\"col_heading level0 col18\" colspan=\"2\">uk</th>\n",
              "      <th id=\"T_50834_level0_col20\" class=\"col_heading level0 col20\" colspan=\"2\">zh</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"blank level1\" >&nbsp;</th>\n",
              "      <th id=\"T_50834_level1_col0\" class=\"col_heading level1 col0\" >mean</th>\n",
              "      <th id=\"T_50834_level1_col1\" class=\"col_heading level1 col1\" >std</th>\n",
              "      <th id=\"T_50834_level1_col2\" class=\"col_heading level1 col2\" >mean</th>\n",
              "      <th id=\"T_50834_level1_col3\" class=\"col_heading level1 col3\" >std</th>\n",
              "      <th id=\"T_50834_level1_col4\" class=\"col_heading level1 col4\" >mean</th>\n",
              "      <th id=\"T_50834_level1_col5\" class=\"col_heading level1 col5\" >std</th>\n",
              "      <th id=\"T_50834_level1_col6\" class=\"col_heading level1 col6\" >mean</th>\n",
              "      <th id=\"T_50834_level1_col7\" class=\"col_heading level1 col7\" >std</th>\n",
              "      <th id=\"T_50834_level1_col8\" class=\"col_heading level1 col8\" >mean</th>\n",
              "      <th id=\"T_50834_level1_col9\" class=\"col_heading level1 col9\" >std</th>\n",
              "      <th id=\"T_50834_level1_col10\" class=\"col_heading level1 col10\" >mean</th>\n",
              "      <th id=\"T_50834_level1_col11\" class=\"col_heading level1 col11\" >std</th>\n",
              "      <th id=\"T_50834_level1_col12\" class=\"col_heading level1 col12\" >mean</th>\n",
              "      <th id=\"T_50834_level1_col13\" class=\"col_heading level1 col13\" >std</th>\n",
              "      <th id=\"T_50834_level1_col14\" class=\"col_heading level1 col14\" >mean</th>\n",
              "      <th id=\"T_50834_level1_col15\" class=\"col_heading level1 col15\" >std</th>\n",
              "      <th id=\"T_50834_level1_col16\" class=\"col_heading level1 col16\" >mean</th>\n",
              "      <th id=\"T_50834_level1_col17\" class=\"col_heading level1 col17\" >std</th>\n",
              "      <th id=\"T_50834_level1_col18\" class=\"col_heading level1 col18\" >mean</th>\n",
              "      <th id=\"T_50834_level1_col19\" class=\"col_heading level1 col19\" >std</th>\n",
              "      <th id=\"T_50834_level1_col20\" class=\"col_heading level1 col20\" >mean</th>\n",
              "      <th id=\"T_50834_level1_col21\" class=\"col_heading level1 col21\" >std</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Train Language</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "      <th class=\"blank col7\" >&nbsp;</th>\n",
              "      <th class=\"blank col8\" >&nbsp;</th>\n",
              "      <th class=\"blank col9\" >&nbsp;</th>\n",
              "      <th class=\"blank col10\" >&nbsp;</th>\n",
              "      <th class=\"blank col11\" >&nbsp;</th>\n",
              "      <th class=\"blank col12\" >&nbsp;</th>\n",
              "      <th class=\"blank col13\" >&nbsp;</th>\n",
              "      <th class=\"blank col14\" >&nbsp;</th>\n",
              "      <th class=\"blank col15\" >&nbsp;</th>\n",
              "      <th class=\"blank col16\" >&nbsp;</th>\n",
              "      <th class=\"blank col17\" >&nbsp;</th>\n",
              "      <th class=\"blank col18\" >&nbsp;</th>\n",
              "      <th class=\"blank col19\" >&nbsp;</th>\n",
              "      <th class=\"blank col20\" >&nbsp;</th>\n",
              "      <th class=\"blank col21\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_50834_level0_row0\" class=\"row_heading level0 row0\" >en</th>\n",
              "      <td id=\"T_50834_row0_col0\" class=\"data row0 col0\" >0.5016</td>\n",
              "      <td id=\"T_50834_row0_col1\" class=\"data row0 col1\" >0.1664</td>\n",
              "      <td id=\"T_50834_row0_col2\" class=\"data row0 col2\" >0.6412</td>\n",
              "      <td id=\"T_50834_row0_col3\" class=\"data row0 col3\" >0.2122</td>\n",
              "      <td id=\"T_50834_row0_col4\" class=\"data row0 col4\" >0.5909</td>\n",
              "      <td id=\"T_50834_row0_col5\" class=\"data row0 col5\" >0.1862</td>\n",
              "      <td id=\"T_50834_row0_col6\" class=\"data row0 col6\" >0.6578</td>\n",
              "      <td id=\"T_50834_row0_col7\" class=\"data row0 col7\" >0.2074</td>\n",
              "      <td id=\"T_50834_row0_col8\" class=\"data row0 col8\" >0.9361</td>\n",
              "      <td id=\"T_50834_row0_col9\" class=\"data row0 col9\" >0.0577</td>\n",
              "      <td id=\"T_50834_row0_col10\" class=\"data row0 col10\" >0.6284</td>\n",
              "      <td id=\"T_50834_row0_col11\" class=\"data row0 col11\" >0.1681</td>\n",
              "      <td id=\"T_50834_row0_col12\" class=\"data row0 col12\" >0.6703</td>\n",
              "      <td id=\"T_50834_row0_col13\" class=\"data row0 col13\" >0.2113</td>\n",
              "      <td id=\"T_50834_row0_col14\" class=\"data row0 col14\" >0.6273</td>\n",
              "      <td id=\"T_50834_row0_col15\" class=\"data row0 col15\" >0.1955</td>\n",
              "      <td id=\"T_50834_row0_col16\" class=\"data row0 col16\" >0.5976</td>\n",
              "      <td id=\"T_50834_row0_col17\" class=\"data row0 col17\" >0.1835</td>\n",
              "      <td id=\"T_50834_row0_col18\" class=\"data row0 col18\" >0.5832</td>\n",
              "      <td id=\"T_50834_row0_col19\" class=\"data row0 col19\" >0.1649</td>\n",
              "      <td id=\"T_50834_row0_col20\" class=\"data row0 col20\" >0.4902</td>\n",
              "      <td id=\"T_50834_row0_col21\" class=\"data row0 col21\" >0.1374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_50834_level0_row1\" class=\"row_heading level0 row1\" >es</th>\n",
              "      <td id=\"T_50834_row1_col0\" class=\"data row1 col0\" >0.6480</td>\n",
              "      <td id=\"T_50834_row1_col1\" class=\"data row1 col1\" >0.2083</td>\n",
              "      <td id=\"T_50834_row1_col2\" class=\"data row1 col2\" >0.8413</td>\n",
              "      <td id=\"T_50834_row1_col3\" class=\"data row1 col3\" >0.1173</td>\n",
              "      <td id=\"T_50834_row1_col4\" class=\"data row1 col4\" >0.7300</td>\n",
              "      <td id=\"T_50834_row1_col5\" class=\"data row1 col5\" >0.2195</td>\n",
              "      <td id=\"T_50834_row1_col6\" class=\"data row1 col6\" >0.7850</td>\n",
              "      <td id=\"T_50834_row1_col7\" class=\"data row1 col7\" >0.1922</td>\n",
              "      <td id=\"T_50834_row1_col8\" class=\"data row1 col8\" >0.5790</td>\n",
              "      <td id=\"T_50834_row1_col9\" class=\"data row1 col9\" >0.2500</td>\n",
              "      <td id=\"T_50834_row1_col10\" class=\"data row1 col10\" >0.9259</td>\n",
              "      <td id=\"T_50834_row1_col11\" class=\"data row1 col11\" >0.0607</td>\n",
              "      <td id=\"T_50834_row1_col12\" class=\"data row1 col12\" >0.7689</td>\n",
              "      <td id=\"T_50834_row1_col13\" class=\"data row1 col13\" >0.1811</td>\n",
              "      <td id=\"T_50834_row1_col14\" class=\"data row1 col14\" >0.8749</td>\n",
              "      <td id=\"T_50834_row1_col15\" class=\"data row1 col15\" >0.0947</td>\n",
              "      <td id=\"T_50834_row1_col16\" class=\"data row1 col16\" >0.6804</td>\n",
              "      <td id=\"T_50834_row1_col17\" class=\"data row1 col17\" >0.2151</td>\n",
              "      <td id=\"T_50834_row1_col18\" class=\"data row1 col18\" >0.6800</td>\n",
              "      <td id=\"T_50834_row1_col19\" class=\"data row1 col19\" >0.2146</td>\n",
              "      <td id=\"T_50834_row1_col20\" class=\"data row1 col20\" >0.6082</td>\n",
              "      <td id=\"T_50834_row1_col21\" class=\"data row1 col21\" >0.2070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_50834_level0_row2\" class=\"row_heading level0 row2\" >ru</th>\n",
              "      <td id=\"T_50834_row2_col0\" class=\"data row2 col0\" >0.7421</td>\n",
              "      <td id=\"T_50834_row2_col1\" class=\"data row2 col1\" >0.1789</td>\n",
              "      <td id=\"T_50834_row2_col2\" class=\"data row2 col2\" >0.5274</td>\n",
              "      <td id=\"T_50834_row2_col3\" class=\"data row2 col3\" >0.2069</td>\n",
              "      <td id=\"T_50834_row2_col4\" class=\"data row2 col4\" >0.6171</td>\n",
              "      <td id=\"T_50834_row2_col5\" class=\"data row2 col5\" >0.2644</td>\n",
              "      <td id=\"T_50834_row2_col6\" class=\"data row2 col6\" >0.5914</td>\n",
              "      <td id=\"T_50834_row2_col7\" class=\"data row2 col7\" >0.2392</td>\n",
              "      <td id=\"T_50834_row2_col8\" class=\"data row2 col8\" >0.4795</td>\n",
              "      <td id=\"T_50834_row2_col9\" class=\"data row2 col9\" >0.2328</td>\n",
              "      <td id=\"T_50834_row2_col10\" class=\"data row2 col10\" >0.5614</td>\n",
              "      <td id=\"T_50834_row2_col11\" class=\"data row2 col11\" >0.2256</td>\n",
              "      <td id=\"T_50834_row2_col12\" class=\"data row2 col12\" >0.5524</td>\n",
              "      <td id=\"T_50834_row2_col13\" class=\"data row2 col13\" >0.2223</td>\n",
              "      <td id=\"T_50834_row2_col14\" class=\"data row2 col14\" >0.5369</td>\n",
              "      <td id=\"T_50834_row2_col15\" class=\"data row2 col15\" >0.2087</td>\n",
              "      <td id=\"T_50834_row2_col16\" class=\"data row2 col16\" >0.8870</td>\n",
              "      <td id=\"T_50834_row2_col17\" class=\"data row2 col17\" >0.1195</td>\n",
              "      <td id=\"T_50834_row2_col18\" class=\"data row2 col18\" >0.8557</td>\n",
              "      <td id=\"T_50834_row2_col19\" class=\"data row2 col19\" >0.1430</td>\n",
              "      <td id=\"T_50834_row2_col20\" class=\"data row2 col20\" >0.6183</td>\n",
              "      <td id=\"T_50834_row2_col21\" class=\"data row2 col21\" >0.1899</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_mean = results_all.reset_index().drop(columns=['Train LLM', 'Model']).groupby(['Train Language']).agg('mean')"
      ],
      "metadata": {
        "id": "ZhlvqaP8SZp_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_std = results_all.reset_index().drop(columns=['Train LLM', 'Model']).groupby(['Train Language']).agg('std')"
      ],
      "metadata": {
        "id": "41-4rL6iSeJz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = temp_mean.copy()\n",
        "for col in temp_mean.columns:\n",
        "  temp[col] = [f\"{str('%.4f' % x)} (±{str('%.2f' % y)})\" for x,y in zip(temp_mean[col], temp_std[col])]"
      ],
      "metadata": {
        "id": "ffZ0PgH5SpFx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "\n",
        "def b_g(s, cmap='PuBu', low=0, high=0):\n",
        "    #if s.name in temp_mean.columns:\n",
        "    #  a = temp_mean.loc[:,s.name].copy()\n",
        "    #else:\n",
        "    a = temp_mean.T.loc[:,s.name].copy() #for axis=1 must be T used\n",
        "    rng = a.max() - a.min()\n",
        "    norm = colors.Normalize(a.min() - (rng * low),\n",
        "                        a.max()+0.1 + (rng * high))\n",
        "    normed = norm(a.values)\n",
        "    c = [colors.rgb2hex(x) for x in matplotlib.colormaps[cmap](normed)]\n",
        "    return ['background-color: %s' % color for color in c]\n",
        "\n",
        "temp.style.apply(b_g,cmap='PuBu', axis=1)"
      ],
      "metadata": {
        "id": "Rc_3v39IX8mi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "026ebc91-48dc-4a46-ea20-f441cbb010ad"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd004d8b8e0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_8b35e_row0_col0 {\n",
              "  background-color: #fcf4fa;\n",
              "}\n",
              "#T_8b35e_row0_col1 {\n",
              "  background-color: #c8cde4;\n",
              "}\n",
              "#T_8b35e_row0_col2 {\n",
              "  background-color: #dfddec;\n",
              "}\n",
              "#T_8b35e_row0_col3 {\n",
              "  background-color: #bdc8e1;\n",
              "}\n",
              "#T_8b35e_row0_col4 {\n",
              "  background-color: #04649d;\n",
              "}\n",
              "#T_8b35e_row0_col5, #T_8b35e_row0_col7 {\n",
              "  background-color: #d0d1e6;\n",
              "}\n",
              "#T_8b35e_row0_col6 {\n",
              "  background-color: #b5c4df;\n",
              "}\n",
              "#T_8b35e_row0_col8 {\n",
              "  background-color: #dcdaeb;\n",
              "}\n",
              "#T_8b35e_row0_col9 {\n",
              "  background-color: #e2dfee;\n",
              "}\n",
              "#T_8b35e_row0_col10, #T_8b35e_row1_col4, #T_8b35e_row2_col4 {\n",
              "  background-color: #fff7fb;\n",
              "}\n",
              "#T_8b35e_row1_col0 {\n",
              "  background-color: #e6e2ef;\n",
              "}\n",
              "#T_8b35e_row1_col1 {\n",
              "  background-color: #4897c4;\n",
              "}\n",
              "#T_8b35e_row1_col2 {\n",
              "  background-color: #b3c3de;\n",
              "}\n",
              "#T_8b35e_row1_col3 {\n",
              "  background-color: #83afd3;\n",
              "}\n",
              "#T_8b35e_row1_col5 {\n",
              "  background-color: #056ba9;\n",
              "}\n",
              "#T_8b35e_row1_col6 {\n",
              "  background-color: #93b5d6;\n",
              "}\n",
              "#T_8b35e_row1_col7 {\n",
              "  background-color: #2786bb;\n",
              "}\n",
              "#T_8b35e_row1_col8 {\n",
              "  background-color: #d5d5e8;\n",
              "}\n",
              "#T_8b35e_row1_col9 {\n",
              "  background-color: #d6d6e9;\n",
              "}\n",
              "#T_8b35e_row1_col10 {\n",
              "  background-color: #f5eff6;\n",
              "}\n",
              "#T_8b35e_row2_col0 {\n",
              "  background-color: #6ba5cd;\n",
              "}\n",
              "#T_8b35e_row2_col1 {\n",
              "  background-color: #f1ebf4;\n",
              "}\n",
              "#T_8b35e_row2_col2, #T_8b35e_row2_col10 {\n",
              "  background-color: #c9cee4;\n",
              "}\n",
              "#T_8b35e_row2_col3 {\n",
              "  background-color: #d7d6e9;\n",
              "}\n",
              "#T_8b35e_row2_col5 {\n",
              "  background-color: #e4e1ef;\n",
              "}\n",
              "#T_8b35e_row2_col6 {\n",
              "  background-color: #e8e4f0;\n",
              "}\n",
              "#T_8b35e_row2_col7 {\n",
              "  background-color: #eee9f3;\n",
              "}\n",
              "#T_8b35e_row2_col8 {\n",
              "  background-color: #0567a1;\n",
              "}\n",
              "#T_8b35e_row2_col9 {\n",
              "  background-color: #0872b1;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_8b35e\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_8b35e_level0_col0\" class=\"col_heading level0 col0\" >ar</th>\n",
              "      <th id=\"T_8b35e_level0_col1\" class=\"col_heading level0 col1\" >ca</th>\n",
              "      <th id=\"T_8b35e_level0_col2\" class=\"col_heading level0 col2\" >cs</th>\n",
              "      <th id=\"T_8b35e_level0_col3\" class=\"col_heading level0 col3\" >de</th>\n",
              "      <th id=\"T_8b35e_level0_col4\" class=\"col_heading level0 col4\" >en</th>\n",
              "      <th id=\"T_8b35e_level0_col5\" class=\"col_heading level0 col5\" >es</th>\n",
              "      <th id=\"T_8b35e_level0_col6\" class=\"col_heading level0 col6\" >nl</th>\n",
              "      <th id=\"T_8b35e_level0_col7\" class=\"col_heading level0 col7\" >pt</th>\n",
              "      <th id=\"T_8b35e_level0_col8\" class=\"col_heading level0 col8\" >ru</th>\n",
              "      <th id=\"T_8b35e_level0_col9\" class=\"col_heading level0 col9\" >uk</th>\n",
              "      <th id=\"T_8b35e_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Train Language</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "      <th class=\"blank col7\" >&nbsp;</th>\n",
              "      <th class=\"blank col8\" >&nbsp;</th>\n",
              "      <th class=\"blank col9\" >&nbsp;</th>\n",
              "      <th class=\"blank col10\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_8b35e_level0_row0\" class=\"row_heading level0 row0\" >en</th>\n",
              "      <td id=\"T_8b35e_row0_col0\" class=\"data row0 col0\" >0.5016 (±0.17)</td>\n",
              "      <td id=\"T_8b35e_row0_col1\" class=\"data row0 col1\" >0.6412 (±0.21)</td>\n",
              "      <td id=\"T_8b35e_row0_col2\" class=\"data row0 col2\" >0.5909 (±0.19)</td>\n",
              "      <td id=\"T_8b35e_row0_col3\" class=\"data row0 col3\" >0.6578 (±0.21)</td>\n",
              "      <td id=\"T_8b35e_row0_col4\" class=\"data row0 col4\" >0.9361 (±0.06)</td>\n",
              "      <td id=\"T_8b35e_row0_col5\" class=\"data row0 col5\" >0.6284 (±0.17)</td>\n",
              "      <td id=\"T_8b35e_row0_col6\" class=\"data row0 col6\" >0.6703 (±0.21)</td>\n",
              "      <td id=\"T_8b35e_row0_col7\" class=\"data row0 col7\" >0.6273 (±0.20)</td>\n",
              "      <td id=\"T_8b35e_row0_col8\" class=\"data row0 col8\" >0.5976 (±0.18)</td>\n",
              "      <td id=\"T_8b35e_row0_col9\" class=\"data row0 col9\" >0.5832 (±0.16)</td>\n",
              "      <td id=\"T_8b35e_row0_col10\" class=\"data row0 col10\" >0.4902 (±0.14)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8b35e_level0_row1\" class=\"row_heading level0 row1\" >es</th>\n",
              "      <td id=\"T_8b35e_row1_col0\" class=\"data row1 col0\" >0.6480 (±0.21)</td>\n",
              "      <td id=\"T_8b35e_row1_col1\" class=\"data row1 col1\" >0.8413 (±0.12)</td>\n",
              "      <td id=\"T_8b35e_row1_col2\" class=\"data row1 col2\" >0.7300 (±0.22)</td>\n",
              "      <td id=\"T_8b35e_row1_col3\" class=\"data row1 col3\" >0.7850 (±0.19)</td>\n",
              "      <td id=\"T_8b35e_row1_col4\" class=\"data row1 col4\" >0.5790 (±0.25)</td>\n",
              "      <td id=\"T_8b35e_row1_col5\" class=\"data row1 col5\" >0.9259 (±0.06)</td>\n",
              "      <td id=\"T_8b35e_row1_col6\" class=\"data row1 col6\" >0.7689 (±0.18)</td>\n",
              "      <td id=\"T_8b35e_row1_col7\" class=\"data row1 col7\" >0.8749 (±0.09)</td>\n",
              "      <td id=\"T_8b35e_row1_col8\" class=\"data row1 col8\" >0.6804 (±0.22)</td>\n",
              "      <td id=\"T_8b35e_row1_col9\" class=\"data row1 col9\" >0.6800 (±0.21)</td>\n",
              "      <td id=\"T_8b35e_row1_col10\" class=\"data row1 col10\" >0.6082 (±0.21)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8b35e_level0_row2\" class=\"row_heading level0 row2\" >ru</th>\n",
              "      <td id=\"T_8b35e_row2_col0\" class=\"data row2 col0\" >0.7421 (±0.18)</td>\n",
              "      <td id=\"T_8b35e_row2_col1\" class=\"data row2 col1\" >0.5274 (±0.21)</td>\n",
              "      <td id=\"T_8b35e_row2_col2\" class=\"data row2 col2\" >0.6171 (±0.26)</td>\n",
              "      <td id=\"T_8b35e_row2_col3\" class=\"data row2 col3\" >0.5914 (±0.24)</td>\n",
              "      <td id=\"T_8b35e_row2_col4\" class=\"data row2 col4\" >0.4795 (±0.23)</td>\n",
              "      <td id=\"T_8b35e_row2_col5\" class=\"data row2 col5\" >0.5614 (±0.23)</td>\n",
              "      <td id=\"T_8b35e_row2_col6\" class=\"data row2 col6\" >0.5524 (±0.22)</td>\n",
              "      <td id=\"T_8b35e_row2_col7\" class=\"data row2 col7\" >0.5369 (±0.21)</td>\n",
              "      <td id=\"T_8b35e_row2_col8\" class=\"data row2 col8\" >0.8870 (±0.12)</td>\n",
              "      <td id=\"T_8b35e_row2_col9\" class=\"data row2 col9\" >0.8557 (±0.14)</td>\n",
              "      <td id=\"T_8b35e_row2_col10\" class=\"data row2 col10\" >0.6183 (±0.19)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#results_all.reset_index().drop(columns=['Train LLM', 'Model']).groupby(['Train Language']).agg('max').style.background_gradient(axis=1).format(na_rep=0, precision=4)\n",
        "temp = results_all.reset_index().drop(columns=['Train LLM', 'Model']).groupby(['Train Language']).agg('max').style.format(na_rep=0, precision=4)\n",
        "display(temp.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None))"
      ],
      "metadata": {
        "id": "hwMfO7-ENtxG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1f33453e-9b95-435b-a579-43ff9850c319"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd004d88df0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_04c08_row0_col0 {\n",
              "  background-color: #84b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_04c08_row0_col1, #T_04c08_row2_col4 {\n",
              "  background-color: #79abd0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_04c08_row0_col2 {\n",
              "  background-color: #73a9cf;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_04c08_row0_col3, #T_04c08_row0_col9, #T_04c08_row2_col1 {\n",
              "  background-color: #7dacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_04c08_row0_col4, #T_04c08_row1_col0, #T_04c08_row1_col2, #T_04c08_row1_col3, #T_04c08_row1_col8, #T_04c08_row1_col9, #T_04c08_row2_col0, #T_04c08_row2_col2, #T_04c08_row2_col8, #T_04c08_row2_col9 {\n",
              "  background-color: #75a9cf;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_04c08_row0_col5, #T_04c08_row2_col3, #T_04c08_row2_col5 {\n",
              "  background-color: #80aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_04c08_row0_col6, #T_04c08_row2_col6 {\n",
              "  background-color: #7bacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_04c08_row0_col7, #T_04c08_row2_col7, #T_04c08_row2_col10 {\n",
              "  background-color: #81aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_04c08_row0_col8, #T_04c08_row1_col10 {\n",
              "  background-color: #7eadd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_04c08_row0_col10 {\n",
              "  background-color: #89b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_04c08_row1_col1, #T_04c08_row1_col5 {\n",
              "  background-color: #76aad0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_04c08_row1_col4, #T_04c08_row1_col6, #T_04c08_row1_col7 {\n",
              "  background-color: #78abd0;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_04c08\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_04c08_level0_col0\" class=\"col_heading level0 col0\" >ar</th>\n",
              "      <th id=\"T_04c08_level0_col1\" class=\"col_heading level0 col1\" >ca</th>\n",
              "      <th id=\"T_04c08_level0_col2\" class=\"col_heading level0 col2\" >cs</th>\n",
              "      <th id=\"T_04c08_level0_col3\" class=\"col_heading level0 col3\" >de</th>\n",
              "      <th id=\"T_04c08_level0_col4\" class=\"col_heading level0 col4\" >en</th>\n",
              "      <th id=\"T_04c08_level0_col5\" class=\"col_heading level0 col5\" >es</th>\n",
              "      <th id=\"T_04c08_level0_col6\" class=\"col_heading level0 col6\" >nl</th>\n",
              "      <th id=\"T_04c08_level0_col7\" class=\"col_heading level0 col7\" >pt</th>\n",
              "      <th id=\"T_04c08_level0_col8\" class=\"col_heading level0 col8\" >ru</th>\n",
              "      <th id=\"T_04c08_level0_col9\" class=\"col_heading level0 col9\" >uk</th>\n",
              "      <th id=\"T_04c08_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Train Language</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "      <th class=\"blank col7\" >&nbsp;</th>\n",
              "      <th class=\"blank col8\" >&nbsp;</th>\n",
              "      <th class=\"blank col9\" >&nbsp;</th>\n",
              "      <th class=\"blank col10\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_04c08_level0_row0\" class=\"row_heading level0 row0\" >en</th>\n",
              "      <td id=\"T_04c08_row0_col0\" class=\"data row0 col0\" >0.9215</td>\n",
              "      <td id=\"T_04c08_row0_col1\" class=\"data row0 col1\" >0.9733</td>\n",
              "      <td id=\"T_04c08_row0_col2\" class=\"data row0 col2\" >1.0000</td>\n",
              "      <td id=\"T_04c08_row0_col3\" class=\"data row0 col3\" >0.9609</td>\n",
              "      <td id=\"T_04c08_row0_col4\" class=\"data row0 col4\" >0.9928</td>\n",
              "      <td id=\"T_04c08_row0_col5\" class=\"data row0 col5\" >0.9435</td>\n",
              "      <td id=\"T_04c08_row0_col6\" class=\"data row0 col6\" >0.9615</td>\n",
              "      <td id=\"T_04c08_row0_col7\" class=\"data row0 col7\" >0.9319</td>\n",
              "      <td id=\"T_04c08_row0_col8\" class=\"data row0 col8\" >0.9471</td>\n",
              "      <td id=\"T_04c08_row0_col9\" class=\"data row0 col9\" >0.9570</td>\n",
              "      <td id=\"T_04c08_row0_col10\" class=\"data row0 col10\" >0.8933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_04c08_level0_row1\" class=\"row_heading level0 row1\" >es</th>\n",
              "      <td id=\"T_04c08_row1_col0\" class=\"data row1 col0\" >0.9931</td>\n",
              "      <td id=\"T_04c08_row1_col1\" class=\"data row1 col1\" >0.9916</td>\n",
              "      <td id=\"T_04c08_row1_col2\" class=\"data row1 col2\" >0.9933</td>\n",
              "      <td id=\"T_04c08_row1_col3\" class=\"data row1 col3\" >0.9932</td>\n",
              "      <td id=\"T_04c08_row1_col4\" class=\"data row1 col4\" >0.9801</td>\n",
              "      <td id=\"T_04c08_row1_col5\" class=\"data row1 col5\" >0.9914</td>\n",
              "      <td id=\"T_04c08_row1_col6\" class=\"data row1 col6\" >0.9799</td>\n",
              "      <td id=\"T_04c08_row1_col7\" class=\"data row1 col7\" >0.9830</td>\n",
              "      <td id=\"T_04c08_row1_col8\" class=\"data row1 col8\" >0.9949</td>\n",
              "      <td id=\"T_04c08_row1_col9\" class=\"data row1 col9\" >0.9931</td>\n",
              "      <td id=\"T_04c08_row1_col10\" class=\"data row1 col10\" >0.9528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_04c08_level0_row2\" class=\"row_heading level0 row2\" >ru</th>\n",
              "      <td id=\"T_04c08_row2_col0\" class=\"data row2 col0\" >0.9931</td>\n",
              "      <td id=\"T_04c08_row2_col1\" class=\"data row2 col1\" >0.9600</td>\n",
              "      <td id=\"T_04c08_row2_col2\" class=\"data row2 col2\" >0.9950</td>\n",
              "      <td id=\"T_04c08_row2_col3\" class=\"data row2 col3\" >0.9442</td>\n",
              "      <td id=\"T_04c08_row2_col4\" class=\"data row2 col4\" >0.9765</td>\n",
              "      <td id=\"T_04c08_row2_col5\" class=\"data row2 col5\" >0.9435</td>\n",
              "      <td id=\"T_04c08_row2_col6\" class=\"data row2 col6\" >0.9633</td>\n",
              "      <td id=\"T_04c08_row2_col7\" class=\"data row2 col7\" >0.9368</td>\n",
              "      <td id=\"T_04c08_row2_col8\" class=\"data row2 col8\" >0.9966</td>\n",
              "      <td id=\"T_04c08_row2_col9\" class=\"data row2 col9\" >0.9948</td>\n",
              "      <td id=\"T_04c08_row2_col10\" class=\"data row2 col10\" >0.9338</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(results_all.reset_index().groupby(['Train Language']).agg('mean').style.format(na_rep=0, precision=4).highlight_max(props='font-weight: bold;', axis=1).applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True))"
      ],
      "metadata": {
        "id": "fELv3Fi1Czxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "884e01be-2111-4d70-92d3-47968dd0445c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{lrrrrrrrrrrr}\n",
            " & \\bfseries ar & \\bfseries ca & \\bfseries cs & \\bfseries de & \\bfseries en & \\bfseries es & \\bfseries nl & \\bfseries pt & \\bfseries ru & \\bfseries uk & \\bfseries zh \\\\\n",
            "Train Language &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
            "\\bfseries en & 0.5016 & 0.6412 & 0.5909 & 0.6578 & \\bfseries 0.9361 & 0.6284 & 0.6703 & 0.6273 & 0.5976 & 0.5832 & 0.4902 \\\\\n",
            "\\bfseries es & 0.6480 & 0.8413 & 0.7300 & 0.7850 & 0.5790 & \\bfseries 0.9259 & 0.7689 & 0.8749 & 0.6804 & 0.6800 & 0.6082 \\\\\n",
            "\\bfseries ru & 0.7421 & 0.5274 & 0.6171 & 0.5914 & 0.4795 & 0.5614 & 0.5524 & 0.5369 & \\bfseries 0.8870 & 0.8557 & 0.6183 \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-2b9115f25c01>:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
            "  print(results_all.reset_index().groupby(['Train Language']).agg('mean').style.format(na_rep=0, precision=4).highlight_max(props='font-weight: bold;', axis=1).applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Results just for English fine-tuned models\n",
        "temp = results_all.loc['en',:]\n",
        "display(temp.style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4))\n",
        "print(temp.style.format(na_rep=0, precision=4).highlight_max(props='font-weight: bold;', axis=1).applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True))\n",
        "\n",
        "means = pd.DataFrame()\n",
        "means = pd.concat([means, temp.agg('mean')], copy=False, axis=1)\n",
        "means.rename(columns={0:'All Detectors Mean'}, inplace=True)\n",
        "means = pd.concat([means, temp.loc[[x in multilingual for x in temp.reset_index().Model], :].agg('mean')], copy=False, axis=1)\n",
        "means.rename(columns={0:'Multilingual Base Models Mean'}, inplace=True)\n",
        "means = pd.concat([means, temp.loc[[x not in multilingual for x in temp.reset_index().Model], :].agg('mean')], copy=False, axis=1)\n",
        "means.rename(columns={0:'Monolingual Base Models Mean'}, inplace=True)\n",
        "means = means.T.style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4)\n",
        "display(means)\n",
        "temp = means.applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True)\n",
        "#little hack to properly format latex table\n",
        "print(temp.replace('\\\\color[HTML]{F1F1F1} ', '\\\\textcolor{white}{').replace('\\\\color[HTML]{000000} ', '\\\\textcolor{black}{').replace(' & {\\\\cellcolor', '} & {\\\\cellcolor').replace(' \\\\\\\\', '} \\\\\\\\').replace('\\n\\\\bfseries', '\\n\\\\multicolumn{2}{r|}{\\\\bfseries').replace('zh} \\\\\\\\', 'zh \\\\\\\\'))"
      ],
      "metadata": {
        "id": "5n5RhGggHT35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "519bdad0-2512-47c0-d21a-2adf47aefc4f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001f30a90>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_3f999_row0_col0, #T_3f999_row2_col9, #T_3f999_row12_col5, #T_3f999_row23_col3, #T_3f999_row23_col6, #T_3f999_row34_col10, #T_3f999_row35_col9, #T_3f999_row54_col3 {\n",
              "  background-color: #cacee5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row0_col1, #T_3f999_row3_col9, #T_3f999_row8_col5, #T_3f999_row14_col5, #T_3f999_row18_col4, #T_3f999_row46_col2, #T_3f999_row48_col7 {\n",
              "  background-color: #97b7d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row0_col2, #T_3f999_row0_col6, #T_3f999_row4_col1, #T_3f999_row4_col7, #T_3f999_row7_col5, #T_3f999_row20_col5, #T_3f999_row28_col4, #T_3f999_row32_col4, #T_3f999_row41_col1, #T_3f999_row42_col2, #T_3f999_row42_col3, #T_3f999_row46_col3, #T_3f999_row49_col7 {\n",
              "  background-color: #91b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row0_col3, #T_3f999_row4_col6, #T_3f999_row7_col9, #T_3f999_row7_col10, #T_3f999_row18_col3, #T_3f999_row20_col3, #T_3f999_row31_col4, #T_3f999_row45_col3, #T_3f999_row48_col3, #T_3f999_row48_col6 {\n",
              "  background-color: #89b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row0_col4, #T_3f999_row38_col4, #T_3f999_row39_col3, #T_3f999_row39_col9 {\n",
              "  background-color: #7dacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row0_col5, #T_3f999_row4_col9, #T_3f999_row47_col6 {\n",
              "  background-color: #a5bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row0_col7, #T_3f999_row0_col9, #T_3f999_row3_col1, #T_3f999_row11_col3, #T_3f999_row52_col7 {\n",
              "  background-color: #9ab8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row0_col8, #T_3f999_row8_col1, #T_3f999_row14_col0, #T_3f999_row14_col10, #T_3f999_row17_col3, #T_3f999_row18_col8, #T_3f999_row30_col4, #T_3f999_row41_col3, #T_3f999_row42_col7, #T_3f999_row42_col10, #T_3f999_row45_col7, #T_3f999_row55_col6 {\n",
              "  background-color: #94b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row0_col10, #T_3f999_row12_col3, #T_3f999_row34_col2, #T_3f999_row35_col8, #T_3f999_row36_col1 {\n",
              "  background-color: #c8cde4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row1_col0, #T_3f999_row24_col5, #T_3f999_row33_col1, #T_3f999_row37_col2, #T_3f999_row37_col3, #T_3f999_row37_col9, #T_3f999_row38_col5, #T_3f999_row45_col10, #T_3f999_row47_col2, #T_3f999_row54_col9 {\n",
              "  background-color: #cccfe5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row1_col1, #T_3f999_row19_col5, #T_3f999_row19_col10, #T_3f999_row20_col2, #T_3f999_row31_col6, #T_3f999_row32_col6, #T_3f999_row38_col9 {\n",
              "  background-color: #d1d2e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row1_col2, #T_3f999_row5_col5, #T_3f999_row5_col9, #T_3f999_row19_col2, #T_3f999_row24_col7, #T_3f999_row24_col9, #T_3f999_row27_col0, #T_3f999_row38_col8, #T_3f999_row40_col10, #T_3f999_row43_col9, #T_3f999_row47_col10, #T_3f999_row50_col6 {\n",
              "  background-color: #d2d3e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row1_col3, #T_3f999_row2_col3, #T_3f999_row16_col8, #T_3f999_row23_col7, #T_3f999_row31_col2, #T_3f999_row50_col0, #T_3f999_row51_col5, #T_3f999_row51_col6 {\n",
              "  background-color: #dcdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row1_col4, #T_3f999_row6_col4, #T_3f999_row7_col4, #T_3f999_row9_col4, #T_3f999_row13_col4, #T_3f999_row36_col4, #T_3f999_row43_col4, #T_3f999_row45_col4, #T_3f999_row50_col4, #T_3f999_row51_col4, #T_3f999_row55_col4 {\n",
              "  background-color: #78abd0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row1_col5, #T_3f999_row29_col3, #T_3f999_row32_col8, #T_3f999_row35_col2, #T_3f999_row37_col5, #T_3f999_row38_col10, #T_3f999_row41_col0 {\n",
              "  background-color: #c2cbe2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row1_col6, #T_3f999_row5_col2, #T_3f999_row12_col8, #T_3f999_row19_col3, #T_3f999_row25_col7, #T_3f999_row28_col5, #T_3f999_row37_col7, #T_3f999_row45_col0, #T_3f999_row46_col0 {\n",
              "  background-color: #c9cee4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row1_col7, #T_3f999_row21_col10, #T_3f999_row24_col10, #T_3f999_row29_col0, #T_3f999_row29_col1, #T_3f999_row30_col0, #T_3f999_row32_col1, #T_3f999_row35_col1, #T_3f999_row38_col7, #T_3f999_row50_col7, #T_3f999_row54_col8 {\n",
              "  background-color: #cdd0e5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row1_col8, #T_3f999_row5_col3, #T_3f999_row13_col10, #T_3f999_row25_col0, #T_3f999_row35_col5, #T_3f999_row35_col6, #T_3f999_row38_col1, #T_3f999_row47_col3, #T_3f999_row53_col7 {\n",
              "  background-color: #ced0e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row1_col9, #T_3f999_row2_col1, #T_3f999_row2_col6, #T_3f999_row12_col0, #T_3f999_row15_col6, #T_3f999_row23_col1, #T_3f999_row28_col6, #T_3f999_row30_col5, #T_3f999_row31_col10, #T_3f999_row34_col6, #T_3f999_row43_col2, #T_3f999_row51_col3, #T_3f999_row54_col0 {\n",
              "  background-color: #d9d8ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row1_col10, #T_3f999_row3_col10, #T_3f999_row24_col8, #T_3f999_row31_col9, #T_3f999_row33_col2, #T_3f999_row34_col1, #T_3f999_row43_col0, #T_3f999_row51_col9, #T_3f999_row53_col9 {\n",
              "  background-color: #d4d4e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row2_col0, #T_3f999_row38_col3, #T_3f999_row41_col8, #T_3f999_row43_col5 {\n",
              "  background-color: #a7bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row2_col2, #T_3f999_row5_col10, #T_3f999_row6_col0, #T_3f999_row6_col10, #T_3f999_row30_col10 {\n",
              "  background-color: #d6d6e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row2_col4, #T_3f999_row3_col4, #T_3f999_row39_col6 {\n",
              "  background-color: #7bacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row2_col5, #T_3f999_row19_col7, #T_3f999_row24_col6, #T_3f999_row30_col1, #T_3f999_row30_col7, #T_3f999_row47_col0, #T_3f999_row52_col0, #T_3f999_row53_col8 {\n",
              "  background-color: #dad9ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row2_col7, #T_3f999_row2_col10, #T_3f999_row9_col0, #T_3f999_row9_col8, #T_3f999_row9_col9, #T_3f999_row15_col9, #T_3f999_row16_col9, #T_3f999_row23_col0, #T_3f999_row30_col2, #T_3f999_row33_col7, #T_3f999_row33_col9 {\n",
              "  background-color: #dfddec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row2_col8, #T_3f999_row10_col10, #T_3f999_row11_col1, #T_3f999_row12_col2, #T_3f999_row15_col1, #T_3f999_row23_col5, #T_3f999_row28_col9, #T_3f999_row29_col6, #T_3f999_row32_col10, #T_3f999_row34_col5, #T_3f999_row37_col6, #T_3f999_row43_col6, #T_3f999_row47_col1 {\n",
              "  background-color: #c6cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row3_col0, #T_3f999_row5_col8, #T_3f999_row8_col0, #T_3f999_row15_col8, #T_3f999_row25_col1, #T_3f999_row33_col3, #T_3f999_row43_col3 {\n",
              "  background-color: #dbdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row3_col2, #T_3f999_row28_col2 {\n",
              "  background-color: #bdc8e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row3_col3, #T_3f999_row3_col6, #T_3f999_row34_col4 {\n",
              "  background-color: #8cb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row3_col5, #T_3f999_row8_col6, #T_3f999_row19_col6 {\n",
              "  background-color: #a8bedc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row3_col7, #T_3f999_row6_col9, #T_3f999_row17_col6, #T_3f999_row25_col2, #T_3f999_row36_col7, #T_3f999_row49_col8 {\n",
              "  background-color: #96b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row3_col8, #T_3f999_row12_col6, #T_3f999_row14_col3, #T_3f999_row49_col9, #T_3f999_row54_col6 {\n",
              "  background-color: #99b8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row4_col0 {\n",
              "  background-color: #f0eaf4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row4_col2, #T_3f999_row6_col6, #T_3f999_row15_col5, #T_3f999_row18_col9, #T_3f999_row20_col8, #T_3f999_row41_col6, #T_3f999_row45_col5, #T_3f999_row45_col8, #T_3f999_row48_col5 {\n",
              "  background-color: #a2bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row4_col3, #T_3f999_row7_col3, #T_3f999_row10_col8, #T_3f999_row21_col4, #T_3f999_row24_col4, #T_3f999_row25_col4, #T_3f999_row35_col4, #T_3f999_row39_col5, #T_3f999_row39_col7, #T_3f999_row42_col1, #T_3f999_row42_col6, #T_3f999_row45_col6 {\n",
              "  background-color: #88b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row4_col4, #T_3f999_row10_col6, #T_3f999_row13_col3, #T_3f999_row18_col5, #T_3f999_row41_col4, #T_3f999_row55_col1 {\n",
              "  background-color: #80aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row4_col5, #T_3f999_row5_col6, #T_3f999_row8_col7, #T_3f999_row11_col2, #T_3f999_row11_col6, #T_3f999_row42_col5, #T_3f999_row46_col10, #T_3f999_row53_col4, #T_3f999_row55_col8 {\n",
              "  background-color: #a1bbda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row4_col8, #T_3f999_row17_col8, #T_3f999_row21_col5, #T_3f999_row27_col3, #T_3f999_row49_col2 {\n",
              "  background-color: #9cb9d9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row4_col10 {\n",
              "  background-color: #e6e2ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row5_col0, #T_3f999_row9_col2, #T_3f999_row16_col7, #T_3f999_row24_col1, #T_3f999_row26_col2, #T_3f999_row26_col5, #T_3f999_row26_col9, #T_3f999_row51_col10 {\n",
              "  background-color: #e1dfed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row5_col1, #T_3f999_row6_col5, #T_3f999_row13_col9, #T_3f999_row27_col9, #T_3f999_row32_col2, #T_3f999_row51_col0, #T_3f999_row53_col3 {\n",
              "  background-color: #bbc7e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row5_col4, #T_3f999_row10_col1, #T_3f999_row13_col6, #T_3f999_row13_col7, #T_3f999_row14_col1, #T_3f999_row14_col6, #T_3f999_row20_col7, #T_3f999_row22_col4, #T_3f999_row39_col4, #T_3f999_row47_col4 {\n",
              "  background-color: #83afd3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row5_col7, #T_3f999_row23_col10, #T_3f999_row27_col6, #T_3f999_row29_col8, #T_3f999_row40_col9, #T_3f999_row47_col5, #T_3f999_row51_col8, #T_3f999_row54_col7, #T_3f999_row55_col10 {\n",
              "  background-color: #d8d7e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row6_col1, #T_3f999_row10_col5, #T_3f999_row27_col2, #T_3f999_row40_col1, #T_3f999_row49_col3, #T_3f999_row52_col3 {\n",
              "  background-color: #93b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row6_col2, #T_3f999_row6_col8, #T_3f999_row14_col7, #T_3f999_row33_col4, #T_3f999_row36_col5 {\n",
              "  background-color: #8eb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row6_col3, #T_3f999_row7_col6, #T_3f999_row18_col6, #T_3f999_row18_col7, #T_3f999_row20_col1, #T_3f999_row37_col4, #T_3f999_row49_col6, #T_3f999_row54_col4 {\n",
              "  background-color: #81aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row6_col7, #T_3f999_row11_col4, #T_3f999_row24_col3, #T_3f999_row25_col3, #T_3f999_row37_col1, #T_3f999_row42_col8, #T_3f999_row55_col7 {\n",
              "  background-color: #acc0dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row7_col0, #T_3f999_row7_col2, #T_3f999_row7_col8, #T_3f999_row10_col7, #T_3f999_row12_col4, #T_3f999_row27_col4, #T_3f999_row40_col6, #T_3f999_row45_col1, #T_3f999_row48_col1 {\n",
              "  background-color: #84b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row7_col1, #T_3f999_row13_col8, #T_3f999_row23_col4, #T_3f999_row26_col4, #T_3f999_row29_col4, #T_3f999_row49_col1, #T_3f999_row52_col6 {\n",
              "  background-color: #8bb2d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row7_col7, #T_3f999_row10_col3, #T_3f999_row10_col9, #T_3f999_row55_col3 {\n",
              "  background-color: #86b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row8_col2, #T_3f999_row15_col2, #T_3f999_row15_col3, #T_3f999_row15_col10, #T_3f999_row16_col10, #T_3f999_row22_col1, #T_3f999_row22_col2, #T_3f999_row22_col6, #T_3f999_row26_col0, #T_3f999_row26_col3, #T_3f999_row26_col8, #T_3f999_row26_col10, #T_3f999_row33_col0, #T_3f999_row33_col8, #T_3f999_row36_col8, #T_3f999_row36_col10, #T_3f999_row44_col0, #T_3f999_row44_col1, #T_3f999_row44_col2, #T_3f999_row44_col3, #T_3f999_row44_col10, #T_3f999_row50_col9 {\n",
              "  background-color: #e3e0ee;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row8_col3, #T_3f999_row8_col9, #T_3f999_row16_col2, #T_3f999_row16_col3, #T_3f999_row20_col10, #T_3f999_row22_col7, #T_3f999_row26_col1, #T_3f999_row33_col10, #T_3f999_row50_col10 {\n",
              "  background-color: #e0deed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row8_col4, #T_3f999_row13_col1, #T_3f999_row14_col4, #T_3f999_row18_col1, #T_3f999_row19_col4, #T_3f999_row39_col1, #T_3f999_row40_col4, #T_3f999_row42_col4, #T_3f999_row49_col4, #T_3f999_row52_col4 {\n",
              "  background-color: #79abd0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row8_col8, #T_3f999_row16_col0, #T_3f999_row16_col6, #T_3f999_row17_col2, #T_3f999_row24_col0, #T_3f999_row32_col0, #T_3f999_row37_col0, #T_3f999_row47_col7, #T_3f999_row50_col1, #T_3f999_row55_col0 {\n",
              "  background-color: #dddbec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row8_col10, #T_3f999_row9_col1, #T_3f999_row9_col3, #T_3f999_row9_col6, #T_3f999_row9_col7, #T_3f999_row9_col10, #T_3f999_row10_col2, #T_3f999_row22_col3, #T_3f999_row26_col7, #T_3f999_row36_col9, #T_3f999_row38_col0, #T_3f999_row40_col0, #T_3f999_row44_col6, #T_3f999_row44_col9, #T_3f999_row50_col8 {\n",
              "  background-color: #e2dfee;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row9_col5, #T_3f999_row22_col10, #T_3f999_row29_col10, #T_3f999_row36_col0, #T_3f999_row44_col5, #T_3f999_row44_col7, #T_3f999_row44_col8 {\n",
              "  background-color: #e4e1ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row10_col0, #T_3f999_row21_col2, #T_3f999_row21_col9, #T_3f999_row29_col7, #T_3f999_row34_col9, #T_3f999_row38_col6, #T_3f999_row39_col10, #T_3f999_row41_col7, #T_3f999_row46_col8, #T_3f999_row48_col2 {\n",
              "  background-color: #afc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row10_col4, #T_3f999_row17_col4, #T_3f999_row20_col4, #T_3f999_row44_col4, #T_3f999_row48_col4 {\n",
              "  background-color: #76aad0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row11_col0, #T_3f999_row13_col5, #T_3f999_row17_col1, #T_3f999_row17_col9, #T_3f999_row40_col2, #T_3f999_row49_col5, #T_3f999_row52_col1 {\n",
              "  background-color: #9fbad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row11_col5, #T_3f999_row21_col1, #T_3f999_row25_col5, #T_3f999_row27_col7, #T_3f999_row31_col7, #T_3f999_row32_col5, #T_3f999_row32_col9, #T_3f999_row50_col2 {\n",
              "  background-color: #bfc9e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row11_col7, #T_3f999_row11_col10, #T_3f999_row21_col7, #T_3f999_row22_col0, #T_3f999_row34_col3 {\n",
              "  background-color: #b3c3de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row11_col8, #T_3f999_row29_col2, #T_3f999_row32_col7, #T_3f999_row38_col2, #T_3f999_row54_col1 {\n",
              "  background-color: #b8c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row11_col9, #T_3f999_row14_col2, #T_3f999_row36_col6, #T_3f999_row41_col9, #T_3f999_row42_col0, #T_3f999_row46_col6 {\n",
              "  background-color: #a4bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row12_col1, #T_3f999_row25_col9, #T_3f999_row34_col8, #T_3f999_row42_col9, #T_3f999_row49_col10 {\n",
              "  background-color: #b4c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row12_col7, #T_3f999_row15_col0, #T_3f999_row25_col10, #T_3f999_row31_col8, #T_3f999_row35_col10, #T_3f999_row48_col10, #T_3f999_row51_col2 {\n",
              "  background-color: #d7d6e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row12_col9, #T_3f999_row30_col9, #T_3f999_row41_col10, #T_3f999_row47_col9, #T_3f999_row53_col6 {\n",
              "  background-color: #bcc7e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row12_col10, #T_3f999_row17_col10, #T_3f999_row35_col7, #T_3f999_row37_col8, #T_3f999_row37_col10, #T_3f999_row47_col8, #T_3f999_row48_col9, #T_3f999_row53_col5, #T_3f999_row54_col10 {\n",
              "  background-color: #d3d4e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row13_col0, #T_3f999_row13_col2, #T_3f999_row18_col0, #T_3f999_row27_col5 {\n",
              "  background-color: #abbfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row14_col8, #T_3f999_row17_col5, #T_3f999_row39_col0, #T_3f999_row49_col0 {\n",
              "  background-color: #9ebad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row14_col9, #T_3f999_row21_col8, #T_3f999_row25_col8, #T_3f999_row52_col9 {\n",
              "  background-color: #adc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row15_col4, #T_3f999_row16_col4 {\n",
              "  background-color: #75a9cf;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row15_col7, #T_3f999_row55_col5 {\n",
              "  background-color: #b1c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row16_col1, #T_3f999_row16_col5, #T_3f999_row22_col5, #T_3f999_row26_col6, #T_3f999_row30_col6, #T_3f999_row40_col8, #T_3f999_row45_col2, #T_3f999_row51_col1 {\n",
              "  background-color: #e0dded;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row17_col0, #T_3f999_row21_col3, #T_3f999_row24_col2, #T_3f999_row27_col8, #T_3f999_row40_col5, #T_3f999_row45_col9 {\n",
              "  background-color: #b0c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row17_col7, #T_3f999_row18_col2, #T_3f999_row41_col2 {\n",
              "  background-color: #8fb4d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row18_col10, #T_3f999_row19_col0, #T_3f999_row21_col6, #T_3f999_row30_col8, #T_3f999_row33_col6, #T_3f999_row43_col1, #T_3f999_row46_col5, #T_3f999_row48_col8 {\n",
              "  background-color: #c4cbe3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row19_col1, #T_3f999_row19_col9, #T_3f999_row20_col0, #T_3f999_row22_col9, #T_3f999_row28_col7, #T_3f999_row54_col2 {\n",
              "  background-color: #c5cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row19_col8, #T_3f999_row31_col3, #T_3f999_row46_col1, #T_3f999_row46_col7, #T_3f999_row52_col8 {\n",
              "  background-color: #b9c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row20_col6, #T_3f999_row39_col8, #T_3f999_row46_col4 {\n",
              "  background-color: #7eadd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row20_col9, #T_3f999_row23_col8, #T_3f999_row25_col6, #T_3f999_row28_col0, #T_3f999_row28_col1, #T_3f999_row35_col0 {\n",
              "  background-color: #d5d5e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row21_col0, #T_3f999_row23_col2, #T_3f999_row28_col3, #T_3f999_row28_col8, #T_3f999_row34_col7, #T_3f999_row35_col3, #T_3f999_row53_col10 {\n",
              "  background-color: #c1cae2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row22_col8, #T_3f999_row23_col9, #T_3f999_row27_col1, #T_3f999_row27_col10, #T_3f999_row34_col0, #T_3f999_row43_col10, #T_3f999_row55_col2, #T_3f999_row55_col9 {\n",
              "  background-color: #d2d2e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row28_col10, #T_3f999_row29_col9, #T_3f999_row31_col1, #T_3f999_row31_col5, #T_3f999_row36_col2, #T_3f999_row43_col8, #T_3f999_row48_col0, #T_3f999_row50_col3, #T_3f999_row52_col2, #T_3f999_row53_col1, #T_3f999_row53_col2, #T_3f999_row54_col5 {\n",
              "  background-color: #d0d1e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row29_col5, #T_3f999_row32_col3, #T_3f999_row36_col3, #T_3f999_row40_col7, #T_3f999_row50_col5, #T_3f999_row52_col5 {\n",
              "  background-color: #b5c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row30_col3, #T_3f999_row33_col5, #T_3f999_row51_col7 {\n",
              "  background-color: #dedcec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row31_col0 {\n",
              "  background-color: #e7e3f0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row39_col2 {\n",
              "  background-color: #73a9cf;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row40_col3 {\n",
              "  background-color: #b7c5df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row41_col5 {\n",
              "  background-color: #a9bfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row43_col7, #T_3f999_row46_col9, #T_3f999_row52_col10 {\n",
              "  background-color: #c0c9e2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_3f999_row53_col0 {\n",
              "  background-color: #f2ecf5;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_3f999\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank\" >&nbsp;</th>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_3f999_level0_col0\" class=\"col_heading level0 col0\" >ar</th>\n",
              "      <th id=\"T_3f999_level0_col1\" class=\"col_heading level0 col1\" >ca</th>\n",
              "      <th id=\"T_3f999_level0_col2\" class=\"col_heading level0 col2\" >cs</th>\n",
              "      <th id=\"T_3f999_level0_col3\" class=\"col_heading level0 col3\" >de</th>\n",
              "      <th id=\"T_3f999_level0_col4\" class=\"col_heading level0 col4\" >en</th>\n",
              "      <th id=\"T_3f999_level0_col5\" class=\"col_heading level0 col5\" >es</th>\n",
              "      <th id=\"T_3f999_level0_col6\" class=\"col_heading level0 col6\" >nl</th>\n",
              "      <th id=\"T_3f999_level0_col7\" class=\"col_heading level0 col7\" >pt</th>\n",
              "      <th id=\"T_3f999_level0_col8\" class=\"col_heading level0 col8\" >ru</th>\n",
              "      <th id=\"T_3f999_level0_col9\" class=\"col_heading level0 col9\" >uk</th>\n",
              "      <th id=\"T_3f999_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Train LLM</th>\n",
              "      <th class=\"index_name level1\" >Model</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "      <th class=\"blank col7\" >&nbsp;</th>\n",
              "      <th class=\"blank col8\" >&nbsp;</th>\n",
              "      <th class=\"blank col9\" >&nbsp;</th>\n",
              "      <th class=\"blank col10\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"7\">alpaca-lora-30b</th>\n",
              "      <th id=\"T_3f999_level1_row0\" class=\"row_heading level1 row0\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_3f999_row0_col0\" class=\"data row0 col0\" >0.5375</td>\n",
              "      <td id=\"T_3f999_row0_col1\" class=\"data row0 col1\" >0.8271</td>\n",
              "      <td id=\"T_3f999_row0_col2\" class=\"data row0 col2\" >0.8546</td>\n",
              "      <td id=\"T_3f999_row0_col3\" class=\"data row0 col3\" >0.8917</td>\n",
              "      <td id=\"T_3f999_row0_col4\" class=\"data row0 col4\" >0.9567</td>\n",
              "      <td id=\"T_3f999_row0_col5\" class=\"data row0 col5\" >0.7517</td>\n",
              "      <td id=\"T_3f999_row0_col6\" class=\"data row0 col6\" >0.8563</td>\n",
              "      <td id=\"T_3f999_row0_col7\" class=\"data row0 col7\" >0.8055</td>\n",
              "      <td id=\"T_3f999_row0_col8\" class=\"data row0 col8\" >0.8374</td>\n",
              "      <td id=\"T_3f999_row0_col9\" class=\"data row0 col9\" >0.8091</td>\n",
              "      <td id=\"T_3f999_row0_col10\" class=\"data row0 col10\" >0.5537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row1\" class=\"row_heading level1 row1\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_3f999_row1_col0\" class=\"data row1 col0\" >0.5270</td>\n",
              "      <td id=\"T_3f999_row1_col1\" class=\"data row1 col1\" >0.4929</td>\n",
              "      <td id=\"T_3f999_row1_col2\" class=\"data row1 col2\" >0.4819</td>\n",
              "      <td id=\"T_3f999_row1_col3\" class=\"data row1 col3\" >0.3956</td>\n",
              "      <td id=\"T_3f999_row1_col4\" class=\"data row1 col4\" >0.9783</td>\n",
              "      <td id=\"T_3f999_row1_col5\" class=\"data row1 col5\" >0.5856</td>\n",
              "      <td id=\"T_3f999_row1_col6\" class=\"data row1 col6\" >0.5448</td>\n",
              "      <td id=\"T_3f999_row1_col7\" class=\"data row1 col7\" >0.5196</td>\n",
              "      <td id=\"T_3f999_row1_col8\" class=\"data row1 col8\" >0.5153</td>\n",
              "      <td id=\"T_3f999_row1_col9\" class=\"data row1 col9\" >0.4215</td>\n",
              "      <td id=\"T_3f999_row1_col10\" class=\"data row1 col10\" >0.4642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row2\" class=\"row_heading level1 row2\" >gpt2-medium</th>\n",
              "      <td id=\"T_3f999_row2_col0\" class=\"data row2 col0\" >0.7438</td>\n",
              "      <td id=\"T_3f999_row2_col1\" class=\"data row2 col1\" >0.4252</td>\n",
              "      <td id=\"T_3f999_row2_col2\" class=\"data row2 col2\" >0.4513</td>\n",
              "      <td id=\"T_3f999_row2_col3\" class=\"data row2 col3\" >0.3928</td>\n",
              "      <td id=\"T_3f999_row2_col4\" class=\"data row2 col4\" >0.9657</td>\n",
              "      <td id=\"T_3f999_row2_col5\" class=\"data row2 col5\" >0.4063</td>\n",
              "      <td id=\"T_3f999_row2_col6\" class=\"data row2 col6\" >0.4233</td>\n",
              "      <td id=\"T_3f999_row2_col7\" class=\"data row2 col7\" >0.3719</td>\n",
              "      <td id=\"T_3f999_row2_col8\" class=\"data row2 col8\" >0.5547</td>\n",
              "      <td id=\"T_3f999_row2_col9\" class=\"data row2 col9\" >0.5326</td>\n",
              "      <td id=\"T_3f999_row2_col10\" class=\"data row2 col10\" >0.3742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row3\" class=\"row_heading level1 row3\" >mGPT</th>\n",
              "      <td id=\"T_3f999_row3_col0\" class=\"data row3 col0\" >0.4024</td>\n",
              "      <td id=\"T_3f999_row3_col1\" class=\"data row3 col1\" >0.8089</td>\n",
              "      <td id=\"T_3f999_row3_col2\" class=\"data row3 col2\" >0.6132</td>\n",
              "      <td id=\"T_3f999_row3_col3\" class=\"data row3 col3\" >0.8763</td>\n",
              "      <td id=\"T_3f999_row3_col4\" class=\"data row3 col4\" >0.9639</td>\n",
              "      <td id=\"T_3f999_row3_col5\" class=\"data row3 col5\" >0.7390</td>\n",
              "      <td id=\"T_3f999_row3_col6\" class=\"data row3 col6\" >0.8791</td>\n",
              "      <td id=\"T_3f999_row3_col7\" class=\"data row3 col7\" >0.8333</td>\n",
              "      <td id=\"T_3f999_row3_col8\" class=\"data row3 col8\" >0.8162</td>\n",
              "      <td id=\"T_3f999_row3_col9\" class=\"data row3 col9\" >0.8210</td>\n",
              "      <td id=\"T_3f999_row3_col10\" class=\"data row3 col10\" >0.4626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row4\" class=\"row_heading level1 row4\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_3f999_row4_col0\" class=\"data row4 col0\" >0.2080</td>\n",
              "      <td id=\"T_3f999_row4_col1\" class=\"data row4 col1\" >0.8592</td>\n",
              "      <td id=\"T_3f999_row4_col2\" class=\"data row4 col2\" >0.7691</td>\n",
              "      <td id=\"T_3f999_row4_col3\" class=\"data row4 col3\" >0.9003</td>\n",
              "      <td id=\"T_3f999_row4_col4\" class=\"data row4 col4\" >0.9439</td>\n",
              "      <td id=\"T_3f999_row4_col5\" class=\"data row4 col5\" >0.7744</td>\n",
              "      <td id=\"T_3f999_row4_col6\" class=\"data row4 col6\" >0.8977</td>\n",
              "      <td id=\"T_3f999_row4_col7\" class=\"data row4 col7\" >0.8570</td>\n",
              "      <td id=\"T_3f999_row4_col8\" class=\"data row4 col8\" >0.7988</td>\n",
              "      <td id=\"T_3f999_row4_col9\" class=\"data row4 col9\" >0.7563</td>\n",
              "      <td id=\"T_3f999_row4_col10\" class=\"data row4 col10\" >0.3099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row5\" class=\"row_heading level1 row5\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_3f999_row5_col0\" class=\"data row5 col0\" >0.3484</td>\n",
              "      <td id=\"T_3f999_row5_col1\" class=\"data row5 col1\" >0.6258</td>\n",
              "      <td id=\"T_3f999_row5_col2\" class=\"data row5 col2\" >0.5430</td>\n",
              "      <td id=\"T_3f999_row5_col3\" class=\"data row5 col3\" >0.5133</td>\n",
              "      <td id=\"T_3f999_row5_col4\" class=\"data row5 col4\" >0.9238</td>\n",
              "      <td id=\"T_3f999_row5_col5\" class=\"data row5 col5\" >0.4809</td>\n",
              "      <td id=\"T_3f999_row5_col6\" class=\"data row5 col6\" >0.7787</td>\n",
              "      <td id=\"T_3f999_row5_col7\" class=\"data row5 col7\" >0.4348</td>\n",
              "      <td id=\"T_3f999_row5_col8\" class=\"data row5 col8\" >0.4029</td>\n",
              "      <td id=\"T_3f999_row5_col9\" class=\"data row5 col9\" >0.4838</td>\n",
              "      <td id=\"T_3f999_row5_col10\" class=\"data row5 col10\" >0.4487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row6\" class=\"row_heading level1 row6\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_3f999_row6_col0\" class=\"data row6 col0\" >0.4474</td>\n",
              "      <td id=\"T_3f999_row6_col1\" class=\"data row6 col1\" >0.8488</td>\n",
              "      <td id=\"T_3f999_row6_col2\" class=\"data row6 col2\" >0.8713</td>\n",
              "      <td id=\"T_3f999_row6_col3\" class=\"data row6 col3\" >0.9324</td>\n",
              "      <td id=\"T_3f999_row6_col4\" class=\"data row6 col4\" >0.9801</td>\n",
              "      <td id=\"T_3f999_row6_col5\" class=\"data row6 col5\" >0.6319</td>\n",
              "      <td id=\"T_3f999_row6_col6\" class=\"data row6 col6\" >0.7706</td>\n",
              "      <td id=\"T_3f999_row6_col7\" class=\"data row6 col7\" >0.7130</td>\n",
              "      <td id=\"T_3f999_row6_col8\" class=\"data row6 col8\" >0.8733</td>\n",
              "      <td id=\"T_3f999_row6_col9\" class=\"data row6 col9\" >0.8319</td>\n",
              "      <td id=\"T_3f999_row6_col10\" class=\"data row6 col10\" >0.4474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level0_row7\" class=\"row_heading level0 row7\" rowspan=\"7\">gpt-3.5-turbo</th>\n",
              "      <th id=\"T_3f999_level1_row7\" class=\"row_heading level1 row7\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_3f999_row7_col0\" class=\"data row7 col0\" >0.9215</td>\n",
              "      <td id=\"T_3f999_row7_col1\" class=\"data row7 col1\" >0.8904</td>\n",
              "      <td id=\"T_3f999_row7_col2\" class=\"data row7 col2\" >0.9150</td>\n",
              "      <td id=\"T_3f999_row7_col3\" class=\"data row7 col3\" >0.9020</td>\n",
              "      <td id=\"T_3f999_row7_col4\" class=\"data row7 col4\" >0.9783</td>\n",
              "      <td id=\"T_3f999_row7_col5\" class=\"data row7 col5\" >0.8545</td>\n",
              "      <td id=\"T_3f999_row7_col6\" class=\"data row7 col6\" >0.9348</td>\n",
              "      <td id=\"T_3f999_row7_col7\" class=\"data row7 col7\" >0.9124</td>\n",
              "      <td id=\"T_3f999_row7_col8\" class=\"data row7 col8\" >0.9183</td>\n",
              "      <td id=\"T_3f999_row7_col9\" class=\"data row7 col9\" >0.8962</td>\n",
              "      <td id=\"T_3f999_row7_col10\" class=\"data row7 col10\" >0.8933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row8\" class=\"row_heading level1 row8\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_3f999_row8_col0\" class=\"data row8 col0\" >0.3985</td>\n",
              "      <td id=\"T_3f999_row8_col1\" class=\"data row8 col1\" >0.8409</td>\n",
              "      <td id=\"T_3f999_row8_col2\" class=\"data row8 col2\" >0.3333</td>\n",
              "      <td id=\"T_3f999_row8_col3\" class=\"data row8 col3\" >0.3576</td>\n",
              "      <td id=\"T_3f999_row8_col4\" class=\"data row8 col4\" >0.9765</td>\n",
              "      <td id=\"T_3f999_row8_col5\" class=\"data row8 col5\" >0.8243</td>\n",
              "      <td id=\"T_3f999_row8_col6\" class=\"data row8 col6\" >0.7355</td>\n",
              "      <td id=\"T_3f999_row8_col7\" class=\"data row8 col7\" >0.7810</td>\n",
              "      <td id=\"T_3f999_row8_col8\" class=\"data row8 col8\" >0.3849</td>\n",
              "      <td id=\"T_3f999_row8_col9\" class=\"data row8 col9\" >0.3526</td>\n",
              "      <td id=\"T_3f999_row8_col10\" class=\"data row8 col10\" >0.3399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row9\" class=\"row_heading level1 row9\" >gpt2-medium</th>\n",
              "      <td id=\"T_3f999_row9_col0\" class=\"data row9 col0\" >0.3673</td>\n",
              "      <td id=\"T_3f999_row9_col1\" class=\"data row9 col1\" >0.3407</td>\n",
              "      <td id=\"T_3f999_row9_col2\" class=\"data row9 col2\" >0.3443</td>\n",
              "      <td id=\"T_3f999_row9_col3\" class=\"data row9 col3\" >0.3369</td>\n",
              "      <td id=\"T_3f999_row9_col4\" class=\"data row9 col4\" >0.9838</td>\n",
              "      <td id=\"T_3f999_row9_col5\" class=\"data row9 col5\" >0.3264</td>\n",
              "      <td id=\"T_3f999_row9_col6\" class=\"data row9 col6\" >0.3432</td>\n",
              "      <td id=\"T_3f999_row9_col7\" class=\"data row9 col7\" >0.3378</td>\n",
              "      <td id=\"T_3f999_row9_col8\" class=\"data row9 col8\" >0.3689</td>\n",
              "      <td id=\"T_3f999_row9_col9\" class=\"data row9 col9\" >0.3710</td>\n",
              "      <td id=\"T_3f999_row9_col10\" class=\"data row9 col10\" >0.3428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row10\" class=\"row_heading level1 row10\" >mGPT</th>\n",
              "      <td id=\"T_3f999_row10_col0\" class=\"data row10 col0\" >0.7009</td>\n",
              "      <td id=\"T_3f999_row10_col1\" class=\"data row10 col1\" >0.9266</td>\n",
              "      <td id=\"T_3f999_row10_col2\" class=\"data row10 col2\" >0.3370</td>\n",
              "      <td id=\"T_3f999_row10_col3\" class=\"data row10 col3\" >0.9071</td>\n",
              "      <td id=\"T_3f999_row10_col4\" class=\"data row10 col4\" >0.9892</td>\n",
              "      <td id=\"T_3f999_row10_col5\" class=\"data row10 col5\" >0.8506</td>\n",
              "      <td id=\"T_3f999_row10_col6\" class=\"data row10 col6\" >0.9432</td>\n",
              "      <td id=\"T_3f999_row10_col7\" class=\"data row10 col7\" >0.9147</td>\n",
              "      <td id=\"T_3f999_row10_col8\" class=\"data row10 col8\" >0.9013</td>\n",
              "      <td id=\"T_3f999_row10_col9\" class=\"data row10 col9\" >0.9080</td>\n",
              "      <td id=\"T_3f999_row10_col10\" class=\"data row10 col10\" >0.5610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row11\" class=\"row_heading level1 row11\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_3f999_row11_col0\" class=\"data row11 col0\" >0.7876</td>\n",
              "      <td id=\"T_3f999_row11_col1\" class=\"data row11 col1\" >0.5599</td>\n",
              "      <td id=\"T_3f999_row11_col2\" class=\"data row11 col2\" >0.7803</td>\n",
              "      <td id=\"T_3f999_row11_col3\" class=\"data row11 col3\" >0.8052</td>\n",
              "      <td id=\"T_3f999_row11_col4\" class=\"data row11 col4\" >0.7178</td>\n",
              "      <td id=\"T_3f999_row11_col5\" class=\"data row11 col5\" >0.6023</td>\n",
              "      <td id=\"T_3f999_row11_col6\" class=\"data row11 col6\" >0.7790</td>\n",
              "      <td id=\"T_3f999_row11_col7\" class=\"data row11 col7\" >0.6782</td>\n",
              "      <td id=\"T_3f999_row11_col8\" class=\"data row11 col8\" >0.6484</td>\n",
              "      <td id=\"T_3f999_row11_col9\" class=\"data row11 col9\" >0.7649</td>\n",
              "      <td id=\"T_3f999_row11_col10\" class=\"data row11 col10\" >0.6733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row12\" class=\"row_heading level1 row12\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_3f999_row12_col0\" class=\"data row12 col0\" >0.4174</td>\n",
              "      <td id=\"T_3f999_row12_col1\" class=\"data row12 col1\" >0.6710</td>\n",
              "      <td id=\"T_3f999_row12_col2\" class=\"data row12 col2\" >0.5623</td>\n",
              "      <td id=\"T_3f999_row12_col3\" class=\"data row12 col3\" >0.5476</td>\n",
              "      <td id=\"T_3f999_row12_col4\" class=\"data row12 col4\" >0.9166</td>\n",
              "      <td id=\"T_3f999_row12_col5\" class=\"data row12 col5\" >0.5356</td>\n",
              "      <td id=\"T_3f999_row12_col6\" class=\"data row12 col6\" >0.8203</td>\n",
              "      <td id=\"T_3f999_row12_col7\" class=\"data row12 col7\" >0.4381</td>\n",
              "      <td id=\"T_3f999_row12_col8\" class=\"data row12 col8\" >0.5442</td>\n",
              "      <td id=\"T_3f999_row12_col9\" class=\"data row12 col9\" >0.6236</td>\n",
              "      <td id=\"T_3f999_row12_col10\" class=\"data row12 col10\" >0.4727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row13\" class=\"row_heading level1 row13\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_3f999_row13_col0\" class=\"data row13 col0\" >0.7218</td>\n",
              "      <td id=\"T_3f999_row13_col1\" class=\"data row13 col1\" >0.9733</td>\n",
              "      <td id=\"T_3f999_row13_col2\" class=\"data row13 col2\" >0.7211</td>\n",
              "      <td id=\"T_3f999_row13_col3\" class=\"data row13 col3\" >0.9392</td>\n",
              "      <td id=\"T_3f999_row13_col4\" class=\"data row13 col4\" >0.9838</td>\n",
              "      <td id=\"T_3f999_row13_col5\" class=\"data row13 col5\" >0.7861</td>\n",
              "      <td id=\"T_3f999_row13_col6\" class=\"data row13 col6\" >0.9229</td>\n",
              "      <td id=\"T_3f999_row13_col7\" class=\"data row13 col7\" >0.9281</td>\n",
              "      <td id=\"T_3f999_row13_col8\" class=\"data row13 col8\" >0.8839</td>\n",
              "      <td id=\"T_3f999_row13_col9\" class=\"data row13 col9\" >0.6290</td>\n",
              "      <td id=\"T_3f999_row13_col10\" class=\"data row13 col10\" >0.5155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level0_row14\" class=\"row_heading level0 row14\" rowspan=\"7\">gpt-4</th>\n",
              "      <th id=\"T_3f999_level1_row14\" class=\"row_heading level1 row14\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_3f999_row14_col0\" class=\"data row14 col0\" >0.8419</td>\n",
              "      <td id=\"T_3f999_row14_col1\" class=\"data row14 col1\" >0.9265</td>\n",
              "      <td id=\"T_3f999_row14_col2\" class=\"data row14 col2\" >0.7592</td>\n",
              "      <td id=\"T_3f999_row14_col3\" class=\"data row14 col3\" >0.8163</td>\n",
              "      <td id=\"T_3f999_row14_col4\" class=\"data row14 col4\" >0.9765</td>\n",
              "      <td id=\"T_3f999_row14_col5\" class=\"data row14 col5\" >0.8261</td>\n",
              "      <td id=\"T_3f999_row14_col6\" class=\"data row14 col6\" >0.9265</td>\n",
              "      <td id=\"T_3f999_row14_col7\" class=\"data row14 col7\" >0.8704</td>\n",
              "      <td id=\"T_3f999_row14_col8\" class=\"data row14 col8\" >0.7929</td>\n",
              "      <td id=\"T_3f999_row14_col9\" class=\"data row14 col9\" >0.7104</td>\n",
              "      <td id=\"T_3f999_row14_col10\" class=\"data row14 col10\" >0.8360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row15\" class=\"row_heading level1 row15\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_3f999_row15_col0\" class=\"data row15 col0\" >0.4448</td>\n",
              "      <td id=\"T_3f999_row15_col1\" class=\"data row15 col1\" >0.5611</td>\n",
              "      <td id=\"T_3f999_row15_col2\" class=\"data row15 col2\" >0.3333</td>\n",
              "      <td id=\"T_3f999_row15_col3\" class=\"data row15 col3\" >0.3303</td>\n",
              "      <td id=\"T_3f999_row15_col4\" class=\"data row15 col4\" >0.9928</td>\n",
              "      <td id=\"T_3f999_row15_col5\" class=\"data row15 col5\" >0.7723</td>\n",
              "      <td id=\"T_3f999_row15_col6\" class=\"data row15 col6\" >0.4206</td>\n",
              "      <td id=\"T_3f999_row15_col7\" class=\"data row15 col7\" >0.6814</td>\n",
              "      <td id=\"T_3f999_row15_col8\" class=\"data row15 col8\" >0.4044</td>\n",
              "      <td id=\"T_3f999_row15_col9\" class=\"data row15 col9\" >0.3697</td>\n",
              "      <td id=\"T_3f999_row15_col10\" class=\"data row15 col10\" >0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row16\" class=\"row_heading level1 row16\" >gpt2-medium</th>\n",
              "      <td id=\"T_3f999_row16_col0\" class=\"data row16 col0\" >0.3853</td>\n",
              "      <td id=\"T_3f999_row16_col1\" class=\"data row16 col1\" >0.3658</td>\n",
              "      <td id=\"T_3f999_row16_col2\" class=\"data row16 col2\" >0.3516</td>\n",
              "      <td id=\"T_3f999_row16_col3\" class=\"data row16 col3\" >0.3593</td>\n",
              "      <td id=\"T_3f999_row16_col4\" class=\"data row16 col4\" >0.9928</td>\n",
              "      <td id=\"T_3f999_row16_col5\" class=\"data row16 col5\" >0.3659</td>\n",
              "      <td id=\"T_3f999_row16_col6\" class=\"data row16 col6\" >0.3887</td>\n",
              "      <td id=\"T_3f999_row16_col7\" class=\"data row16 col7\" >0.3486</td>\n",
              "      <td id=\"T_3f999_row16_col8\" class=\"data row16 col8\" >0.3969</td>\n",
              "      <td id=\"T_3f999_row16_col9\" class=\"data row16 col9\" >0.3732</td>\n",
              "      <td id=\"T_3f999_row16_col10\" class=\"data row16 col10\" >0.3318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row17\" class=\"row_heading level1 row17\" >mGPT</th>\n",
              "      <td id=\"T_3f999_row17_col0\" class=\"data row17 col0\" >0.6945</td>\n",
              "      <td id=\"T_3f999_row17_col1\" class=\"data row17 col1\" >0.7888</td>\n",
              "      <td id=\"T_3f999_row17_col2\" class=\"data row17 col2\" >0.3866</td>\n",
              "      <td id=\"T_3f999_row17_col3\" class=\"data row17 col3\" >0.8365</td>\n",
              "      <td id=\"T_3f999_row17_col4\" class=\"data row17 col4\" >0.9910</td>\n",
              "      <td id=\"T_3f999_row17_col5\" class=\"data row17 col5\" >0.7954</td>\n",
              "      <td id=\"T_3f999_row17_col6\" class=\"data row17 col6\" >0.8334</td>\n",
              "      <td id=\"T_3f999_row17_col7\" class=\"data row17 col7\" >0.8658</td>\n",
              "      <td id=\"T_3f999_row17_col8\" class=\"data row17 col8\" >0.8008</td>\n",
              "      <td id=\"T_3f999_row17_col9\" class=\"data row17 col9\" >0.7849</td>\n",
              "      <td id=\"T_3f999_row17_col10\" class=\"data row17 col10\" >0.4734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row18\" class=\"row_heading level1 row18\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_3f999_row18_col0\" class=\"data row18 col0\" >0.7199</td>\n",
              "      <td id=\"T_3f999_row18_col1\" class=\"data row18 col1\" >0.9733</td>\n",
              "      <td id=\"T_3f999_row18_col2\" class=\"data row18 col2\" >0.8626</td>\n",
              "      <td id=\"T_3f999_row18_col3\" class=\"data row18 col3\" >0.8944</td>\n",
              "      <td id=\"T_3f999_row18_col4\" class=\"data row18 col4\" >0.8255</td>\n",
              "      <td id=\"T_3f999_row18_col5\" class=\"data row18 col5\" >0.9435</td>\n",
              "      <td id=\"T_3f999_row18_col6\" class=\"data row18 col6\" >0.9331</td>\n",
              "      <td id=\"T_3f999_row18_col7\" class=\"data row18 col7\" >0.9319</td>\n",
              "      <td id=\"T_3f999_row18_col8\" class=\"data row18 col8\" >0.8395</td>\n",
              "      <td id=\"T_3f999_row18_col9\" class=\"data row18 col9\" >0.7668</td>\n",
              "      <td id=\"T_3f999_row18_col10\" class=\"data row18 col10\" >0.5706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row19\" class=\"row_heading level1 row19\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_3f999_row19_col0\" class=\"data row19 col0\" >0.5765</td>\n",
              "      <td id=\"T_3f999_row19_col1\" class=\"data row19 col1\" >0.5661</td>\n",
              "      <td id=\"T_3f999_row19_col2\" class=\"data row19 col2\" >0.4808</td>\n",
              "      <td id=\"T_3f999_row19_col3\" class=\"data row19 col3\" >0.5395</td>\n",
              "      <td id=\"T_3f999_row19_col4\" class=\"data row19 col4\" >0.9765</td>\n",
              "      <td id=\"T_3f999_row19_col5\" class=\"data row19 col5\" >0.4974</td>\n",
              "      <td id=\"T_3f999_row19_col6\" class=\"data row19 col6\" >0.7401</td>\n",
              "      <td id=\"T_3f999_row19_col7\" class=\"data row19 col7\" >0.4077</td>\n",
              "      <td id=\"T_3f999_row19_col8\" class=\"data row19 col8\" >0.6384</td>\n",
              "      <td id=\"T_3f999_row19_col9\" class=\"data row19 col9\" >0.5689</td>\n",
              "      <td id=\"T_3f999_row19_col10\" class=\"data row19 col10\" >0.4995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row20\" class=\"row_heading level1 row20\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_3f999_row20_col0\" class=\"data row20 col0\" >0.5659</td>\n",
              "      <td id=\"T_3f999_row20_col1\" class=\"data row20 col1\" >0.9348</td>\n",
              "      <td id=\"T_3f999_row20_col2\" class=\"data row20 col2\" >0.4987</td>\n",
              "      <td id=\"T_3f999_row20_col3\" class=\"data row20 col3\" >0.8927</td>\n",
              "      <td id=\"T_3f999_row20_col4\" class=\"data row20 col4\" >0.9892</td>\n",
              "      <td id=\"T_3f999_row20_col5\" class=\"data row20 col5\" >0.8592</td>\n",
              "      <td id=\"T_3f999_row20_col6\" class=\"data row20 col6\" >0.9499</td>\n",
              "      <td id=\"T_3f999_row20_col7\" class=\"data row20 col7\" >0.9267</td>\n",
              "      <td id=\"T_3f999_row20_col8\" class=\"data row20 col8\" >0.7676</td>\n",
              "      <td id=\"T_3f999_row20_col9\" class=\"data row20 col9\" >0.4567</td>\n",
              "      <td id=\"T_3f999_row20_col10\" class=\"data row20 col10\" >0.3587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level0_row21\" class=\"row_heading level0 row21\" rowspan=\"7\">llama-65b</th>\n",
              "      <th id=\"T_3f999_level1_row21\" class=\"row_heading level1 row21\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_3f999_row21_col0\" class=\"data row21 col0\" >0.5896</td>\n",
              "      <td id=\"T_3f999_row21_col1\" class=\"data row21 col1\" >0.6070</td>\n",
              "      <td id=\"T_3f999_row21_col2\" class=\"data row21 col2\" >0.6955</td>\n",
              "      <td id=\"T_3f999_row21_col3\" class=\"data row21 col3\" >0.6895</td>\n",
              "      <td id=\"T_3f999_row21_col4\" class=\"data row21 col4\" >0.9017</td>\n",
              "      <td id=\"T_3f999_row21_col5\" class=\"data row21 col5\" >0.7998</td>\n",
              "      <td id=\"T_3f999_row21_col6\" class=\"data row21 col6\" >0.5771</td>\n",
              "      <td id=\"T_3f999_row21_col7\" class=\"data row21 col7\" >0.6752</td>\n",
              "      <td id=\"T_3f999_row21_col8\" class=\"data row21 col8\" >0.7032</td>\n",
              "      <td id=\"T_3f999_row21_col9\" class=\"data row21 col9\" >0.7020</td>\n",
              "      <td id=\"T_3f999_row21_col10\" class=\"data row21 col10\" >0.5194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row22\" class=\"row_heading level1 row22\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_3f999_row22_col0\" class=\"data row22 col0\" >0.6770</td>\n",
              "      <td id=\"T_3f999_row22_col1\" class=\"data row22 col1\" >0.3311</td>\n",
              "      <td id=\"T_3f999_row22_col2\" class=\"data row22 col2\" >0.3299</td>\n",
              "      <td id=\"T_3f999_row22_col3\" class=\"data row22 col3\" >0.3386</td>\n",
              "      <td id=\"T_3f999_row22_col4\" class=\"data row22 col4\" >0.9220</td>\n",
              "      <td id=\"T_3f999_row22_col5\" class=\"data row22 col5\" >0.3613</td>\n",
              "      <td id=\"T_3f999_row22_col6\" class=\"data row22 col6\" >0.3322</td>\n",
              "      <td id=\"T_3f999_row22_col7\" class=\"data row22 col7\" >0.3538</td>\n",
              "      <td id=\"T_3f999_row22_col8\" class=\"data row22 col8\" >0.4911</td>\n",
              "      <td id=\"T_3f999_row22_col9\" class=\"data row22 col9\" >0.5700</td>\n",
              "      <td id=\"T_3f999_row22_col10\" class=\"data row22 col10\" >0.3281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row23\" class=\"row_heading level1 row23\" >gpt2-medium</th>\n",
              "      <td id=\"T_3f999_row23_col0\" class=\"data row23 col0\" >0.3735</td>\n",
              "      <td id=\"T_3f999_row23_col1\" class=\"data row23 col1\" >0.4296</td>\n",
              "      <td id=\"T_3f999_row23_col2\" class=\"data row23 col2\" >0.5929</td>\n",
              "      <td id=\"T_3f999_row23_col3\" class=\"data row23 col3\" >0.5370</td>\n",
              "      <td id=\"T_3f999_row23_col4\" class=\"data row23 col4\" >0.8854</td>\n",
              "      <td id=\"T_3f999_row23_col5\" class=\"data row23 col5\" >0.5623</td>\n",
              "      <td id=\"T_3f999_row23_col6\" class=\"data row23 col6\" >0.5372</td>\n",
              "      <td id=\"T_3f999_row23_col7\" class=\"data row23 col7\" >0.3950</td>\n",
              "      <td id=\"T_3f999_row23_col8\" class=\"data row23 col8\" >0.4543</td>\n",
              "      <td id=\"T_3f999_row23_col9\" class=\"data row23 col9\" >0.4893</td>\n",
              "      <td id=\"T_3f999_row23_col10\" class=\"data row23 col10\" >0.4342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row24\" class=\"row_heading level1 row24\" >mGPT</th>\n",
              "      <td id=\"T_3f999_row24_col0\" class=\"data row24 col0\" >0.3848</td>\n",
              "      <td id=\"T_3f999_row24_col1\" class=\"data row24 col1\" >0.3508</td>\n",
              "      <td id=\"T_3f999_row24_col2\" class=\"data row24 col2\" >0.6898</td>\n",
              "      <td id=\"T_3f999_row24_col3\" class=\"data row24 col3\" >0.7115</td>\n",
              "      <td id=\"T_3f999_row24_col4\" class=\"data row24 col4\" >0.9037</td>\n",
              "      <td id=\"T_3f999_row24_col5\" class=\"data row24 col5\" >0.5248</td>\n",
              "      <td id=\"T_3f999_row24_col6\" class=\"data row24 col6\" >0.4102</td>\n",
              "      <td id=\"T_3f999_row24_col7\" class=\"data row24 col7\" >0.4835</td>\n",
              "      <td id=\"T_3f999_row24_col8\" class=\"data row24 col8\" >0.4669</td>\n",
              "      <td id=\"T_3f999_row24_col9\" class=\"data row24 col9\" >0.4779</td>\n",
              "      <td id=\"T_3f999_row24_col10\" class=\"data row24 col10\" >0.5233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row25\" class=\"row_heading level1 row25\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_3f999_row25_col0\" class=\"data row25 col0\" >0.5115</td>\n",
              "      <td id=\"T_3f999_row25_col1\" class=\"data row25 col1\" >0.4060</td>\n",
              "      <td id=\"T_3f999_row25_col2\" class=\"data row25 col2\" >0.8307</td>\n",
              "      <td id=\"T_3f999_row25_col3\" class=\"data row25 col3\" >0.7162</td>\n",
              "      <td id=\"T_3f999_row25_col4\" class=\"data row25 col4\" >0.9003</td>\n",
              "      <td id=\"T_3f999_row25_col5\" class=\"data row25 col5\" >0.6074</td>\n",
              "      <td id=\"T_3f999_row25_col6\" class=\"data row25 col6\" >0.4591</td>\n",
              "      <td id=\"T_3f999_row25_col7\" class=\"data row25 col7\" >0.5413</td>\n",
              "      <td id=\"T_3f999_row25_col8\" class=\"data row25 col8\" >0.7103</td>\n",
              "      <td id=\"T_3f999_row25_col9\" class=\"data row25 col9\" >0.6693</td>\n",
              "      <td id=\"T_3f999_row25_col10\" class=\"data row25 col10\" >0.4384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row26\" class=\"row_heading level1 row26\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_3f999_row26_col0\" class=\"data row26 col0\" >0.3303</td>\n",
              "      <td id=\"T_3f999_row26_col1\" class=\"data row26 col1\" >0.3528</td>\n",
              "      <td id=\"T_3f999_row26_col2\" class=\"data row26 col2\" >0.3446</td>\n",
              "      <td id=\"T_3f999_row26_col3\" class=\"data row26 col3\" >0.3341</td>\n",
              "      <td id=\"T_3f999_row26_col4\" class=\"data row26 col4\" >0.8890</td>\n",
              "      <td id=\"T_3f999_row26_col5\" class=\"data row26 col5\" >0.3460</td>\n",
              "      <td id=\"T_3f999_row26_col6\" class=\"data row26 col6\" >0.3612</td>\n",
              "      <td id=\"T_3f999_row26_col7\" class=\"data row26 col7\" >0.3371</td>\n",
              "      <td id=\"T_3f999_row26_col8\" class=\"data row26 col8\" >0.3311</td>\n",
              "      <td id=\"T_3f999_row26_col9\" class=\"data row26 col9\" >0.3449</td>\n",
              "      <td id=\"T_3f999_row26_col10\" class=\"data row26 col10\" >0.3332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row27\" class=\"row_heading level1 row27\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_3f999_row27_col0\" class=\"data row27 col0\" >0.4779</td>\n",
              "      <td id=\"T_3f999_row27_col1\" class=\"data row27 col1\" >0.4892</td>\n",
              "      <td id=\"T_3f999_row27_col2\" class=\"data row27 col2\" >0.8507</td>\n",
              "      <td id=\"T_3f999_row27_col3\" class=\"data row27 col3\" >0.7983</td>\n",
              "      <td id=\"T_3f999_row27_col4\" class=\"data row27 col4\" >0.9165</td>\n",
              "      <td id=\"T_3f999_row27_col5\" class=\"data row27 col5\" >0.7241</td>\n",
              "      <td id=\"T_3f999_row27_col6\" class=\"data row27 col6\" >0.4368</td>\n",
              "      <td id=\"T_3f999_row27_col7\" class=\"data row27 col7\" >0.6082</td>\n",
              "      <td id=\"T_3f999_row27_col8\" class=\"data row27 col8\" >0.6899</td>\n",
              "      <td id=\"T_3f999_row27_col9\" class=\"data row27 col9\" >0.6290</td>\n",
              "      <td id=\"T_3f999_row27_col10\" class=\"data row27 col10\" >0.4892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level0_row28\" class=\"row_heading level0 row28\" rowspan=\"7\">opt-66b</th>\n",
              "      <th id=\"T_3f999_level1_row28\" class=\"row_heading level1 row28\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_3f999_row28_col0\" class=\"data row28 col0\" >0.4557</td>\n",
              "      <td id=\"T_3f999_row28_col1\" class=\"data row28 col1\" >0.4586</td>\n",
              "      <td id=\"T_3f999_row28_col2\" class=\"data row28 col2\" >0.6107</td>\n",
              "      <td id=\"T_3f999_row28_col3\" class=\"data row28 col3\" >0.5890</td>\n",
              "      <td id=\"T_3f999_row28_col4\" class=\"data row28 col4\" >0.8592</td>\n",
              "      <td id=\"T_3f999_row28_col5\" class=\"data row28 col5\" >0.5394</td>\n",
              "      <td id=\"T_3f999_row28_col6\" class=\"data row28 col6\" >0.4268</td>\n",
              "      <td id=\"T_3f999_row28_col7\" class=\"data row28 col7\" >0.5635</td>\n",
              "      <td id=\"T_3f999_row28_col8\" class=\"data row28 col8\" >0.5876</td>\n",
              "      <td id=\"T_3f999_row28_col9\" class=\"data row28 col9\" >0.5594</td>\n",
              "      <td id=\"T_3f999_row28_col10\" class=\"data row28 col10\" >0.5041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row29\" class=\"row_heading level1 row29\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_3f999_row29_col0\" class=\"data row29 col0\" >0.5167</td>\n",
              "      <td id=\"T_3f999_row29_col1\" class=\"data row29 col1\" >0.5157</td>\n",
              "      <td id=\"T_3f999_row29_col2\" class=\"data row29 col2\" >0.6473</td>\n",
              "      <td id=\"T_3f999_row29_col3\" class=\"data row29 col3\" >0.5853</td>\n",
              "      <td id=\"T_3f999_row29_col4\" class=\"data row29 col4\" >0.8899</td>\n",
              "      <td id=\"T_3f999_row29_col5\" class=\"data row29 col5\" >0.6591</td>\n",
              "      <td id=\"T_3f999_row29_col6\" class=\"data row29 col6\" >0.5600</td>\n",
              "      <td id=\"T_3f999_row29_col7\" class=\"data row29 col7\" >0.6993</td>\n",
              "      <td id=\"T_3f999_row29_col8\" class=\"data row29 col8\" >0.4347</td>\n",
              "      <td id=\"T_3f999_row29_col9\" class=\"data row29 col9\" >0.5061</td>\n",
              "      <td id=\"T_3f999_row29_col10\" class=\"data row29 col10\" >0.3273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row30\" class=\"row_heading level1 row30\" >gpt2-medium</th>\n",
              "      <td id=\"T_3f999_row30_col0\" class=\"data row30 col0\" >0.5226</td>\n",
              "      <td id=\"T_3f999_row30_col1\" class=\"data row30 col1\" >0.4138</td>\n",
              "      <td id=\"T_3f999_row30_col2\" class=\"data row30 col2\" >0.3720</td>\n",
              "      <td id=\"T_3f999_row30_col3\" class=\"data row30 col3\" >0.3827</td>\n",
              "      <td id=\"T_3f999_row30_col4\" class=\"data row30 col4\" >0.8427</td>\n",
              "      <td id=\"T_3f999_row30_col5\" class=\"data row30 col5\" >0.4202</td>\n",
              "      <td id=\"T_3f999_row30_col6\" class=\"data row30 col6\" >0.3647</td>\n",
              "      <td id=\"T_3f999_row30_col7\" class=\"data row30 col7\" >0.4089</td>\n",
              "      <td id=\"T_3f999_row30_col8\" class=\"data row30 col8\" >0.5713</td>\n",
              "      <td id=\"T_3f999_row30_col9\" class=\"data row30 col9\" >0.6213</td>\n",
              "      <td id=\"T_3f999_row30_col10\" class=\"data row30 col10\" >0.4473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row31\" class=\"row_heading level1 row31\" >mGPT</th>\n",
              "      <td id=\"T_3f999_row31_col0\" class=\"data row31 col0\" >0.2959</td>\n",
              "      <td id=\"T_3f999_row31_col1\" class=\"data row31 col1\" >0.5049</td>\n",
              "      <td id=\"T_3f999_row31_col2\" class=\"data row31 col2\" >0.3954</td>\n",
              "      <td id=\"T_3f999_row31_col3\" class=\"data row31 col3\" >0.6351</td>\n",
              "      <td id=\"T_3f999_row31_col4\" class=\"data row31 col4\" >0.8931</td>\n",
              "      <td id=\"T_3f999_row31_col5\" class=\"data row31 col5\" >0.5064</td>\n",
              "      <td id=\"T_3f999_row31_col6\" class=\"data row31 col6\" >0.4942</td>\n",
              "      <td id=\"T_3f999_row31_col7\" class=\"data row31 col7\" >0.6017</td>\n",
              "      <td id=\"T_3f999_row31_col8\" class=\"data row31 col8\" >0.4416</td>\n",
              "      <td id=\"T_3f999_row31_col9\" class=\"data row31 col9\" >0.4645</td>\n",
              "      <td id=\"T_3f999_row31_col10\" class=\"data row31 col10\" >0.4161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row32\" class=\"row_heading level1 row32\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_3f999_row32_col0\" class=\"data row32 col0\" >0.3903</td>\n",
              "      <td id=\"T_3f999_row32_col1\" class=\"data row32 col1\" >0.5181</td>\n",
              "      <td id=\"T_3f999_row32_col2\" class=\"data row32 col2\" >0.6278</td>\n",
              "      <td id=\"T_3f999_row32_col3\" class=\"data row32 col3\" >0.6568</td>\n",
              "      <td id=\"T_3f999_row32_col4\" class=\"data row32 col4\" >0.8538</td>\n",
              "      <td id=\"T_3f999_row32_col5\" class=\"data row32 col5\" >0.6053</td>\n",
              "      <td id=\"T_3f999_row32_col6\" class=\"data row32 col6\" >0.4927</td>\n",
              "      <td id=\"T_3f999_row32_col7\" class=\"data row32 col7\" >0.6431</td>\n",
              "      <td id=\"T_3f999_row32_col8\" class=\"data row32 col8\" >0.5799</td>\n",
              "      <td id=\"T_3f999_row32_col9\" class=\"data row32 col9\" >0.6016</td>\n",
              "      <td id=\"T_3f999_row32_col10\" class=\"data row32 col10\" >0.5617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row33\" class=\"row_heading level1 row33\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_3f999_row33_col0\" class=\"data row33 col0\" >0.3315</td>\n",
              "      <td id=\"T_3f999_row33_col1\" class=\"data row33 col1\" >0.5268</td>\n",
              "      <td id=\"T_3f999_row33_col2\" class=\"data row33 col2\" >0.4635</td>\n",
              "      <td id=\"T_3f999_row33_col3\" class=\"data row33 col3\" >0.4038</td>\n",
              "      <td id=\"T_3f999_row33_col4\" class=\"data row33 col4\" >0.8711</td>\n",
              "      <td id=\"T_3f999_row33_col5\" class=\"data row33 col5\" >0.3792</td>\n",
              "      <td id=\"T_3f999_row33_col6\" class=\"data row33 col6\" >0.5761</td>\n",
              "      <td id=\"T_3f999_row33_col7\" class=\"data row33 col7\" >0.3714</td>\n",
              "      <td id=\"T_3f999_row33_col8\" class=\"data row33 col8\" >0.3293</td>\n",
              "      <td id=\"T_3f999_row33_col9\" class=\"data row33 col9\" >0.3733</td>\n",
              "      <td id=\"T_3f999_row33_col10\" class=\"data row33 col10\" >0.3591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row34\" class=\"row_heading level1 row34\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_3f999_row34_col0\" class=\"data row34 col0\" >0.4910</td>\n",
              "      <td id=\"T_3f999_row34_col1\" class=\"data row34 col1\" >0.4613</td>\n",
              "      <td id=\"T_3f999_row34_col2\" class=\"data row34 col2\" >0.5495</td>\n",
              "      <td id=\"T_3f999_row34_col3\" class=\"data row34 col3\" >0.6735</td>\n",
              "      <td id=\"T_3f999_row34_col4\" class=\"data row34 col4\" >0.8827</td>\n",
              "      <td id=\"T_3f999_row34_col5\" class=\"data row34 col5\" >0.5613</td>\n",
              "      <td id=\"T_3f999_row34_col6\" class=\"data row34 col6\" >0.4187</td>\n",
              "      <td id=\"T_3f999_row34_col7\" class=\"data row34 col7\" >0.5897</td>\n",
              "      <td id=\"T_3f999_row34_col8\" class=\"data row34 col8\" >0.6706</td>\n",
              "      <td id=\"T_3f999_row34_col9\" class=\"data row34 col9\" >0.6961</td>\n",
              "      <td id=\"T_3f999_row34_col10\" class=\"data row34 col10\" >0.5358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level0_row35\" class=\"row_heading level0 row35\" rowspan=\"7\">opt-iml-max-1.3b</th>\n",
              "      <th id=\"T_3f999_level1_row35\" class=\"row_heading level1 row35\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_3f999_row35_col0\" class=\"data row35 col0\" >0.4592</td>\n",
              "      <td id=\"T_3f999_row35_col1\" class=\"data row35 col1\" >0.5171</td>\n",
              "      <td id=\"T_3f999_row35_col2\" class=\"data row35 col2\" >0.5792</td>\n",
              "      <td id=\"T_3f999_row35_col3\" class=\"data row35 col3\" >0.5916</td>\n",
              "      <td id=\"T_3f999_row35_col4\" class=\"data row35 col4\" >0.9061</td>\n",
              "      <td id=\"T_3f999_row35_col5\" class=\"data row35 col5\" >0.5143</td>\n",
              "      <td id=\"T_3f999_row35_col6\" class=\"data row35 col6\" >0.5136</td>\n",
              "      <td id=\"T_3f999_row35_col7\" class=\"data row35 col7\" >0.4721</td>\n",
              "      <td id=\"T_3f999_row35_col8\" class=\"data row35 col8\" >0.5494</td>\n",
              "      <td id=\"T_3f999_row35_col9\" class=\"data row35 col9\" >0.5333</td>\n",
              "      <td id=\"T_3f999_row35_col10\" class=\"data row35 col10\" >0.4390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row36\" class=\"row_heading level1 row36\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_3f999_row36_col0\" class=\"data row36 col0\" >0.3264</td>\n",
              "      <td id=\"T_3f999_row36_col1\" class=\"data row36 col1\" >0.5542</td>\n",
              "      <td id=\"T_3f999_row36_col2\" class=\"data row36 col2\" >0.5036</td>\n",
              "      <td id=\"T_3f999_row36_col3\" class=\"data row36 col3\" >0.6618</td>\n",
              "      <td id=\"T_3f999_row36_col4\" class=\"data row36 col4\" >0.9819</td>\n",
              "      <td id=\"T_3f999_row36_col5\" class=\"data row36 col5\" >0.8709</td>\n",
              "      <td id=\"T_3f999_row36_col6\" class=\"data row36 col6\" >0.7625</td>\n",
              "      <td id=\"T_3f999_row36_col7\" class=\"data row36 col7\" >0.8334</td>\n",
              "      <td id=\"T_3f999_row36_col8\" class=\"data row36 col8\" >0.3309</td>\n",
              "      <td id=\"T_3f999_row36_col9\" class=\"data row36 col9\" >0.3390</td>\n",
              "      <td id=\"T_3f999_row36_col10\" class=\"data row36 col10\" >0.3299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row37\" class=\"row_heading level1 row37\" >gpt2-medium</th>\n",
              "      <td id=\"T_3f999_row37_col0\" class=\"data row37 col0\" >0.3869</td>\n",
              "      <td id=\"T_3f999_row37_col1\" class=\"data row37 col1\" >0.7116</td>\n",
              "      <td id=\"T_3f999_row37_col2\" class=\"data row37 col2\" >0.5300</td>\n",
              "      <td id=\"T_3f999_row37_col3\" class=\"data row37 col3\" >0.5259</td>\n",
              "      <td id=\"T_3f999_row37_col4\" class=\"data row37 col4\" >0.9368</td>\n",
              "      <td id=\"T_3f999_row37_col5\" class=\"data row37 col5\" >0.5789</td>\n",
              "      <td id=\"T_3f999_row37_col6\" class=\"data row37 col6\" >0.5600</td>\n",
              "      <td id=\"T_3f999_row37_col7\" class=\"data row37 col7\" >0.5453</td>\n",
              "      <td id=\"T_3f999_row37_col8\" class=\"data row37 col8\" >0.4733</td>\n",
              "      <td id=\"T_3f999_row37_col9\" class=\"data row37 col9\" >0.5295</td>\n",
              "      <td id=\"T_3f999_row37_col10\" class=\"data row37 col10\" >0.4720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row38\" class=\"row_heading level1 row38\" >mGPT</th>\n",
              "      <td id=\"T_3f999_row38_col0\" class=\"data row38 col0\" >0.3380</td>\n",
              "      <td id=\"T_3f999_row38_col1\" class=\"data row38 col1\" >0.5134</td>\n",
              "      <td id=\"T_3f999_row38_col2\" class=\"data row38 col2\" >0.6410</td>\n",
              "      <td id=\"T_3f999_row38_col3\" class=\"data row38 col3\" >0.7450</td>\n",
              "      <td id=\"T_3f999_row38_col4\" class=\"data row38 col4\" >0.9585</td>\n",
              "      <td id=\"T_3f999_row38_col5\" class=\"data row38 col5\" >0.5284</td>\n",
              "      <td id=\"T_3f999_row38_col6\" class=\"data row38 col6\" >0.6955</td>\n",
              "      <td id=\"T_3f999_row38_col7\" class=\"data row38 col7\" >0.5212</td>\n",
              "      <td id=\"T_3f999_row38_col8\" class=\"data row38 col8\" >0.4829</td>\n",
              "      <td id=\"T_3f999_row38_col9\" class=\"data row38 col9\" >0.4925</td>\n",
              "      <td id=\"T_3f999_row38_col10\" class=\"data row38 col10\" >0.5788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row39\" class=\"row_heading level1 row39\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_3f999_row39_col0\" class=\"data row39 col0\" >0.7932</td>\n",
              "      <td id=\"T_3f999_row39_col1\" class=\"data row39 col1\" >0.9731</td>\n",
              "      <td id=\"T_3f999_row39_col2\" class=\"data row39 col2\" >1.0000</td>\n",
              "      <td id=\"T_3f999_row39_col3\" class=\"data row39 col3\" >0.9609</td>\n",
              "      <td id=\"T_3f999_row39_col4\" class=\"data row39 col4\" >0.9257</td>\n",
              "      <td id=\"T_3f999_row39_col5\" class=\"data row39 col5\" >0.9023</td>\n",
              "      <td id=\"T_3f999_row39_col6\" class=\"data row39 col6\" >0.9615</td>\n",
              "      <td id=\"T_3f999_row39_col7\" class=\"data row39 col7\" >0.9025</td>\n",
              "      <td id=\"T_3f999_row39_col8\" class=\"data row39 col8\" >0.9471</td>\n",
              "      <td id=\"T_3f999_row39_col9\" class=\"data row39 col9\" >0.9570</td>\n",
              "      <td id=\"T_3f999_row39_col10\" class=\"data row39 col10\" >0.7016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row40\" class=\"row_heading level1 row40\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_3f999_row40_col0\" class=\"data row40 col0\" >0.3427</td>\n",
              "      <td id=\"T_3f999_row40_col1\" class=\"data row40 col1\" >0.8511</td>\n",
              "      <td id=\"T_3f999_row40_col2\" class=\"data row40 col2\" >0.7842</td>\n",
              "      <td id=\"T_3f999_row40_col3\" class=\"data row40 col3\" >0.6490</td>\n",
              "      <td id=\"T_3f999_row40_col4\" class=\"data row40 col4\" >0.9729</td>\n",
              "      <td id=\"T_3f999_row40_col5\" class=\"data row40 col5\" >0.6895</td>\n",
              "      <td id=\"T_3f999_row40_col6\" class=\"data row40 col6\" >0.9211</td>\n",
              "      <td id=\"T_3f999_row40_col7\" class=\"data row40 col7\" >0.6590</td>\n",
              "      <td id=\"T_3f999_row40_col8\" class=\"data row40 col8\" >0.3614</td>\n",
              "      <td id=\"T_3f999_row40_col9\" class=\"data row40 col9\" >0.4318</td>\n",
              "      <td id=\"T_3f999_row40_col10\" class=\"data row40 col10\" >0.4777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row41\" class=\"row_heading level1 row41\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_3f999_row41_col0\" class=\"data row41 col0\" >0.5810</td>\n",
              "      <td id=\"T_3f999_row41_col1\" class=\"data row41 col1\" >0.8567</td>\n",
              "      <td id=\"T_3f999_row41_col2\" class=\"data row41 col2\" >0.8657</td>\n",
              "      <td id=\"T_3f999_row41_col3\" class=\"data row41 col3\" >0.8416</td>\n",
              "      <td id=\"T_3f999_row41_col4\" class=\"data row41 col4\" >0.9439</td>\n",
              "      <td id=\"T_3f999_row41_col5\" class=\"data row41 col5\" >0.7268</td>\n",
              "      <td id=\"T_3f999_row41_col6\" class=\"data row41 col6\" >0.7684</td>\n",
              "      <td id=\"T_3f999_row41_col7\" class=\"data row41 col7\" >0.6991</td>\n",
              "      <td id=\"T_3f999_row41_col8\" class=\"data row41 col8\" >0.7483</td>\n",
              "      <td id=\"T_3f999_row41_col9\" class=\"data row41 col9\" >0.7587</td>\n",
              "      <td id=\"T_3f999_row41_col10\" class=\"data row41 col10\" >0.6237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level0_row42\" class=\"row_heading level0 row42\" rowspan=\"7\">text-davinci-003</th>\n",
              "      <th id=\"T_3f999_level1_row42\" class=\"row_heading level1 row42\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_3f999_row42_col0\" class=\"data row42 col0\" >0.7647</td>\n",
              "      <td id=\"T_3f999_row42_col1\" class=\"data row42 col1\" >0.9014</td>\n",
              "      <td id=\"T_3f999_row42_col2\" class=\"data row42 col2\" >0.8537</td>\n",
              "      <td id=\"T_3f999_row42_col3\" class=\"data row42 col3\" >0.8554</td>\n",
              "      <td id=\"T_3f999_row42_col4\" class=\"data row42 col4\" >0.9693</td>\n",
              "      <td id=\"T_3f999_row42_col5\" class=\"data row42 col5\" >0.7750</td>\n",
              "      <td id=\"T_3f999_row42_col6\" class=\"data row42 col6\" >0.9032</td>\n",
              "      <td id=\"T_3f999_row42_col7\" class=\"data row42 col7\" >0.8408</td>\n",
              "      <td id=\"T_3f999_row42_col8\" class=\"data row42 col8\" >0.7161</td>\n",
              "      <td id=\"T_3f999_row42_col9\" class=\"data row42 col9\" >0.6657</td>\n",
              "      <td id=\"T_3f999_row42_col10\" class=\"data row42 col10\" >0.8409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row43\" class=\"row_heading level1 row43\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_3f999_row43_col0\" class=\"data row43 col0\" >0.4628</td>\n",
              "      <td id=\"T_3f999_row43_col1\" class=\"data row43 col1\" >0.5751</td>\n",
              "      <td id=\"T_3f999_row43_col2\" class=\"data row43 col2\" >0.4275</td>\n",
              "      <td id=\"T_3f999_row43_col3\" class=\"data row43 col3\" >0.4024</td>\n",
              "      <td id=\"T_3f999_row43_col4\" class=\"data row43 col4\" >0.9783</td>\n",
              "      <td id=\"T_3f999_row43_col5\" class=\"data row43 col5\" >0.7471</td>\n",
              "      <td id=\"T_3f999_row43_col6\" class=\"data row43 col6\" >0.5568</td>\n",
              "      <td id=\"T_3f999_row43_col7\" class=\"data row43 col7\" >0.6000</td>\n",
              "      <td id=\"T_3f999_row43_col8\" class=\"data row43 col8\" >0.5075</td>\n",
              "      <td id=\"T_3f999_row43_col9\" class=\"data row43 col9\" >0.4826</td>\n",
              "      <td id=\"T_3f999_row43_col10\" class=\"data row43 col10\" >0.4880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row44\" class=\"row_heading level1 row44\" >gpt2-medium</th>\n",
              "      <td id=\"T_3f999_row44_col0\" class=\"data row44 col0\" >0.3322</td>\n",
              "      <td id=\"T_3f999_row44_col1\" class=\"data row44 col1\" >0.3333</td>\n",
              "      <td id=\"T_3f999_row44_col2\" class=\"data row44 col2\" >0.3333</td>\n",
              "      <td id=\"T_3f999_row44_col3\" class=\"data row44 col3\" >0.3303</td>\n",
              "      <td id=\"T_3f999_row44_col4\" class=\"data row44 col4\" >0.9856</td>\n",
              "      <td id=\"T_3f999_row44_col5\" class=\"data row44 col5\" >0.3264</td>\n",
              "      <td id=\"T_3f999_row44_col6\" class=\"data row44 col6\" >0.3367</td>\n",
              "      <td id=\"T_3f999_row44_col7\" class=\"data row44 col7\" >0.3268</td>\n",
              "      <td id=\"T_3f999_row44_col8\" class=\"data row44 col8\" >0.3267</td>\n",
              "      <td id=\"T_3f999_row44_col9\" class=\"data row44 col9\" >0.3361</td>\n",
              "      <td id=\"T_3f999_row44_col10\" class=\"data row44 col10\" >0.3326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row45\" class=\"row_heading level1 row45\" >mGPT</th>\n",
              "      <td id=\"T_3f999_row45_col0\" class=\"data row45 col0\" >0.5393</td>\n",
              "      <td id=\"T_3f999_row45_col1\" class=\"data row45 col1\" >0.9199</td>\n",
              "      <td id=\"T_3f999_row45_col2\" class=\"data row45 col2\" >0.3623</td>\n",
              "      <td id=\"T_3f999_row45_col3\" class=\"data row45 col3\" >0.8936</td>\n",
              "      <td id=\"T_3f999_row45_col4\" class=\"data row45 col4\" >0.9783</td>\n",
              "      <td id=\"T_3f999_row45_col5\" class=\"data row45 col5\" >0.7678</td>\n",
              "      <td id=\"T_3f999_row45_col6\" class=\"data row45 col6\" >0.9027</td>\n",
              "      <td id=\"T_3f999_row45_col7\" class=\"data row45 col7\" >0.8389</td>\n",
              "      <td id=\"T_3f999_row45_col8\" class=\"data row45 col8\" >0.7713</td>\n",
              "      <td id=\"T_3f999_row45_col9\" class=\"data row45 col9\" >0.6932</td>\n",
              "      <td id=\"T_3f999_row45_col10\" class=\"data row45 col10\" >0.5246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row46\" class=\"row_heading level1 row46\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_3f999_row46_col0\" class=\"data row46 col0\" >0.5432</td>\n",
              "      <td id=\"T_3f999_row46_col1\" class=\"data row46 col1\" >0.6398</td>\n",
              "      <td id=\"T_3f999_row46_col2\" class=\"data row46 col2\" >0.8239</td>\n",
              "      <td id=\"T_3f999_row46_col3\" class=\"data row46 col3\" >0.8563</td>\n",
              "      <td id=\"T_3f999_row46_col4\" class=\"data row46 col4\" >0.9513</td>\n",
              "      <td id=\"T_3f999_row46_col5\" class=\"data row46 col5\" >0.5774</td>\n",
              "      <td id=\"T_3f999_row46_col6\" class=\"data row46 col6\" >0.7596</td>\n",
              "      <td id=\"T_3f999_row46_col7\" class=\"data row46 col7\" >0.6335</td>\n",
              "      <td id=\"T_3f999_row46_col8\" class=\"data row46 col8\" >0.6969</td>\n",
              "      <td id=\"T_3f999_row46_col9\" class=\"data row46 col9\" >0.6015</td>\n",
              "      <td id=\"T_3f999_row46_col10\" class=\"data row46 col10\" >0.7779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row47\" class=\"row_heading level1 row47\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_3f999_row47_col0\" class=\"data row47 col0\" >0.4087</td>\n",
              "      <td id=\"T_3f999_row47_col1\" class=\"data row47 col1\" >0.5610</td>\n",
              "      <td id=\"T_3f999_row47_col2\" class=\"data row47 col2\" >0.5307</td>\n",
              "      <td id=\"T_3f999_row47_col3\" class=\"data row47 col3\" >0.5112</td>\n",
              "      <td id=\"T_3f999_row47_col4\" class=\"data row47 col4\" >0.9293</td>\n",
              "      <td id=\"T_3f999_row47_col5\" class=\"data row47 col5\" >0.4369</td>\n",
              "      <td id=\"T_3f999_row47_col6\" class=\"data row47 col6\" >0.7561</td>\n",
              "      <td id=\"T_3f999_row47_col7\" class=\"data row47 col7\" >0.3902</td>\n",
              "      <td id=\"T_3f999_row47_col8\" class=\"data row47 col8\" >0.4746</td>\n",
              "      <td id=\"T_3f999_row47_col9\" class=\"data row47 col9\" >0.6221</td>\n",
              "      <td id=\"T_3f999_row47_col10\" class=\"data row47 col10\" >0.4820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row48\" class=\"row_heading level1 row48\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_3f999_row48_col0\" class=\"data row48 col0\" >0.5035</td>\n",
              "      <td id=\"T_3f999_row48_col1\" class=\"data row48 col1\" >0.9213</td>\n",
              "      <td id=\"T_3f999_row48_col2\" class=\"data row48 col2\" >0.6965</td>\n",
              "      <td id=\"T_3f999_row48_col3\" class=\"data row48 col3\" >0.8913</td>\n",
              "      <td id=\"T_3f999_row48_col4\" class=\"data row48 col4\" >0.9856</td>\n",
              "      <td id=\"T_3f999_row48_col5\" class=\"data row48 col5\" >0.7715</td>\n",
              "      <td id=\"T_3f999_row48_col6\" class=\"data row48 col6\" >0.8973</td>\n",
              "      <td id=\"T_3f999_row48_col7\" class=\"data row48 col7\" >0.8232</td>\n",
              "      <td id=\"T_3f999_row48_col8\" class=\"data row48 col8\" >0.5769</td>\n",
              "      <td id=\"T_3f999_row48_col9\" class=\"data row48 col9\" >0.4739</td>\n",
              "      <td id=\"T_3f999_row48_col10\" class=\"data row48 col10\" >0.4377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level0_row49\" class=\"row_heading level0 row49\" rowspan=\"7\">vicuna-13b</th>\n",
              "      <th id=\"T_3f999_level1_row49\" class=\"row_heading level1 row49\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_3f999_row49_col0\" class=\"data row49 col0\" >0.7913</td>\n",
              "      <td id=\"T_3f999_row49_col1\" class=\"data row49 col1\" >0.8856</td>\n",
              "      <td id=\"T_3f999_row49_col2\" class=\"data row49 col2\" >0.8012</td>\n",
              "      <td id=\"T_3f999_row49_col3\" class=\"data row49 col3\" >0.8446</td>\n",
              "      <td id=\"T_3f999_row49_col4\" class=\"data row49 col4\" >0.9747</td>\n",
              "      <td id=\"T_3f999_row49_col5\" class=\"data row49 col5\" >0.7859</td>\n",
              "      <td id=\"T_3f999_row49_col6\" class=\"data row49 col6\" >0.9365</td>\n",
              "      <td id=\"T_3f999_row49_col7\" class=\"data row49 col7\" >0.8520</td>\n",
              "      <td id=\"T_3f999_row49_col8\" class=\"data row49 col8\" >0.8309</td>\n",
              "      <td id=\"T_3f999_row49_col9\" class=\"data row49 col9\" >0.8137</td>\n",
              "      <td id=\"T_3f999_row49_col10\" class=\"data row49 col10\" >0.6667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row50\" class=\"row_heading level1 row50\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_3f999_row50_col0\" class=\"data row50 col0\" >0.3912</td>\n",
              "      <td id=\"T_3f999_row50_col1\" class=\"data row50 col1\" >0.3848</td>\n",
              "      <td id=\"T_3f999_row50_col2\" class=\"data row50 col2\" >0.6025</td>\n",
              "      <td id=\"T_3f999_row50_col3\" class=\"data row50 col3\" >0.5077</td>\n",
              "      <td id=\"T_3f999_row50_col4\" class=\"data row50 col4\" >0.9838</td>\n",
              "      <td id=\"T_3f999_row50_col5\" class=\"data row50 col5\" >0.6577</td>\n",
              "      <td id=\"T_3f999_row50_col6\" class=\"data row50 col6\" >0.4839</td>\n",
              "      <td id=\"T_3f999_row50_col7\" class=\"data row50 col7\" >0.5162</td>\n",
              "      <td id=\"T_3f999_row50_col8\" class=\"data row50 col8\" >0.3420</td>\n",
              "      <td id=\"T_3f999_row50_col9\" class=\"data row50 col9\" >0.3296</td>\n",
              "      <td id=\"T_3f999_row50_col10\" class=\"data row50 col10\" >0.3551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row51\" class=\"row_heading level1 row51\" >gpt2-medium</th>\n",
              "      <td id=\"T_3f999_row51_col0\" class=\"data row51 col0\" >0.6275</td>\n",
              "      <td id=\"T_3f999_row51_col1\" class=\"data row51 col1\" >0.3658</td>\n",
              "      <td id=\"T_3f999_row51_col2\" class=\"data row51 col2\" >0.4420</td>\n",
              "      <td id=\"T_3f999_row51_col3\" class=\"data row51 col3\" >0.4286</td>\n",
              "      <td id=\"T_3f999_row51_col4\" class=\"data row51 col4\" >0.9783</td>\n",
              "      <td id=\"T_3f999_row51_col5\" class=\"data row51 col5\" >0.3931</td>\n",
              "      <td id=\"T_3f999_row51_col6\" class=\"data row51 col6\" >0.3921</td>\n",
              "      <td id=\"T_3f999_row51_col7\" class=\"data row51 col7\" >0.3814</td>\n",
              "      <td id=\"T_3f999_row51_col8\" class=\"data row51 col8\" >0.4329</td>\n",
              "      <td id=\"T_3f999_row51_col9\" class=\"data row51 col9\" >0.4687</td>\n",
              "      <td id=\"T_3f999_row51_col10\" class=\"data row51 col10\" >0.3502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row52\" class=\"row_heading level1 row52\" >mGPT</th>\n",
              "      <td id=\"T_3f999_row52_col0\" class=\"data row52 col0\" >0.4086</td>\n",
              "      <td id=\"T_3f999_row52_col1\" class=\"data row52 col1\" >0.7888</td>\n",
              "      <td id=\"T_3f999_row52_col2\" class=\"data row52 col2\" >0.5043</td>\n",
              "      <td id=\"T_3f999_row52_col3\" class=\"data row52 col3\" >0.8511</td>\n",
              "      <td id=\"T_3f999_row52_col4\" class=\"data row52 col4\" >0.9765</td>\n",
              "      <td id=\"T_3f999_row52_col5\" class=\"data row52 col5\" >0.6633</td>\n",
              "      <td id=\"T_3f999_row52_col6\" class=\"data row52 col6\" >0.8876</td>\n",
              "      <td id=\"T_3f999_row52_col7\" class=\"data row52 col7\" >0.8071</td>\n",
              "      <td id=\"T_3f999_row52_col8\" class=\"data row52 col8\" >0.6347</td>\n",
              "      <td id=\"T_3f999_row52_col9\" class=\"data row52 col9\" >0.7088</td>\n",
              "      <td id=\"T_3f999_row52_col10\" class=\"data row52 col10\" >0.5941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row53\" class=\"row_heading level1 row53\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_3f999_row53_col0\" class=\"data row53 col0\" >0.1784</td>\n",
              "      <td id=\"T_3f999_row53_col1\" class=\"data row53 col1\" >0.5000</td>\n",
              "      <td id=\"T_3f999_row53_col2\" class=\"data row53 col2\" >0.5018</td>\n",
              "      <td id=\"T_3f999_row53_col3\" class=\"data row53 col3\" >0.6295</td>\n",
              "      <td id=\"T_3f999_row53_col4\" class=\"data row53 col4\" >0.7775</td>\n",
              "      <td id=\"T_3f999_row53_col5\" class=\"data row53 col5\" >0.4754</td>\n",
              "      <td id=\"T_3f999_row53_col6\" class=\"data row53 col6\" >0.6191</td>\n",
              "      <td id=\"T_3f999_row53_col7\" class=\"data row53 col7\" >0.5078</td>\n",
              "      <td id=\"T_3f999_row53_col8\" class=\"data row53 col8\" >0.4121</td>\n",
              "      <td id=\"T_3f999_row53_col9\" class=\"data row53 col9\" >0.4635</td>\n",
              "      <td id=\"T_3f999_row53_col10\" class=\"data row53 col10\" >0.5888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row54\" class=\"row_heading level1 row54\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_3f999_row54_col0\" class=\"data row54 col0\" >0.4179</td>\n",
              "      <td id=\"T_3f999_row54_col1\" class=\"data row54 col1\" >0.6409</td>\n",
              "      <td id=\"T_3f999_row54_col2\" class=\"data row54 col2\" >0.5661</td>\n",
              "      <td id=\"T_3f999_row54_col3\" class=\"data row54 col3\" >0.5341</td>\n",
              "      <td id=\"T_3f999_row54_col4\" class=\"data row54 col4\" >0.9348</td>\n",
              "      <td id=\"T_3f999_row54_col5\" class=\"data row54 col5\" >0.5070</td>\n",
              "      <td id=\"T_3f999_row54_col6\" class=\"data row54 col6\" >0.8146</td>\n",
              "      <td id=\"T_3f999_row54_col7\" class=\"data row54 col7\" >0.4350</td>\n",
              "      <td id=\"T_3f999_row54_col8\" class=\"data row54 col8\" >0.5180</td>\n",
              "      <td id=\"T_3f999_row54_col9\" class=\"data row54 col9\" >0.5303</td>\n",
              "      <td id=\"T_3f999_row54_col10\" class=\"data row54 col10\" >0.4740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_3f999_level1_row55\" class=\"row_heading level1 row55\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_3f999_row55_col0\" class=\"data row55 col0\" >0.3864</td>\n",
              "      <td id=\"T_3f999_row55_col1\" class=\"data row55 col1\" >0.9417</td>\n",
              "      <td id=\"T_3f999_row55_col2\" class=\"data row55 col2\" >0.4894</td>\n",
              "      <td id=\"T_3f999_row55_col3\" class=\"data row55 col3\" >0.9086</td>\n",
              "      <td id=\"T_3f999_row55_col4\" class=\"data row55 col4\" >0.9801</td>\n",
              "      <td id=\"T_3f999_row55_col5\" class=\"data row55 col5\" >0.6866</td>\n",
              "      <td id=\"T_3f999_row55_col6\" class=\"data row55 col6\" >0.8400</td>\n",
              "      <td id=\"T_3f999_row55_col7\" class=\"data row55 col7\" >0.7163</td>\n",
              "      <td id=\"T_3f999_row55_col8\" class=\"data row55 col8\" >0.7787</td>\n",
              "      <td id=\"T_3f999_row55_col9\" class=\"data row55 col9\" >0.4897</td>\n",
              "      <td id=\"T_3f999_row55_col10\" class=\"data row55 col10\" >0.4370</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{llrrrrrrrrrrr}\n",
            " &  & \\bfseries ar & \\bfseries ca & \\bfseries cs & \\bfseries de & \\bfseries en & \\bfseries es & \\bfseries nl & \\bfseries pt & \\bfseries ru & \\bfseries uk & \\bfseries zh \\\\\n",
            "Train LLM & Model &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries alpaca-lora-30b} & \\bfseries bert-base-multilingual-cased & 0.5375 & 0.8271 & 0.8546 & 0.8917 & \\bfseries 0.9567 & 0.7517 & 0.8563 & 0.8055 & 0.8374 & 0.8091 & 0.5537 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.5270 & 0.4929 & 0.4819 & 0.3956 & \\bfseries 0.9783 & 0.5856 & 0.5448 & 0.5196 & 0.5153 & 0.4215 & 0.4642 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.7438 & 0.4252 & 0.4513 & 0.3928 & \\bfseries 0.9657 & 0.4063 & 0.4233 & 0.3719 & 0.5547 & 0.5326 & 0.3742 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.4024 & 0.8089 & 0.6132 & 0.8763 & \\bfseries 0.9639 & 0.7390 & 0.8791 & 0.8333 & 0.8162 & 0.8210 & 0.4626 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.2080 & 0.8592 & 0.7691 & 0.9003 & \\bfseries 0.9439 & 0.7744 & 0.8977 & 0.8570 & 0.7988 & 0.7563 & 0.3099 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.3484 & 0.6258 & 0.5430 & 0.5133 & \\bfseries 0.9238 & 0.4809 & 0.7787 & 0.4348 & 0.4029 & 0.4838 & 0.4487 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.4474 & 0.8488 & 0.8713 & 0.9324 & \\bfseries 0.9801 & 0.6319 & 0.7706 & 0.7130 & 0.8733 & 0.8319 & 0.4474 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries gpt-3.5-turbo} & \\bfseries bert-base-multilingual-cased & 0.9215 & 0.8904 & 0.9150 & 0.9020 & \\bfseries 0.9783 & 0.8545 & 0.9348 & 0.9124 & 0.9183 & 0.8962 & 0.8933 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.3985 & 0.8409 & 0.3333 & 0.3576 & \\bfseries 0.9765 & 0.8243 & 0.7355 & 0.7810 & 0.3849 & 0.3526 & 0.3399 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.3673 & 0.3407 & 0.3443 & 0.3369 & \\bfseries 0.9838 & 0.3264 & 0.3432 & 0.3378 & 0.3689 & 0.3710 & 0.3428 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.7009 & 0.9266 & 0.3370 & 0.9071 & \\bfseries 0.9892 & 0.8506 & 0.9432 & 0.9147 & 0.9013 & 0.9080 & 0.5610 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.7876 & 0.5599 & 0.7803 & \\bfseries 0.8052 & 0.7178 & 0.6023 & 0.7790 & 0.6782 & 0.6484 & 0.7649 & 0.6733 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.4174 & 0.6710 & 0.5623 & 0.5476 & \\bfseries 0.9166 & 0.5356 & 0.8203 & 0.4381 & 0.5442 & 0.6236 & 0.4727 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.7218 & 0.9733 & 0.7211 & 0.9392 & \\bfseries 0.9838 & 0.7861 & 0.9229 & 0.9281 & 0.8839 & 0.6290 & 0.5155 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries gpt-4} & \\bfseries bert-base-multilingual-cased & 0.8419 & 0.9265 & 0.7592 & 0.8163 & \\bfseries 0.9765 & 0.8261 & 0.9265 & 0.8704 & 0.7929 & 0.7104 & 0.8360 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.4448 & 0.5611 & 0.3333 & 0.3303 & \\bfseries 0.9928 & 0.7723 & 0.4206 & 0.6814 & 0.4044 & 0.3697 & 0.3333 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.3853 & 0.3658 & 0.3516 & 0.3593 & \\bfseries 0.9928 & 0.3659 & 0.3887 & 0.3486 & 0.3969 & 0.3732 & 0.3318 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.6945 & 0.7888 & 0.3866 & 0.8365 & \\bfseries 0.9910 & 0.7954 & 0.8334 & 0.8658 & 0.8008 & 0.7849 & 0.4734 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.7199 & \\bfseries 0.9733 & 0.8626 & 0.8944 & 0.8255 & 0.9435 & 0.9331 & 0.9319 & 0.8395 & 0.7668 & 0.5706 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.5765 & 0.5661 & 0.4808 & 0.5395 & \\bfseries 0.9765 & 0.4974 & 0.7401 & 0.4077 & 0.6384 & 0.5689 & 0.4995 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.5659 & 0.9348 & 0.4987 & 0.8927 & \\bfseries 0.9892 & 0.8592 & 0.9499 & 0.9267 & 0.7676 & 0.4567 & 0.3587 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries llama-65b} & \\bfseries bert-base-multilingual-cased & 0.5896 & 0.6070 & 0.6955 & 0.6895 & \\bfseries 0.9017 & 0.7998 & 0.5771 & 0.6752 & 0.7032 & 0.7020 & 0.5194 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.6770 & 0.3311 & 0.3299 & 0.3386 & \\bfseries 0.9220 & 0.3613 & 0.3322 & 0.3538 & 0.4911 & 0.5700 & 0.3281 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.3735 & 0.4296 & 0.5929 & 0.5370 & \\bfseries 0.8854 & 0.5623 & 0.5372 & 0.3950 & 0.4543 & 0.4893 & 0.4342 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.3848 & 0.3508 & 0.6898 & 0.7115 & \\bfseries 0.9037 & 0.5248 & 0.4102 & 0.4835 & 0.4669 & 0.4779 & 0.5233 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.5115 & 0.4060 & 0.8307 & 0.7162 & \\bfseries 0.9003 & 0.6074 & 0.4591 & 0.5413 & 0.7103 & 0.6693 & 0.4384 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.3303 & 0.3528 & 0.3446 & 0.3341 & \\bfseries 0.8890 & 0.3460 & 0.3612 & 0.3371 & 0.3311 & 0.3449 & 0.3332 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.4779 & 0.4892 & 0.8507 & 0.7983 & \\bfseries 0.9165 & 0.7241 & 0.4368 & 0.6082 & 0.6899 & 0.6290 & 0.4892 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries opt-66b} & \\bfseries bert-base-multilingual-cased & 0.4557 & 0.4586 & 0.6107 & 0.5890 & \\bfseries 0.8592 & 0.5394 & 0.4268 & 0.5635 & 0.5876 & 0.5594 & 0.5041 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.5167 & 0.5157 & 0.6473 & 0.5853 & \\bfseries 0.8899 & 0.6591 & 0.5600 & 0.6993 & 0.4347 & 0.5061 & 0.3273 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.5226 & 0.4138 & 0.3720 & 0.3827 & \\bfseries 0.8427 & 0.4202 & 0.3647 & 0.4089 & 0.5713 & 0.6213 & 0.4473 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.2959 & 0.5049 & 0.3954 & 0.6351 & \\bfseries 0.8931 & 0.5064 & 0.4942 & 0.6017 & 0.4416 & 0.4645 & 0.4161 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.3903 & 0.5181 & 0.6278 & 0.6568 & \\bfseries 0.8538 & 0.6053 & 0.4927 & 0.6431 & 0.5799 & 0.6016 & 0.5617 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.3315 & 0.5268 & 0.4635 & 0.4038 & \\bfseries 0.8711 & 0.3792 & 0.5761 & 0.3714 & 0.3293 & 0.3733 & 0.3591 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.4910 & 0.4613 & 0.5495 & 0.6735 & \\bfseries 0.8827 & 0.5613 & 0.4187 & 0.5897 & 0.6706 & 0.6961 & 0.5358 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries opt-iml-max-1.3b} & \\bfseries bert-base-multilingual-cased & 0.4592 & 0.5171 & 0.5792 & 0.5916 & \\bfseries 0.9061 & 0.5143 & 0.5136 & 0.4721 & 0.5494 & 0.5333 & 0.4390 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.3264 & 0.5542 & 0.5036 & 0.6618 & \\bfseries 0.9819 & 0.8709 & 0.7625 & 0.8334 & 0.3309 & 0.3390 & 0.3299 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.3869 & 0.7116 & 0.5300 & 0.5259 & \\bfseries 0.9368 & 0.5789 & 0.5600 & 0.5453 & 0.4733 & 0.5295 & 0.4720 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.3380 & 0.5134 & 0.6410 & 0.7450 & \\bfseries 0.9585 & 0.5284 & 0.6955 & 0.5212 & 0.4829 & 0.4925 & 0.5788 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.7932 & 0.9731 & \\bfseries 1.0000 & 0.9609 & 0.9257 & 0.9023 & 0.9615 & 0.9025 & 0.9471 & 0.9570 & 0.7016 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.3427 & 0.8511 & 0.7842 & 0.6490 & \\bfseries 0.9729 & 0.6895 & 0.9211 & 0.6590 & 0.3614 & 0.4318 & 0.4777 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.5810 & 0.8567 & 0.8657 & 0.8416 & \\bfseries 0.9439 & 0.7268 & 0.7684 & 0.6991 & 0.7483 & 0.7587 & 0.6237 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries text-davinci-003} & \\bfseries bert-base-multilingual-cased & 0.7647 & 0.9014 & 0.8537 & 0.8554 & \\bfseries 0.9693 & 0.7750 & 0.9032 & 0.8408 & 0.7161 & 0.6657 & 0.8409 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.4628 & 0.5751 & 0.4275 & 0.4024 & \\bfseries 0.9783 & 0.7471 & 0.5568 & 0.6000 & 0.5075 & 0.4826 & 0.4880 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.3322 & 0.3333 & 0.3333 & 0.3303 & \\bfseries 0.9856 & 0.3264 & 0.3367 & 0.3268 & 0.3267 & 0.3361 & 0.3326 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.5393 & 0.9199 & 0.3623 & 0.8936 & \\bfseries 0.9783 & 0.7678 & 0.9027 & 0.8389 & 0.7713 & 0.6932 & 0.5246 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.5432 & 0.6398 & 0.8239 & 0.8563 & \\bfseries 0.9513 & 0.5774 & 0.7596 & 0.6335 & 0.6969 & 0.6015 & 0.7779 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.4087 & 0.5610 & 0.5307 & 0.5112 & \\bfseries 0.9293 & 0.4369 & 0.7561 & 0.3902 & 0.4746 & 0.6221 & 0.4820 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.5035 & 0.9213 & 0.6965 & 0.8913 & \\bfseries 0.9856 & 0.7715 & 0.8973 & 0.8232 & 0.5769 & 0.4739 & 0.4377 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries vicuna-13b} & \\bfseries bert-base-multilingual-cased & 0.7913 & 0.8856 & 0.8012 & 0.8446 & \\bfseries 0.9747 & 0.7859 & 0.9365 & 0.8520 & 0.8309 & 0.8137 & 0.6667 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.3912 & 0.3848 & 0.6025 & 0.5077 & \\bfseries 0.9838 & 0.6577 & 0.4839 & 0.5162 & 0.3420 & 0.3296 & 0.3551 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.6275 & 0.3658 & 0.4420 & 0.4286 & \\bfseries 0.9783 & 0.3931 & 0.3921 & 0.3814 & 0.4329 & 0.4687 & 0.3502 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.4086 & 0.7888 & 0.5043 & 0.8511 & \\bfseries 0.9765 & 0.6633 & 0.8876 & 0.8071 & 0.6347 & 0.7088 & 0.5941 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.1784 & 0.5000 & 0.5018 & 0.6295 & \\bfseries 0.7775 & 0.4754 & 0.6191 & 0.5078 & 0.4121 & 0.4635 & 0.5888 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.4179 & 0.6409 & 0.5661 & 0.5341 & \\bfseries 0.9348 & 0.5070 & 0.8146 & 0.4350 & 0.5180 & 0.5303 & 0.4740 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.3864 & 0.9417 & 0.4894 & 0.9086 & \\bfseries 0.9801 & 0.6866 & 0.8400 & 0.7163 & 0.7787 & 0.4897 & 0.4370 \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd004f1d300>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_c3da0_row0_col0 {\n",
              "  background-color: #d0d1e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row0_col1 {\n",
              "  background-color: #b8c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row0_col2 {\n",
              "  background-color: #c1cae2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row0_col3 {\n",
              "  background-color: #b5c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row0_col4 {\n",
              "  background-color: #81aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row0_col5, #T_c3da0_row0_col7 {\n",
              "  background-color: #bbc7e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row0_col6 {\n",
              "  background-color: #b4c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row0_col8 {\n",
              "  background-color: #c0c9e2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row0_col9 {\n",
              "  background-color: #c2cbe2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row0_col10 {\n",
              "  background-color: #d2d2e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row1_col0 {\n",
              "  background-color: #c9cee4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row1_col1 {\n",
              "  background-color: #a9bfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row1_col2, #T_c3da0_row1_col9 {\n",
              "  background-color: #b3c3de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row1_col3 {\n",
              "  background-color: #9ab8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row1_col4 {\n",
              "  background-color: #83afd3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row1_col5 {\n",
              "  background-color: #afc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row1_col6 {\n",
              "  background-color: #a5bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row1_col7 {\n",
              "  background-color: #a8bedc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row1_col8 {\n",
              "  background-color: #acc0dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row1_col10 {\n",
              "  background-color: #c6cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row2_col0, #T_c3da0_row2_col8 {\n",
              "  background-color: #d7d6e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row2_col1 {\n",
              "  background-color: #cdd0e5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row2_col2 {\n",
              "  background-color: #d3d4e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row2_col3 {\n",
              "  background-color: #d5d5e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row2_col4 {\n",
              "  background-color: #7eadd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row2_col5 {\n",
              "  background-color: #cccfe5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row2_col6 {\n",
              "  background-color: #c5cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row2_col7 {\n",
              "  background-color: #d2d3e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row2_col9 {\n",
              "  background-color: #d4d4e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c3da0_row2_col10 {\n",
              "  background-color: #dbdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_c3da0\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_c3da0_level0_col0\" class=\"col_heading level0 col0\" >ar</th>\n",
              "      <th id=\"T_c3da0_level0_col1\" class=\"col_heading level0 col1\" >ca</th>\n",
              "      <th id=\"T_c3da0_level0_col2\" class=\"col_heading level0 col2\" >cs</th>\n",
              "      <th id=\"T_c3da0_level0_col3\" class=\"col_heading level0 col3\" >de</th>\n",
              "      <th id=\"T_c3da0_level0_col4\" class=\"col_heading level0 col4\" >en</th>\n",
              "      <th id=\"T_c3da0_level0_col5\" class=\"col_heading level0 col5\" >es</th>\n",
              "      <th id=\"T_c3da0_level0_col6\" class=\"col_heading level0 col6\" >nl</th>\n",
              "      <th id=\"T_c3da0_level0_col7\" class=\"col_heading level0 col7\" >pt</th>\n",
              "      <th id=\"T_c3da0_level0_col8\" class=\"col_heading level0 col8\" >ru</th>\n",
              "      <th id=\"T_c3da0_level0_col9\" class=\"col_heading level0 col9\" >uk</th>\n",
              "      <th id=\"T_c3da0_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_c3da0_level0_row0\" class=\"row_heading level0 row0\" >All Detectors Mean</th>\n",
              "      <td id=\"T_c3da0_row0_col0\" class=\"data row0 col0\" >0.5016</td>\n",
              "      <td id=\"T_c3da0_row0_col1\" class=\"data row0 col1\" >0.6412</td>\n",
              "      <td id=\"T_c3da0_row0_col2\" class=\"data row0 col2\" >0.5909</td>\n",
              "      <td id=\"T_c3da0_row0_col3\" class=\"data row0 col3\" >0.6578</td>\n",
              "      <td id=\"T_c3da0_row0_col4\" class=\"data row0 col4\" >0.9361</td>\n",
              "      <td id=\"T_c3da0_row0_col5\" class=\"data row0 col5\" >0.6284</td>\n",
              "      <td id=\"T_c3da0_row0_col6\" class=\"data row0 col6\" >0.6703</td>\n",
              "      <td id=\"T_c3da0_row0_col7\" class=\"data row0 col7\" >0.6273</td>\n",
              "      <td id=\"T_c3da0_row0_col8\" class=\"data row0 col8\" >0.5976</td>\n",
              "      <td id=\"T_c3da0_row0_col9\" class=\"data row0 col9\" >0.5832</td>\n",
              "      <td id=\"T_c3da0_row0_col10\" class=\"data row0 col10\" >0.4902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c3da0_level0_row1\" class=\"row_heading level0 row1\" >Multilingual Base Models Mean</th>\n",
              "      <td id=\"T_c3da0_row1_col0\" class=\"data row1 col0\" >0.5448</td>\n",
              "      <td id=\"T_c3da0_row1_col1\" class=\"data row1 col1\" >0.7335</td>\n",
              "      <td id=\"T_c3da0_row1_col2\" class=\"data row1 col2\" >0.6793</td>\n",
              "      <td id=\"T_c3da0_row1_col3\" class=\"data row1 col3\" >0.8104</td>\n",
              "      <td id=\"T_c3da0_row1_col4\" class=\"data row1 col4\" >0.9292</td>\n",
              "      <td id=\"T_c3da0_row1_col5\" class=\"data row1 col5\" >0.7018</td>\n",
              "      <td id=\"T_c3da0_row1_col6\" class=\"data row1 col6\" >0.7508</td>\n",
              "      <td id=\"T_c3da0_row1_col7\" class=\"data row1 col7\" >0.7362</td>\n",
              "      <td id=\"T_c3da0_row1_col8\" class=\"data row1 col8\" >0.7148</td>\n",
              "      <td id=\"T_c3da0_row1_col9\" class=\"data row1 col9\" >0.6746</td>\n",
              "      <td id=\"T_c3da0_row1_col10\" class=\"data row1 col10\" >0.5580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c3da0_level0_row2\" class=\"row_heading level0 row2\" >Monolingual Base Models Mean</th>\n",
              "      <td id=\"T_c3da0_row2_col0\" class=\"data row2 col0\" >0.4440</td>\n",
              "      <td id=\"T_c3da0_row2_col1\" class=\"data row2 col1\" >0.5182</td>\n",
              "      <td id=\"T_c3da0_row2_col2\" class=\"data row2 col2\" >0.4730</td>\n",
              "      <td id=\"T_c3da0_row2_col3\" class=\"data row2 col3\" >0.4544</td>\n",
              "      <td id=\"T_c3da0_row2_col4\" class=\"data row2 col4\" >0.9454</td>\n",
              "      <td id=\"T_c3da0_row2_col5\" class=\"data row2 col5\" >0.5304</td>\n",
              "      <td id=\"T_c3da0_row2_col6\" class=\"data row2 col6\" >0.5629</td>\n",
              "      <td id=\"T_c3da0_row2_col7\" class=\"data row2 col7\" >0.4822</td>\n",
              "      <td id=\"T_c3da0_row2_col8\" class=\"data row2 col8\" >0.4412</td>\n",
              "      <td id=\"T_c3da0_row2_col9\" class=\"data row2 col9\" >0.4613</td>\n",
              "      <td id=\"T_c3da0_row2_col10\" class=\"data row2 col10\" >0.3999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{lrrrrrrrrrrr}\n",
            " & \\bfseries ar & \\bfseries ca & \\bfseries cs & \\bfseries de & \\bfseries en & \\bfseries es & \\bfseries nl & \\bfseries pt & \\bfseries ru & \\bfseries uk & \\bfseries zh \\\\\n",
            "\\multicolumn{2}{r|}{\\bfseries All Detectors Mean} & {\\cellcolor[HTML]{D0D1E6}} \\textcolor{black}{0.5016} & {\\cellcolor[HTML]{B8C6E0}} \\textcolor{black}{0.6412} & {\\cellcolor[HTML]{C1CAE2}} \\textcolor{black}{0.5909} & {\\cellcolor[HTML]{B5C4DF}} \\textcolor{black}{0.6578} & {\\cellcolor[HTML]{81AED2}} \\textcolor{black}{0.9361} & {\\cellcolor[HTML]{BBC7E0}} \\textcolor{black}{0.6284} & {\\cellcolor[HTML]{B4C4DF}} \\textcolor{black}{0.6703} & {\\cellcolor[HTML]{BBC7E0}} \\textcolor{black}{0.6273} & {\\cellcolor[HTML]{C0C9E2}} \\textcolor{black}{0.5976} & {\\cellcolor[HTML]{C2CBE2}} \\textcolor{black}{0.5832} & {\\cellcolor[HTML]{D2D2E7}} \\textcolor{black}{0.4902} \\\\\n",
            "\\multicolumn{2}{r|}{\\bfseries Multilingual Base Models Mean} & {\\cellcolor[HTML]{C9CEE4}} \\textcolor{black}{0.5448} & {\\cellcolor[HTML]{A9BFDC}} \\textcolor{black}{0.7335} & {\\cellcolor[HTML]{B3C3DE}} \\textcolor{black}{0.6793} & {\\cellcolor[HTML]{9AB8D8}} \\textcolor{black}{0.8104} & {\\cellcolor[HTML]{83AFD3}} \\textcolor{black}{0.9292} & {\\cellcolor[HTML]{AFC1DD}} \\textcolor{black}{0.7018} & {\\cellcolor[HTML]{A5BDDB}} \\textcolor{black}{0.7508} & {\\cellcolor[HTML]{A8BEDC}} \\textcolor{black}{0.7362} & {\\cellcolor[HTML]{ACC0DD}} \\textcolor{black}{0.7148} & {\\cellcolor[HTML]{B3C3DE}} \\textcolor{black}{0.6746} & {\\cellcolor[HTML]{C6CCE3}} \\textcolor{black}{0.5580} \\\\\n",
            "\\multicolumn{2}{r|}{\\bfseries Monolingual Base Models Mean} & {\\cellcolor[HTML]{D7D6E9}} \\textcolor{black}{0.4440} & {\\cellcolor[HTML]{CDD0E5}} \\textcolor{black}{0.5182} & {\\cellcolor[HTML]{D3D4E7}} \\textcolor{black}{0.4730} & {\\cellcolor[HTML]{D5D5E8}} \\textcolor{black}{0.4544} & {\\cellcolor[HTML]{7EADD1}} \\textcolor{black}{0.9454} & {\\cellcolor[HTML]{CCCFE5}} \\textcolor{black}{0.5304} & {\\cellcolor[HTML]{C5CCE3}} \\textcolor{black}{0.5629} & {\\cellcolor[HTML]{D2D3E7}} \\textcolor{black}{0.4822} & {\\cellcolor[HTML]{D7D6E9}} \\textcolor{black}{0.4412} & {\\cellcolor[HTML]{D4D4E8}} \\textcolor{black}{0.4613} & {\\cellcolor[HTML]{DBDAEB}} \\textcolor{black}{0.3999} \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Results just for Spanish fine-tuned models\n",
        "temp = results_all.loc['es',:]\n",
        "display(temp.style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4))\n",
        "print(temp.style.format(na_rep=0, precision=4).highlight_max(props='font-weight: bold;', axis=1).applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True))\n",
        "\n",
        "means = pd.DataFrame()\n",
        "means = pd.concat([means, temp.agg('mean')], copy=False, axis=1)\n",
        "means.rename(columns={0:'All Detectors Mean'}, inplace=True)\n",
        "means = pd.concat([means, temp.loc[[x in multilingual for x in temp.reset_index().Model], :].agg('mean')], copy=False, axis=1)\n",
        "means.rename(columns={0:'Multilingual Base Models Mean'}, inplace=True)\n",
        "means = pd.concat([means, temp.loc[[x not in multilingual for x in temp.reset_index().Model], :].agg('mean')], copy=False, axis=1)\n",
        "means.rename(columns={0:'Monolingual Base Models Mean'}, inplace=True)\n",
        "means = means.T.style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4)\n",
        "display(means)\n",
        "temp = means.applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True)\n",
        "#little hack to properly format latex table\n",
        "print(temp.replace('\\\\color[HTML]{F1F1F1} ', '\\\\textcolor{white}{').replace('\\\\color[HTML]{000000} ', '\\\\textcolor{black}{').replace(' & {\\\\cellcolor', '} & {\\\\cellcolor').replace(' \\\\\\\\', '} \\\\\\\\').replace('\\n\\\\bfseries', '\\n\\\\multicolumn{2}{r|}{\\\\bfseries').replace('zh} \\\\\\\\', 'zh \\\\\\\\'))"
      ],
      "metadata": {
        "id": "ziNSLJ9j333d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ef92905d-c4a9-4eae-8c94-766923e6892b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd004dcf910>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_f1af6_row0_col0, #T_f1af6_row34_col0, #T_f1af6_row37_col0 {\n",
              "  background-color: #bfc9e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row0_col1, #T_f1af6_row0_col6, #T_f1af6_row0_col7, #T_f1af6_row2_col5, #T_f1af6_row7_col8, #T_f1af6_row9_col5, #T_f1af6_row10_col0, #T_f1af6_row10_col6, #T_f1af6_row10_col8, #T_f1af6_row14_col8, #T_f1af6_row26_col1, #T_f1af6_row36_col3, #T_f1af6_row38_col8, #T_f1af6_row43_col7, #T_f1af6_row48_col8, #T_f1af6_row53_col7, #T_f1af6_row55_col7 {\n",
              "  background-color: #89b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row0_col2, #T_f1af6_row2_col1, #T_f1af6_row15_col1, #T_f1af6_row19_col2, #T_f1af6_row34_col3, #T_f1af6_row34_col5, #T_f1af6_row34_col8, #T_f1af6_row46_col5 {\n",
              "  background-color: #9ab8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row0_col3, #T_f1af6_row4_col4, #T_f1af6_row7_col10, #T_f1af6_row11_col1, #T_f1af6_row14_col0, #T_f1af6_row14_col3, #T_f1af6_row14_col9, #T_f1af6_row14_col10, #T_f1af6_row18_col4, #T_f1af6_row19_col3, #T_f1af6_row19_col7, #T_f1af6_row26_col7, #T_f1af6_row32_col5, #T_f1af6_row34_col2, #T_f1af6_row35_col7, #T_f1af6_row36_col6, #T_f1af6_row48_col10, #T_f1af6_row49_col7, #T_f1af6_row53_col6, #T_f1af6_row55_col1 {\n",
              "  background-color: #8cb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row0_col4, #T_f1af6_row5_col10, #T_f1af6_row24_col3, #T_f1af6_row26_col2, #T_f1af6_row28_col9, #T_f1af6_row41_col8, #T_f1af6_row44_col3, #T_f1af6_row44_col8, #T_f1af6_row49_col4 {\n",
              "  background-color: #a5bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row0_col5, #T_f1af6_row4_col2, #T_f1af6_row4_col9, #T_f1af6_row11_col6, #T_f1af6_row11_col7, #T_f1af6_row11_col9, #T_f1af6_row13_col8, #T_f1af6_row14_col6, #T_f1af6_row18_col9, #T_f1af6_row19_col1, #T_f1af6_row23_col3, #T_f1af6_row25_col0, #T_f1af6_row32_col0, #T_f1af6_row37_col5, #T_f1af6_row40_col7, #T_f1af6_row46_col6 {\n",
              "  background-color: #86b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row0_col8, #T_f1af6_row1_col7, #T_f1af6_row2_col7, #T_f1af6_row3_col1, #T_f1af6_row5_col7, #T_f1af6_row12_col1, #T_f1af6_row13_col2, #T_f1af6_row24_col2, #T_f1af6_row42_col3, #T_f1af6_row42_col7, #T_f1af6_row46_col8, #T_f1af6_row49_col0, #T_f1af6_row53_col8, #T_f1af6_row55_col3 {\n",
              "  background-color: #8fb4d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row0_col9, #T_f1af6_row26_col6, #T_f1af6_row33_col5, #T_f1af6_row41_col10 {\n",
              "  background-color: #99b8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row0_col10, #T_f1af6_row13_col0, #T_f1af6_row21_col3, #T_f1af6_row28_col5, #T_f1af6_row28_col7, #T_f1af6_row29_col5, #T_f1af6_row30_col5, #T_f1af6_row45_col10, #T_f1af6_row47_col2, #T_f1af6_row49_col2, #T_f1af6_row51_col0 {\n",
              "  background-color: #a2bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row1_col0, #T_f1af6_row20_col0, #T_f1af6_row37_col3 {\n",
              "  background-color: #ced0e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row1_col1, #T_f1af6_row28_col4, #T_f1af6_row30_col2, #T_f1af6_row30_col10, #T_f1af6_row34_col6, #T_f1af6_row37_col4 {\n",
              "  background-color: #d3d4e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row1_col2, #T_f1af6_row1_col3, #T_f1af6_row15_col4, #T_f1af6_row15_col10, #T_f1af6_row16_col4, #T_f1af6_row16_col10, #T_f1af6_row17_col2, #T_f1af6_row19_col8, #T_f1af6_row26_col8, #T_f1af6_row43_col4, #T_f1af6_row43_col10, #T_f1af6_row47_col9 {\n",
              "  background-color: #e2dfee;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row1_col4, #T_f1af6_row8_col2, #T_f1af6_row8_col3, #T_f1af6_row8_col4, #T_f1af6_row8_col10, #T_f1af6_row9_col10, #T_f1af6_row12_col0, #T_f1af6_row12_col8, #T_f1af6_row12_col9, #T_f1af6_row15_col2, #T_f1af6_row15_col3, #T_f1af6_row19_col0, #T_f1af6_row19_col9, #T_f1af6_row22_col4, #T_f1af6_row22_col10, #T_f1af6_row26_col0, #T_f1af6_row26_col4, #T_f1af6_row26_col10, #T_f1af6_row29_col4, #T_f1af6_row29_col10, #T_f1af6_row33_col4, #T_f1af6_row36_col4, #T_f1af6_row36_col9, #T_f1af6_row36_col10, #T_f1af6_row43_col2, #T_f1af6_row43_col3, #T_f1af6_row44_col10, #T_f1af6_row47_col0, #T_f1af6_row47_col8, #T_f1af6_row50_col4 {\n",
              "  background-color: #e3e0ee;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row1_col5, #T_f1af6_row4_col8, #T_f1af6_row6_col7, #T_f1af6_row7_col1, #T_f1af6_row10_col3, #T_f1af6_row11_col4, #T_f1af6_row14_col2, #T_f1af6_row18_col0, #T_f1af6_row18_col3, #T_f1af6_row18_col6, #T_f1af6_row23_col1, #T_f1af6_row39_col6, #T_f1af6_row42_col5, #T_f1af6_row48_col7 {\n",
              "  background-color: #81aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row1_col6, #T_f1af6_row8_col0, #T_f1af6_row51_col9 {\n",
              "  background-color: #cccfe5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row1_col8, #T_f1af6_row9_col2, #T_f1af6_row15_col0, #T_f1af6_row31_col0, #T_f1af6_row31_col6, #T_f1af6_row45_col2 {\n",
              "  background-color: #dddbec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row1_col9, #T_f1af6_row9_col3, #T_f1af6_row15_col8, #T_f1af6_row50_col2, #T_f1af6_row50_col3, #T_f1af6_row54_col9 {\n",
              "  background-color: #dedcec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row1_col10, #T_f1af6_row16_col9, #T_f1af6_row20_col9, #T_f1af6_row20_col10, #T_f1af6_row23_col4, #T_f1af6_row38_col4 {\n",
              "  background-color: #dbdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row2_col0, #T_f1af6_row24_col1, #T_f1af6_row44_col9, #T_f1af6_row51_col3, #T_f1af6_row52_col6 {\n",
              "  background-color: #b0c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row2_col2, #T_f1af6_row3_col4, #T_f1af6_row25_col6, #T_f1af6_row37_col6 {\n",
              "  background-color: #b5c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row2_col3, #T_f1af6_row4_col0, #T_f1af6_row4_col5, #T_f1af6_row6_col9, #T_f1af6_row21_col8, #T_f1af6_row28_col10, #T_f1af6_row49_col8, #T_f1af6_row55_col6 {\n",
              "  background-color: #97b7d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row2_col4, #T_f1af6_row4_col1, #T_f1af6_row14_col4, #T_f1af6_row30_col1, #T_f1af6_row31_col1, #T_f1af6_row33_col7, #T_f1af6_row42_col0, #T_f1af6_row51_col6 {\n",
              "  background-color: #a9bfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row2_col6, #T_f1af6_row5_col6, #T_f1af6_row6_col1, #T_f1af6_row9_col7, #T_f1af6_row24_col9, #T_f1af6_row27_col1, #T_f1af6_row44_col7, #T_f1af6_row49_col3, #T_f1af6_row51_col7, #T_f1af6_row53_col10 {\n",
              "  background-color: #96b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row2_col8, #T_f1af6_row31_col8 {\n",
              "  background-color: #c4cbe3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row2_col9, #T_f1af6_row3_col10, #T_f1af6_row20_col2, #T_f1af6_row33_col9, #T_f1af6_row43_col9 {\n",
              "  background-color: #c5cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row2_col10, #T_f1af6_row8_col9, #T_f1af6_row16_col8, #T_f1af6_row34_col4 {\n",
              "  background-color: #d7d6e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row3_col0, #T_f1af6_row3_col2, #T_f1af6_row6_col0, #T_f1af6_row29_col3, #T_f1af6_row31_col3, #T_f1af6_row32_col10 {\n",
              "  background-color: #afc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row3_col3, #T_f1af6_row6_col2, #T_f1af6_row7_col2, #T_f1af6_row17_col8, #T_f1af6_row18_col7, #T_f1af6_row18_col10, #T_f1af6_row22_col1, #T_f1af6_row35_col8, #T_f1af6_row37_col1, #T_f1af6_row42_col1, #T_f1af6_row44_col5, #T_f1af6_row45_col4, #T_f1af6_row47_col7, #T_f1af6_row49_col6, #T_f1af6_row53_col1, #T_f1af6_row53_col3, #T_f1af6_row54_col7, #T_f1af6_row55_col4 {\n",
              "  background-color: #88b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row3_col5, #T_f1af6_row3_col7, #T_f1af6_row11_col10, #T_f1af6_row23_col7, #T_f1af6_row27_col8, #T_f1af6_row38_col0, #T_f1af6_row40_col6, #T_f1af6_row45_col6, #T_f1af6_row46_col4, #T_f1af6_row47_col5, #T_f1af6_row48_col4, #T_f1af6_row54_col5, #T_f1af6_row55_col5 {\n",
              "  background-color: #80aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row3_col6, #T_f1af6_row5_col1, #T_f1af6_row28_col6, #T_f1af6_row37_col9, #T_f1af6_row41_col9 {\n",
              "  background-color: #adc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row3_col8, #T_f1af6_row5_col3, #T_f1af6_row10_col4, #T_f1af6_row12_col7, #T_f1af6_row34_col9, #T_f1af6_row36_col2, #T_f1af6_row38_col6, #T_f1af6_row42_col10, #T_f1af6_row53_col2 {\n",
              "  background-color: #94b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row3_col9, #T_f1af6_row6_col8, #T_f1af6_row7_col0, #T_f1af6_row7_col3, #T_f1af6_row7_col7, #T_f1af6_row9_col1, #T_f1af6_row11_col8, #T_f1af6_row14_col1, #T_f1af6_row18_col1, #T_f1af6_row42_col2, #T_f1af6_row42_col6, #T_f1af6_row54_col1 {\n",
              "  background-color: #8bb2d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row4_col3, #T_f1af6_row7_col4, #T_f1af6_row8_col7, #T_f1af6_row10_col9, #T_f1af6_row12_col5, #T_f1af6_row16_col5, #T_f1af6_row17_col3, #T_f1af6_row18_col5, #T_f1af6_row21_col1, #T_f1af6_row22_col3, #T_f1af6_row36_col7, #T_f1af6_row38_col2, #T_f1af6_row45_col7, #T_f1af6_row46_col10, #T_f1af6_row49_col1, #T_f1af6_row51_col5, #T_f1af6_row52_col3, #T_f1af6_row52_col7, #T_f1af6_row53_col4 {\n",
              "  background-color: #84b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row4_col6, #T_f1af6_row20_col3, #T_f1af6_row31_col5, #T_f1af6_row42_col8, #T_f1af6_row45_col1, #T_f1af6_row48_col0 {\n",
              "  background-color: #9cb9d9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row4_col7, #T_f1af6_row44_col6, #T_f1af6_row47_col6, #T_f1af6_row55_col8 {\n",
              "  background-color: #a1bbda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row4_col10, #T_f1af6_row16_col1, #T_f1af6_row21_col0, #T_f1af6_row48_col9, #T_f1af6_row52_col9 {\n",
              "  background-color: #9ebad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row5_col0, #T_f1af6_row5_col8, #T_f1af6_row10_col2, #T_f1af6_row25_col10, #T_f1af6_row36_col0 {\n",
              "  background-color: #e0deed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row5_col2, #T_f1af6_row10_col10, #T_f1af6_row28_col8, #T_f1af6_row52_col4, #T_f1af6_row53_col0, #T_f1af6_row54_col2 {\n",
              "  background-color: #b3c3de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row5_col4 {\n",
              "  background-color: #eae6f1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row5_col5, #T_f1af6_row6_col4, #T_f1af6_row6_col5, #T_f1af6_row7_col5, #T_f1af6_row7_col6, #T_f1af6_row11_col0, #T_f1af6_row14_col5, #T_f1af6_row17_col6, #T_f1af6_row19_col5, #T_f1af6_row21_col9, #T_f1af6_row22_col2, #T_f1af6_row25_col1, #T_f1af6_row44_col1, #T_f1af6_row45_col3, #T_f1af6_row46_col2, #T_f1af6_row46_col3, #T_f1af6_row48_col1, #T_f1af6_row49_col5, #T_f1af6_row51_col1 {\n",
              "  background-color: #83afd3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row5_col9, #T_f1af6_row9_col4, #T_f1af6_row15_col9, #T_f1af6_row19_col4, #T_f1af6_row21_col4, #T_f1af6_row22_col8, #T_f1af6_row24_col6, #T_f1af6_row26_col9 {\n",
              "  background-color: #e0dded;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row6_col3, #T_f1af6_row15_col7, #T_f1af6_row17_col4, #T_f1af6_row24_col7, #T_f1af6_row26_col5, #T_f1af6_row32_col3, #T_f1af6_row35_col5, #T_f1af6_row35_col10, #T_f1af6_row36_col5, #T_f1af6_row40_col2, #T_f1af6_row48_col2, #T_f1af6_row48_col3, #T_f1af6_row53_col5 {\n",
              "  background-color: #7eadd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row6_col6, #T_f1af6_row10_col7, #T_f1af6_row11_col3, #T_f1af6_row11_col5, #T_f1af6_row13_col1, #T_f1af6_row15_col5, #T_f1af6_row18_col2, #T_f1af6_row20_col1, #T_f1af6_row25_col3, #T_f1af6_row27_col3, #T_f1af6_row35_col0, #T_f1af6_row40_col5, #T_f1af6_row43_col5 {\n",
              "  background-color: #7dacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row6_col10, #T_f1af6_row29_col2, #T_f1af6_row33_col1, #T_f1af6_row44_col0 {\n",
              "  background-color: #bdc8e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row7_col9, #T_f1af6_row12_col3, #T_f1af6_row12_col6, #T_f1af6_row14_col7, #T_f1af6_row17_col1, #T_f1af6_row23_col2, #T_f1af6_row23_col6, #T_f1af6_row35_col3, #T_f1af6_row39_col3, #T_f1af6_row39_col10, #T_f1af6_row47_col3, #T_f1af6_row50_col7, #T_f1af6_row52_col1, #T_f1af6_row54_col3 {\n",
              "  background-color: #8eb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row8_col1, #T_f1af6_row13_col9, #T_f1af6_row28_col0, #T_f1af6_row42_col9, #T_f1af6_row47_col10 {\n",
              "  background-color: #a8bedc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row8_col5, #T_f1af6_row10_col5, #T_f1af6_row13_col5, #T_f1af6_row20_col4, #T_f1af6_row22_col5, #T_f1af6_row22_col7, #T_f1af6_row25_col8, #T_f1af6_row27_col7, #T_f1af6_row32_col8, #T_f1af6_row35_col1, #T_f1af6_row35_col6, #T_f1af6_row35_col9, #T_f1af6_row48_col6, #T_f1af6_row50_col5 {\n",
              "  background-color: #79abd0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row8_col6, #T_f1af6_row16_col0, #T_f1af6_row51_col10 {\n",
              "  background-color: #d9d8ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row8_col8, #T_f1af6_row23_col10, #T_f1af6_row34_col10, #T_f1af6_row54_col0 {\n",
              "  background-color: #cacee5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row9_col0, #T_f1af6_row16_col2, #T_f1af6_row16_col3, #T_f1af6_row29_col8, #T_f1af6_row54_col8 {\n",
              "  background-color: #d8d7e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row9_col6, #T_f1af6_row22_col0, #T_f1af6_row43_col0 {\n",
              "  background-color: #d2d2e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row9_col8 {\n",
              "  background-color: #d1d2e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row9_col9, #T_f1af6_row30_col0, #T_f1af6_row30_col4, #T_f1af6_row43_col6 {\n",
              "  background-color: #d6d6e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row10_col1, #T_f1af6_row11_col2, #T_f1af6_row13_col6, #T_f1af6_row13_col7, #T_f1af6_row20_col6, #T_f1af6_row20_col7, #T_f1af6_row21_col2, #T_f1af6_row23_col5, #T_f1af6_row24_col5, #T_f1af6_row27_col9, #T_f1af6_row32_col9, #T_f1af6_row35_col2, #T_f1af6_row36_col1, #T_f1af6_row38_col1, #T_f1af6_row40_col1, #T_f1af6_row45_col5, #T_f1af6_row52_col5 {\n",
              "  background-color: #7bacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row12_col2, #T_f1af6_row13_col10, #T_f1af6_row17_col0, #T_f1af6_row17_col9, #T_f1af6_row19_col6, #T_f1af6_row22_col6, #T_f1af6_row27_col0, #T_f1af6_row32_col1, #T_f1af6_row46_col7, #T_f1af6_row49_col10, #T_f1af6_row54_col6, #T_f1af6_row55_col0 {\n",
              "  background-color: #93b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row12_col4 {\n",
              "  background-color: #f0eaf4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row12_col10, #T_f1af6_row28_col1, #T_f1af6_row37_col2, #T_f1af6_row42_col4, #T_f1af6_row46_col1 {\n",
              "  background-color: #a4bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row13_col3, #T_f1af6_row18_col8, #T_f1af6_row32_col6, #T_f1af6_row33_col2, #T_f1af6_row33_col6, #T_f1af6_row41_col2, #T_f1af6_row45_col8, #T_f1af6_row53_col9 {\n",
              "  background-color: #91b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row13_col4, #T_f1af6_row17_col5, #T_f1af6_row17_col7, #T_f1af6_row20_col5, #T_f1af6_row21_col7, #T_f1af6_row25_col7, #T_f1af6_row27_col2, #T_f1af6_row38_col7, #T_f1af6_row39_col5, #T_f1af6_row39_col7, #T_f1af6_row41_col0, #T_f1af6_row41_col5, #T_f1af6_row41_col6, #T_f1af6_row41_col7, #T_f1af6_row48_col5 {\n",
              "  background-color: #78abd0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row15_col6, #T_f1af6_row24_col8, #T_f1af6_row30_col6, #T_f1af6_row40_col8, #T_f1af6_row52_col2 {\n",
              "  background-color: #c9cee4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row16_col6, #T_f1af6_row22_col9, #T_f1af6_row27_col6, #T_f1af6_row29_col0, #T_f1af6_row31_col2, #T_f1af6_row31_col10 {\n",
              "  background-color: #d5d5e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row16_col7, #T_f1af6_row32_col7, #T_f1af6_row49_col9 {\n",
              "  background-color: #9fbad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row17_col10, #T_f1af6_row21_col6, #T_f1af6_row27_col4, #T_f1af6_row37_col7, #T_f1af6_row55_col2 {\n",
              "  background-color: #b4c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row19_col10, #T_f1af6_row25_col4, #T_f1af6_row44_col2 {\n",
              "  background-color: #bbc7e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row20_col8, #T_f1af6_row30_col7, #T_f1af6_row30_col8, #T_f1af6_row31_col7, #T_f1af6_row47_col1, #T_f1af6_row54_col10 {\n",
              "  background-color: #c0c9e2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row21_col5, #T_f1af6_row25_col2, #T_f1af6_row25_col5, #T_f1af6_row25_col9, #T_f1af6_row27_col5, #T_f1af6_row32_col2, #T_f1af6_row38_col5, #T_f1af6_row38_col9, #T_f1af6_row39_col1, #T_f1af6_row41_col1, #T_f1af6_row41_col3 {\n",
              "  background-color: #76aad0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row21_col10, #T_f1af6_row34_col7, #T_f1af6_row55_col9 {\n",
              "  background-color: #b9c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row23_col0, #T_f1af6_row24_col10, #T_f1af6_row31_col4 {\n",
              "  background-color: #dfddec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row23_col8, #T_f1af6_row50_col0, #T_f1af6_row50_col6 {\n",
              "  background-color: #d2d3e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row23_col9, #T_f1af6_row33_col8, #T_f1af6_row33_col10 {\n",
              "  background-color: #c6cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row24_col0, #T_f1af6_row28_col3, #T_f1af6_row30_col3, #T_f1af6_row39_col4, #T_f1af6_row44_col4, #T_f1af6_row51_col2 {\n",
              "  background-color: #c1cae2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row24_col4 {\n",
              "  background-color: #e1dfed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row26_col3, #T_f1af6_row34_col1, #T_f1af6_row46_col9 {\n",
              "  background-color: #a7bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row27_col10, #T_f1af6_row29_col6, #T_f1af6_row35_col4, #T_f1af6_row40_col9 {\n",
              "  background-color: #b1c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row28_col2, #T_f1af6_row30_col9, #T_f1af6_row31_col9, #T_f1af6_row46_col0, #T_f1af6_row55_col10 {\n",
              "  background-color: #b8c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row29_col1, #T_f1af6_row29_col7, #T_f1af6_row40_col3, #T_f1af6_row45_col0, #T_f1af6_row51_col4, #T_f1af6_row52_col0, #T_f1af6_row52_col8 {\n",
              "  background-color: #abbfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row29_col9, #T_f1af6_row40_col10 {\n",
              "  background-color: #cdd0e5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row32_col4, #T_f1af6_row41_col4, #T_f1af6_row52_col10 {\n",
              "  background-color: #bcc7e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row33_col0 {\n",
              "  background-color: #d4d4e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row33_col3, #T_f1af6_row37_col8 {\n",
              "  background-color: #b7c5df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row36_col8 {\n",
              "  background-color: #e4e1ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row37_col10 {\n",
              "  background-color: #dcdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row38_col3, #T_f1af6_row39_col0, #T_f1af6_row39_col2, #T_f1af6_row39_col8, #T_f1af6_row39_col9 {\n",
              "  background-color: #75a9cf;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row38_col10, #T_f1af6_row43_col1, #T_f1af6_row50_col1, #T_f1af6_row50_col8 {\n",
              "  background-color: #c2cbe2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row40_col0, #T_f1af6_row50_col10 {\n",
              "  background-color: #dad9ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row40_col4, #T_f1af6_row47_col4 {\n",
              "  background-color: #efe9f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row43_col8, #T_f1af6_row50_col9 {\n",
              "  background-color: #c8cde4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row45_col9 {\n",
              "  background-color: #acc0dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row51_col8 {\n",
              "  background-color: #d0d1e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_f1af6_row54_col4 {\n",
              "  background-color: #e5e1ef;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_f1af6\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank\" >&nbsp;</th>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_f1af6_level0_col0\" class=\"col_heading level0 col0\" >ar</th>\n",
              "      <th id=\"T_f1af6_level0_col1\" class=\"col_heading level0 col1\" >ca</th>\n",
              "      <th id=\"T_f1af6_level0_col2\" class=\"col_heading level0 col2\" >cs</th>\n",
              "      <th id=\"T_f1af6_level0_col3\" class=\"col_heading level0 col3\" >de</th>\n",
              "      <th id=\"T_f1af6_level0_col4\" class=\"col_heading level0 col4\" >en</th>\n",
              "      <th id=\"T_f1af6_level0_col5\" class=\"col_heading level0 col5\" >es</th>\n",
              "      <th id=\"T_f1af6_level0_col6\" class=\"col_heading level0 col6\" >nl</th>\n",
              "      <th id=\"T_f1af6_level0_col7\" class=\"col_heading level0 col7\" >pt</th>\n",
              "      <th id=\"T_f1af6_level0_col8\" class=\"col_heading level0 col8\" >ru</th>\n",
              "      <th id=\"T_f1af6_level0_col9\" class=\"col_heading level0 col9\" >uk</th>\n",
              "      <th id=\"T_f1af6_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Train LLM</th>\n",
              "      <th class=\"index_name level1\" >Model</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "      <th class=\"blank col7\" >&nbsp;</th>\n",
              "      <th class=\"blank col8\" >&nbsp;</th>\n",
              "      <th class=\"blank col9\" >&nbsp;</th>\n",
              "      <th class=\"blank col10\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"7\">alpaca-lora-30b</th>\n",
              "      <th id=\"T_f1af6_level1_row0\" class=\"row_heading level1 row0\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_f1af6_row0_col0\" class=\"data row0 col0\" >0.6019</td>\n",
              "      <td id=\"T_f1af6_row0_col1\" class=\"data row0 col1\" >0.8950</td>\n",
              "      <td id=\"T_f1af6_row0_col2\" class=\"data row0 col2\" >0.8108</td>\n",
              "      <td id=\"T_f1af6_row0_col3\" class=\"data row0 col3\" >0.8765</td>\n",
              "      <td id=\"T_f1af6_row0_col4\" class=\"data row0 col4\" >0.7517</td>\n",
              "      <td id=\"T_f1af6_row0_col5\" class=\"data row0 col5\" >0.9092</td>\n",
              "      <td id=\"T_f1af6_row0_col6\" class=\"data row0 col6\" >0.8941</td>\n",
              "      <td id=\"T_f1af6_row0_col7\" class=\"data row0 col7\" >0.8943</td>\n",
              "      <td id=\"T_f1af6_row0_col8\" class=\"data row0 col8\" >0.8600</td>\n",
              "      <td id=\"T_f1af6_row0_col9\" class=\"data row0 col9\" >0.8182</td>\n",
              "      <td id=\"T_f1af6_row0_col10\" class=\"data row0 col10\" >0.7713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row1\" class=\"row_heading level1 row1\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_f1af6_row1_col0\" class=\"data row1 col0\" >0.5116</td>\n",
              "      <td id=\"T_f1af6_row1_col1\" class=\"data row1 col1\" >0.4754</td>\n",
              "      <td id=\"T_f1af6_row1_col2\" class=\"data row1 col2\" >0.3370</td>\n",
              "      <td id=\"T_f1af6_row1_col3\" class=\"data row1 col3\" >0.3361</td>\n",
              "      <td id=\"T_f1af6_row1_col4\" class=\"data row1 col4\" >0.3325</td>\n",
              "      <td id=\"T_f1af6_row1_col5\" class=\"data row1 col5\" >0.9348</td>\n",
              "      <td id=\"T_f1af6_row1_col6\" class=\"data row1 col6\" >0.5286</td>\n",
              "      <td id=\"T_f1af6_row1_col7\" class=\"data row1 col7\" >0.8665</td>\n",
              "      <td id=\"T_f1af6_row1_col8\" class=\"data row1 col8\" >0.3891</td>\n",
              "      <td id=\"T_f1af6_row1_col9\" class=\"data row1 col9\" >0.3820</td>\n",
              "      <td id=\"T_f1af6_row1_col10\" class=\"data row1 col10\" >0.3986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row2\" class=\"row_heading level1 row2\" >gpt2-medium</th>\n",
              "      <td id=\"T_f1af6_row2_col0\" class=\"data row2 col0\" >0.6937</td>\n",
              "      <td id=\"T_f1af6_row2_col1\" class=\"data row2 col1\" >0.8090</td>\n",
              "      <td id=\"T_f1af6_row2_col2\" class=\"data row2 col2\" >0.6564</td>\n",
              "      <td id=\"T_f1af6_row2_col3\" class=\"data row2 col3\" >0.8226</td>\n",
              "      <td id=\"T_f1af6_row2_col4\" class=\"data row2 col4\" >0.7332</td>\n",
              "      <td id=\"T_f1af6_row2_col5\" class=\"data row2 col5\" >0.8935</td>\n",
              "      <td id=\"T_f1af6_row2_col6\" class=\"data row2 col6\" >0.8291</td>\n",
              "      <td id=\"T_f1af6_row2_col7\" class=\"data row2 col7\" >0.8602</td>\n",
              "      <td id=\"T_f1af6_row2_col8\" class=\"data row2 col8\" >0.5735</td>\n",
              "      <td id=\"T_f1af6_row2_col9\" class=\"data row2 col9\" >0.5679</td>\n",
              "      <td id=\"T_f1af6_row2_col10\" class=\"data row2 col10\" >0.4391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row3\" class=\"row_heading level1 row3\" >mGPT</th>\n",
              "      <td id=\"T_f1af6_row3_col0\" class=\"data row3 col0\" >0.6980</td>\n",
              "      <td id=\"T_f1af6_row3_col1\" class=\"data row3 col1\" >0.8639</td>\n",
              "      <td id=\"T_f1af6_row3_col2\" class=\"data row3 col2\" >0.6989</td>\n",
              "      <td id=\"T_f1af6_row3_col3\" class=\"data row3 col3\" >0.9001</td>\n",
              "      <td id=\"T_f1af6_row3_col4\" class=\"data row3 col4\" >0.6589</td>\n",
              "      <td id=\"T_f1af6_row3_col5\" class=\"data row3 col5\" >0.9452</td>\n",
              "      <td id=\"T_f1af6_row3_col6\" class=\"data row3 col6\" >0.7088</td>\n",
              "      <td id=\"T_f1af6_row3_col7\" class=\"data row3 col7\" >0.9403</td>\n",
              "      <td id=\"T_f1af6_row3_col8\" class=\"data row3 col8\" >0.8366</td>\n",
              "      <td id=\"T_f1af6_row3_col9\" class=\"data row3 col9\" >0.8880</td>\n",
              "      <td id=\"T_f1af6_row3_col10\" class=\"data row3 col10\" >0.5691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row4\" class=\"row_heading level1 row4\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_f1af6_row4_col0\" class=\"data row4 col0\" >0.8255</td>\n",
              "      <td id=\"T_f1af6_row4_col1\" class=\"data row4 col1\" >0.7338</td>\n",
              "      <td id=\"T_f1af6_row4_col2\" class=\"data row4 col2\" >0.9110</td>\n",
              "      <td id=\"T_f1af6_row4_col3\" class=\"data row4 col3\" >0.9171</td>\n",
              "      <td id=\"T_f1af6_row4_col4\" class=\"data row4 col4\" >0.8771</td>\n",
              "      <td id=\"T_f1af6_row4_col5\" class=\"data row4 col5\" >0.8256</td>\n",
              "      <td id=\"T_f1af6_row4_col6\" class=\"data row4 col6\" >0.8038</td>\n",
              "      <td id=\"T_f1af6_row4_col7\" class=\"data row4 col7\" >0.7777</td>\n",
              "      <td id=\"T_f1af6_row4_col8\" class=\"data row4 col8\" >0.9317</td>\n",
              "      <td id=\"T_f1af6_row4_col9\" class=\"data row4 col9\" >0.9096</td>\n",
              "      <td id=\"T_f1af6_row4_col10\" class=\"data row4 col10\" >0.7954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row5\" class=\"row_heading level1 row5\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_f1af6_row5_col0\" class=\"data row5 col0\" >0.3578</td>\n",
              "      <td id=\"T_f1af6_row5_col1\" class=\"data row5 col1\" >0.7052</td>\n",
              "      <td id=\"T_f1af6_row5_col2\" class=\"data row5 col2\" >0.6789</td>\n",
              "      <td id=\"T_f1af6_row5_col3\" class=\"data row5 col3\" >0.8414</td>\n",
              "      <td id=\"T_f1af6_row5_col4\" class=\"data row5 col4\" >0.2717</td>\n",
              "      <td id=\"T_f1af6_row5_col5\" class=\"data row5 col5\" >0.9295</td>\n",
              "      <td id=\"T_f1af6_row5_col6\" class=\"data row5 col6\" >0.8296</td>\n",
              "      <td id=\"T_f1af6_row5_col7\" class=\"data row5 col7\" >0.8671</td>\n",
              "      <td id=\"T_f1af6_row5_col8\" class=\"data row5 col8\" >0.3587</td>\n",
              "      <td id=\"T_f1af6_row5_col9\" class=\"data row5 col9\" >0.3634</td>\n",
              "      <td id=\"T_f1af6_row5_col10\" class=\"data row5 col10\" >0.7537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row6\" class=\"row_heading level1 row6\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_f1af6_row6_col0\" class=\"data row6 col0\" >0.6999</td>\n",
              "      <td id=\"T_f1af6_row6_col1\" class=\"data row6 col1\" >0.8349</td>\n",
              "      <td id=\"T_f1af6_row6_col2\" class=\"data row6 col2\" >0.9008</td>\n",
              "      <td id=\"T_f1af6_row6_col3\" class=\"data row6 col3\" >0.9493</td>\n",
              "      <td id=\"T_f1af6_row6_col4\" class=\"data row6 col4\" >0.9295</td>\n",
              "      <td id=\"T_f1af6_row6_col5\" class=\"data row6 col5\" >0.9281</td>\n",
              "      <td id=\"T_f1af6_row6_col6\" class=\"data row6 col6\" >0.9532</td>\n",
              "      <td id=\"T_f1af6_row6_col7\" class=\"data row6 col7\" >0.9301</td>\n",
              "      <td id=\"T_f1af6_row6_col8\" class=\"data row6 col8\" >0.8845</td>\n",
              "      <td id=\"T_f1af6_row6_col9\" class=\"data row6 col9\" >0.8278</td>\n",
              "      <td id=\"T_f1af6_row6_col10\" class=\"data row6 col10\" >0.6164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level0_row7\" class=\"row_heading level0 row7\" rowspan=\"7\">gpt-3.5-turbo</th>\n",
              "      <th id=\"T_f1af6_level1_row7\" class=\"row_heading level1 row7\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_f1af6_row7_col0\" class=\"data row7 col0\" >0.8896</td>\n",
              "      <td id=\"T_f1af6_row7_col1\" class=\"data row7 col1\" >0.9365</td>\n",
              "      <td id=\"T_f1af6_row7_col2\" class=\"data row7 col2\" >0.8997</td>\n",
              "      <td id=\"T_f1af6_row7_col3\" class=\"data row7 col3\" >0.8900</td>\n",
              "      <td id=\"T_f1af6_row7_col4\" class=\"data row7 col4\" >0.9169</td>\n",
              "      <td id=\"T_f1af6_row7_col5\" class=\"data row7 col5\" >0.9263</td>\n",
              "      <td id=\"T_f1af6_row7_col6\" class=\"data row7 col6\" >0.9232</td>\n",
              "      <td id=\"T_f1af6_row7_col7\" class=\"data row7 col7\" >0.8858</td>\n",
              "      <td id=\"T_f1af6_row7_col8\" class=\"data row7 col8\" >0.8949</td>\n",
              "      <td id=\"T_f1af6_row7_col9\" class=\"data row7 col9\" >0.8722</td>\n",
              "      <td id=\"T_f1af6_row7_col10\" class=\"data row7 col10\" >0.8765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row8\" class=\"row_heading level1 row8\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_f1af6_row8_col0\" class=\"data row8 col0\" >0.5268</td>\n",
              "      <td id=\"T_f1af6_row8_col1\" class=\"data row8 col1\" >0.7374</td>\n",
              "      <td id=\"T_f1af6_row8_col2\" class=\"data row8 col2\" >0.3333</td>\n",
              "      <td id=\"T_f1af6_row8_col3\" class=\"data row8 col3\" >0.3340</td>\n",
              "      <td id=\"T_f1af6_row8_col4\" class=\"data row8 col4\" >0.3333</td>\n",
              "      <td id=\"T_f1af6_row8_col5\" class=\"data row8 col5\" >0.9743</td>\n",
              "      <td id=\"T_f1af6_row8_col6\" class=\"data row8 col6\" >0.4290</td>\n",
              "      <td id=\"T_f1af6_row8_col7\" class=\"data row8 col7\" >0.9215</td>\n",
              "      <td id=\"T_f1af6_row8_col8\" class=\"data row8 col8\" >0.5390</td>\n",
              "      <td id=\"T_f1af6_row8_col9\" class=\"data row8 col9\" >0.4383</td>\n",
              "      <td id=\"T_f1af6_row8_col10\" class=\"data row8 col10\" >0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row9\" class=\"row_heading level1 row9\" >gpt2-medium</th>\n",
              "      <td id=\"T_f1af6_row9_col0\" class=\"data row9 col0\" >0.4313</td>\n",
              "      <td id=\"T_f1af6_row9_col1\" class=\"data row9 col1\" >0.8889</td>\n",
              "      <td id=\"T_f1af6_row9_col2\" class=\"data row9 col2\" >0.3900</td>\n",
              "      <td id=\"T_f1af6_row9_col3\" class=\"data row9 col3\" >0.3827</td>\n",
              "      <td id=\"T_f1af6_row9_col4\" class=\"data row9 col4\" >0.3669</td>\n",
              "      <td id=\"T_f1af6_row9_col5\" class=\"data row9 col5\" >0.8970</td>\n",
              "      <td id=\"T_f1af6_row9_col6\" class=\"data row9 col6\" >0.4874</td>\n",
              "      <td id=\"T_f1af6_row9_col7\" class=\"data row9 col7\" >0.8320</td>\n",
              "      <td id=\"T_f1af6_row9_col8\" class=\"data row9 col8\" >0.4949</td>\n",
              "      <td id=\"T_f1af6_row9_col9\" class=\"data row9 col9\" >0.4526</td>\n",
              "      <td id=\"T_f1af6_row9_col10\" class=\"data row9 col10\" >0.3355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row10\" class=\"row_heading level1 row10\" >mGPT</th>\n",
              "      <td id=\"T_f1af6_row10_col0\" class=\"data row10 col0\" >0.8941</td>\n",
              "      <td id=\"T_f1af6_row10_col1\" class=\"data row10 col1\" >0.9650</td>\n",
              "      <td id=\"T_f1af6_row10_col2\" class=\"data row10 col2\" >0.3552</td>\n",
              "      <td id=\"T_f1af6_row10_col3\" class=\"data row10 col3\" >0.9307</td>\n",
              "      <td id=\"T_f1af6_row10_col4\" class=\"data row10 col4\" >0.8397</td>\n",
              "      <td id=\"T_f1af6_row10_col5\" class=\"data row10 col5\" >0.9692</td>\n",
              "      <td id=\"T_f1af6_row10_col6\" class=\"data row10 col6\" >0.8974</td>\n",
              "      <td id=\"T_f1af6_row10_col7\" class=\"data row10 col7\" >0.9556</td>\n",
              "      <td id=\"T_f1af6_row10_col8\" class=\"data row10 col8\" >0.8942</td>\n",
              "      <td id=\"T_f1af6_row10_col9\" class=\"data row10 col9\" >0.9196</td>\n",
              "      <td id=\"T_f1af6_row10_col10\" class=\"data row10 col10\" >0.6781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row11\" class=\"row_heading level1 row11\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_f1af6_row11_col0\" class=\"data row11 col0\" >0.9281</td>\n",
              "      <td id=\"T_f1af6_row11_col1\" class=\"data row11 col1\" >0.8817</td>\n",
              "      <td id=\"T_f1af6_row11_col2\" class=\"data row11 col2\" >0.9666</td>\n",
              "      <td id=\"T_f1af6_row11_col3\" class=\"data row11 col3\" >0.9577</td>\n",
              "      <td id=\"T_f1af6_row11_col4\" class=\"data row11 col4\" >0.9368</td>\n",
              "      <td id=\"T_f1af6_row11_col5\" class=\"data row11 col5\" >0.9570</td>\n",
              "      <td id=\"T_f1af6_row11_col6\" class=\"data row11 col6\" >0.9109</td>\n",
              "      <td id=\"T_f1af6_row11_col7\" class=\"data row11 col7\" >0.9089</td>\n",
              "      <td id=\"T_f1af6_row11_col8\" class=\"data row11 col8\" >0.8890</td>\n",
              "      <td id=\"T_f1af6_row11_col9\" class=\"data row11 col9\" >0.9128</td>\n",
              "      <td id=\"T_f1af6_row11_col10\" class=\"data row11 col10\" >0.9416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row12\" class=\"row_heading level1 row12\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_f1af6_row12_col0\" class=\"data row12 col0\" >0.3307</td>\n",
              "      <td id=\"T_f1af6_row12_col1\" class=\"data row12 col1\" >0.8610</td>\n",
              "      <td id=\"T_f1af6_row12_col2\" class=\"data row12 col2\" >0.8449</td>\n",
              "      <td id=\"T_f1af6_row12_col3\" class=\"data row12 col3\" >0.8700</td>\n",
              "      <td id=\"T_f1af6_row12_col4\" class=\"data row12 col4\" >0.2073</td>\n",
              "      <td id=\"T_f1af6_row12_col5\" class=\"data row12 col5\" >0.9153</td>\n",
              "      <td id=\"T_f1af6_row12_col6\" class=\"data row12 col6\" >0.8743</td>\n",
              "      <td id=\"T_f1af6_row12_col7\" class=\"data row12 col7\" >0.8392</td>\n",
              "      <td id=\"T_f1af6_row12_col8\" class=\"data row12 col8\" >0.3333</td>\n",
              "      <td id=\"T_f1af6_row12_col9\" class=\"data row12 col9\" >0.3311</td>\n",
              "      <td id=\"T_f1af6_row12_col10\" class=\"data row12 col10\" >0.7601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row13\" class=\"row_heading level1 row13\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_f1af6_row13_col0\" class=\"data row13 col0\" >0.7671</td>\n",
              "      <td id=\"T_f1af6_row13_col1\" class=\"data row13 col1\" >0.9550</td>\n",
              "      <td id=\"T_f1af6_row13_col2\" class=\"data row13 col2\" >0.8609</td>\n",
              "      <td id=\"T_f1af6_row13_col3\" class=\"data row13 col3\" >0.8539</td>\n",
              "      <td id=\"T_f1af6_row13_col4\" class=\"data row13 col4\" >0.9801</td>\n",
              "      <td id=\"T_f1af6_row13_col5\" class=\"data row13 col5\" >0.9743</td>\n",
              "      <td id=\"T_f1af6_row13_col6\" class=\"data row13 col6\" >0.9616</td>\n",
              "      <td id=\"T_f1af6_row13_col7\" class=\"data row13 col7\" >0.9676</td>\n",
              "      <td id=\"T_f1af6_row13_col8\" class=\"data row13 col8\" >0.9093</td>\n",
              "      <td id=\"T_f1af6_row13_col9\" class=\"data row13 col9\" >0.7368</td>\n",
              "      <td id=\"T_f1af6_row13_col10\" class=\"data row13 col10\" >0.8504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level0_row14\" class=\"row_heading level0 row14\" rowspan=\"7\">gpt-4</th>\n",
              "      <th id=\"T_f1af6_level1_row14\" class=\"row_heading level1 row14\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_f1af6_row14_col0\" class=\"data row14 col0\" >0.8764</td>\n",
              "      <td id=\"T_f1af6_row14_col1\" class=\"data row14 col1\" >0.8852</td>\n",
              "      <td id=\"T_f1af6_row14_col2\" class=\"data row14 col2\" >0.9300</td>\n",
              "      <td id=\"T_f1af6_row14_col3\" class=\"data row14 col3\" >0.8808</td>\n",
              "      <td id=\"T_f1af6_row14_col4\" class=\"data row14 col4\" >0.7306</td>\n",
              "      <td id=\"T_f1af6_row14_col5\" class=\"data row14 col5\" >0.9277</td>\n",
              "      <td id=\"T_f1af6_row14_col6\" class=\"data row14 col6\" >0.9075</td>\n",
              "      <td id=\"T_f1af6_row14_col7\" class=\"data row14 col7\" >0.8679</td>\n",
              "      <td id=\"T_f1af6_row14_col8\" class=\"data row14 col8\" >0.8916</td>\n",
              "      <td id=\"T_f1af6_row14_col9\" class=\"data row14 col9\" >0.8792</td>\n",
              "      <td id=\"T_f1af6_row14_col10\" class=\"data row14 col10\" >0.8799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row15\" class=\"row_heading level1 row15\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_f1af6_row15_col0\" class=\"data row15 col0\" >0.3860</td>\n",
              "      <td id=\"T_f1af6_row15_col1\" class=\"data row15 col1\" >0.8103</td>\n",
              "      <td id=\"T_f1af6_row15_col2\" class=\"data row15 col2\" >0.3333</td>\n",
              "      <td id=\"T_f1af6_row15_col3\" class=\"data row15 col3\" >0.3303</td>\n",
              "      <td id=\"T_f1af6_row15_col4\" class=\"data row15 col4\" >0.3373</td>\n",
              "      <td id=\"T_f1af6_row15_col5\" class=\"data row15 col5\" >0.9571</td>\n",
              "      <td id=\"T_f1af6_row15_col6\" class=\"data row15 col6\" >0.5424</td>\n",
              "      <td id=\"T_f1af6_row15_col7\" class=\"data row15 col7\" >0.9506</td>\n",
              "      <td id=\"T_f1af6_row15_col8\" class=\"data row15 col8\" >0.3798</td>\n",
              "      <td id=\"T_f1af6_row15_col9\" class=\"data row15 col9\" >0.3670</td>\n",
              "      <td id=\"T_f1af6_row15_col10\" class=\"data row15 col10\" >0.3407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row16\" class=\"row_heading level1 row16\" >gpt2-medium</th>\n",
              "      <td id=\"T_f1af6_row16_col0\" class=\"data row16 col0\" >0.4260</td>\n",
              "      <td id=\"T_f1af6_row16_col1\" class=\"data row16 col1\" >0.7906</td>\n",
              "      <td id=\"T_f1af6_row16_col2\" class=\"data row16 col2\" >0.4326</td>\n",
              "      <td id=\"T_f1af6_row16_col3\" class=\"data row16 col3\" >0.4318</td>\n",
              "      <td id=\"T_f1af6_row16_col4\" class=\"data row16 col4\" >0.3362</td>\n",
              "      <td id=\"T_f1af6_row16_col5\" class=\"data row16 col5\" >0.9143</td>\n",
              "      <td id=\"T_f1af6_row16_col6\" class=\"data row16 col6\" >0.4571</td>\n",
              "      <td id=\"T_f1af6_row16_col7\" class=\"data row16 col7\" >0.7844</td>\n",
              "      <td id=\"T_f1af6_row16_col8\" class=\"data row16 col8\" >0.4378</td>\n",
              "      <td id=\"T_f1af6_row16_col9\" class=\"data row16 col9\" >0.4042</td>\n",
              "      <td id=\"T_f1af6_row16_col10\" class=\"data row16 col10\" >0.3399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row17\" class=\"row_heading level1 row17\" >mGPT</th>\n",
              "      <td id=\"T_f1af6_row17_col0\" class=\"data row17 col0\" >0.8491</td>\n",
              "      <td id=\"T_f1af6_row17_col1\" class=\"data row17 col1\" >0.8705</td>\n",
              "      <td id=\"T_f1af6_row17_col2\" class=\"data row17 col2\" >0.3370</td>\n",
              "      <td id=\"T_f1af6_row17_col3\" class=\"data row17 col3\" >0.9187</td>\n",
              "      <td id=\"T_f1af6_row17_col4\" class=\"data row17 col4\" >0.9476</td>\n",
              "      <td id=\"T_f1af6_row17_col5\" class=\"data row17 col5\" >0.9829</td>\n",
              "      <td id=\"T_f1af6_row17_col6\" class=\"data row17 col6\" >0.9245</td>\n",
              "      <td id=\"T_f1af6_row17_col7\" class=\"data row17 col7\" >0.9830</td>\n",
              "      <td id=\"T_f1af6_row17_col8\" class=\"data row17 col8\" >0.9016</td>\n",
              "      <td id=\"T_f1af6_row17_col9\" class=\"data row17 col9\" >0.8454</td>\n",
              "      <td id=\"T_f1af6_row17_col10\" class=\"data row17 col10\" >0.6649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row18\" class=\"row_heading level1 row18\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_f1af6_row18_col0\" class=\"data row18 col0\" >0.9331</td>\n",
              "      <td id=\"T_f1af6_row18_col1\" class=\"data row18 col1\" >0.8897</td>\n",
              "      <td id=\"T_f1af6_row18_col2\" class=\"data row18 col2\" >0.9549</td>\n",
              "      <td id=\"T_f1af6_row18_col3\" class=\"data row18 col3\" >0.9356</td>\n",
              "      <td id=\"T_f1af6_row18_col4\" class=\"data row18 col4\" >0.8826</td>\n",
              "      <td id=\"T_f1af6_row18_col5\" class=\"data row18 col5\" >0.9195</td>\n",
              "      <td id=\"T_f1af6_row18_col6\" class=\"data row18 col6\" >0.9331</td>\n",
              "      <td id=\"T_f1af6_row18_col7\" class=\"data row18 col7\" >0.9027</td>\n",
              "      <td id=\"T_f1af6_row18_col8\" class=\"data row18 col8\" >0.8545</td>\n",
              "      <td id=\"T_f1af6_row18_col9\" class=\"data row18 col9\" >0.9096</td>\n",
              "      <td id=\"T_f1af6_row18_col10\" class=\"data row18 col10\" >0.9045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row19\" class=\"row_heading level1 row19\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_f1af6_row19_col0\" class=\"data row19 col0\" >0.3322</td>\n",
              "      <td id=\"T_f1af6_row19_col1\" class=\"data row19 col1\" >0.9128</td>\n",
              "      <td id=\"T_f1af6_row19_col2\" class=\"data row19 col2\" >0.8091</td>\n",
              "      <td id=\"T_f1af6_row19_col3\" class=\"data row19 col3\" >0.8765</td>\n",
              "      <td id=\"T_f1af6_row19_col4\" class=\"data row19 col4\" >0.3602</td>\n",
              "      <td id=\"T_f1af6_row19_col5\" class=\"data row19 col5\" >0.9293</td>\n",
              "      <td id=\"T_f1af6_row19_col6\" class=\"data row19 col6\" >0.8449</td>\n",
              "      <td id=\"T_f1af6_row19_col7\" class=\"data row19 col7\" >0.8768</td>\n",
              "      <td id=\"T_f1af6_row19_col8\" class=\"data row19 col8\" >0.3370</td>\n",
              "      <td id=\"T_f1af6_row19_col9\" class=\"data row19 col9\" >0.3318</td>\n",
              "      <td id=\"T_f1af6_row19_col10\" class=\"data row19 col10\" >0.6283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row20\" class=\"row_heading level1 row20\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_f1af6_row20_col0\" class=\"data row20 col0\" >0.5124</td>\n",
              "      <td id=\"T_f1af6_row20_col1\" class=\"data row20 col1\" >0.9599</td>\n",
              "      <td id=\"T_f1af6_row20_col2\" class=\"data row20 col2\" >0.5688</td>\n",
              "      <td id=\"T_f1af6_row20_col3\" class=\"data row20 col3\" >0.8011</td>\n",
              "      <td id=\"T_f1af6_row20_col4\" class=\"data row20 col4\" >0.9729</td>\n",
              "      <td id=\"T_f1af6_row20_col5\" class=\"data row20 col5\" >0.9812</td>\n",
              "      <td id=\"T_f1af6_row20_col6\" class=\"data row20 col6\" >0.9616</td>\n",
              "      <td id=\"T_f1af6_row20_col7\" class=\"data row20 col7\" >0.9676</td>\n",
              "      <td id=\"T_f1af6_row20_col8\" class=\"data row20 col8\" >0.5987</td>\n",
              "      <td id=\"T_f1af6_row20_col9\" class=\"data row20 col9\" >0.3994</td>\n",
              "      <td id=\"T_f1af6_row20_col10\" class=\"data row20 col10\" >0.4001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level0_row21\" class=\"row_heading level0 row21\" rowspan=\"7\">llama-65b</th>\n",
              "      <th id=\"T_f1af6_level1_row21\" class=\"row_heading level1 row21\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_f1af6_row21_col0\" class=\"data row21 col0\" >0.7956</td>\n",
              "      <td id=\"T_f1af6_row21_col1\" class=\"data row21 col1\" >0.9177</td>\n",
              "      <td id=\"T_f1af6_row21_col2\" class=\"data row21 col2\" >0.9645</td>\n",
              "      <td id=\"T_f1af6_row21_col3\" class=\"data row21 col3\" >0.7711</td>\n",
              "      <td id=\"T_f1af6_row21_col4\" class=\"data row21 col4\" >0.3611</td>\n",
              "      <td id=\"T_f1af6_row21_col5\" class=\"data row21 col5\" >0.9914</td>\n",
              "      <td id=\"T_f1af6_row21_col6\" class=\"data row21 col6\" >0.6665</td>\n",
              "      <td id=\"T_f1af6_row21_col7\" class=\"data row21 col7\" >0.9794</td>\n",
              "      <td id=\"T_f1af6_row21_col8\" class=\"data row21 col8\" >0.8227</td>\n",
              "      <td id=\"T_f1af6_row21_col9\" class=\"data row21 col9\" >0.9290</td>\n",
              "      <td id=\"T_f1af6_row21_col10\" class=\"data row21 col10\" >0.6396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row22\" class=\"row_heading level1 row22\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_f1af6_row22_col0\" class=\"data row22 col0\" >0.4887</td>\n",
              "      <td id=\"T_f1af6_row22_col1\" class=\"data row22 col1\" >0.9026</td>\n",
              "      <td id=\"T_f1af6_row22_col2\" class=\"data row22 col2\" >0.9272</td>\n",
              "      <td id=\"T_f1af6_row22_col3\" class=\"data row22 col3\" >0.9214</td>\n",
              "      <td id=\"T_f1af6_row22_col4\" class=\"data row22 col4\" >0.3325</td>\n",
              "      <td id=\"T_f1af6_row22_col5\" class=\"data row22 col5\" >0.9741</td>\n",
              "      <td id=\"T_f1af6_row22_col6\" class=\"data row22 col6\" >0.8515</td>\n",
              "      <td id=\"T_f1af6_row22_col7\" class=\"data row22 col7\" >0.9691</td>\n",
              "      <td id=\"T_f1af6_row22_col8\" class=\"data row22 col8\" >0.3620</td>\n",
              "      <td id=\"T_f1af6_row22_col9\" class=\"data row22 col9\" >0.4606</td>\n",
              "      <td id=\"T_f1af6_row22_col10\" class=\"data row22 col10\" >0.3318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row23\" class=\"row_heading level1 row23\" >gpt2-medium</th>\n",
              "      <td id=\"T_f1af6_row23_col0\" class=\"data row23 col0\" >0.3680</td>\n",
              "      <td id=\"T_f1af6_row23_col1\" class=\"data row23 col1\" >0.9365</td>\n",
              "      <td id=\"T_f1af6_row23_col2\" class=\"data row23 col2\" >0.8738</td>\n",
              "      <td id=\"T_f1af6_row23_col3\" class=\"data row23 col3\" >0.9079</td>\n",
              "      <td id=\"T_f1af6_row23_col4\" class=\"data row23 col4\" >0.3990</td>\n",
              "      <td id=\"T_f1af6_row23_col5\" class=\"data row23 col5\" >0.9621</td>\n",
              "      <td id=\"T_f1af6_row23_col6\" class=\"data row23 col6\" >0.8744</td>\n",
              "      <td id=\"T_f1af6_row23_col7\" class=\"data row23 col7\" >0.9398</td>\n",
              "      <td id=\"T_f1af6_row23_col8\" class=\"data row23 col8\" >0.4793</td>\n",
              "      <td id=\"T_f1af6_row23_col9\" class=\"data row23 col9\" >0.5608</td>\n",
              "      <td id=\"T_f1af6_row23_col10\" class=\"data row23 col10\" >0.5344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row24\" class=\"row_heading level1 row24\" >mGPT</th>\n",
              "      <td id=\"T_f1af6_row24_col0\" class=\"data row24 col0\" >0.5912</td>\n",
              "      <td id=\"T_f1af6_row24_col1\" class=\"data row24 col1\" >0.6899</td>\n",
              "      <td id=\"T_f1af6_row24_col2\" class=\"data row24 col2\" >0.8606</td>\n",
              "      <td id=\"T_f1af6_row24_col3\" class=\"data row24 col3\" >0.7540</td>\n",
              "      <td id=\"T_f1af6_row24_col4\" class=\"data row24 col4\" >0.3505</td>\n",
              "      <td id=\"T_f1af6_row24_col5\" class=\"data row24 col5\" >0.9673</td>\n",
              "      <td id=\"T_f1af6_row24_col6\" class=\"data row24 col6\" >0.3604</td>\n",
              "      <td id=\"T_f1af6_row24_col7\" class=\"data row24 col7\" >0.9501</td>\n",
              "      <td id=\"T_f1af6_row24_col8\" class=\"data row24 col8\" >0.5401</td>\n",
              "      <td id=\"T_f1af6_row24_col9\" class=\"data row24 col9\" >0.8348</td>\n",
              "      <td id=\"T_f1af6_row24_col10\" class=\"data row24 col10\" >0.3731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row25\" class=\"row_heading level1 row25\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_f1af6_row25_col0\" class=\"data row25 col0\" >0.9135</td>\n",
              "      <td id=\"T_f1af6_row25_col1\" class=\"data row25 col1\" >0.9262</td>\n",
              "      <td id=\"T_f1af6_row25_col2\" class=\"data row25 col2\" >0.9882</td>\n",
              "      <td id=\"T_f1af6_row25_col3\" class=\"data row25 col3\" >0.9608</td>\n",
              "      <td id=\"T_f1af6_row25_col4\" class=\"data row25 col4\" >0.6260</td>\n",
              "      <td id=\"T_f1af6_row25_col5\" class=\"data row25 col5\" >0.9862</td>\n",
              "      <td id=\"T_f1af6_row25_col6\" class=\"data row25 col6\" >0.6591</td>\n",
              "      <td id=\"T_f1af6_row25_col7\" class=\"data row25 col7\" >0.9777</td>\n",
              "      <td id=\"T_f1af6_row25_col8\" class=\"data row25 col8\" >0.9765</td>\n",
              "      <td id=\"T_f1af6_row25_col9\" class=\"data row25 col9\" >0.9848</td>\n",
              "      <td id=\"T_f1af6_row25_col10\" class=\"data row25 col10\" >0.3539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row26\" class=\"row_heading level1 row26\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_f1af6_row26_col0\" class=\"data row26 col0\" >0.3296</td>\n",
              "      <td id=\"T_f1af6_row26_col1\" class=\"data row26 col1\" >0.8944</td>\n",
              "      <td id=\"T_f1af6_row26_col2\" class=\"data row26 col2\" >0.7558</td>\n",
              "      <td id=\"T_f1af6_row26_col3\" class=\"data row26 col3\" >0.7449</td>\n",
              "      <td id=\"T_f1af6_row26_col4\" class=\"data row26 col4\" >0.3357</td>\n",
              "      <td id=\"T_f1af6_row26_col5\" class=\"data row26 col5\" >0.9517</td>\n",
              "      <td id=\"T_f1af6_row26_col6\" class=\"data row26 col6\" >0.8188</td>\n",
              "      <td id=\"T_f1af6_row26_col7\" class=\"data row26 col7\" >0.8808</td>\n",
              "      <td id=\"T_f1af6_row26_col8\" class=\"data row26 col8\" >0.3392</td>\n",
              "      <td id=\"T_f1af6_row26_col9\" class=\"data row26 col9\" >0.3617</td>\n",
              "      <td id=\"T_f1af6_row26_col10\" class=\"data row26 col10\" >0.3296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row27\" class=\"row_heading level1 row27\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_f1af6_row27_col0\" class=\"data row27 col0\" >0.8461</td>\n",
              "      <td id=\"T_f1af6_row27_col1\" class=\"data row27 col1\" >0.8307</td>\n",
              "      <td id=\"T_f1af6_row27_col2\" class=\"data row27 col2\" >0.9831</td>\n",
              "      <td id=\"T_f1af6_row27_col3\" class=\"data row27 col3\" >0.9557</td>\n",
              "      <td id=\"T_f1af6_row27_col4\" class=\"data row27 col4\" >0.6680</td>\n",
              "      <td id=\"T_f1af6_row27_col5\" class=\"data row27 col5\" >0.9879</td>\n",
              "      <td id=\"T_f1af6_row27_col6\" class=\"data row27 col6\" >0.4583</td>\n",
              "      <td id=\"T_f1af6_row27_col7\" class=\"data row27 col7\" >0.9725</td>\n",
              "      <td id=\"T_f1af6_row27_col8\" class=\"data row27 col8\" >0.9412</td>\n",
              "      <td id=\"T_f1af6_row27_col9\" class=\"data row27 col9\" >0.9680</td>\n",
              "      <td id=\"T_f1af6_row27_col10\" class=\"data row27 col10\" >0.6842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level0_row28\" class=\"row_heading level0 row28\" rowspan=\"7\">opt-66b</th>\n",
              "      <th id=\"T_f1af6_level1_row28\" class=\"row_heading level1 row28\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_f1af6_row28_col0\" class=\"data row28 col0\" >0.7378</td>\n",
              "      <td id=\"T_f1af6_row28_col1\" class=\"data row28 col1\" >0.7608</td>\n",
              "      <td id=\"T_f1af6_row28_col2\" class=\"data row28 col2\" >0.6443</td>\n",
              "      <td id=\"T_f1af6_row28_col3\" class=\"data row28 col3\" >0.5872</td>\n",
              "      <td id=\"T_f1af6_row28_col4\" class=\"data row28 col4\" >0.4705</td>\n",
              "      <td id=\"T_f1af6_row28_col5\" class=\"data row28 col5\" >0.7658</td>\n",
              "      <td id=\"T_f1af6_row28_col6\" class=\"data row28 col6\" >0.7038</td>\n",
              "      <td id=\"T_f1af6_row28_col7\" class=\"data row28 col7\" >0.7662</td>\n",
              "      <td id=\"T_f1af6_row28_col8\" class=\"data row28 col8\" >0.6748</td>\n",
              "      <td id=\"T_f1af6_row28_col9\" class=\"data row28 col9\" >0.7543</td>\n",
              "      <td id=\"T_f1af6_row28_col10\" class=\"data row28 col10\" >0.8225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row29\" class=\"row_heading level1 row29\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_f1af6_row29_col0\" class=\"data row29 col0\" >0.4594</td>\n",
              "      <td id=\"T_f1af6_row29_col1\" class=\"data row29 col1\" >0.7202</td>\n",
              "      <td id=\"T_f1af6_row29_col2\" class=\"data row29 col2\" >0.6132</td>\n",
              "      <td id=\"T_f1af6_row29_col3\" class=\"data row29 col3\" >0.6954</td>\n",
              "      <td id=\"T_f1af6_row29_col4\" class=\"data row29 col4\" >0.3333</td>\n",
              "      <td id=\"T_f1af6_row29_col5\" class=\"data row29 col5\" >0.7692</td>\n",
              "      <td id=\"T_f1af6_row29_col6\" class=\"data row29 col6\" >0.6844</td>\n",
              "      <td id=\"T_f1af6_row29_col7\" class=\"data row29 col7\" >0.7265</td>\n",
              "      <td id=\"T_f1af6_row29_col8\" class=\"data row29 col8\" >0.4311</td>\n",
              "      <td id=\"T_f1af6_row29_col9\" class=\"data row29 col9\" >0.5203</td>\n",
              "      <td id=\"T_f1af6_row29_col10\" class=\"data row29 col10\" >0.3311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row30\" class=\"row_heading level1 row30\" >gpt2-medium</th>\n",
              "      <td id=\"T_f1af6_row30_col0\" class=\"data row30 col0\" >0.4455</td>\n",
              "      <td id=\"T_f1af6_row30_col1\" class=\"data row30 col1\" >0.7310</td>\n",
              "      <td id=\"T_f1af6_row30_col2\" class=\"data row30 col2\" >0.4703</td>\n",
              "      <td id=\"T_f1af6_row30_col3\" class=\"data row30 col3\" >0.5913</td>\n",
              "      <td id=\"T_f1af6_row30_col4\" class=\"data row30 col4\" >0.4524</td>\n",
              "      <td id=\"T_f1af6_row30_col5\" class=\"data row30 col5\" >0.7703</td>\n",
              "      <td id=\"T_f1af6_row30_col6\" class=\"data row30 col6\" >0.5466</td>\n",
              "      <td id=\"T_f1af6_row30_col7\" class=\"data row30 col7\" >0.5972</td>\n",
              "      <td id=\"T_f1af6_row30_col8\" class=\"data row30 col8\" >0.5993</td>\n",
              "      <td id=\"T_f1af6_row30_col9\" class=\"data row30 col9\" >0.6464</td>\n",
              "      <td id=\"T_f1af6_row30_col10\" class=\"data row30 col10\" >0.4690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row31\" class=\"row_heading level1 row31\" >mGPT</th>\n",
              "      <td id=\"T_f1af6_row31_col0\" class=\"data row31 col0\" >0.3906</td>\n",
              "      <td id=\"T_f1af6_row31_col1\" class=\"data row31 col1\" >0.7297</td>\n",
              "      <td id=\"T_f1af6_row31_col2\" class=\"data row31 col2\" >0.4550</td>\n",
              "      <td id=\"T_f1af6_row31_col3\" class=\"data row31 col3\" >0.6957</td>\n",
              "      <td id=\"T_f1af6_row31_col4\" class=\"data row31 col4\" >0.3722</td>\n",
              "      <td id=\"T_f1af6_row31_col5\" class=\"data row31 col5\" >0.7992</td>\n",
              "      <td id=\"T_f1af6_row31_col6\" class=\"data row31 col6\" >0.3866</td>\n",
              "      <td id=\"T_f1af6_row31_col7\" class=\"data row31 col7\" >0.5949</td>\n",
              "      <td id=\"T_f1af6_row31_col8\" class=\"data row31 col8\" >0.5713</td>\n",
              "      <td id=\"T_f1af6_row31_col9\" class=\"data row31 col9\" >0.6426</td>\n",
              "      <td id=\"T_f1af6_row31_col10\" class=\"data row31 col10\" >0.4554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row32\" class=\"row_heading level1 row32\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_f1af6_row32_col0\" class=\"data row32 col0\" >0.9139</td>\n",
              "      <td id=\"T_f1af6_row32_col1\" class=\"data row32 col1\" >0.8500</td>\n",
              "      <td id=\"T_f1af6_row32_col2\" class=\"data row32 col2\" >0.9883</td>\n",
              "      <td id=\"T_f1af6_row32_col3\" class=\"data row32 col3\" >0.9493</td>\n",
              "      <td id=\"T_f1af6_row32_col4\" class=\"data row32 col4\" >0.6240</td>\n",
              "      <td id=\"T_f1af6_row32_col5\" class=\"data row32 col5\" >0.8762</td>\n",
              "      <td id=\"T_f1af6_row32_col6\" class=\"data row32 col6\" >0.8538</td>\n",
              "      <td id=\"T_f1af6_row32_col7\" class=\"data row32 col7\" >0.7872</td>\n",
              "      <td id=\"T_f1af6_row32_col8\" class=\"data row32 col8\" >0.9696</td>\n",
              "      <td id=\"T_f1af6_row32_col9\" class=\"data row32 col9\" >0.9627</td>\n",
              "      <td id=\"T_f1af6_row32_col10\" class=\"data row32 col10\" >0.6997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row33\" class=\"row_heading level1 row33\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_f1af6_row33_col0\" class=\"data row33 col0\" >0.4668</td>\n",
              "      <td id=\"T_f1af6_row33_col1\" class=\"data row33 col1\" >0.6129</td>\n",
              "      <td id=\"T_f1af6_row33_col2\" class=\"data row33 col2\" >0.8550</td>\n",
              "      <td id=\"T_f1af6_row33_col3\" class=\"data row33 col3\" >0.6502</td>\n",
              "      <td id=\"T_f1af6_row33_col4\" class=\"data row33 col4\" >0.3355</td>\n",
              "      <td id=\"T_f1af6_row33_col5\" class=\"data row33 col5\" >0.8141</td>\n",
              "      <td id=\"T_f1af6_row33_col6\" class=\"data row33 col6\" >0.8564</td>\n",
              "      <td id=\"T_f1af6_row33_col7\" class=\"data row33 col7\" >0.7337</td>\n",
              "      <td id=\"T_f1af6_row33_col8\" class=\"data row33 col8\" >0.5548</td>\n",
              "      <td id=\"T_f1af6_row33_col9\" class=\"data row33 col9\" >0.5646</td>\n",
              "      <td id=\"T_f1af6_row33_col10\" class=\"data row33 col10\" >0.5596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row34\" class=\"row_heading level1 row34\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_f1af6_row34_col0\" class=\"data row34 col0\" >0.6058</td>\n",
              "      <td id=\"T_f1af6_row34_col1\" class=\"data row34 col1\" >0.7452</td>\n",
              "      <td id=\"T_f1af6_row34_col2\" class=\"data row34 col2\" >0.8765</td>\n",
              "      <td id=\"T_f1af6_row34_col3\" class=\"data row34 col3\" >0.8104</td>\n",
              "      <td id=\"T_f1af6_row34_col4\" class=\"data row34 col4\" >0.4443</td>\n",
              "      <td id=\"T_f1af6_row34_col5\" class=\"data row34 col5\" >0.8123</td>\n",
              "      <td id=\"T_f1af6_row34_col6\" class=\"data row34 col6\" >0.4702</td>\n",
              "      <td id=\"T_f1af6_row34_col7\" class=\"data row34 col7\" >0.6335</td>\n",
              "      <td id=\"T_f1af6_row34_col8\" class=\"data row34 col8\" >0.8106</td>\n",
              "      <td id=\"T_f1af6_row34_col9\" class=\"data row34 col9\" >0.8408</td>\n",
              "      <td id=\"T_f1af6_row34_col10\" class=\"data row34 col10\" >0.5381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level0_row35\" class=\"row_heading level0 row35\" rowspan=\"7\">opt-iml-max-1.3b</th>\n",
              "      <th id=\"T_f1af6_level1_row35\" class=\"row_heading level1 row35\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_f1af6_row35_col0\" class=\"data row35 col0\" >0.9537</td>\n",
              "      <td id=\"T_f1af6_row35_col1\" class=\"data row35 col1\" >0.9697</td>\n",
              "      <td id=\"T_f1af6_row35_col2\" class=\"data row35 col2\" >0.9632</td>\n",
              "      <td id=\"T_f1af6_row35_col3\" class=\"data row35 col3\" >0.8676</td>\n",
              "      <td id=\"T_f1af6_row35_col4\" class=\"data row35 col4\" >0.6840</td>\n",
              "      <td id=\"T_f1af6_row35_col5\" class=\"data row35 col5\" >0.9482</td>\n",
              "      <td id=\"T_f1af6_row35_col6\" class=\"data row35 col6\" >0.9699</td>\n",
              "      <td id=\"T_f1af6_row35_col7\" class=\"data row35 col7\" >0.8820</td>\n",
              "      <td id=\"T_f1af6_row35_col8\" class=\"data row35 col8\" >0.9027</td>\n",
              "      <td id=\"T_f1af6_row35_col9\" class=\"data row35 col9\" >0.9707</td>\n",
              "      <td id=\"T_f1af6_row35_col10\" class=\"data row35 col10\" >0.9528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row36\" class=\"row_heading level1 row36\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_f1af6_row36_col0\" class=\"data row36 col0\" >0.3527</td>\n",
              "      <td id=\"T_f1af6_row36_col1\" class=\"data row36 col1\" >0.9681</td>\n",
              "      <td id=\"T_f1af6_row36_col2\" class=\"data row36 col2\" >0.8392</td>\n",
              "      <td id=\"T_f1af6_row36_col3\" class=\"data row36 col3\" >0.8930</td>\n",
              "      <td id=\"T_f1af6_row36_col4\" class=\"data row36 col4\" >0.3333</td>\n",
              "      <td id=\"T_f1af6_row36_col5\" class=\"data row36 col5\" >0.9515</td>\n",
              "      <td id=\"T_f1af6_row36_col6\" class=\"data row36 col6\" >0.8798</td>\n",
              "      <td id=\"T_f1af6_row36_col7\" class=\"data row36 col7\" >0.9189</td>\n",
              "      <td id=\"T_f1af6_row36_col8\" class=\"data row36 col8\" >0.3280</td>\n",
              "      <td id=\"T_f1af6_row36_col9\" class=\"data row36 col9\" >0.3305</td>\n",
              "      <td id=\"T_f1af6_row36_col10\" class=\"data row36 col10\" >0.3307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row37\" class=\"row_heading level1 row37\" >gpt2-medium</th>\n",
              "      <td id=\"T_f1af6_row37_col0\" class=\"data row37 col0\" >0.6084</td>\n",
              "      <td id=\"T_f1af6_row37_col1\" class=\"data row37 col1\" >0.9049</td>\n",
              "      <td id=\"T_f1af6_row37_col2\" class=\"data row37 col2\" >0.7598</td>\n",
              "      <td id=\"T_f1af6_row37_col3\" class=\"data row37 col3\" >0.5085</td>\n",
              "      <td id=\"T_f1af6_row37_col4\" class=\"data row37 col4\" >0.4730</td>\n",
              "      <td id=\"T_f1af6_row37_col5\" class=\"data row37 col5\" >0.9100</td>\n",
              "      <td id=\"T_f1af6_row37_col6\" class=\"data row37 col6\" >0.6565</td>\n",
              "      <td id=\"T_f1af6_row37_col7\" class=\"data row37 col7\" >0.6696</td>\n",
              "      <td id=\"T_f1af6_row37_col8\" class=\"data row37 col8\" >0.6559</td>\n",
              "      <td id=\"T_f1af6_row37_col9\" class=\"data row37 col9\" >0.7088</td>\n",
              "      <td id=\"T_f1af6_row37_col10\" class=\"data row37 col10\" >0.3941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row38\" class=\"row_heading level1 row38\" >mGPT</th>\n",
              "      <td id=\"T_f1af6_row38_col0\" class=\"data row38 col0\" >0.9434</td>\n",
              "      <td id=\"T_f1af6_row38_col1\" class=\"data row38 col1\" >0.9613</td>\n",
              "      <td id=\"T_f1af6_row38_col2\" class=\"data row38 col2\" >0.9210</td>\n",
              "      <td id=\"T_f1af6_row38_col3\" class=\"data row38 col3\" >0.9932</td>\n",
              "      <td id=\"T_f1af6_row38_col4\" class=\"data row38 col4\" >0.4045</td>\n",
              "      <td id=\"T_f1af6_row38_col5\" class=\"data row38 col5\" >0.9879</td>\n",
              "      <td id=\"T_f1af6_row38_col6\" class=\"data row38 col6\" >0.8410</td>\n",
              "      <td id=\"T_f1af6_row38_col7\" class=\"data row38 col7\" >0.9810</td>\n",
              "      <td id=\"T_f1af6_row38_col8\" class=\"data row38 col8\" >0.8970</td>\n",
              "      <td id=\"T_f1af6_row38_col9\" class=\"data row38 col9\" >0.9845</td>\n",
              "      <td id=\"T_f1af6_row38_col10\" class=\"data row38 col10\" >0.5838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row39\" class=\"row_heading level1 row39\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_f1af6_row39_col0\" class=\"data row39 col0\" >0.9931</td>\n",
              "      <td id=\"T_f1af6_row39_col1\" class=\"data row39 col1\" >0.9866</td>\n",
              "      <td id=\"T_f1af6_row39_col2\" class=\"data row39 col2\" >0.9933</td>\n",
              "      <td id=\"T_f1af6_row39_col3\" class=\"data row39 col3\" >0.8710</td>\n",
              "      <td id=\"T_f1af6_row39_col4\" class=\"data row39 col4\" >0.5865</td>\n",
              "      <td id=\"T_f1af6_row39_col5\" class=\"data row39 col5\" >0.9827</td>\n",
              "      <td id=\"T_f1af6_row39_col6\" class=\"data row39 col6\" >0.9329</td>\n",
              "      <td id=\"T_f1af6_row39_col7\" class=\"data row39 col7\" >0.9828</td>\n",
              "      <td id=\"T_f1af6_row39_col8\" class=\"data row39 col8\" >0.9949</td>\n",
              "      <td id=\"T_f1af6_row39_col9\" class=\"data row39 col9\" >0.9931</td>\n",
              "      <td id=\"T_f1af6_row39_col10\" class=\"data row39 col10\" >0.8744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row40\" class=\"row_heading level1 row40\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_f1af6_row40_col0\" class=\"data row40 col0\" >0.4128</td>\n",
              "      <td id=\"T_f1af6_row40_col1\" class=\"data row40 col1\" >0.9646</td>\n",
              "      <td id=\"T_f1af6_row40_col2\" class=\"data row40 col2\" >0.9498</td>\n",
              "      <td id=\"T_f1af6_row40_col3\" class=\"data row40 col3\" >0.7265</td>\n",
              "      <td id=\"T_f1af6_row40_col4\" class=\"data row40 col4\" >0.2160</td>\n",
              "      <td id=\"T_f1af6_row40_col5\" class=\"data row40 col5\" >0.9602</td>\n",
              "      <td id=\"T_f1af6_row40_col6\" class=\"data row40 col6\" >0.9381</td>\n",
              "      <td id=\"T_f1af6_row40_col7\" class=\"data row40 col7\" >0.9103</td>\n",
              "      <td id=\"T_f1af6_row40_col8\" class=\"data row40 col8\" >0.5456</td>\n",
              "      <td id=\"T_f1af6_row40_col9\" class=\"data row40 col9\" >0.6839</td>\n",
              "      <td id=\"T_f1af6_row40_col10\" class=\"data row40 col10\" >0.5212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row41\" class=\"row_heading level1 row41\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_f1af6_row41_col0\" class=\"data row41 col0\" >0.9794</td>\n",
              "      <td id=\"T_f1af6_row41_col1\" class=\"data row41 col1\" >0.9916</td>\n",
              "      <td id=\"T_f1af6_row41_col2\" class=\"data row41 col2\" >0.8583</td>\n",
              "      <td id=\"T_f1af6_row41_col3\" class=\"data row41 col3\" >0.9881</td>\n",
              "      <td id=\"T_f1af6_row41_col4\" class=\"data row41 col4\" >0.6192</td>\n",
              "      <td id=\"T_f1af6_row41_col5\" class=\"data row41 col5\" >0.9810</td>\n",
              "      <td id=\"T_f1af6_row41_col6\" class=\"data row41 col6\" >0.9799</td>\n",
              "      <td id=\"T_f1af6_row41_col7\" class=\"data row41 col7\" >0.9793</td>\n",
              "      <td id=\"T_f1af6_row41_col8\" class=\"data row41 col8\" >0.7578</td>\n",
              "      <td id=\"T_f1af6_row41_col9\" class=\"data row41 col9\" >0.7060</td>\n",
              "      <td id=\"T_f1af6_row41_col10\" class=\"data row41 col10\" >0.8187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level0_row42\" class=\"row_heading level0 row42\" rowspan=\"7\">text-davinci-003</th>\n",
              "      <th id=\"T_f1af6_level1_row42\" class=\"row_heading level1 row42\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_f1af6_row42_col0\" class=\"data row42 col0\" >0.7319</td>\n",
              "      <td id=\"T_f1af6_row42_col1\" class=\"data row42 col1\" >0.9047</td>\n",
              "      <td id=\"T_f1af6_row42_col2\" class=\"data row42 col2\" >0.8833</td>\n",
              "      <td id=\"T_f1af6_row42_col3\" class=\"data row42 col3\" >0.8602</td>\n",
              "      <td id=\"T_f1af6_row42_col4\" class=\"data row42 col4\" >0.7605</td>\n",
              "      <td id=\"T_f1af6_row42_col5\" class=\"data row42 col5\" >0.9312</td>\n",
              "      <td id=\"T_f1af6_row42_col6\" class=\"data row42 col6\" >0.8892</td>\n",
              "      <td id=\"T_f1af6_row42_col7\" class=\"data row42 col7\" >0.8631</td>\n",
              "      <td id=\"T_f1af6_row42_col8\" class=\"data row42 col8\" >0.8027</td>\n",
              "      <td id=\"T_f1af6_row42_col9\" class=\"data row42 col9\" >0.7405</td>\n",
              "      <td id=\"T_f1af6_row42_col10\" class=\"data row42 col10\" >0.8400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row43\" class=\"row_heading level1 row43\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_f1af6_row43_col0\" class=\"data row43 col0\" >0.4855</td>\n",
              "      <td id=\"T_f1af6_row43_col1\" class=\"data row43 col1\" >0.5814</td>\n",
              "      <td id=\"T_f1af6_row43_col2\" class=\"data row43 col2\" >0.3333</td>\n",
              "      <td id=\"T_f1af6_row43_col3\" class=\"data row43 col3\" >0.3296</td>\n",
              "      <td id=\"T_f1af6_row43_col4\" class=\"data row43 col4\" >0.3373</td>\n",
              "      <td id=\"T_f1af6_row43_col5\" class=\"data row43 col5\" >0.9605</td>\n",
              "      <td id=\"T_f1af6_row43_col6\" class=\"data row43 col6\" >0.4529</td>\n",
              "      <td id=\"T_f1af6_row43_col7\" class=\"data row43 col7\" >0.8925</td>\n",
              "      <td id=\"T_f1af6_row43_col8\" class=\"data row43 col8\" >0.5528</td>\n",
              "      <td id=\"T_f1af6_row43_col9\" class=\"data row43 col9\" >0.5678</td>\n",
              "      <td id=\"T_f1af6_row43_col10\" class=\"data row43 col10\" >0.3370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row44\" class=\"row_heading level1 row44\" >gpt2-medium</th>\n",
              "      <td id=\"T_f1af6_row44_col0\" class=\"data row44 col0\" >0.6145</td>\n",
              "      <td id=\"T_f1af6_row44_col1\" class=\"data row44 col1\" >0.9250</td>\n",
              "      <td id=\"T_f1af6_row44_col2\" class=\"data row44 col2\" >0.6295</td>\n",
              "      <td id=\"T_f1af6_row44_col3\" class=\"data row44 col3\" >0.7554</td>\n",
              "      <td id=\"T_f1af6_row44_col4\" class=\"data row44 col4\" >0.5875</td>\n",
              "      <td id=\"T_f1af6_row44_col5\" class=\"data row44 col5\" >0.9047</td>\n",
              "      <td id=\"T_f1af6_row44_col6\" class=\"data row44 col6\" >0.7750</td>\n",
              "      <td id=\"T_f1af6_row44_col7\" class=\"data row44 col7\" >0.8313</td>\n",
              "      <td id=\"T_f1af6_row44_col8\" class=\"data row44 col8\" >0.7541</td>\n",
              "      <td id=\"T_f1af6_row44_col9\" class=\"data row44 col9\" >0.6885</td>\n",
              "      <td id=\"T_f1af6_row44_col10\" class=\"data row44 col10\" >0.3355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row45\" class=\"row_heading level1 row45\" >mGPT</th>\n",
              "      <td id=\"T_f1af6_row45_col0\" class=\"data row45 col0\" >0.7188</td>\n",
              "      <td id=\"T_f1af6_row45_col1\" class=\"data row45 col1\" >0.8020</td>\n",
              "      <td id=\"T_f1af6_row45_col2\" class=\"data row45 col2\" >0.3832</td>\n",
              "      <td id=\"T_f1af6_row45_col3\" class=\"data row45 col3\" >0.9274</td>\n",
              "      <td id=\"T_f1af6_row45_col4\" class=\"data row45 col4\" >0.9039</td>\n",
              "      <td id=\"T_f1af6_row45_col5\" class=\"data row45 col5\" >0.9657</td>\n",
              "      <td id=\"T_f1af6_row45_col6\" class=\"data row45 col6\" >0.9382</td>\n",
              "      <td id=\"T_f1af6_row45_col7\" class=\"data row45 col7\" >0.9165</td>\n",
              "      <td id=\"T_f1af6_row45_col8\" class=\"data row45 col8\" >0.8578</td>\n",
              "      <td id=\"T_f1af6_row45_col9\" class=\"data row45 col9\" >0.7165</td>\n",
              "      <td id=\"T_f1af6_row45_col10\" class=\"data row45 col10\" >0.7663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row46\" class=\"row_heading level1 row46\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_f1af6_row46_col0\" class=\"data row46 col0\" >0.6436</td>\n",
              "      <td id=\"T_f1af6_row46_col1\" class=\"data row46 col1\" >0.7615</td>\n",
              "      <td id=\"T_f1af6_row46_col2\" class=\"data row46 col2\" >0.9248</td>\n",
              "      <td id=\"T_f1af6_row46_col3\" class=\"data row46 col3\" >0.9239</td>\n",
              "      <td id=\"T_f1af6_row46_col4\" class=\"data row46 col4\" >0.9404</td>\n",
              "      <td id=\"T_f1af6_row46_col5\" class=\"data row46 col5\" >0.8059</td>\n",
              "      <td id=\"T_f1af6_row46_col6\" class=\"data row46 col6\" >0.9065</td>\n",
              "      <td id=\"T_f1af6_row46_col7\" class=\"data row46 col7\" >0.8484</td>\n",
              "      <td id=\"T_f1af6_row46_col8\" class=\"data row46 col8\" >0.8597</td>\n",
              "      <td id=\"T_f1af6_row46_col9\" class=\"data row46 col9\" >0.7446</td>\n",
              "      <td id=\"T_f1af6_row46_col10\" class=\"data row46 col10\" >0.9166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row47\" class=\"row_heading level1 row47\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_f1af6_row47_col0\" class=\"data row47 col0\" >0.3329</td>\n",
              "      <td id=\"T_f1af6_row47_col1\" class=\"data row47 col1\" >0.5973</td>\n",
              "      <td id=\"T_f1af6_row47_col2\" class=\"data row47 col2\" >0.7689</td>\n",
              "      <td id=\"T_f1af6_row47_col3\" class=\"data row47 col3\" >0.8681</td>\n",
              "      <td id=\"T_f1af6_row47_col4\" class=\"data row47 col4\" >0.2113</td>\n",
              "      <td id=\"T_f1af6_row47_col5\" class=\"data row47 col5\" >0.9415</td>\n",
              "      <td id=\"T_f1af6_row47_col6\" class=\"data row47 col6\" >0.7787</td>\n",
              "      <td id=\"T_f1af6_row47_col7\" class=\"data row47 col7\" >0.9003</td>\n",
              "      <td id=\"T_f1af6_row47_col8\" class=\"data row47 col8\" >0.3337</td>\n",
              "      <td id=\"T_f1af6_row47_col9\" class=\"data row47 col9\" >0.3388</td>\n",
              "      <td id=\"T_f1af6_row47_col10\" class=\"data row47 col10\" >0.7381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row48\" class=\"row_heading level1 row48\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_f1af6_row48_col0\" class=\"data row48 col0\" >0.7993</td>\n",
              "      <td id=\"T_f1af6_row48_col1\" class=\"data row48 col1\" >0.9264</td>\n",
              "      <td id=\"T_f1af6_row48_col2\" class=\"data row48 col2\" >0.9482</td>\n",
              "      <td id=\"T_f1af6_row48_col3\" class=\"data row48 col3\" >0.9510</td>\n",
              "      <td id=\"T_f1af6_row48_col4\" class=\"data row48 col4\" >0.9422</td>\n",
              "      <td id=\"T_f1af6_row48_col5\" class=\"data row48 col5\" >0.9777</td>\n",
              "      <td id=\"T_f1af6_row48_col6\" class=\"data row48 col6\" >0.9733</td>\n",
              "      <td id=\"T_f1af6_row48_col7\" class=\"data row48 col7\" >0.9352</td>\n",
              "      <td id=\"T_f1af6_row48_col8\" class=\"data row48 col8\" >0.8944</td>\n",
              "      <td id=\"T_f1af6_row48_col9\" class=\"data row48 col9\" >0.7921</td>\n",
              "      <td id=\"T_f1af6_row48_col10\" class=\"data row48 col10\" >0.8768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level0_row49\" class=\"row_heading level0 row49\" rowspan=\"7\">vicuna-13b</th>\n",
              "      <th id=\"T_f1af6_level1_row49\" class=\"row_heading level1 row49\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_f1af6_row49_col0\" class=\"data row49 col0\" >0.8628</td>\n",
              "      <td id=\"T_f1af6_row49_col1\" class=\"data row49 col1\" >0.9149</td>\n",
              "      <td id=\"T_f1af6_row49_col2\" class=\"data row49 col2\" >0.7705</td>\n",
              "      <td id=\"T_f1af6_row49_col3\" class=\"data row49 col3\" >0.8341</td>\n",
              "      <td id=\"T_f1af6_row49_col4\" class=\"data row49 col4\" >0.7503</td>\n",
              "      <td id=\"T_f1af6_row49_col5\" class=\"data row49 col5\" >0.9281</td>\n",
              "      <td id=\"T_f1af6_row49_col6\" class=\"data row49 col6\" >0.8996</td>\n",
              "      <td id=\"T_f1af6_row49_col7\" class=\"data row49 col7\" >0.8823</td>\n",
              "      <td id=\"T_f1af6_row49_col8\" class=\"data row49 col8\" >0.8226</td>\n",
              "      <td id=\"T_f1af6_row49_col9\" class=\"data row49 col9\" >0.7869</td>\n",
              "      <td id=\"T_f1af6_row49_col10\" class=\"data row49 col10\" >0.8514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row50\" class=\"row_heading level1 row50\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_f1af6_row50_col0\" class=\"data row50 col0\" >0.4812</td>\n",
              "      <td id=\"T_f1af6_row50_col1\" class=\"data row50 col1\" >0.5839</td>\n",
              "      <td id=\"T_f1af6_row50_col2\" class=\"data row50 col2\" >0.3798</td>\n",
              "      <td id=\"T_f1af6_row50_col3\" class=\"data row50 col3\" >0.3827</td>\n",
              "      <td id=\"T_f1af6_row50_col4\" class=\"data row50 col4\" >0.3333</td>\n",
              "      <td id=\"T_f1af6_row50_col5\" class=\"data row50 col5\" >0.9743</td>\n",
              "      <td id=\"T_f1af6_row50_col6\" class=\"data row50 col6\" >0.4827</td>\n",
              "      <td id=\"T_f1af6_row50_col7\" class=\"data row50 col7\" >0.8677</td>\n",
              "      <td id=\"T_f1af6_row50_col8\" class=\"data row50 col8\" >0.5833</td>\n",
              "      <td id=\"T_f1af6_row50_col9\" class=\"data row50 col9\" >0.5508</td>\n",
              "      <td id=\"T_f1af6_row50_col10\" class=\"data row50 col10\" >0.4081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row51\" class=\"row_heading level1 row51\" >gpt2-medium</th>\n",
              "      <td id=\"T_f1af6_row51_col0\" class=\"data row51 col0\" >0.7679</td>\n",
              "      <td id=\"T_f1af6_row51_col1\" class=\"data row51 col1\" >0.9232</td>\n",
              "      <td id=\"T_f1af6_row51_col2\" class=\"data row51 col2\" >0.5873</td>\n",
              "      <td id=\"T_f1af6_row51_col3\" class=\"data row51 col3\" >0.6943</td>\n",
              "      <td id=\"T_f1af6_row51_col4\" class=\"data row51 col4\" >0.7214</td>\n",
              "      <td id=\"T_f1af6_row51_col5\" class=\"data row51 col5\" >0.9159</td>\n",
              "      <td id=\"T_f1af6_row51_col6\" class=\"data row51 col6\" >0.7333</td>\n",
              "      <td id=\"T_f1af6_row51_col7\" class=\"data row51 col7\" >0.8299</td>\n",
              "      <td id=\"T_f1af6_row51_col8\" class=\"data row51 col8\" >0.5051</td>\n",
              "      <td id=\"T_f1af6_row51_col9\" class=\"data row51 col9\" >0.5253</td>\n",
              "      <td id=\"T_f1af6_row51_col10\" class=\"data row51 col10\" >0.4208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row52\" class=\"row_heading level1 row52\" >mGPT</th>\n",
              "      <td id=\"T_f1af6_row52_col0\" class=\"data row52 col0\" >0.7234</td>\n",
              "      <td id=\"T_f1af6_row52_col1\" class=\"data row52 col1\" >0.8716</td>\n",
              "      <td id=\"T_f1af6_row52_col2\" class=\"data row52 col2\" >0.5427</td>\n",
              "      <td id=\"T_f1af6_row52_col3\" class=\"data row52 col3\" >0.9206</td>\n",
              "      <td id=\"T_f1af6_row52_col4\" class=\"data row52 col4\" >0.6739</td>\n",
              "      <td id=\"T_f1af6_row52_col5\" class=\"data row52 col5\" >0.9674</td>\n",
              "      <td id=\"T_f1af6_row52_col6\" class=\"data row52 col6\" >0.6880</td>\n",
              "      <td id=\"T_f1af6_row52_col7\" class=\"data row52 col7\" >0.9164</td>\n",
              "      <td id=\"T_f1af6_row52_col8\" class=\"data row52 col8\" >0.7205</td>\n",
              "      <td id=\"T_f1af6_row52_col9\" class=\"data row52 col9\" >0.7893</td>\n",
              "      <td id=\"T_f1af6_row52_col10\" class=\"data row52 col10\" >0.6191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row53\" class=\"row_heading level1 row53\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_f1af6_row53_col0\" class=\"data row53 col0\" >0.6784</td>\n",
              "      <td id=\"T_f1af6_row53_col1\" class=\"data row53 col1\" >0.8994</td>\n",
              "      <td id=\"T_f1af6_row53_col2\" class=\"data row53 col2\" >0.8387</td>\n",
              "      <td id=\"T_f1af6_row53_col3\" class=\"data row53 col3\" >0.9052</td>\n",
              "      <td id=\"T_f1af6_row53_col4\" class=\"data row53 col4\" >0.9169</td>\n",
              "      <td id=\"T_f1af6_row53_col5\" class=\"data row53 col5\" >0.9520</td>\n",
              "      <td id=\"T_f1af6_row53_col6\" class=\"data row53 col6\" >0.8753</td>\n",
              "      <td id=\"T_f1af6_row53_col7\" class=\"data row53 col7\" >0.8956</td>\n",
              "      <td id=\"T_f1af6_row53_col8\" class=\"data row53 col8\" >0.8595</td>\n",
              "      <td id=\"T_f1af6_row53_col9\" class=\"data row53 col9\" >0.8562</td>\n",
              "      <td id=\"T_f1af6_row53_col10\" class=\"data row53 col10\" >0.8291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row54\" class=\"row_heading level1 row54\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_f1af6_row54_col0\" class=\"data row54 col0\" >0.5358</td>\n",
              "      <td id=\"T_f1af6_row54_col1\" class=\"data row54 col1\" >0.8853</td>\n",
              "      <td id=\"T_f1af6_row54_col2\" class=\"data row54 col2\" >0.6725</td>\n",
              "      <td id=\"T_f1af6_row54_col3\" class=\"data row54 col3\" >0.8681</td>\n",
              "      <td id=\"T_f1af6_row54_col4\" class=\"data row54 col4\" >0.3147</td>\n",
              "      <td id=\"T_f1af6_row54_col5\" class=\"data row54 col5\" >0.9432</td>\n",
              "      <td id=\"T_f1af6_row54_col6\" class=\"data row54 col6\" >0.8460</td>\n",
              "      <td id=\"T_f1af6_row54_col7\" class=\"data row54 col7\" >0.9058</td>\n",
              "      <td id=\"T_f1af6_row54_col8\" class=\"data row54 col8\" >0.4371</td>\n",
              "      <td id=\"T_f1af6_row54_col9\" class=\"data row54 col9\" >0.3755</td>\n",
              "      <td id=\"T_f1af6_row54_col10\" class=\"data row54 col10\" >0.5963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f1af6_level1_row55\" class=\"row_heading level1 row55\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_f1af6_row55_col0\" class=\"data row55 col0\" >0.8447</td>\n",
              "      <td id=\"T_f1af6_row55_col1\" class=\"data row55 col1\" >0.8789</td>\n",
              "      <td id=\"T_f1af6_row55_col2\" class=\"data row55 col2\" >0.6681</td>\n",
              "      <td id=\"T_f1af6_row55_col3\" class=\"data row55 col3\" >0.8614</td>\n",
              "      <td id=\"T_f1af6_row55_col4\" class=\"data row55 col4\" >0.9061</td>\n",
              "      <td id=\"T_f1af6_row55_col5\" class=\"data row55 col5\" >0.9434</td>\n",
              "      <td id=\"T_f1af6_row55_col6\" class=\"data row55 col6\" >0.8267</td>\n",
              "      <td id=\"T_f1af6_row55_col7\" class=\"data row55 col7\" >0.8959</td>\n",
              "      <td id=\"T_f1af6_row55_col8\" class=\"data row55 col8\" >0.7773</td>\n",
              "      <td id=\"T_f1af6_row55_col9\" class=\"data row55 col9\" >0.6400</td>\n",
              "      <td id=\"T_f1af6_row55_col10\" class=\"data row55 col10\" >0.6473</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{llrrrrrrrrrrr}\n",
            " &  & \\bfseries ar & \\bfseries ca & \\bfseries cs & \\bfseries de & \\bfseries en & \\bfseries es & \\bfseries nl & \\bfseries pt & \\bfseries ru & \\bfseries uk & \\bfseries zh \\\\\n",
            "Train LLM & Model &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries alpaca-lora-30b} & \\bfseries bert-base-multilingual-cased & 0.6019 & 0.8950 & 0.8108 & 0.8765 & 0.7517 & \\bfseries 0.9092 & 0.8941 & 0.8943 & 0.8600 & 0.8182 & 0.7713 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.5116 & 0.4754 & 0.3370 & 0.3361 & 0.3325 & \\bfseries 0.9348 & 0.5286 & 0.8665 & 0.3891 & 0.3820 & 0.3986 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.6937 & 0.8090 & 0.6564 & 0.8226 & 0.7332 & \\bfseries 0.8935 & 0.8291 & 0.8602 & 0.5735 & 0.5679 & 0.4391 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.6980 & 0.8639 & 0.6989 & 0.9001 & 0.6589 & \\bfseries 0.9452 & 0.7088 & 0.9403 & 0.8366 & 0.8880 & 0.5691 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.8255 & 0.7338 & 0.9110 & 0.9171 & 0.8771 & 0.8256 & 0.8038 & 0.7777 & \\bfseries 0.9317 & 0.9096 & 0.7954 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.3578 & 0.7052 & 0.6789 & 0.8414 & 0.2717 & \\bfseries 0.9295 & 0.8296 & 0.8671 & 0.3587 & 0.3634 & 0.7537 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.6999 & 0.8349 & 0.9008 & 0.9493 & 0.9295 & 0.9281 & \\bfseries 0.9532 & 0.9301 & 0.8845 & 0.8278 & 0.6164 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries gpt-3.5-turbo} & \\bfseries bert-base-multilingual-cased & 0.8896 & \\bfseries 0.9365 & 0.8997 & 0.8900 & 0.9169 & 0.9263 & 0.9232 & 0.8858 & 0.8949 & 0.8722 & 0.8765 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.5268 & 0.7374 & 0.3333 & 0.3340 & 0.3333 & \\bfseries 0.9743 & 0.4290 & 0.9215 & 0.5390 & 0.4383 & 0.3333 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.4313 & 0.8889 & 0.3900 & 0.3827 & 0.3669 & \\bfseries 0.8970 & 0.4874 & 0.8320 & 0.4949 & 0.4526 & 0.3355 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.8941 & 0.9650 & 0.3552 & 0.9307 & 0.8397 & \\bfseries 0.9692 & 0.8974 & 0.9556 & 0.8942 & 0.9196 & 0.6781 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.9281 & 0.8817 & \\bfseries 0.9666 & 0.9577 & 0.9368 & 0.9570 & 0.9109 & 0.9089 & 0.8890 & 0.9128 & 0.9416 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.3307 & 0.8610 & 0.8449 & 0.8700 & 0.2073 & \\bfseries 0.9153 & 0.8743 & 0.8392 & 0.3333 & 0.3311 & 0.7601 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.7671 & 0.9550 & 0.8609 & 0.8539 & \\bfseries 0.9801 & 0.9743 & 0.9616 & 0.9676 & 0.9093 & 0.7368 & 0.8504 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries gpt-4} & \\bfseries bert-base-multilingual-cased & 0.8764 & 0.8852 & \\bfseries 0.9300 & 0.8808 & 0.7306 & 0.9277 & 0.9075 & 0.8679 & 0.8916 & 0.8792 & 0.8799 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.3860 & 0.8103 & 0.3333 & 0.3303 & 0.3373 & \\bfseries 0.9571 & 0.5424 & 0.9506 & 0.3798 & 0.3670 & 0.3407 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.4260 & 0.7906 & 0.4326 & 0.4318 & 0.3362 & \\bfseries 0.9143 & 0.4571 & 0.7844 & 0.4378 & 0.4042 & 0.3399 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.8491 & 0.8705 & 0.3370 & 0.9187 & 0.9476 & 0.9829 & 0.9245 & \\bfseries 0.9830 & 0.9016 & 0.8454 & 0.6649 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.9331 & 0.8897 & \\bfseries 0.9549 & 0.9356 & 0.8826 & 0.9195 & 0.9331 & 0.9027 & 0.8545 & 0.9096 & 0.9045 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.3322 & 0.9128 & 0.8091 & 0.8765 & 0.3602 & \\bfseries 0.9293 & 0.8449 & 0.8768 & 0.3370 & 0.3318 & 0.6283 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.5124 & 0.9599 & 0.5688 & 0.8011 & 0.9729 & \\bfseries 0.9812 & 0.9616 & 0.9676 & 0.5987 & 0.3994 & 0.4001 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries llama-65b} & \\bfseries bert-base-multilingual-cased & 0.7956 & 0.9177 & 0.9645 & 0.7711 & 0.3611 & \\bfseries 0.9914 & 0.6665 & 0.9794 & 0.8227 & 0.9290 & 0.6396 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.4887 & 0.9026 & 0.9272 & 0.9214 & 0.3325 & \\bfseries 0.9741 & 0.8515 & 0.9691 & 0.3620 & 0.4606 & 0.3318 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.3680 & 0.9365 & 0.8738 & 0.9079 & 0.3990 & \\bfseries 0.9621 & 0.8744 & 0.9398 & 0.4793 & 0.5608 & 0.5344 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.5912 & 0.6899 & 0.8606 & 0.7540 & 0.3505 & \\bfseries 0.9673 & 0.3604 & 0.9501 & 0.5401 & 0.8348 & 0.3731 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.9135 & 0.9262 & \\bfseries 0.9882 & 0.9608 & 0.6260 & 0.9862 & 0.6591 & 0.9777 & 0.9765 & 0.9848 & 0.3539 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.3296 & 0.8944 & 0.7558 & 0.7449 & 0.3357 & \\bfseries 0.9517 & 0.8188 & 0.8808 & 0.3392 & 0.3617 & 0.3296 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.8461 & 0.8307 & 0.9831 & 0.9557 & 0.6680 & \\bfseries 0.9879 & 0.4583 & 0.9725 & 0.9412 & 0.9680 & 0.6842 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries opt-66b} & \\bfseries bert-base-multilingual-cased & 0.7378 & 0.7608 & 0.6443 & 0.5872 & 0.4705 & 0.7658 & 0.7038 & 0.7662 & 0.6748 & 0.7543 & \\bfseries 0.8225 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.4594 & 0.7202 & 0.6132 & 0.6954 & 0.3333 & \\bfseries 0.7692 & 0.6844 & 0.7265 & 0.4311 & 0.5203 & 0.3311 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.4455 & 0.7310 & 0.4703 & 0.5913 & 0.4524 & \\bfseries 0.7703 & 0.5466 & 0.5972 & 0.5993 & 0.6464 & 0.4690 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.3906 & 0.7297 & 0.4550 & 0.6957 & 0.3722 & \\bfseries 0.7992 & 0.3866 & 0.5949 & 0.5713 & 0.6426 & 0.4554 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.9139 & 0.8500 & \\bfseries 0.9883 & 0.9493 & 0.6240 & 0.8762 & 0.8538 & 0.7872 & 0.9696 & 0.9627 & 0.6997 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.4668 & 0.6129 & 0.8550 & 0.6502 & 0.3355 & 0.8141 & \\bfseries 0.8564 & 0.7337 & 0.5548 & 0.5646 & 0.5596 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.6058 & 0.7452 & \\bfseries 0.8765 & 0.8104 & 0.4443 & 0.8123 & 0.4702 & 0.6335 & 0.8106 & 0.8408 & 0.5381 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries opt-iml-max-1.3b} & \\bfseries bert-base-multilingual-cased & 0.9537 & 0.9697 & 0.9632 & 0.8676 & 0.6840 & 0.9482 & 0.9699 & 0.8820 & 0.9027 & \\bfseries 0.9707 & 0.9528 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.3527 & \\bfseries 0.9681 & 0.8392 & 0.8930 & 0.3333 & 0.9515 & 0.8798 & 0.9189 & 0.3280 & 0.3305 & 0.3307 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.6084 & 0.9049 & 0.7598 & 0.5085 & 0.4730 & \\bfseries 0.9100 & 0.6565 & 0.6696 & 0.6559 & 0.7088 & 0.3941 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.9434 & 0.9613 & 0.9210 & \\bfseries 0.9932 & 0.4045 & 0.9879 & 0.8410 & 0.9810 & 0.8970 & 0.9845 & 0.5838 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.9931 & 0.9866 & 0.9933 & 0.8710 & 0.5865 & 0.9827 & 0.9329 & 0.9828 & \\bfseries 0.9949 & 0.9931 & 0.8744 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.4128 & \\bfseries 0.9646 & 0.9498 & 0.7265 & 0.2160 & 0.9602 & 0.9381 & 0.9103 & 0.5456 & 0.6839 & 0.5212 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.9794 & \\bfseries 0.9916 & 0.8583 & 0.9881 & 0.6192 & 0.9810 & 0.9799 & 0.9793 & 0.7578 & 0.7060 & 0.8187 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries text-davinci-003} & \\bfseries bert-base-multilingual-cased & 0.7319 & 0.9047 & 0.8833 & 0.8602 & 0.7605 & \\bfseries 0.9312 & 0.8892 & 0.8631 & 0.8027 & 0.7405 & 0.8400 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.4855 & 0.5814 & 0.3333 & 0.3296 & 0.3373 & \\bfseries 0.9605 & 0.4529 & 0.8925 & 0.5528 & 0.5678 & 0.3370 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.6145 & \\bfseries 0.9250 & 0.6295 & 0.7554 & 0.5875 & 0.9047 & 0.7750 & 0.8313 & 0.7541 & 0.6885 & 0.3355 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.7188 & 0.8020 & 0.3832 & 0.9274 & 0.9039 & \\bfseries 0.9657 & 0.9382 & 0.9165 & 0.8578 & 0.7165 & 0.7663 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.6436 & 0.7615 & 0.9248 & 0.9239 & \\bfseries 0.9404 & 0.8059 & 0.9065 & 0.8484 & 0.8597 & 0.7446 & 0.9166 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.3329 & 0.5973 & 0.7689 & 0.8681 & 0.2113 & \\bfseries 0.9415 & 0.7787 & 0.9003 & 0.3337 & 0.3388 & 0.7381 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.7993 & 0.9264 & 0.9482 & 0.9510 & 0.9422 & \\bfseries 0.9777 & 0.9733 & 0.9352 & 0.8944 & 0.7921 & 0.8768 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries vicuna-13b} & \\bfseries bert-base-multilingual-cased & 0.8628 & 0.9149 & 0.7705 & 0.8341 & 0.7503 & \\bfseries 0.9281 & 0.8996 & 0.8823 & 0.8226 & 0.7869 & 0.8514 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.4812 & 0.5839 & 0.3798 & 0.3827 & 0.3333 & \\bfseries 0.9743 & 0.4827 & 0.8677 & 0.5833 & 0.5508 & 0.4081 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.7679 & \\bfseries 0.9232 & 0.5873 & 0.6943 & 0.7214 & 0.9159 & 0.7333 & 0.8299 & 0.5051 & 0.5253 & 0.4208 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.7234 & 0.8716 & 0.5427 & 0.9206 & 0.6739 & \\bfseries 0.9674 & 0.6880 & 0.9164 & 0.7205 & 0.7893 & 0.6191 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.6784 & 0.8994 & 0.8387 & 0.9052 & 0.9169 & \\bfseries 0.9520 & 0.8753 & 0.8956 & 0.8595 & 0.8562 & 0.8291 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.5358 & 0.8853 & 0.6725 & 0.8681 & 0.3147 & \\bfseries 0.9432 & 0.8460 & 0.9058 & 0.4371 & 0.3755 & 0.5963 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.8447 & 0.8789 & 0.6681 & 0.8614 & 0.9061 & \\bfseries 0.9434 & 0.8267 & 0.8959 & 0.7773 & 0.6400 & 0.6473 \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd004d621a0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_46770_row0_col0 {\n",
              "  background-color: #b8c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row0_col1, #T_46770_row1_col8 {\n",
              "  background-color: #94b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row0_col2, #T_46770_row1_col4 {\n",
              "  background-color: #a9bfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row0_col3, #T_46770_row1_col0 {\n",
              "  background-color: #9fbad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row0_col4 {\n",
              "  background-color: #c2cbe2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row0_col5 {\n",
              "  background-color: #83afd3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row0_col6 {\n",
              "  background-color: #a2bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row0_col7, #T_46770_row1_col1 {\n",
              "  background-color: #8eb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row0_col8, #T_46770_row0_col9 {\n",
              "  background-color: #b1c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row0_col10 {\n",
              "  background-color: #bfc9e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row1_col2 {\n",
              "  background-color: #9cb9d9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row1_col3 {\n",
              "  background-color: #8cb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row1_col5 {\n",
              "  background-color: #81aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row1_col6 {\n",
              "  background-color: #99b8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row1_col7 {\n",
              "  background-color: #89b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row1_col9 {\n",
              "  background-color: #96b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row1_col10 {\n",
              "  background-color: #abbfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row2_col0 {\n",
              "  background-color: #d4d4e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row2_col1 {\n",
              "  background-color: #9ebad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row2_col2 {\n",
              "  background-color: #b9c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row2_col3 {\n",
              "  background-color: #b5c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row2_col4 {\n",
              "  background-color: #dfddec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row2_col5 {\n",
              "  background-color: #84b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row2_col6 {\n",
              "  background-color: #adc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row2_col7 {\n",
              "  background-color: #93b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row2_col8 {\n",
              "  background-color: #d3d4e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row2_col9 {\n",
              "  background-color: #d2d3e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_46770_row2_col10 {\n",
              "  background-color: #d5d5e8;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_46770\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_46770_level0_col0\" class=\"col_heading level0 col0\" >ar</th>\n",
              "      <th id=\"T_46770_level0_col1\" class=\"col_heading level0 col1\" >ca</th>\n",
              "      <th id=\"T_46770_level0_col2\" class=\"col_heading level0 col2\" >cs</th>\n",
              "      <th id=\"T_46770_level0_col3\" class=\"col_heading level0 col3\" >de</th>\n",
              "      <th id=\"T_46770_level0_col4\" class=\"col_heading level0 col4\" >en</th>\n",
              "      <th id=\"T_46770_level0_col5\" class=\"col_heading level0 col5\" >es</th>\n",
              "      <th id=\"T_46770_level0_col6\" class=\"col_heading level0 col6\" >nl</th>\n",
              "      <th id=\"T_46770_level0_col7\" class=\"col_heading level0 col7\" >pt</th>\n",
              "      <th id=\"T_46770_level0_col8\" class=\"col_heading level0 col8\" >ru</th>\n",
              "      <th id=\"T_46770_level0_col9\" class=\"col_heading level0 col9\" >uk</th>\n",
              "      <th id=\"T_46770_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_46770_level0_row0\" class=\"row_heading level0 row0\" >All Detectors Mean</th>\n",
              "      <td id=\"T_46770_row0_col0\" class=\"data row0 col0\" >0.6480</td>\n",
              "      <td id=\"T_46770_row0_col1\" class=\"data row0 col1\" >0.8413</td>\n",
              "      <td id=\"T_46770_row0_col2\" class=\"data row0 col2\" >0.7300</td>\n",
              "      <td id=\"T_46770_row0_col3\" class=\"data row0 col3\" >0.7850</td>\n",
              "      <td id=\"T_46770_row0_col4\" class=\"data row0 col4\" >0.5790</td>\n",
              "      <td id=\"T_46770_row0_col5\" class=\"data row0 col5\" >0.9259</td>\n",
              "      <td id=\"T_46770_row0_col6\" class=\"data row0 col6\" >0.7689</td>\n",
              "      <td id=\"T_46770_row0_col7\" class=\"data row0 col7\" >0.8749</td>\n",
              "      <td id=\"T_46770_row0_col8\" class=\"data row0 col8\" >0.6804</td>\n",
              "      <td id=\"T_46770_row0_col9\" class=\"data row0 col9\" >0.6800</td>\n",
              "      <td id=\"T_46770_row0_col10\" class=\"data row0 col10\" >0.6082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_46770_level0_row1\" class=\"row_heading level0 row1\" >Multilingual Base Models Mean</th>\n",
              "      <td id=\"T_46770_row1_col0\" class=\"data row1 col0\" >0.7857</td>\n",
              "      <td id=\"T_46770_row1_col1\" class=\"data row1 col1\" >0.8747</td>\n",
              "      <td id=\"T_46770_row1_col2\" class=\"data row1 col2\" >0.8016</td>\n",
              "      <td id=\"T_46770_row1_col3\" class=\"data row1 col3\" >0.8812</td>\n",
              "      <td id=\"T_46770_row1_col4\" class=\"data row1 col4\" >0.7322</td>\n",
              "      <td id=\"T_46770_row1_col5\" class=\"data row1 col5\" >0.9314</td>\n",
              "      <td id=\"T_46770_row1_col6\" class=\"data row1 col6\" >0.8143</td>\n",
              "      <td id=\"T_46770_row1_col7\" class=\"data row1 col7\" >0.8944</td>\n",
              "      <td id=\"T_46770_row1_col8\" class=\"data row1 col8\" >0.8375</td>\n",
              "      <td id=\"T_46770_row1_col9\" class=\"data row1 col9\" >0.8299</td>\n",
              "      <td id=\"T_46770_row1_col10\" class=\"data row1 col10\" >0.7216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_46770_level0_row2\" class=\"row_heading level0 row2\" >Monolingual Base Models Mean</th>\n",
              "      <td id=\"T_46770_row2_col0\" class=\"data row2 col0\" >0.4644</td>\n",
              "      <td id=\"T_46770_row2_col1\" class=\"data row2 col1\" >0.7967</td>\n",
              "      <td id=\"T_46770_row2_col2\" class=\"data row2 col2\" >0.6346</td>\n",
              "      <td id=\"T_46770_row2_col3\" class=\"data row2 col3\" >0.6568</td>\n",
              "      <td id=\"T_46770_row2_col4\" class=\"data row2 col4\" >0.3748</td>\n",
              "      <td id=\"T_46770_row2_col5\" class=\"data row2 col5\" >0.9187</td>\n",
              "      <td id=\"T_46770_row2_col6\" class=\"data row2 col6\" >0.7082</td>\n",
              "      <td id=\"T_46770_row2_col7\" class=\"data row2 col7\" >0.8488</td>\n",
              "      <td id=\"T_46770_row2_col8\" class=\"data row2 col8\" >0.4710</td>\n",
              "      <td id=\"T_46770_row2_col9\" class=\"data row2 col9\" >0.4801</td>\n",
              "      <td id=\"T_46770_row2_col10\" class=\"data row2 col10\" >0.4569</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{lrrrrrrrrrrr}\n",
            " & \\bfseries ar & \\bfseries ca & \\bfseries cs & \\bfseries de & \\bfseries en & \\bfseries es & \\bfseries nl & \\bfseries pt & \\bfseries ru & \\bfseries uk & \\bfseries zh \\\\\n",
            "\\multicolumn{2}{r|}{\\bfseries All Detectors Mean} & {\\cellcolor[HTML]{B8C6E0}} \\textcolor{black}{0.6480} & {\\cellcolor[HTML]{94B6D7}} \\textcolor{black}{0.8413} & {\\cellcolor[HTML]{A9BFDC}} \\textcolor{black}{0.7300} & {\\cellcolor[HTML]{9FBAD9}} \\textcolor{black}{0.7850} & {\\cellcolor[HTML]{C2CBE2}} \\textcolor{black}{0.5790} & {\\cellcolor[HTML]{83AFD3}} \\textcolor{black}{0.9259} & {\\cellcolor[HTML]{A2BCDA}} \\textcolor{black}{0.7689} & {\\cellcolor[HTML]{8EB3D5}} \\textcolor{black}{0.8749} & {\\cellcolor[HTML]{B1C2DE}} \\textcolor{black}{0.6804} & {\\cellcolor[HTML]{B1C2DE}} \\textcolor{black}{0.6800} & {\\cellcolor[HTML]{BFC9E1}} \\textcolor{black}{0.6082} \\\\\n",
            "\\multicolumn{2}{r|}{\\bfseries Multilingual Base Models Mean} & {\\cellcolor[HTML]{9FBAD9}} \\textcolor{black}{0.7857} & {\\cellcolor[HTML]{8EB3D5}} \\textcolor{black}{0.8747} & {\\cellcolor[HTML]{9CB9D9}} \\textcolor{black}{0.8016} & {\\cellcolor[HTML]{8CB3D5}} \\textcolor{black}{0.8812} & {\\cellcolor[HTML]{A9BFDC}} \\textcolor{black}{0.7322} & {\\cellcolor[HTML]{81AED2}} \\textcolor{black}{0.9314} & {\\cellcolor[HTML]{99B8D8}} \\textcolor{black}{0.8143} & {\\cellcolor[HTML]{89B1D4}} \\textcolor{black}{0.8944} & {\\cellcolor[HTML]{94B6D7}} \\textcolor{black}{0.8375} & {\\cellcolor[HTML]{96B6D7}} \\textcolor{black}{0.8299} & {\\cellcolor[HTML]{ABBFDC}} \\textcolor{black}{0.7216} \\\\\n",
            "\\multicolumn{2}{r|}{\\bfseries Monolingual Base Models Mean} & {\\cellcolor[HTML]{D4D4E8}} \\textcolor{black}{0.4644} & {\\cellcolor[HTML]{9EBAD9}} \\textcolor{black}{0.7967} & {\\cellcolor[HTML]{B9C6E0}} \\textcolor{black}{0.6346} & {\\cellcolor[HTML]{B5C4DF}} \\textcolor{black}{0.6568} & {\\cellcolor[HTML]{DFDDEC}} \\textcolor{black}{0.3748} & {\\cellcolor[HTML]{84B0D3}} \\textcolor{black}{0.9187} & {\\cellcolor[HTML]{ADC1DD}} \\textcolor{black}{0.7082} & {\\cellcolor[HTML]{93B5D6}} \\textcolor{black}{0.8488} & {\\cellcolor[HTML]{D3D4E7}} \\textcolor{black}{0.4710} & {\\cellcolor[HTML]{D2D3E7}} \\textcolor{black}{0.4801} & {\\cellcolor[HTML]{D5D5E8}} \\textcolor{black}{0.4569} \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Results just for Russian fine-tuned models\n",
        "temp = results_all.loc['ru',:]\n",
        "display(temp.style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4))\n",
        "print(temp.style.format(na_rep=0, precision=4).highlight_max(props='font-weight: bold;', axis=1).applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True))\n",
        "\n",
        "means = pd.DataFrame()\n",
        "means = pd.concat([means, temp.agg('mean')], copy=False, axis=1)\n",
        "means.rename(columns={0:'All Detectors Mean'}, inplace=True)\n",
        "means = pd.concat([means, temp.loc[[x in multilingual for x in temp.reset_index().Model], :].agg('mean')], copy=False, axis=1)\n",
        "means.rename(columns={0:'Multilingual Base Models Mean'}, inplace=True)\n",
        "means = pd.concat([means, temp.loc[[x not in multilingual for x in temp.reset_index().Model], :].agg('mean')], copy=False, axis=1)\n",
        "means.rename(columns={0:'Monolingual Base Models Mean'}, inplace=True)\n",
        "means = means.T.style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4)\n",
        "display(means)\n",
        "temp = means.applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True)\n",
        "#little hack to properly format latex table\n",
        "print(temp.replace('\\\\color[HTML]{F1F1F1} ', '\\\\textcolor{white}{').replace('\\\\color[HTML]{000000} ', '\\\\textcolor{black}{').replace(' & {\\\\cellcolor', '} & {\\\\cellcolor').replace(' \\\\\\\\', '} \\\\\\\\').replace('\\n\\\\bfseries', '\\n\\\\multicolumn{2}{r|}{\\\\bfseries').replace('zh} \\\\\\\\', 'zh \\\\\\\\'))"
      ],
      "metadata": {
        "id": "IqrzxQSW39S0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e0a402ca-f3c2-48c7-ac75-00515d80abb3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001f306d0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_03488_row0_col0, #T_03488_row21_col7, #T_03488_row22_col9, #T_03488_row28_col1, #T_03488_row28_col3, #T_03488_row31_col6 {\n",
              "  background-color: #bfc9e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row0_col1, #T_03488_row10_col10, #T_03488_row23_col2, #T_03488_row27_col10, #T_03488_row32_col6 {\n",
              "  background-color: #bbc7e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row0_col2, #T_03488_row4_col1, #T_03488_row9_col9, #T_03488_row14_col10, #T_03488_row42_col3 {\n",
              "  background-color: #9ab8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row0_col3, #T_03488_row46_col10 {\n",
              "  background-color: #a8bedc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row0_col4, #T_03488_row2_col2, #T_03488_row36_col6 {\n",
              "  background-color: #d4d4e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row0_col5, #T_03488_row0_col7, #T_03488_row2_col4, #T_03488_row8_col0, #T_03488_row23_col3, #T_03488_row38_col1 {\n",
              "  background-color: #b5c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row0_col6, #T_03488_row0_col10, #T_03488_row19_col10 {\n",
              "  background-color: #b4c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row0_col8, #T_03488_row3_col9, #T_03488_row10_col3, #T_03488_row30_col8, #T_03488_row42_col9 {\n",
              "  background-color: #86b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row0_col9, #T_03488_row4_col5, #T_03488_row4_col6, #T_03488_row10_col1, #T_03488_row27_col0, #T_03488_row33_col9, #T_03488_row39_col0, #T_03488_row54_col8 {\n",
              "  background-color: #8bb2d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row1_col0, #T_03488_row14_col4, #T_03488_row23_col7, #T_03488_row24_col5 {\n",
              "  background-color: #d1d2e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row1_col1, #T_03488_row12_col2, #T_03488_row15_col4, #T_03488_row15_col10, #T_03488_row22_col10, #T_03488_row27_col4, #T_03488_row27_col6, #T_03488_row36_col1, #T_03488_row38_col4, #T_03488_row49_col7, #T_03488_row52_col4 {\n",
              "  background-color: #dfddec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row1_col2, #T_03488_row8_col4, #T_03488_row9_col1, #T_03488_row12_col0, #T_03488_row12_col1, #T_03488_row12_col3, #T_03488_row12_col5, #T_03488_row16_col3, #T_03488_row16_col7, #T_03488_row16_col10, #T_03488_row19_col2, #T_03488_row19_col3, #T_03488_row31_col4, #T_03488_row34_col4, #T_03488_row46_col4, #T_03488_row47_col3, #T_03488_row47_col5, #T_03488_row49_col10 {\n",
              "  background-color: #e1dfed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row1_col3, #T_03488_row10_col2, #T_03488_row16_col1, #T_03488_row17_col2, #T_03488_row30_col3, #T_03488_row40_col6, #T_03488_row45_col2, #T_03488_row49_col6 {\n",
              "  background-color: #e0deed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row1_col4, #T_03488_row1_col7, #T_03488_row37_col7, #T_03488_row38_col5 {\n",
              "  background-color: #dddbec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row1_col5, #T_03488_row16_col0, #T_03488_row21_col6, #T_03488_row46_col5 {\n",
              "  background-color: #cdd0e5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row1_col6, #T_03488_row2_col3, #T_03488_row3_col1, #T_03488_row37_col5 {\n",
              "  background-color: #dad9ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row1_col8, #T_03488_row5_col10, #T_03488_row29_col0, #T_03488_row34_col3, #T_03488_row34_col5, #T_03488_row36_col2, #T_03488_row36_col5, #T_03488_row50_col8, #T_03488_row50_col10, #T_03488_row52_col7 {\n",
              "  background-color: #c8cde4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row1_col9, #T_03488_row31_col1, #T_03488_row37_col1, #T_03488_row39_col1, #T_03488_row43_col8, #T_03488_row46_col7, #T_03488_row50_col0 {\n",
              "  background-color: #ced0e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row1_col10, #T_03488_row36_col3, #T_03488_row51_col10 {\n",
              "  background-color: #d6d6e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row2_col0, #T_03488_row25_col5, #T_03488_row29_col8, #T_03488_row41_col1 {\n",
              "  background-color: #a9bfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row2_col1, #T_03488_row51_col2 {\n",
              "  background-color: #d2d3e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row2_col5 {\n",
              "  background-color: #b7c5df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row2_col6, #T_03488_row23_col1, #T_03488_row23_col4, #T_03488_row25_col1, #T_03488_row25_col6, #T_03488_row31_col5, #T_03488_row31_col7, #T_03488_row33_col0, #T_03488_row35_col7, #T_03488_row47_col2, #T_03488_row47_col6 {\n",
              "  background-color: #d9d8ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row2_col7, #T_03488_row3_col6, #T_03488_row9_col0, #T_03488_row24_col0, #T_03488_row25_col7, #T_03488_row35_col6, #T_03488_row43_col4 {\n",
              "  background-color: #cacee5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row2_col8, #T_03488_row5_col8, #T_03488_row7_col10, #T_03488_row14_col0, #T_03488_row41_col6, #T_03488_row45_col3, #T_03488_row48_col0, #T_03488_row52_col2, #T_03488_row53_col10 {\n",
              "  background-color: #8fb4d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row2_col9, #T_03488_row4_col10, #T_03488_row21_col5, #T_03488_row35_col10 {\n",
              "  background-color: #97b7d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row2_col10, #T_03488_row26_col10, #T_03488_row30_col5, #T_03488_row37_col3, #T_03488_row39_col5, #T_03488_row39_col7, #T_03488_row40_col2, #T_03488_row41_col5, #T_03488_row43_col10, #T_03488_row51_col6 {\n",
              "  background-color: #dcdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row3_col0, #T_03488_row23_col10, #T_03488_row48_col5, #T_03488_row48_col7 {\n",
              "  background-color: #c5cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row3_col2, #T_03488_row28_col7, #T_03488_row36_col0 {\n",
              "  background-color: #b3c3de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row3_col3, #T_03488_row7_col1, #T_03488_row9_col8, #T_03488_row12_col8, #T_03488_row17_col4, #T_03488_row35_col2, #T_03488_row45_col5, #T_03488_row51_col8 {\n",
              "  background-color: #8eb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row3_col4, #T_03488_row21_col1, #T_03488_row37_col10, #T_03488_row50_col9 {\n",
              "  background-color: #cccfe5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row3_col5, #T_03488_row28_col2, #T_03488_row34_col6, #T_03488_row38_col10 {\n",
              "  background-color: #a2bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row3_col7, #T_03488_row42_col10 {\n",
              "  background-color: #9ebad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row3_col8, #T_03488_row7_col9, #T_03488_row11_col0, #T_03488_row13_col0, #T_03488_row14_col8, #T_03488_row18_col0, #T_03488_row20_col5, #T_03488_row20_col7, #T_03488_row24_col9, #T_03488_row26_col9, #T_03488_row28_col8, #T_03488_row37_col8, #T_03488_row40_col8, #T_03488_row41_col10, #T_03488_row45_col8, #T_03488_row55_col5 {\n",
              "  background-color: #81aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row3_col10, #T_03488_row24_col7, #T_03488_row32_col3, #T_03488_row46_col6 {\n",
              "  background-color: #d2d2e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row4_col0, #T_03488_row6_col2, #T_03488_row20_col4, #T_03488_row21_col9, #T_03488_row31_col0, #T_03488_row39_col9, #T_03488_row55_col0 {\n",
              "  background-color: #79abd0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row4_col2, #T_03488_row13_col2, #T_03488_row21_col8, #T_03488_row25_col2, #T_03488_row25_col8, #T_03488_row27_col2, #T_03488_row27_col9, #T_03488_row32_col8, #T_03488_row34_col2, #T_03488_row34_col8, #T_03488_row34_col9, #T_03488_row41_col2, #T_03488_row53_col0 {\n",
              "  background-color: #76aad0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row4_col3, #T_03488_row6_col3, #T_03488_row6_col6, #T_03488_row6_col8, #T_03488_row6_col9, #T_03488_row7_col0, #T_03488_row13_col1, #T_03488_row13_col3, #T_03488_row13_col5, #T_03488_row17_col5, #T_03488_row20_col3, #T_03488_row20_col9, #T_03488_row23_col8, #T_03488_row23_col9, #T_03488_row32_col0, #T_03488_row39_col2, #T_03488_row46_col8, #T_03488_row48_col8, #T_03488_row53_col2, #T_03488_row55_col3 {\n",
              "  background-color: #80aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row4_col4, #T_03488_row6_col10, #T_03488_row7_col3, #T_03488_row17_col9, #T_03488_row19_col9, #T_03488_row25_col3, #T_03488_row34_col1, #T_03488_row34_col10, #T_03488_row36_col9, #T_03488_row44_col8, #T_03488_row52_col3, #T_03488_row55_col4 {\n",
              "  background-color: #91b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row4_col7, #T_03488_row42_col5, #T_03488_row45_col9, #T_03488_row47_col9, #T_03488_row53_col4 {\n",
              "  background-color: #99b8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row4_col8, #T_03488_row4_col9, #T_03488_row13_col4, #T_03488_row13_col9, #T_03488_row20_col6, #T_03488_row26_col8, #T_03488_row31_col8, #T_03488_row38_col0, #T_03488_row55_col8 {\n",
              "  background-color: #7bacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row5_col0, #T_03488_row14_col1, #T_03488_row17_col10, #T_03488_row21_col0, #T_03488_row36_col10 {\n",
              "  background-color: #adc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row5_col1, #T_03488_row8_col1, #T_03488_row8_col2, #T_03488_row8_col3, #T_03488_row8_col6, #T_03488_row8_col10, #T_03488_row9_col7, #T_03488_row9_col10, #T_03488_row15_col1, #T_03488_row15_col2, #T_03488_row15_col3, #T_03488_row15_col6, #T_03488_row15_col7, #T_03488_row21_col4, #T_03488_row22_col1, #T_03488_row22_col2, #T_03488_row22_col3, #T_03488_row22_col4, #T_03488_row22_col6, #T_03488_row22_col7, #T_03488_row24_col1, #T_03488_row24_col4, #T_03488_row26_col1, #T_03488_row26_col3, #T_03488_row26_col6, #T_03488_row26_col7, #T_03488_row29_col1, #T_03488_row29_col2, #T_03488_row29_col3, #T_03488_row29_col4, #T_03488_row29_col6, #T_03488_row33_col2, #T_03488_row33_col3, #T_03488_row33_col6, #T_03488_row35_col4, #T_03488_row39_col4, #T_03488_row43_col1, #T_03488_row43_col2, #T_03488_row43_col3, #T_03488_row43_col6, #T_03488_row43_col7, #T_03488_row44_col1, #T_03488_row44_col2, #T_03488_row44_col3, #T_03488_row44_col10, #T_03488_row49_col4, #T_03488_row50_col1, #T_03488_row50_col3, #T_03488_row50_col4, #T_03488_row50_col6, #T_03488_row54_col1, #T_03488_row54_col2, #T_03488_row54_col3, #T_03488_row54_col6 {\n",
              "  background-color: #e3e0ee;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row5_col2, #T_03488_row5_col3, #T_03488_row5_col5, #T_03488_row5_col6, #T_03488_row5_col7, #T_03488_row9_col2, #T_03488_row9_col3, #T_03488_row9_col4, #T_03488_row9_col6, #T_03488_row12_col7, #T_03488_row15_col5, #T_03488_row16_col4, #T_03488_row19_col1, #T_03488_row19_col5, #T_03488_row19_col7, #T_03488_row22_col5, #T_03488_row24_col6, #T_03488_row26_col4, #T_03488_row26_col5, #T_03488_row29_col5, #T_03488_row29_col7, #T_03488_row29_col10, #T_03488_row30_col6, #T_03488_row32_col4, #T_03488_row32_col5, #T_03488_row33_col5, #T_03488_row33_col7, #T_03488_row41_col4, #T_03488_row43_col5, #T_03488_row44_col6, #T_03488_row47_col7, #T_03488_row50_col2, #T_03488_row50_col5, #T_03488_row50_col7, #T_03488_row52_col1, #T_03488_row54_col5, #T_03488_row54_col7 {\n",
              "  background-color: #e2dfee;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row5_col4 {\n",
              "  background-color: #ede8f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row5_col9, #T_03488_row53_col7 {\n",
              "  background-color: #93b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row6_col0, #T_03488_row6_col4, #T_03488_row11_col6, #T_03488_row13_col7, #T_03488_row14_col9, #T_03488_row17_col3, #T_03488_row20_col0, #T_03488_row20_col10, #T_03488_row27_col3, #T_03488_row37_col9, #T_03488_row42_col8 {\n",
              "  background-color: #84b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row6_col1, #T_03488_row17_col0, #T_03488_row17_col1, #T_03488_row18_col9, #T_03488_row52_col9 {\n",
              "  background-color: #89b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row6_col5, #T_03488_row6_col7, #T_03488_row7_col2, #T_03488_row10_col7, #T_03488_row11_col7, #T_03488_row13_col10, #T_03488_row33_col8, #T_03488_row53_col3, #T_03488_row53_col5, #T_03488_row55_col10 {\n",
              "  background-color: #88b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row7_col4, #T_03488_row16_col9, #T_03488_row18_col10, #T_03488_row48_col10 {\n",
              "  background-color: #a1bbda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row7_col5, #T_03488_row7_col6, #T_03488_row10_col4, #T_03488_row18_col2, #T_03488_row21_col2, #T_03488_row30_col9, #T_03488_row45_col0, #T_03488_row46_col0, #T_03488_row47_col8, #T_03488_row53_col6 {\n",
              "  background-color: #8cb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row7_col7, #T_03488_row11_col1, #T_03488_row36_col8, #T_03488_row38_col6, #T_03488_row44_col9, #T_03488_row49_col9, #T_03488_row55_col7 {\n",
              "  background-color: #94b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row7_col8, #T_03488_row10_col8, #T_03488_row11_col4, #T_03488_row17_col8, #T_03488_row20_col1, #T_03488_row20_col8, #T_03488_row34_col0, #T_03488_row35_col0, #T_03488_row46_col9, #T_03488_row52_col8, #T_03488_row53_col8, #T_03488_row53_col9 {\n",
              "  background-color: #7dacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row8_col5, #T_03488_row8_col7, #T_03488_row9_col5, #T_03488_row26_col2, #T_03488_row33_col1, #T_03488_row40_col3, #T_03488_row40_col5, #T_03488_row40_col7, #T_03488_row44_col4, #T_03488_row44_col5, #T_03488_row44_col7 {\n",
              "  background-color: #e4e1ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row8_col8, #T_03488_row45_col1 {\n",
              "  background-color: #b9c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row8_col9, #T_03488_row18_col1, #T_03488_row25_col10, #T_03488_row38_col3 {\n",
              "  background-color: #c6cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row10_col0, #T_03488_row14_col2 {\n",
              "  background-color: #9fbad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row10_col5, #T_03488_row10_col6, #T_03488_row10_col9, #T_03488_row11_col3, #T_03488_row11_col5, #T_03488_row11_col8, #T_03488_row17_col6, #T_03488_row17_col7, #T_03488_row18_col3, #T_03488_row18_col4, #T_03488_row28_col9, #T_03488_row39_col8, #T_03488_row40_col9, #T_03488_row49_col8, #T_03488_row55_col2, #T_03488_row55_col9 {\n",
              "  background-color: #83afd3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row11_col2, #T_03488_row13_col8, #T_03488_row24_col8, #T_03488_row25_col9, #T_03488_row31_col9, #T_03488_row32_col2, #T_03488_row35_col8, #T_03488_row35_col9, #T_03488_row38_col9 {\n",
              "  background-color: #78abd0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row11_col9, #T_03488_row13_col6, #T_03488_row48_col9 {\n",
              "  background-color: #7eadd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row11_col10, #T_03488_row18_col8, #T_03488_row37_col0, #T_03488_row39_col10 {\n",
              "  background-color: #96b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row12_col4, #T_03488_row47_col4 {\n",
              "  background-color: #f2ecf5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row12_col6, #T_03488_row16_col2, #T_03488_row16_col5, #T_03488_row16_col6, #T_03488_row19_col6, #T_03488_row30_col7, #T_03488_row32_col7, #T_03488_row36_col4, #T_03488_row38_col2, #T_03488_row47_col1, #T_03488_row49_col1, #T_03488_row52_col6 {\n",
              "  background-color: #e0dded;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row12_col9, #T_03488_row27_col7, #T_03488_row46_col3, #T_03488_row51_col0 {\n",
              "  background-color: #c2cbe2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row12_col10, #T_03488_row15_col9, #T_03488_row39_col6, #T_03488_row52_col5 {\n",
              "  background-color: #bcc7e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row14_col3, #T_03488_row14_col6, #T_03488_row16_col8, #T_03488_row41_col3, #T_03488_row55_col1 {\n",
              "  background-color: #a5bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row14_col5, #T_03488_row15_col8, #T_03488_row30_col0, #T_03488_row45_col4, #T_03488_row52_col0 {\n",
              "  background-color: #b1c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row14_col7, #T_03488_row18_col7, #T_03488_row29_col9, #T_03488_row46_col2, #T_03488_row47_col0 {\n",
              "  background-color: #b0c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row15_col0, #T_03488_row23_col6, #T_03488_row27_col5, #T_03488_row35_col3, #T_03488_row51_col4, #T_03488_row54_col0 {\n",
              "  background-color: #d0d1e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row18_col5, #T_03488_row18_col6, #T_03488_row25_col0, #T_03488_row28_col0, #T_03488_row31_col10, #T_03488_row32_col10, #T_03488_row42_col7 {\n",
              "  background-color: #a4bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row19_col0, #T_03488_row48_col6 {\n",
              "  background-color: #c1cae2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row19_col4, #T_03488_row33_col4 {\n",
              "  background-color: #e7e3f0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row19_col8, #T_03488_row45_col6, #T_03488_row45_col7, #T_03488_row53_col1 {\n",
              "  background-color: #9cb9d9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row20_col2, #T_03488_row27_col8, #T_03488_row32_col9, #T_03488_row38_col8, #T_03488_row41_col0, #T_03488_row41_col8, #T_03488_row41_col9 {\n",
              "  background-color: #75a9cf;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row21_col3, #T_03488_row24_col3, #T_03488_row26_col0, #T_03488_row28_col5, #T_03488_row28_col6, #T_03488_row28_col10, #T_03488_row42_col2, #T_03488_row45_col10 {\n",
              "  background-color: #b8c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row21_col10, #T_03488_row28_col4, #T_03488_row30_col4, #T_03488_row43_col9, #T_03488_row49_col5, #T_03488_row51_col3 {\n",
              "  background-color: #d3d4e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row22_col0, #T_03488_row42_col6, #T_03488_row48_col3 {\n",
              "  background-color: #acc0dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row22_col8, #T_03488_row24_col2, #T_03488_row34_col7, #T_03488_row47_col10, #T_03488_row51_col9, #T_03488_row54_col10 {\n",
              "  background-color: #bdc8e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row23_col0, #T_03488_row54_col9 {\n",
              "  background-color: #c0c9e2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row23_col5, #T_03488_row32_col1, #T_03488_row42_col0, #T_03488_row48_col2 {\n",
              "  background-color: #a7bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row24_col10, #T_03488_row31_col3, #T_03488_row37_col4, #T_03488_row37_col6 {\n",
              "  background-color: #dbdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row25_col4, #T_03488_row30_col1, #T_03488_row30_col2, #T_03488_row33_col10, #T_03488_row48_col4, #T_03488_row51_col1, #T_03488_row51_col5 {\n",
              "  background-color: #dedcec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row27_col1, #T_03488_row31_col2, #T_03488_row39_col3, #T_03488_row49_col2 {\n",
              "  background-color: #c4cbe3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row30_col10, #T_03488_row51_col7 {\n",
              "  background-color: #d8d7e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row35_col1, #T_03488_row36_col7, #T_03488_row40_col10, #T_03488_row49_col0, #T_03488_row52_col10 {\n",
              "  background-color: #c9cee4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row35_col5, #T_03488_row37_col2, #T_03488_row38_col7, #T_03488_row41_col7, #T_03488_row43_col0, #T_03488_row46_col1, #T_03488_row48_col1 {\n",
              "  background-color: #d7d6e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row40_col0, #T_03488_row42_col4, #T_03488_row44_col0 {\n",
              "  background-color: #abbfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row40_col1, #T_03488_row54_col4 {\n",
              "  background-color: #e5e1ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row40_col4 {\n",
              "  background-color: #f5eff6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row42_col1, #T_03488_row55_col6 {\n",
              "  background-color: #afc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_03488_row49_col3 {\n",
              "  background-color: #d5d5e8;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_03488\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank\" >&nbsp;</th>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_03488_level0_col0\" class=\"col_heading level0 col0\" >ar</th>\n",
              "      <th id=\"T_03488_level0_col1\" class=\"col_heading level0 col1\" >ca</th>\n",
              "      <th id=\"T_03488_level0_col2\" class=\"col_heading level0 col2\" >cs</th>\n",
              "      <th id=\"T_03488_level0_col3\" class=\"col_heading level0 col3\" >de</th>\n",
              "      <th id=\"T_03488_level0_col4\" class=\"col_heading level0 col4\" >en</th>\n",
              "      <th id=\"T_03488_level0_col5\" class=\"col_heading level0 col5\" >es</th>\n",
              "      <th id=\"T_03488_level0_col6\" class=\"col_heading level0 col6\" >nl</th>\n",
              "      <th id=\"T_03488_level0_col7\" class=\"col_heading level0 col7\" >pt</th>\n",
              "      <th id=\"T_03488_level0_col8\" class=\"col_heading level0 col8\" >ru</th>\n",
              "      <th id=\"T_03488_level0_col9\" class=\"col_heading level0 col9\" >uk</th>\n",
              "      <th id=\"T_03488_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Train LLM</th>\n",
              "      <th class=\"index_name level1\" >Model</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "      <th class=\"blank col7\" >&nbsp;</th>\n",
              "      <th class=\"blank col8\" >&nbsp;</th>\n",
              "      <th class=\"blank col9\" >&nbsp;</th>\n",
              "      <th class=\"blank col10\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"7\">alpaca-lora-30b</th>\n",
              "      <th id=\"T_03488_level1_row0\" class=\"row_heading level1 row0\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_03488_row0_col0\" class=\"data row0 col0\" >0.6026</td>\n",
              "      <td id=\"T_03488_row0_col1\" class=\"data row0 col1\" >0.6327</td>\n",
              "      <td id=\"T_03488_row0_col2\" class=\"data row0 col2\" >0.8067</td>\n",
              "      <td id=\"T_03488_row0_col3\" class=\"data row0 col3\" >0.7409</td>\n",
              "      <td id=\"T_03488_row0_col4\" class=\"data row0 col4\" >0.4634</td>\n",
              "      <td id=\"T_03488_row0_col5\" class=\"data row0 col5\" >0.6572</td>\n",
              "      <td id=\"T_03488_row0_col6\" class=\"data row0 col6\" >0.6705</td>\n",
              "      <td id=\"T_03488_row0_col7\" class=\"data row0 col7\" >0.6567</td>\n",
              "      <td id=\"T_03488_row0_col8\" class=\"data row0 col8\" >0.9099</td>\n",
              "      <td id=\"T_03488_row0_col9\" class=\"data row0 col9\" >0.8829</td>\n",
              "      <td id=\"T_03488_row0_col10\" class=\"data row0 col10\" >0.6696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row1\" class=\"row_heading level1 row1\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_03488_row1_col0\" class=\"data row1 col0\" >0.4945</td>\n",
              "      <td id=\"T_03488_row1_col1\" class=\"data row1 col1\" >0.3680</td>\n",
              "      <td id=\"T_03488_row1_col2\" class=\"data row1 col2\" >0.3443</td>\n",
              "      <td id=\"T_03488_row1_col3\" class=\"data row1 col3\" >0.3516</td>\n",
              "      <td id=\"T_03488_row1_col4\" class=\"data row1 col4\" >0.3829</td>\n",
              "      <td id=\"T_03488_row1_col5\" class=\"data row1 col5\" >0.5214</td>\n",
              "      <td id=\"T_03488_row1_col6\" class=\"data row1 col6\" >0.4081</td>\n",
              "      <td id=\"T_03488_row1_col7\" class=\"data row1 col7\" >0.3884</td>\n",
              "      <td id=\"T_03488_row1_col8\" class=\"data row1 col8\" >0.5506</td>\n",
              "      <td id=\"T_03488_row1_col9\" class=\"data row1 col9\" >0.5100</td>\n",
              "      <td id=\"T_03488_row1_col10\" class=\"data row1 col10\" >0.4514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row2\" class=\"row_heading level1 row2\" >gpt2-medium</th>\n",
              "      <td id=\"T_03488_row2_col0\" class=\"data row2 col0\" >0.7291</td>\n",
              "      <td id=\"T_03488_row2_col1\" class=\"data row2 col1\" >0.4784</td>\n",
              "      <td id=\"T_03488_row2_col2\" class=\"data row2 col2\" >0.4665</td>\n",
              "      <td id=\"T_03488_row2_col3\" class=\"data row2 col3\" >0.4094</td>\n",
              "      <td id=\"T_03488_row2_col4\" class=\"data row2 col4\" >0.6639</td>\n",
              "      <td id=\"T_03488_row2_col5\" class=\"data row2 col5\" >0.6505</td>\n",
              "      <td id=\"T_03488_row2_col6\" class=\"data row2 col6\" >0.4265</td>\n",
              "      <td id=\"T_03488_row2_col7\" class=\"data row2 col7\" >0.5331</td>\n",
              "      <td id=\"T_03488_row2_col8\" class=\"data row2 col8\" >0.8599</td>\n",
              "      <td id=\"T_03488_row2_col9\" class=\"data row2 col9\" >0.8249</td>\n",
              "      <td id=\"T_03488_row2_col10\" class=\"data row2 col10\" >0.3950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row3\" class=\"row_heading level1 row3\" >mGPT</th>\n",
              "      <td id=\"T_03488_row3_col0\" class=\"data row3 col0\" >0.5635</td>\n",
              "      <td id=\"T_03488_row3_col1\" class=\"data row3 col1\" >0.4123</td>\n",
              "      <td id=\"T_03488_row3_col2\" class=\"data row3 col2\" >0.6725</td>\n",
              "      <td id=\"T_03488_row3_col3\" class=\"data row3 col3\" >0.8716</td>\n",
              "      <td id=\"T_03488_row3_col4\" class=\"data row3 col4\" >0.5272</td>\n",
              "      <td id=\"T_03488_row3_col5\" class=\"data row3 col5\" >0.7714</td>\n",
              "      <td id=\"T_03488_row3_col6\" class=\"data row3 col6\" >0.5325</td>\n",
              "      <td id=\"T_03488_row3_col7\" class=\"data row3 col7\" >0.7920</td>\n",
              "      <td id=\"T_03488_row3_col8\" class=\"data row3 col8\" >0.9350</td>\n",
              "      <td id=\"T_03488_row3_col9\" class=\"data row3 col9\" >0.9129</td>\n",
              "      <td id=\"T_03488_row3_col10\" class=\"data row3 col10\" >0.4906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row4\" class=\"row_heading level1 row4\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_03488_row4_col0\" class=\"data row4 col0\" >0.9716</td>\n",
              "      <td id=\"T_03488_row4_col1\" class=\"data row4 col1\" >0.8123</td>\n",
              "      <td id=\"T_03488_row4_col2\" class=\"data row4 col2\" >0.9850</td>\n",
              "      <td id=\"T_03488_row4_col3\" class=\"data row4 col3\" >0.9409</td>\n",
              "      <td id=\"T_03488_row4_col4\" class=\"data row4 col4\" >0.8550</td>\n",
              "      <td id=\"T_03488_row4_col5\" class=\"data row4 col5\" >0.8885</td>\n",
              "      <td id=\"T_03488_row4_col6\" class=\"data row4 col6\" >0.8868</td>\n",
              "      <td id=\"T_03488_row4_col7\" class=\"data row4 col7\" >0.8199</td>\n",
              "      <td id=\"T_03488_row4_col8\" class=\"data row4 col8\" >0.9617</td>\n",
              "      <td id=\"T_03488_row4_col9\" class=\"data row4 col9\" >0.9615</td>\n",
              "      <td id=\"T_03488_row4_col10\" class=\"data row4 col10\" >0.8237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row5\" class=\"row_heading level1 row5\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_03488_row5_col0\" class=\"data row5 col0\" >0.7062</td>\n",
              "      <td id=\"T_03488_row5_col1\" class=\"data row5 col1\" >0.3326</td>\n",
              "      <td id=\"T_03488_row5_col2\" class=\"data row5 col2\" >0.3407</td>\n",
              "      <td id=\"T_03488_row5_col3\" class=\"data row5 col3\" >0.3363</td>\n",
              "      <td id=\"T_03488_row5_col4\" class=\"data row5 col4\" >0.2381</td>\n",
              "      <td id=\"T_03488_row5_col5\" class=\"data row5 col5\" >0.3394</td>\n",
              "      <td id=\"T_03488_row5_col6\" class=\"data row5 col6\" >0.3370</td>\n",
              "      <td id=\"T_03488_row5_col7\" class=\"data row5 col7\" >0.3421</td>\n",
              "      <td id=\"T_03488_row5_col8\" class=\"data row5 col8\" >0.8599</td>\n",
              "      <td id=\"T_03488_row5_col9\" class=\"data row5 col9\" >0.8464</td>\n",
              "      <td id=\"T_03488_row5_col10\" class=\"data row5 col10\" >0.5544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row6\" class=\"row_heading level1 row6\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_03488_row6_col0\" class=\"data row6 col0\" >0.9180</td>\n",
              "      <td id=\"T_03488_row6_col1\" class=\"data row6 col1\" >0.8983</td>\n",
              "      <td id=\"T_03488_row6_col2\" class=\"data row6 col2\" >0.9700</td>\n",
              "      <td id=\"T_03488_row6_col3\" class=\"data row6 col3\" >0.9442</td>\n",
              "      <td id=\"T_03488_row6_col4\" class=\"data row6 col4\" >0.9169</td>\n",
              "      <td id=\"T_03488_row6_col5\" class=\"data row6 col5\" >0.9058</td>\n",
              "      <td id=\"T_03488_row6_col6\" class=\"data row6 col6\" >0.9431</td>\n",
              "      <td id=\"T_03488_row6_col7\" class=\"data row6 col7\" >0.8993</td>\n",
              "      <td id=\"T_03488_row6_col8\" class=\"data row6 col8\" >0.9433</td>\n",
              "      <td id=\"T_03488_row6_col9\" class=\"data row6 col9\" >0.9397</td>\n",
              "      <td id=\"T_03488_row6_col10\" class=\"data row6 col10\" >0.8522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level0_row7\" class=\"row_heading level0 row7\" rowspan=\"7\">gpt-3.5-turbo</th>\n",
              "      <th id=\"T_03488_level1_row7\" class=\"row_heading level1 row7\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_03488_row7_col0\" class=\"data row7 col0\" >0.9416</td>\n",
              "      <td id=\"T_03488_row7_col1\" class=\"data row7 col1\" >0.8680</td>\n",
              "      <td id=\"T_03488_row7_col2\" class=\"data row7 col2\" >0.9045</td>\n",
              "      <td id=\"T_03488_row7_col3\" class=\"data row7 col3\" >0.8538</td>\n",
              "      <td id=\"T_03488_row7_col4\" class=\"data row7 col4\" >0.7810</td>\n",
              "      <td id=\"T_03488_row7_col5\" class=\"data row7 col5\" >0.8754</td>\n",
              "      <td id=\"T_03488_row7_col6\" class=\"data row7 col6\" >0.8783</td>\n",
              "      <td id=\"T_03488_row7_col7\" class=\"data row7 col7\" >0.8378</td>\n",
              "      <td id=\"T_03488_row7_col8\" class=\"data row7 col8\" >0.9567</td>\n",
              "      <td id=\"T_03488_row7_col9\" class=\"data row7 col9\" >0.9331</td>\n",
              "      <td id=\"T_03488_row7_col10\" class=\"data row7 col10\" >0.8666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row8\" class=\"row_heading level1 row8\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_03488_row8_col0\" class=\"data row8 col0\" >0.6636</td>\n",
              "      <td id=\"T_03488_row8_col1\" class=\"data row8 col1\" >0.3333</td>\n",
              "      <td id=\"T_03488_row8_col2\" class=\"data row8 col2\" >0.3333</td>\n",
              "      <td id=\"T_03488_row8_col3\" class=\"data row8 col3\" >0.3303</td>\n",
              "      <td id=\"T_03488_row8_col4\" class=\"data row8 col4\" >0.3457</td>\n",
              "      <td id=\"T_03488_row8_col5\" class=\"data row8 col5\" >0.3272</td>\n",
              "      <td id=\"T_03488_row8_col6\" class=\"data row8 col6\" >0.3330</td>\n",
              "      <td id=\"T_03488_row8_col7\" class=\"data row8 col7\" >0.3276</td>\n",
              "      <td id=\"T_03488_row8_col8\" class=\"data row8 col8\" >0.6336</td>\n",
              "      <td id=\"T_03488_row8_col9\" class=\"data row8 col9\" >0.5592</td>\n",
              "      <td id=\"T_03488_row8_col10\" class=\"data row8 col10\" >0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row9\" class=\"row_heading level1 row9\" >gpt2-medium</th>\n",
              "      <td id=\"T_03488_row9_col0\" class=\"data row9 col0\" >0.5384</td>\n",
              "      <td id=\"T_03488_row9_col1\" class=\"data row9 col1\" >0.3443</td>\n",
              "      <td id=\"T_03488_row9_col2\" class=\"data row9 col2\" >0.3407</td>\n",
              "      <td id=\"T_03488_row9_col3\" class=\"data row9 col3\" >0.3369</td>\n",
              "      <td id=\"T_03488_row9_col4\" class=\"data row9 col4\" >0.3423</td>\n",
              "      <td id=\"T_03488_row9_col5\" class=\"data row9 col5\" >0.3264</td>\n",
              "      <td id=\"T_03488_row9_col6\" class=\"data row9 col6\" >0.3403</td>\n",
              "      <td id=\"T_03488_row9_col7\" class=\"data row9 col7\" >0.3350</td>\n",
              "      <td id=\"T_03488_row9_col8\" class=\"data row9 col8\" >0.8716</td>\n",
              "      <td id=\"T_03488_row9_col9\" class=\"data row9 col9\" >0.8061</td>\n",
              "      <td id=\"T_03488_row9_col10\" class=\"data row9 col10\" >0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row10\" class=\"row_heading level1 row10\" >mGPT</th>\n",
              "      <td id=\"T_03488_row10_col0\" class=\"data row10 col0\" >0.7846</td>\n",
              "      <td id=\"T_03488_row10_col1\" class=\"data row10 col1\" >0.8857</td>\n",
              "      <td id=\"T_03488_row10_col2\" class=\"data row10 col2\" >0.3552</td>\n",
              "      <td id=\"T_03488_row10_col3\" class=\"data row10 col3\" >0.9070</td>\n",
              "      <td id=\"T_03488_row10_col4\" class=\"data row10 col4\" >0.8817</td>\n",
              "      <td id=\"T_03488_row10_col5\" class=\"data row10 col5\" >0.9228</td>\n",
              "      <td id=\"T_03488_row10_col6\" class=\"data row10 col6\" >0.9230</td>\n",
              "      <td id=\"T_03488_row10_col7\" class=\"data row10 col7\" >0.9043</td>\n",
              "      <td id=\"T_03488_row10_col8\" class=\"data row10 col8\" >0.9533</td>\n",
              "      <td id=\"T_03488_row10_col9\" class=\"data row10 col9\" >0.9244</td>\n",
              "      <td id=\"T_03488_row10_col10\" class=\"data row10 col10\" >0.6258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row11\" class=\"row_heading level1 row11\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_03488_row11_col0\" class=\"data row11 col0\" >0.9365</td>\n",
              "      <td id=\"T_03488_row11_col1\" class=\"data row11 col1\" >0.8387</td>\n",
              "      <td id=\"T_03488_row11_col2\" class=\"data row11 col2\" >0.9817</td>\n",
              "      <td id=\"T_03488_row11_col3\" class=\"data row11 col3\" >0.9256</td>\n",
              "      <td id=\"T_03488_row11_col4\" class=\"data row11 col4\" >0.9603</td>\n",
              "      <td id=\"T_03488_row11_col5\" class=\"data row11 col5\" >0.9296</td>\n",
              "      <td id=\"T_03488_row11_col6\" class=\"data row11 col6\" >0.9146</td>\n",
              "      <td id=\"T_03488_row11_col7\" class=\"data row11 col7\" >0.9023</td>\n",
              "      <td id=\"T_03488_row11_col8\" class=\"data row11 col8\" >0.9283</td>\n",
              "      <td id=\"T_03488_row11_col9\" class=\"data row11 col9\" >0.9482</td>\n",
              "      <td id=\"T_03488_row11_col10\" class=\"data row11 col10\" >0.8329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row12\" class=\"row_heading level1 row12\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_03488_row12_col0\" class=\"data row12 col0\" >0.3496</td>\n",
              "      <td id=\"T_03488_row12_col1\" class=\"data row12 col1\" >0.3472</td>\n",
              "      <td id=\"T_03488_row12_col2\" class=\"data row12 col2\" >0.3720</td>\n",
              "      <td id=\"T_03488_row12_col3\" class=\"data row12 col3\" >0.3476</td>\n",
              "      <td id=\"T_03488_row12_col4\" class=\"data row12 col4\" >0.1697</td>\n",
              "      <td id=\"T_03488_row12_col5\" class=\"data row12 col5\" >0.3471</td>\n",
              "      <td id=\"T_03488_row12_col6\" class=\"data row12 col6\" >0.3663</td>\n",
              "      <td id=\"T_03488_row12_col7\" class=\"data row12 col7\" >0.3421</td>\n",
              "      <td id=\"T_03488_row12_col8\" class=\"data row12 col8\" >0.8695</td>\n",
              "      <td id=\"T_03488_row12_col9\" class=\"data row12 col9\" >0.5855</td>\n",
              "      <td id=\"T_03488_row12_col10\" class=\"data row12 col10\" >0.6240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row13\" class=\"row_heading level1 row13\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_03488_row13_col0\" class=\"data row13 col0\" >0.9332</td>\n",
              "      <td id=\"T_03488_row13_col1\" class=\"data row13 col1\" >0.9432</td>\n",
              "      <td id=\"T_03488_row13_col2\" class=\"data row13 col2\" >0.9883</td>\n",
              "      <td id=\"T_03488_row13_col3\" class=\"data row13 col3\" >0.9391</td>\n",
              "      <td id=\"T_03488_row13_col4\" class=\"data row13 col4\" >0.9675</td>\n",
              "      <td id=\"T_03488_row13_col5\" class=\"data row13 col5\" >0.9435</td>\n",
              "      <td id=\"T_03488_row13_col6\" class=\"data row13 col6\" >0.9466</td>\n",
              "      <td id=\"T_03488_row13_col7\" class=\"data row13 col7\" >0.9143</td>\n",
              "      <td id=\"T_03488_row13_col8\" class=\"data row13 col8\" >0.9767</td>\n",
              "      <td id=\"T_03488_row13_col9\" class=\"data row13 col9\" >0.9615</td>\n",
              "      <td id=\"T_03488_row13_col10\" class=\"data row13 col10\" >0.9048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level0_row14\" class=\"row_heading level0 row14\" rowspan=\"7\">gpt-4</th>\n",
              "      <th id=\"T_03488_level1_row14\" class=\"row_heading level1 row14\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_03488_row14_col0\" class=\"data row14 col0\" >0.8667</td>\n",
              "      <td id=\"T_03488_row14_col1\" class=\"data row14 col1\" >0.7067</td>\n",
              "      <td id=\"T_03488_row14_col2\" class=\"data row14 col2\" >0.7828</td>\n",
              "      <td id=\"T_03488_row14_col3\" class=\"data row14 col3\" >0.7535</td>\n",
              "      <td id=\"T_03488_row14_col4\" class=\"data row14 col4\" >0.4958</td>\n",
              "      <td id=\"T_03488_row14_col5\" class=\"data row14 col5\" >0.6805</td>\n",
              "      <td id=\"T_03488_row14_col6\" class=\"data row14 col6\" >0.7514</td>\n",
              "      <td id=\"T_03488_row14_col7\" class=\"data row14 col7\" >0.6918</td>\n",
              "      <td id=\"T_03488_row14_col8\" class=\"data row14 col8\" >0.9333</td>\n",
              "      <td id=\"T_03488_row14_col9\" class=\"data row14 col9\" >0.9214</td>\n",
              "      <td id=\"T_03488_row14_col10\" class=\"data row14 col10\" >0.8122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row15\" class=\"row_heading level1 row15\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_03488_row15_col0\" class=\"data row15 col0\" >0.5003</td>\n",
              "      <td id=\"T_03488_row15_col1\" class=\"data row15 col1\" >0.3311</td>\n",
              "      <td id=\"T_03488_row15_col2\" class=\"data row15 col2\" >0.3333</td>\n",
              "      <td id=\"T_03488_row15_col3\" class=\"data row15 col3\" >0.3303</td>\n",
              "      <td id=\"T_03488_row15_col4\" class=\"data row15 col4\" >0.3718</td>\n",
              "      <td id=\"T_03488_row15_col5\" class=\"data row15 col5\" >0.3374</td>\n",
              "      <td id=\"T_03488_row15_col6\" class=\"data row15 col6\" >0.3292</td>\n",
              "      <td id=\"T_03488_row15_col7\" class=\"data row15 col7\" >0.3326</td>\n",
              "      <td id=\"T_03488_row15_col8\" class=\"data row15 col8\" >0.6837</td>\n",
              "      <td id=\"T_03488_row15_col9\" class=\"data row15 col9\" >0.6241</td>\n",
              "      <td id=\"T_03488_row15_col10\" class=\"data row15 col10\" >0.3677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row16\" class=\"row_heading level1 row16\" >gpt2-medium</th>\n",
              "      <td id=\"T_03488_row16_col0\" class=\"data row16 col0\" >0.5234</td>\n",
              "      <td id=\"T_03488_row16_col1\" class=\"data row16 col1\" >0.3587</td>\n",
              "      <td id=\"T_03488_row16_col2\" class=\"data row16 col2\" >0.3623</td>\n",
              "      <td id=\"T_03488_row16_col3\" class=\"data row16 col3\" >0.3514</td>\n",
              "      <td id=\"T_03488_row16_col4\" class=\"data row16 col4\" >0.3368</td>\n",
              "      <td id=\"T_03488_row16_col5\" class=\"data row16 col5\" >0.3659</td>\n",
              "      <td id=\"T_03488_row16_col6\" class=\"data row16 col6\" >0.3655</td>\n",
              "      <td id=\"T_03488_row16_col7\" class=\"data row16 col7\" >0.3458</td>\n",
              "      <td id=\"T_03488_row16_col8\" class=\"data row16 col8\" >0.7535</td>\n",
              "      <td id=\"T_03488_row16_col9\" class=\"data row16 col9\" >0.7794</td>\n",
              "      <td id=\"T_03488_row16_col10\" class=\"data row16 col10\" >0.3508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row17\" class=\"row_heading level1 row17\" >mGPT</th>\n",
              "      <td id=\"T_03488_row17_col0\" class=\"data row17 col0\" >0.8978</td>\n",
              "      <td id=\"T_03488_row17_col1\" class=\"data row17 col1\" >0.8978</td>\n",
              "      <td id=\"T_03488_row17_col2\" class=\"data row17 col2\" >0.3587</td>\n",
              "      <td id=\"T_03488_row17_col3\" class=\"data row17 col3\" >0.9188</td>\n",
              "      <td id=\"T_03488_row17_col4\" class=\"data row17 col4\" >0.8713</td>\n",
              "      <td id=\"T_03488_row17_col5\" class=\"data row17 col5\" >0.9383</td>\n",
              "      <td id=\"T_03488_row17_col6\" class=\"data row17 col6\" >0.9246</td>\n",
              "      <td id=\"T_03488_row17_col7\" class=\"data row17 col7\" >0.9264</td>\n",
              "      <td id=\"T_03488_row17_col8\" class=\"data row17 col8\" >0.9566</td>\n",
              "      <td id=\"T_03488_row17_col9\" class=\"data row17 col9\" >0.8570</td>\n",
              "      <td id=\"T_03488_row17_col10\" class=\"data row17 col10\" >0.7078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row18\" class=\"row_heading level1 row18\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_03488_row18_col0\" class=\"data row18 col0\" >0.9349</td>\n",
              "      <td id=\"T_03488_row18_col1\" class=\"data row18 col1\" >0.5556</td>\n",
              "      <td id=\"T_03488_row18_col2\" class=\"data row18 col2\" >0.8788</td>\n",
              "      <td id=\"T_03488_row18_col3\" class=\"data row18 col3\" >0.9221</td>\n",
              "      <td id=\"T_03488_row18_col4\" class=\"data row18 col4\" >0.9259</td>\n",
              "      <td id=\"T_03488_row18_col5\" class=\"data row18 col5\" >0.7611</td>\n",
              "      <td id=\"T_03488_row18_col6\" class=\"data row18 col6\" >0.7609</td>\n",
              "      <td id=\"T_03488_row18_col7\" class=\"data row18 col7\" >0.6932</td>\n",
              "      <td id=\"T_03488_row18_col8\" class=\"data row18 col8\" >0.8305</td>\n",
              "      <td id=\"T_03488_row18_col9\" class=\"data row18 col9\" >0.8940</td>\n",
              "      <td id=\"T_03488_row18_col10\" class=\"data row18 col10\" >0.7797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row19\" class=\"row_heading level1 row19\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_03488_row19_col0\" class=\"data row19 col0\" >0.5936</td>\n",
              "      <td id=\"T_03488_row19_col1\" class=\"data row19 col1\" >0.3407</td>\n",
              "      <td id=\"T_03488_row19_col2\" class=\"data row19 col2\" >0.3480</td>\n",
              "      <td id=\"T_03488_row19_col3\" class=\"data row19 col3\" >0.3439</td>\n",
              "      <td id=\"T_03488_row19_col4\" class=\"data row19 col4\" >0.2994</td>\n",
              "      <td id=\"T_03488_row19_col5\" class=\"data row19 col5\" >0.3433</td>\n",
              "      <td id=\"T_03488_row19_col6\" class=\"data row19 col6\" >0.3628</td>\n",
              "      <td id=\"T_03488_row19_col7\" class=\"data row19 col7\" >0.3421</td>\n",
              "      <td id=\"T_03488_row19_col8\" class=\"data row19 col8\" >0.7991</td>\n",
              "      <td id=\"T_03488_row19_col9\" class=\"data row19 col9\" >0.8526</td>\n",
              "      <td id=\"T_03488_row19_col10\" class=\"data row19 col10\" >0.6649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row20\" class=\"row_heading level1 row20\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_03488_row20_col0\" class=\"data row20 col0\" >0.9215</td>\n",
              "      <td id=\"T_03488_row20_col1\" class=\"data row20 col1\" >0.9600</td>\n",
              "      <td id=\"T_03488_row20_col2\" class=\"data row20 col2\" >0.9950</td>\n",
              "      <td id=\"T_03488_row20_col3\" class=\"data row20 col3\" >0.9391</td>\n",
              "      <td id=\"T_03488_row20_col4\" class=\"data row20 col4\" >0.9765</td>\n",
              "      <td id=\"T_03488_row20_col5\" class=\"data row20 col5\" >0.9349</td>\n",
              "      <td id=\"T_03488_row20_col6\" class=\"data row20 col6\" >0.9633</td>\n",
              "      <td id=\"T_03488_row20_col7\" class=\"data row20 col7\" >0.9368</td>\n",
              "      <td id=\"T_03488_row20_col8\" class=\"data row20 col8\" >0.9567</td>\n",
              "      <td id=\"T_03488_row20_col9\" class=\"data row20 col9\" >0.9447</td>\n",
              "      <td id=\"T_03488_row20_col10\" class=\"data row20 col10\" >0.9214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level0_row21\" class=\"row_heading level0 row21\" rowspan=\"7\">llama-65b</th>\n",
              "      <th id=\"T_03488_level1_row21\" class=\"row_heading level1 row21\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_03488_row21_col0\" class=\"data row21 col0\" >0.7078</td>\n",
              "      <td id=\"T_03488_row21_col1\" class=\"data row21 col1\" >0.5300</td>\n",
              "      <td id=\"T_03488_row21_col2\" class=\"data row21 col2\" >0.8750</td>\n",
              "      <td id=\"T_03488_row21_col3\" class=\"data row21 col3\" >0.6477</td>\n",
              "      <td id=\"T_03488_row21_col4\" class=\"data row21 col4\" >0.3325</td>\n",
              "      <td id=\"T_03488_row21_col5\" class=\"data row21 col5\" >0.8211</td>\n",
              "      <td id=\"T_03488_row21_col6\" class=\"data row21 col6\" >0.5191</td>\n",
              "      <td id=\"T_03488_row21_col7\" class=\"data row21 col7\" >0.6043</td>\n",
              "      <td id=\"T_03488_row21_col8\" class=\"data row21 col8\" >0.9849</td>\n",
              "      <td id=\"T_03488_row21_col9\" class=\"data row21 col9\" >0.9764</td>\n",
              "      <td id=\"T_03488_row21_col10\" class=\"data row21 col10\" >0.4765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row22\" class=\"row_heading level1 row22\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_03488_row22_col0\" class=\"data row22 col0\" >0.7140</td>\n",
              "      <td id=\"T_03488_row22_col1\" class=\"data row22 col1\" >0.3303</td>\n",
              "      <td id=\"T_03488_row22_col2\" class=\"data row22 col2\" >0.3299</td>\n",
              "      <td id=\"T_03488_row22_col3\" class=\"data row22 col3\" >0.3348</td>\n",
              "      <td id=\"T_03488_row22_col4\" class=\"data row22 col4\" >0.3325</td>\n",
              "      <td id=\"T_03488_row22_col5\" class=\"data row22 col5\" >0.3383</td>\n",
              "      <td id=\"T_03488_row22_col6\" class=\"data row22 col6\" >0.3330</td>\n",
              "      <td id=\"T_03488_row22_col7\" class=\"data row22 col7\" >0.3356</td>\n",
              "      <td id=\"T_03488_row22_col8\" class=\"data row22 col8\" >0.6109</td>\n",
              "      <td id=\"T_03488_row22_col9\" class=\"data row22 col9\" >0.6025</td>\n",
              "      <td id=\"T_03488_row22_col10\" class=\"data row22 col10\" >0.3676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row23\" class=\"row_heading level1 row23\" >gpt2-medium</th>\n",
              "      <td id=\"T_03488_row23_col0\" class=\"data row23 col0\" >0.6015</td>\n",
              "      <td id=\"T_03488_row23_col1\" class=\"data row23 col1\" >0.4216</td>\n",
              "      <td id=\"T_03488_row23_col2\" class=\"data row23 col2\" >0.6322</td>\n",
              "      <td id=\"T_03488_row23_col3\" class=\"data row23 col3\" >0.6629</td>\n",
              "      <td id=\"T_03488_row23_col4\" class=\"data row23 col4\" >0.4170</td>\n",
              "      <td id=\"T_03488_row23_col5\" class=\"data row23 col5\" >0.7424</td>\n",
              "      <td id=\"T_03488_row23_col6\" class=\"data row23 col6\" >0.5012</td>\n",
              "      <td id=\"T_03488_row23_col7\" class=\"data row23 col7\" >0.4982</td>\n",
              "      <td id=\"T_03488_row23_col8\" class=\"data row23 col8\" >0.9395</td>\n",
              "      <td id=\"T_03488_row23_col9\" class=\"data row23 col9\" >0.9394</td>\n",
              "      <td id=\"T_03488_row23_col10\" class=\"data row23 col10\" >0.5690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row24\" class=\"row_heading level1 row24\" >mGPT</th>\n",
              "      <td id=\"T_03488_row24_col0\" class=\"data row24 col0\" >0.5347</td>\n",
              "      <td id=\"T_03488_row24_col1\" class=\"data row24 col1\" >0.3326</td>\n",
              "      <td id=\"T_03488_row24_col2\" class=\"data row24 col2\" >0.6154</td>\n",
              "      <td id=\"T_03488_row24_col3\" class=\"data row24 col3\" >0.6470</td>\n",
              "      <td id=\"T_03488_row24_col4\" class=\"data row24 col4\" >0.3325</td>\n",
              "      <td id=\"T_03488_row24_col5\" class=\"data row24 col5\" >0.4964</td>\n",
              "      <td id=\"T_03488_row24_col6\" class=\"data row24 col6\" >0.3404</td>\n",
              "      <td id=\"T_03488_row24_col7\" class=\"data row24 col7\" >0.4847</td>\n",
              "      <td id=\"T_03488_row24_col8\" class=\"data row24 col8\" >0.9782</td>\n",
              "      <td id=\"T_03488_row24_col9\" class=\"data row24 col9\" >0.9359</td>\n",
              "      <td id=\"T_03488_row24_col10\" class=\"data row24 col10\" >0.4056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row25\" class=\"row_heading level1 row25\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_03488_row25_col0\" class=\"data row25 col0\" >0.7629</td>\n",
              "      <td id=\"T_03488_row25_col1\" class=\"data row25 col1\" >0.4277</td>\n",
              "      <td id=\"T_03488_row25_col2\" class=\"data row25 col2\" >0.9848</td>\n",
              "      <td id=\"T_03488_row25_col3\" class=\"data row25 col3\" >0.8540</td>\n",
              "      <td id=\"T_03488_row25_col4\" class=\"data row25 col4\" >0.3824</td>\n",
              "      <td id=\"T_03488_row25_col5\" class=\"data row25 col5\" >0.7305</td>\n",
              "      <td id=\"T_03488_row25_col6\" class=\"data row25 col6\" >0.4145</td>\n",
              "      <td id=\"T_03488_row25_col7\" class=\"data row25 col7\" >0.5383</td>\n",
              "      <td id=\"T_03488_row25_col8\" class=\"data row25 col8\" >0.9883</td>\n",
              "      <td id=\"T_03488_row25_col9\" class=\"data row25 col9\" >0.9815</td>\n",
              "      <td id=\"T_03488_row25_col10\" class=\"data row25 col10\" >0.5622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row26\" class=\"row_heading level1 row26\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_03488_row26_col0\" class=\"data row26 col0\" >0.6454</td>\n",
              "      <td id=\"T_03488_row26_col1\" class=\"data row26 col1\" >0.3311</td>\n",
              "      <td id=\"T_03488_row26_col2\" class=\"data row26 col2\" >0.3269</td>\n",
              "      <td id=\"T_03488_row26_col3\" class=\"data row26 col3\" >0.3341</td>\n",
              "      <td id=\"T_03488_row26_col4\" class=\"data row26 col4\" >0.3388</td>\n",
              "      <td id=\"T_03488_row26_col5\" class=\"data row26 col5\" >0.3375</td>\n",
              "      <td id=\"T_03488_row26_col6\" class=\"data row26 col6\" >0.3307</td>\n",
              "      <td id=\"T_03488_row26_col7\" class=\"data row26 col7\" >0.3356</td>\n",
              "      <td id=\"T_03488_row26_col8\" class=\"data row26 col8\" >0.9631</td>\n",
              "      <td id=\"T_03488_row26_col9\" class=\"data row26 col9\" >0.9309</td>\n",
              "      <td id=\"T_03488_row26_col10\" class=\"data row26 col10\" >0.3969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row27\" class=\"row_heading level1 row27\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_03488_row27_col0\" class=\"data row27 col0\" >0.8858</td>\n",
              "      <td id=\"T_03488_row27_col1\" class=\"data row27 col1\" >0.5705</td>\n",
              "      <td id=\"T_03488_row27_col2\" class=\"data row27 col2\" >0.9865</td>\n",
              "      <td id=\"T_03488_row27_col3\" class=\"data row27 col3\" >0.9160</td>\n",
              "      <td id=\"T_03488_row27_col4\" class=\"data row27 col4\" >0.3705</td>\n",
              "      <td id=\"T_03488_row27_col5\" class=\"data row27 col5\" >0.5024</td>\n",
              "      <td id=\"T_03488_row27_col6\" class=\"data row27 col6\" >0.3674</td>\n",
              "      <td id=\"T_03488_row27_col7\" class=\"data row27 col7\" >0.5826</td>\n",
              "      <td id=\"T_03488_row27_col8\" class=\"data row27 col8\" >0.9933</td>\n",
              "      <td id=\"T_03488_row27_col9\" class=\"data row27 col9\" >0.9882</td>\n",
              "      <td id=\"T_03488_row27_col10\" class=\"data row27 col10\" >0.6314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level0_row28\" class=\"row_heading level0 row28\" rowspan=\"7\">opt-66b</th>\n",
              "      <th id=\"T_03488_level1_row28\" class=\"row_heading level1 row28\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_03488_row28_col0\" class=\"data row28 col0\" >0.7593</td>\n",
              "      <td id=\"T_03488_row28_col1\" class=\"data row28 col1\" >0.6061</td>\n",
              "      <td id=\"T_03488_row28_col2\" class=\"data row28 col2\" >0.7673</td>\n",
              "      <td id=\"T_03488_row28_col3\" class=\"data row28 col3\" >0.6078</td>\n",
              "      <td id=\"T_03488_row28_col4\" class=\"data row28 col4\" >0.4720</td>\n",
              "      <td id=\"T_03488_row28_col5\" class=\"data row28 col5\" >0.6423</td>\n",
              "      <td id=\"T_03488_row28_col6\" class=\"data row28 col6\" >0.6428</td>\n",
              "      <td id=\"T_03488_row28_col7\" class=\"data row28 col7\" >0.6734</td>\n",
              "      <td id=\"T_03488_row28_col8\" class=\"data row28 col8\" >0.9340</td>\n",
              "      <td id=\"T_03488_row28_col9\" class=\"data row28 col9\" >0.9254</td>\n",
              "      <td id=\"T_03488_row28_col10\" class=\"data row28 col10\" >0.6408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row29\" class=\"row_heading level1 row29\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_03488_row29_col0\" class=\"data row29 col0\" >0.5543</td>\n",
              "      <td id=\"T_03488_row29_col1\" class=\"data row29 col1\" >0.3326</td>\n",
              "      <td id=\"T_03488_row29_col2\" class=\"data row29 col2\" >0.3333</td>\n",
              "      <td id=\"T_03488_row29_col3\" class=\"data row29 col3\" >0.3356</td>\n",
              "      <td id=\"T_03488_row29_col4\" class=\"data row29 col4\" >0.3325</td>\n",
              "      <td id=\"T_03488_row29_col5\" class=\"data row29 col5\" >0.3394</td>\n",
              "      <td id=\"T_03488_row29_col6\" class=\"data row29 col6\" >0.3337</td>\n",
              "      <td id=\"T_03488_row29_col7\" class=\"data row29 col7\" >0.3375</td>\n",
              "      <td id=\"T_03488_row29_col8\" class=\"data row29 col8\" >0.7304</td>\n",
              "      <td id=\"T_03488_row29_col9\" class=\"data row29 col9\" >0.6932</td>\n",
              "      <td id=\"T_03488_row29_col10\" class=\"data row29 col10\" >0.3425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row30\" class=\"row_heading level1 row30\" >gpt2-medium</th>\n",
              "      <td id=\"T_03488_row30_col0\" class=\"data row30 col0\" >0.6817</td>\n",
              "      <td id=\"T_03488_row30_col1\" class=\"data row30 col1\" >0.3755</td>\n",
              "      <td id=\"T_03488_row30_col2\" class=\"data row30 col2\" >0.3814</td>\n",
              "      <td id=\"T_03488_row30_col3\" class=\"data row30 col3\" >0.3576</td>\n",
              "      <td id=\"T_03488_row30_col4\" class=\"data row30 col4\" >0.4726</td>\n",
              "      <td id=\"T_03488_row30_col5\" class=\"data row30 col5\" >0.3972</td>\n",
              "      <td id=\"T_03488_row30_col6\" class=\"data row30 col6\" >0.3388</td>\n",
              "      <td id=\"T_03488_row30_col7\" class=\"data row30 col7\" >0.3598</td>\n",
              "      <td id=\"T_03488_row30_col8\" class=\"data row30 col8\" >0.9085</td>\n",
              "      <td id=\"T_03488_row30_col9\" class=\"data row30 col9\" >0.8797</td>\n",
              "      <td id=\"T_03488_row30_col10\" class=\"data row30 col10\" >0.4361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row31\" class=\"row_heading level1 row31\" >mGPT</th>\n",
              "      <td id=\"T_03488_row31_col0\" class=\"data row31 col0\" >0.9714</td>\n",
              "      <td id=\"T_03488_row31_col1\" class=\"data row31 col1\" >0.5123</td>\n",
              "      <td id=\"T_03488_row31_col2\" class=\"data row31 col2\" >0.5776</td>\n",
              "      <td id=\"T_03488_row31_col3\" class=\"data row31 col3\" >0.3999</td>\n",
              "      <td id=\"T_03488_row31_col4\" class=\"data row31 col4\" >0.3473</td>\n",
              "      <td id=\"T_03488_row31_col5\" class=\"data row31 col5\" >0.4262</td>\n",
              "      <td id=\"T_03488_row31_col6\" class=\"data row31 col6\" >0.6050</td>\n",
              "      <td id=\"T_03488_row31_col7\" class=\"data row31 col7\" >0.4165</td>\n",
              "      <td id=\"T_03488_row31_col8\" class=\"data row31 col8\" >0.9662</td>\n",
              "      <td id=\"T_03488_row31_col9\" class=\"data row31 col9\" >0.9814</td>\n",
              "      <td id=\"T_03488_row31_col10\" class=\"data row31 col10\" >0.7649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row32\" class=\"row_heading level1 row32\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_03488_row32_col0\" class=\"data row32 col0\" >0.9393</td>\n",
              "      <td id=\"T_03488_row32_col1\" class=\"data row32 col1\" >0.7428</td>\n",
              "      <td id=\"T_03488_row32_col2\" class=\"data row32 col2\" >0.9833</td>\n",
              "      <td id=\"T_03488_row32_col3\" class=\"data row32 col3\" >0.4860</td>\n",
              "      <td id=\"T_03488_row32_col4\" class=\"data row32 col4\" >0.3413</td>\n",
              "      <td id=\"T_03488_row32_col5\" class=\"data row32 col5\" >0.3394</td>\n",
              "      <td id=\"T_03488_row32_col6\" class=\"data row32 col6\" >0.6255</td>\n",
              "      <td id=\"T_03488_row32_col7\" class=\"data row32 col7\" >0.3610</td>\n",
              "      <td id=\"T_03488_row32_col8\" class=\"data row32 col8\" >0.9848</td>\n",
              "      <td id=\"T_03488_row32_col9\" class=\"data row32 col9\" >0.9932</td>\n",
              "      <td id=\"T_03488_row32_col10\" class=\"data row32 col10\" >0.7645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row33\" class=\"row_heading level1 row33\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_03488_row33_col0\" class=\"data row33 col0\" >0.4195</td>\n",
              "      <td id=\"T_03488_row33_col1\" class=\"data row33 col1\" >0.3266</td>\n",
              "      <td id=\"T_03488_row33_col2\" class=\"data row33 col2\" >0.3326</td>\n",
              "      <td id=\"T_03488_row33_col3\" class=\"data row33 col3\" >0.3348</td>\n",
              "      <td id=\"T_03488_row33_col4\" class=\"data row33 col4\" >0.3046</td>\n",
              "      <td id=\"T_03488_row33_col5\" class=\"data row33 col5\" >0.3386</td>\n",
              "      <td id=\"T_03488_row33_col6\" class=\"data row33 col6\" >0.3322</td>\n",
              "      <td id=\"T_03488_row33_col7\" class=\"data row33 col7\" >0.3375</td>\n",
              "      <td id=\"T_03488_row33_col8\" class=\"data row33 col8\" >0.9003</td>\n",
              "      <td id=\"T_03488_row33_col9\" class=\"data row33 col9\" >0.8830</td>\n",
              "      <td id=\"T_03488_row33_col10\" class=\"data row33 col10\" >0.3759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row34\" class=\"row_heading level1 row34\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_03488_row34_col0\" class=\"data row34 col0\" >0.9546</td>\n",
              "      <td id=\"T_03488_row34_col1\" class=\"data row34 col1\" >0.8552</td>\n",
              "      <td id=\"T_03488_row34_col2\" class=\"data row34 col2\" >0.9917</td>\n",
              "      <td id=\"T_03488_row34_col3\" class=\"data row34 col3\" >0.5544</td>\n",
              "      <td id=\"T_03488_row34_col4\" class=\"data row34 col4\" >0.3453</td>\n",
              "      <td id=\"T_03488_row34_col5\" class=\"data row34 col5\" >0.5474</td>\n",
              "      <td id=\"T_03488_row34_col6\" class=\"data row34 col6\" >0.7734</td>\n",
              "      <td id=\"T_03488_row34_col7\" class=\"data row34 col7\" >0.6168</td>\n",
              "      <td id=\"T_03488_row34_col8\" class=\"data row34 col8\" >0.9848</td>\n",
              "      <td id=\"T_03488_row34_col9\" class=\"data row34 col9\" >0.9864</td>\n",
              "      <td id=\"T_03488_row34_col10\" class=\"data row34 col10\" >0.8530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level0_row35\" class=\"row_heading level0 row35\" rowspan=\"7\">opt-iml-max-1.3b</th>\n",
              "      <th id=\"T_03488_level1_row35\" class=\"row_heading level1 row35\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_03488_row35_col0\" class=\"data row35 col0\" >0.9588</td>\n",
              "      <td id=\"T_03488_row35_col1\" class=\"data row35 col1\" >0.5420</td>\n",
              "      <td id=\"T_03488_row35_col2\" class=\"data row35 col2\" >0.8707</td>\n",
              "      <td id=\"T_03488_row35_col3\" class=\"data row35 col3\" >0.5073</td>\n",
              "      <td id=\"T_03488_row35_col4\" class=\"data row35 col4\" >0.3333</td>\n",
              "      <td id=\"T_03488_row35_col5\" class=\"data row35 col5\" >0.4417</td>\n",
              "      <td id=\"T_03488_row35_col6\" class=\"data row35 col6\" >0.5379</td>\n",
              "      <td id=\"T_03488_row35_col7\" class=\"data row35 col7\" >0.4153</td>\n",
              "      <td id=\"T_03488_row35_col8\" class=\"data row35 col8\" >0.9795</td>\n",
              "      <td id=\"T_03488_row35_col9\" class=\"data row35 col9\" >0.9828</td>\n",
              "      <td id=\"T_03488_row35_col10\" class=\"data row35 col10\" >0.8243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row36\" class=\"row_heading level1 row36\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_03488_row36_col0\" class=\"data row36 col0\" >0.6770</td>\n",
              "      <td id=\"T_03488_row36_col1\" class=\"data row36 col1\" >0.3709</td>\n",
              "      <td id=\"T_03488_row36_col2\" class=\"data row36 col2\" >0.5542</td>\n",
              "      <td id=\"T_03488_row36_col3\" class=\"data row36 col3\" >0.4481</td>\n",
              "      <td id=\"T_03488_row36_col4\" class=\"data row36 col4\" >0.3669</td>\n",
              "      <td id=\"T_03488_row36_col5\" class=\"data row36 col5\" >0.5480</td>\n",
              "      <td id=\"T_03488_row36_col6\" class=\"data row36 col6\" >0.4653</td>\n",
              "      <td id=\"T_03488_row36_col7\" class=\"data row36 col7\" >0.5424</td>\n",
              "      <td id=\"T_03488_row36_col8\" class=\"data row36 col8\" >0.8379</td>\n",
              "      <td id=\"T_03488_row36_col9\" class=\"data row36 col9\" >0.8536</td>\n",
              "      <td id=\"T_03488_row36_col10\" class=\"data row36 col10\" >0.7043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row37\" class=\"row_heading level1 row37\" >gpt2-medium</th>\n",
              "      <td id=\"T_03488_row37_col0\" class=\"data row37 col0\" >0.8338</td>\n",
              "      <td id=\"T_03488_row37_col1\" class=\"data row37 col1\" >0.5143</td>\n",
              "      <td id=\"T_03488_row37_col2\" class=\"data row37 col2\" >0.4434</td>\n",
              "      <td id=\"T_03488_row37_col3\" class=\"data row37 col3\" >0.3979</td>\n",
              "      <td id=\"T_03488_row37_col4\" class=\"data row37 col4\" >0.3991</td>\n",
              "      <td id=\"T_03488_row37_col5\" class=\"data row37 col5\" >0.4127</td>\n",
              "      <td id=\"T_03488_row37_col6\" class=\"data row37 col6\" >0.4003</td>\n",
              "      <td id=\"T_03488_row37_col7\" class=\"data row37 col7\" >0.3891</td>\n",
              "      <td id=\"T_03488_row37_col8\" class=\"data row37 col8\" >0.9368</td>\n",
              "      <td id=\"T_03488_row37_col9\" class=\"data row37 col9\" >0.9188</td>\n",
              "      <td id=\"T_03488_row37_col10\" class=\"data row37 col10\" >0.5273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row38\" class=\"row_heading level1 row38\" >mGPT</th>\n",
              "      <td id=\"T_03488_row38_col0\" class=\"data row38 col0\" >0.9640</td>\n",
              "      <td id=\"T_03488_row38_col1\" class=\"data row38 col1\" >0.6583</td>\n",
              "      <td id=\"T_03488_row38_col2\" class=\"data row38 col2\" >0.3597</td>\n",
              "      <td id=\"T_03488_row38_col3\" class=\"data row38 col3\" >0.5613</td>\n",
              "      <td id=\"T_03488_row38_col4\" class=\"data row38 col4\" >0.3750</td>\n",
              "      <td id=\"T_03488_row38_col5\" class=\"data row38 col5\" >0.3873</td>\n",
              "      <td id=\"T_03488_row38_col6\" class=\"data row38 col6\" >0.8374</td>\n",
              "      <td id=\"T_03488_row38_col7\" class=\"data row38 col7\" >0.4378</td>\n",
              "      <td id=\"T_03488_row38_col8\" class=\"data row38 col8\" >0.9949</td>\n",
              "      <td id=\"T_03488_row38_col9\" class=\"data row38 col9\" >0.9810</td>\n",
              "      <td id=\"T_03488_row38_col10\" class=\"data row38 col10\" >0.7678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row39\" class=\"row_heading level1 row39\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_03488_row39_col0\" class=\"data row39 col0\" >0.8902</td>\n",
              "      <td id=\"T_03488_row39_col1\" class=\"data row39 col1\" >0.5114</td>\n",
              "      <td id=\"T_03488_row39_col2\" class=\"data row39 col2\" >0.9379</td>\n",
              "      <td id=\"T_03488_row39_col3\" class=\"data row39 col3\" >0.5767</td>\n",
              "      <td id=\"T_03488_row39_col4\" class=\"data row39 col4\" >0.3333</td>\n",
              "      <td id=\"T_03488_row39_col5\" class=\"data row39 col5\" >0.3926</td>\n",
              "      <td id=\"T_03488_row39_col6\" class=\"data row39 col6\" >0.6230</td>\n",
              "      <td id=\"T_03488_row39_col7\" class=\"data row39 col7\" >0.3984</td>\n",
              "      <td id=\"T_03488_row39_col8\" class=\"data row39 col8\" >0.9224</td>\n",
              "      <td id=\"T_03488_row39_col9\" class=\"data row39 col9\" >0.9707</td>\n",
              "      <td id=\"T_03488_row39_col10\" class=\"data row39 col10\" >0.8335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row40\" class=\"row_heading level1 row40\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_03488_row40_col0\" class=\"data row40 col0\" >0.7226</td>\n",
              "      <td id=\"T_03488_row40_col1\" class=\"data row40 col1\" >0.3149</td>\n",
              "      <td id=\"T_03488_row40_col2\" class=\"data row40 col2\" >0.3979</td>\n",
              "      <td id=\"T_03488_row40_col3\" class=\"data row40 col3\" >0.3242</td>\n",
              "      <td id=\"T_03488_row40_col4\" class=\"data row40 col4\" >0.1294</td>\n",
              "      <td id=\"T_03488_row40_col5\" class=\"data row40 col5\" >0.3216</td>\n",
              "      <td id=\"T_03488_row40_col6\" class=\"data row40 col6\" >0.3521</td>\n",
              "      <td id=\"T_03488_row40_col7\" class=\"data row40 col7\" >0.3245</td>\n",
              "      <td id=\"T_03488_row40_col8\" class=\"data row40 col8\" >0.9333</td>\n",
              "      <td id=\"T_03488_row40_col9\" class=\"data row40 col9\" >0.9293</td>\n",
              "      <td id=\"T_03488_row40_col10\" class=\"data row40 col10\" >0.5404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row41\" class=\"row_heading level1 row41\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_03488_row41_col0\" class=\"data row41 col0\" >0.9931</td>\n",
              "      <td id=\"T_03488_row41_col1\" class=\"data row41 col1\" >0.7275</td>\n",
              "      <td id=\"T_03488_row41_col2\" class=\"data row41 col2\" >0.9900</td>\n",
              "      <td id=\"T_03488_row41_col3\" class=\"data row41 col3\" >0.7551</td>\n",
              "      <td id=\"T_03488_row41_col4\" class=\"data row41 col4\" >0.3373</td>\n",
              "      <td id=\"T_03488_row41_col5\" class=\"data row41 col5\" >0.3944</td>\n",
              "      <td id=\"T_03488_row41_col6\" class=\"data row41 col6\" >0.8638</td>\n",
              "      <td id=\"T_03488_row41_col7\" class=\"data row41 col7\" >0.4432</td>\n",
              "      <td id=\"T_03488_row41_col8\" class=\"data row41 col8\" >0.9966</td>\n",
              "      <td id=\"T_03488_row41_col9\" class=\"data row41 col9\" >0.9948</td>\n",
              "      <td id=\"T_03488_row41_col10\" class=\"data row41 col10\" >0.9338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level0_row42\" class=\"row_heading level0 row42\" rowspan=\"7\">text-davinci-003</th>\n",
              "      <th id=\"T_03488_level1_row42\" class=\"row_heading level1 row42\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_03488_row42_col0\" class=\"data row42 col0\" >0.7480</td>\n",
              "      <td id=\"T_03488_row42_col1\" class=\"data row42 col1\" >0.6998</td>\n",
              "      <td id=\"T_03488_row42_col2\" class=\"data row42 col2\" >0.6476</td>\n",
              "      <td id=\"T_03488_row42_col3\" class=\"data row42 col3\" >0.8087</td>\n",
              "      <td id=\"T_03488_row42_col4\" class=\"data row42 col4\" >0.7249</td>\n",
              "      <td id=\"T_03488_row42_col5\" class=\"data row42 col5\" >0.8167</td>\n",
              "      <td id=\"T_03488_row42_col6\" class=\"data row42 col6\" >0.7157</td>\n",
              "      <td id=\"T_03488_row42_col7\" class=\"data row42 col7\" >0.7645</td>\n",
              "      <td id=\"T_03488_row42_col8\" class=\"data row42 col8\" >0.9146</td>\n",
              "      <td id=\"T_03488_row42_col9\" class=\"data row42 col9\" >0.9079</td>\n",
              "      <td id=\"T_03488_row42_col10\" class=\"data row42 col10\" >0.7911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row43\" class=\"row_heading level1 row43\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_03488_row43_col0\" class=\"data row43 col0\" >0.4395</td>\n",
              "      <td id=\"T_03488_row43_col1\" class=\"data row43 col1\" >0.3348</td>\n",
              "      <td id=\"T_03488_row43_col2\" class=\"data row43 col2\" >0.3326</td>\n",
              "      <td id=\"T_03488_row43_col3\" class=\"data row43 col3\" >0.3332</td>\n",
              "      <td id=\"T_03488_row43_col4\" class=\"data row43 col4\" >0.5357</td>\n",
              "      <td id=\"T_03488_row43_col5\" class=\"data row43 col5\" >0.3413</td>\n",
              "      <td id=\"T_03488_row43_col6\" class=\"data row43 col6\" >0.3306</td>\n",
              "      <td id=\"T_03488_row43_col7\" class=\"data row43 col7\" >0.3318</td>\n",
              "      <td id=\"T_03488_row43_col8\" class=\"data row43 col8\" >0.5136</td>\n",
              "      <td id=\"T_03488_row43_col9\" class=\"data row43 col9\" >0.4716</td>\n",
              "      <td id=\"T_03488_row43_col10\" class=\"data row43 col10\" >0.3935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row44\" class=\"row_heading level1 row44\" >gpt2-medium</th>\n",
              "      <td id=\"T_03488_row44_col0\" class=\"data row44 col0\" >0.7234</td>\n",
              "      <td id=\"T_03488_row44_col1\" class=\"data row44 col1\" >0.3333</td>\n",
              "      <td id=\"T_03488_row44_col2\" class=\"data row44 col2\" >0.3333</td>\n",
              "      <td id=\"T_03488_row44_col3\" class=\"data row44 col3\" >0.3296</td>\n",
              "      <td id=\"T_03488_row44_col4\" class=\"data row44 col4\" >0.3244</td>\n",
              "      <td id=\"T_03488_row44_col5\" class=\"data row44 col5\" >0.3264</td>\n",
              "      <td id=\"T_03488_row44_col6\" class=\"data row44 col6\" >0.3367</td>\n",
              "      <td id=\"T_03488_row44_col7\" class=\"data row44 col7\" >0.3245</td>\n",
              "      <td id=\"T_03488_row44_col8\" class=\"data row44 col8\" >0.8528</td>\n",
              "      <td id=\"T_03488_row44_col9\" class=\"data row44 col9\" >0.8408</td>\n",
              "      <td id=\"T_03488_row44_col10\" class=\"data row44 col10\" >0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row45\" class=\"row_heading level1 row45\" >mGPT</th>\n",
              "      <td id=\"T_03488_row45_col0\" class=\"data row45 col0\" >0.8798</td>\n",
              "      <td id=\"T_03488_row45_col1\" class=\"data row45 col1\" >0.6376</td>\n",
              "      <td id=\"T_03488_row45_col2\" class=\"data row45 col2\" >0.3552</td>\n",
              "      <td id=\"T_03488_row45_col3\" class=\"data row45 col3\" >0.8629</td>\n",
              "      <td id=\"T_03488_row45_col4\" class=\"data row45 col4\" >0.6831</td>\n",
              "      <td id=\"T_03488_row45_col5\" class=\"data row45 col5\" >0.8696</td>\n",
              "      <td id=\"T_03488_row45_col6\" class=\"data row45 col6\" >0.7987</td>\n",
              "      <td id=\"T_03488_row45_col7\" class=\"data row45 col7\" >0.7996</td>\n",
              "      <td id=\"T_03488_row45_col8\" class=\"data row45 col8\" >0.9315</td>\n",
              "      <td id=\"T_03488_row45_col9\" class=\"data row45 col9\" >0.8193</td>\n",
              "      <td id=\"T_03488_row45_col10\" class=\"data row45 col10\" >0.6410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row46\" class=\"row_heading level1 row46\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_03488_row46_col0\" class=\"data row46 col0\" >0.8800</td>\n",
              "      <td id=\"T_03488_row46_col1\" class=\"data row46 col1\" >0.4399</td>\n",
              "      <td id=\"T_03488_row46_col2\" class=\"data row46 col2\" >0.6877</td>\n",
              "      <td id=\"T_03488_row46_col3\" class=\"data row46 col3\" >0.5837</td>\n",
              "      <td id=\"T_03488_row46_col4\" class=\"data row46 col4\" >0.3453</td>\n",
              "      <td id=\"T_03488_row46_col5\" class=\"data row46 col5\" >0.5184</td>\n",
              "      <td id=\"T_03488_row46_col6\" class=\"data row46 col6\" >0.4845</td>\n",
              "      <td id=\"T_03488_row46_col7\" class=\"data row46 col7\" >0.5132</td>\n",
              "      <td id=\"T_03488_row46_col8\" class=\"data row46 col8\" >0.9381</td>\n",
              "      <td id=\"T_03488_row46_col9\" class=\"data row46 col9\" >0.9598</td>\n",
              "      <td id=\"T_03488_row46_col10\" class=\"data row46 col10\" >0.7358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row47\" class=\"row_heading level1 row47\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_03488_row47_col0\" class=\"data row47 col0\" >0.6924</td>\n",
              "      <td id=\"T_03488_row47_col1\" class=\"data row47 col1\" >0.3623</td>\n",
              "      <td id=\"T_03488_row47_col2\" class=\"data row47 col2\" >0.4165</td>\n",
              "      <td id=\"T_03488_row47_col3\" class=\"data row47 col3\" >0.3514</td>\n",
              "      <td id=\"T_03488_row47_col4\" class=\"data row47 col4\" >0.1657</td>\n",
              "      <td id=\"T_03488_row47_col5\" class=\"data row47 col5\" >0.3471</td>\n",
              "      <td id=\"T_03488_row47_col6\" class=\"data row47 col6\" >0.4236</td>\n",
              "      <td id=\"T_03488_row47_col7\" class=\"data row47 col7\" >0.3421</td>\n",
              "      <td id=\"T_03488_row47_col8\" class=\"data row47 col8\" >0.8814</td>\n",
              "      <td id=\"T_03488_row47_col9\" class=\"data row47 col9\" >0.8155</td>\n",
              "      <td id=\"T_03488_row47_col10\" class=\"data row47 col10\" >0.6138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row48\" class=\"row_heading level1 row48\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_03488_row48_col0\" class=\"data row48 col0\" >0.8664</td>\n",
              "      <td id=\"T_03488_row48_col1\" class=\"data row48 col1\" >0.4440</td>\n",
              "      <td id=\"T_03488_row48_col2\" class=\"data row48 col2\" >0.7422</td>\n",
              "      <td id=\"T_03488_row48_col3\" class=\"data row48 col3\" >0.7117</td>\n",
              "      <td id=\"T_03488_row48_col4\" class=\"data row48 col4\" >0.3778</td>\n",
              "      <td id=\"T_03488_row48_col5\" class=\"data row48 col5\" >0.5679</td>\n",
              "      <td id=\"T_03488_row48_col6\" class=\"data row48 col6\" >0.5897</td>\n",
              "      <td id=\"T_03488_row48_col7\" class=\"data row48 col7\" >0.5700</td>\n",
              "      <td id=\"T_03488_row48_col8\" class=\"data row48 col8\" >0.9382</td>\n",
              "      <td id=\"T_03488_row48_col9\" class=\"data row48 col9\" >0.9497</td>\n",
              "      <td id=\"T_03488_row48_col10\" class=\"data row48 col10\" >0.7750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level0_row49\" class=\"row_heading level0 row49\" rowspan=\"7\">vicuna-13b</th>\n",
              "      <th id=\"T_03488_level1_row49\" class=\"row_heading level1 row49\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_03488_row49_col0\" class=\"data row49 col0\" >0.5450</td>\n",
              "      <td id=\"T_03488_row49_col1\" class=\"data row49 col1\" >0.3623</td>\n",
              "      <td id=\"T_03488_row49_col2\" class=\"data row49 col2\" >0.5724</td>\n",
              "      <td id=\"T_03488_row49_col3\" class=\"data row49 col3\" >0.4549</td>\n",
              "      <td id=\"T_03488_row49_col4\" class=\"data row49 col4\" >0.3333</td>\n",
              "      <td id=\"T_03488_row49_col5\" class=\"data row49 col5\" >0.4751</td>\n",
              "      <td id=\"T_03488_row49_col6\" class=\"data row49 col6\" >0.3548</td>\n",
              "      <td id=\"T_03488_row49_col7\" class=\"data row49 col7\" >0.3721</td>\n",
              "      <td id=\"T_03488_row49_col8\" class=\"data row49 col8\" >0.9264</td>\n",
              "      <td id=\"T_03488_row49_col9\" class=\"data row49 col9\" >0.8401</td>\n",
              "      <td id=\"T_03488_row49_col10\" class=\"data row49 col10\" >0.3472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row50\" class=\"row_heading level1 row50\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_03488_row50_col0\" class=\"data row50 col0\" >0.5084</td>\n",
              "      <td id=\"T_03488_row50_col1\" class=\"data row50 col1\" >0.3355</td>\n",
              "      <td id=\"T_03488_row50_col2\" class=\"data row50 col2\" >0.3376</td>\n",
              "      <td id=\"T_03488_row50_col3\" class=\"data row50 col3\" >0.3318</td>\n",
              "      <td id=\"T_03488_row50_col4\" class=\"data row50 col4\" >0.3333</td>\n",
              "      <td id=\"T_03488_row50_col5\" class=\"data row50 col5\" >0.3386</td>\n",
              "      <td id=\"T_03488_row50_col6\" class=\"data row50 col6\" >0.3306</td>\n",
              "      <td id=\"T_03488_row50_col7\" class=\"data row50 col7\" >0.3421</td>\n",
              "      <td id=\"T_03488_row50_col8\" class=\"data row50 col8\" >0.5535</td>\n",
              "      <td id=\"T_03488_row50_col9\" class=\"data row50 col9\" >0.5256</td>\n",
              "      <td id=\"T_03488_row50_col10\" class=\"data row50 col10\" >0.5498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row51\" class=\"row_heading level1 row51\" >gpt2-medium</th>\n",
              "      <td id=\"T_03488_row51_col0\" class=\"data row51 col0\" >0.5858</td>\n",
              "      <td id=\"T_03488_row51_col1\" class=\"data row51 col1\" >0.3787</td>\n",
              "      <td id=\"T_03488_row51_col2\" class=\"data row51 col2\" >0.4784</td>\n",
              "      <td id=\"T_03488_row51_col3\" class=\"data row51 col3\" >0.4744</td>\n",
              "      <td id=\"T_03488_row51_col4\" class=\"data row51 col4\" >0.5022</td>\n",
              "      <td id=\"T_03488_row51_col5\" class=\"data row51 col5\" >0.3811</td>\n",
              "      <td id=\"T_03488_row51_col6\" class=\"data row51 col6\" >0.3954</td>\n",
              "      <td id=\"T_03488_row51_col7\" class=\"data row51 col7\" >0.4326</td>\n",
              "      <td id=\"T_03488_row51_col8\" class=\"data row51 col8\" >0.8712</td>\n",
              "      <td id=\"T_03488_row51_col9\" class=\"data row51 col9\" >0.6147</td>\n",
              "      <td id=\"T_03488_row51_col10\" class=\"data row51 col10\" >0.4456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row52\" class=\"row_heading level1 row52\" >mGPT</th>\n",
              "      <td id=\"T_03488_row52_col0\" class=\"data row52 col0\" >0.6846</td>\n",
              "      <td id=\"T_03488_row52_col1\" class=\"data row52 col1\" >0.3407</td>\n",
              "      <td id=\"T_03488_row52_col2\" class=\"data row52 col2\" >0.8659</td>\n",
              "      <td id=\"T_03488_row52_col3\" class=\"data row52 col3\" >0.8576</td>\n",
              "      <td id=\"T_03488_row52_col4\" class=\"data row52 col4\" >0.3722</td>\n",
              "      <td id=\"T_03488_row52_col5\" class=\"data row52 col5\" >0.6188</td>\n",
              "      <td id=\"T_03488_row52_col6\" class=\"data row52 col6\" >0.3663</td>\n",
              "      <td id=\"T_03488_row52_col7\" class=\"data row52 col7\" >0.5499</td>\n",
              "      <td id=\"T_03488_row52_col8\" class=\"data row52 col8\" >0.9548</td>\n",
              "      <td id=\"T_03488_row52_col9\" class=\"data row52 col9\" >0.8978</td>\n",
              "      <td id=\"T_03488_row52_col10\" class=\"data row52 col10\" >0.5443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row53\" class=\"row_heading level1 row53\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_03488_row53_col0\" class=\"data row53 col0\" >0.9883</td>\n",
              "      <td id=\"T_03488_row53_col1\" class=\"data row53 col1\" >0.7980</td>\n",
              "      <td id=\"T_03488_row53_col2\" class=\"data row53 col2\" >0.9381</td>\n",
              "      <td id=\"T_03488_row53_col3\" class=\"data row53 col3\" >0.9033</td>\n",
              "      <td id=\"T_03488_row53_col4\" class=\"data row53 col4\" >0.8136</td>\n",
              "      <td id=\"T_03488_row53_col5\" class=\"data row53 col5\" >0.9023</td>\n",
              "      <td id=\"T_03488_row53_col6\" class=\"data row53 col6\" >0.8760</td>\n",
              "      <td id=\"T_03488_row53_col7\" class=\"data row53 col7\" >0.8464</td>\n",
              "      <td id=\"T_03488_row53_col8\" class=\"data row53 col8\" >0.9565</td>\n",
              "      <td id=\"T_03488_row53_col9\" class=\"data row53 col9\" >0.9582</td>\n",
              "      <td id=\"T_03488_row53_col10\" class=\"data row53 col10\" >0.8594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row54\" class=\"row_heading level1 row54\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_03488_row54_col0\" class=\"data row54 col0\" >0.5037</td>\n",
              "      <td id=\"T_03488_row54_col1\" class=\"data row54 col1\" >0.3326</td>\n",
              "      <td id=\"T_03488_row54_col2\" class=\"data row54 col2\" >0.3318</td>\n",
              "      <td id=\"T_03488_row54_col3\" class=\"data row54 col3\" >0.3356</td>\n",
              "      <td id=\"T_03488_row54_col4\" class=\"data row54 col4\" >0.3138</td>\n",
              "      <td id=\"T_03488_row54_col5\" class=\"data row54 col5\" >0.3394</td>\n",
              "      <td id=\"T_03488_row54_col6\" class=\"data row54 col6\" >0.3330</td>\n",
              "      <td id=\"T_03488_row54_col7\" class=\"data row54 col7\" >0.3382</td>\n",
              "      <td id=\"T_03488_row54_col8\" class=\"data row54 col8\" >0.8845</td>\n",
              "      <td id=\"T_03488_row54_col9\" class=\"data row54 col9\" >0.5946</td>\n",
              "      <td id=\"T_03488_row54_col10\" class=\"data row54 col10\" >0.6119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_03488_level1_row55\" class=\"row_heading level1 row55\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_03488_row55_col0\" class=\"data row55 col0\" >0.9716</td>\n",
              "      <td id=\"T_03488_row55_col1\" class=\"data row55 col1\" >0.7526</td>\n",
              "      <td id=\"T_03488_row55_col2\" class=\"data row55 col2\" >0.9280</td>\n",
              "      <td id=\"T_03488_row55_col3\" class=\"data row55 col3\" >0.9391</td>\n",
              "      <td id=\"T_03488_row55_col4\" class=\"data row55 col4\" >0.8572</td>\n",
              "      <td id=\"T_03488_row55_col5\" class=\"data row55 col5\" >0.9298</td>\n",
              "      <td id=\"T_03488_row55_col6\" class=\"data row55 col6\" >0.6979</td>\n",
              "      <td id=\"T_03488_row55_col7\" class=\"data row55 col7\" >0.8416</td>\n",
              "      <td id=\"T_03488_row55_col8\" class=\"data row55 col8\" >0.9615</td>\n",
              "      <td id=\"T_03488_row55_col9\" class=\"data row55 col9\" >0.9278</td>\n",
              "      <td id=\"T_03488_row55_col10\" class=\"data row55 col10\" >0.8998</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{llrrrrrrrrrrr}\n",
            " &  & \\bfseries ar & \\bfseries ca & \\bfseries cs & \\bfseries de & \\bfseries en & \\bfseries es & \\bfseries nl & \\bfseries pt & \\bfseries ru & \\bfseries uk & \\bfseries zh \\\\\n",
            "Train LLM & Model &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries alpaca-lora-30b} & \\bfseries bert-base-multilingual-cased & 0.6026 & 0.6327 & 0.8067 & 0.7409 & 0.4634 & 0.6572 & 0.6705 & 0.6567 & \\bfseries 0.9099 & 0.8829 & 0.6696 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.4945 & 0.3680 & 0.3443 & 0.3516 & 0.3829 & 0.5214 & 0.4081 & 0.3884 & \\bfseries 0.5506 & 0.5100 & 0.4514 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.7291 & 0.4784 & 0.4665 & 0.4094 & 0.6639 & 0.6505 & 0.4265 & 0.5331 & \\bfseries 0.8599 & 0.8249 & 0.3950 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.5635 & 0.4123 & 0.6725 & 0.8716 & 0.5272 & 0.7714 & 0.5325 & 0.7920 & \\bfseries 0.9350 & 0.9129 & 0.4906 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.9716 & 0.8123 & \\bfseries 0.9850 & 0.9409 & 0.8550 & 0.8885 & 0.8868 & 0.8199 & 0.9617 & 0.9615 & 0.8237 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.7062 & 0.3326 & 0.3407 & 0.3363 & 0.2381 & 0.3394 & 0.3370 & 0.3421 & \\bfseries 0.8599 & 0.8464 & 0.5544 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.9180 & 0.8983 & \\bfseries 0.9700 & 0.9442 & 0.9169 & 0.9058 & 0.9431 & 0.8993 & 0.9433 & 0.9397 & 0.8522 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries gpt-3.5-turbo} & \\bfseries bert-base-multilingual-cased & 0.9416 & 0.8680 & 0.9045 & 0.8538 & 0.7810 & 0.8754 & 0.8783 & 0.8378 & \\bfseries 0.9567 & 0.9331 & 0.8666 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & \\bfseries 0.6636 & 0.3333 & 0.3333 & 0.3303 & 0.3457 & 0.3272 & 0.3330 & 0.3276 & 0.6336 & 0.5592 & 0.3333 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.5384 & 0.3443 & 0.3407 & 0.3369 & 0.3423 & 0.3264 & 0.3403 & 0.3350 & \\bfseries 0.8716 & 0.8061 & 0.3333 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.7846 & 0.8857 & 0.3552 & 0.9070 & 0.8817 & 0.9228 & 0.9230 & 0.9043 & \\bfseries 0.9533 & 0.9244 & 0.6258 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.9365 & 0.8387 & \\bfseries 0.9817 & 0.9256 & 0.9603 & 0.9296 & 0.9146 & 0.9023 & 0.9283 & 0.9482 & 0.8329 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.3496 & 0.3472 & 0.3720 & 0.3476 & 0.1697 & 0.3471 & 0.3663 & 0.3421 & \\bfseries 0.8695 & 0.5855 & 0.6240 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.9332 & 0.9432 & \\bfseries 0.9883 & 0.9391 & 0.9675 & 0.9435 & 0.9466 & 0.9143 & 0.9767 & 0.9615 & 0.9048 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries gpt-4} & \\bfseries bert-base-multilingual-cased & 0.8667 & 0.7067 & 0.7828 & 0.7535 & 0.4958 & 0.6805 & 0.7514 & 0.6918 & \\bfseries 0.9333 & 0.9214 & 0.8122 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.5003 & 0.3311 & 0.3333 & 0.3303 & 0.3718 & 0.3374 & 0.3292 & 0.3326 & \\bfseries 0.6837 & 0.6241 & 0.3677 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.5234 & 0.3587 & 0.3623 & 0.3514 & 0.3368 & 0.3659 & 0.3655 & 0.3458 & 0.7535 & \\bfseries 0.7794 & 0.3508 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.8978 & 0.8978 & 0.3587 & 0.9188 & 0.8713 & 0.9383 & 0.9246 & 0.9264 & \\bfseries 0.9566 & 0.8570 & 0.7078 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & \\bfseries 0.9349 & 0.5556 & 0.8788 & 0.9221 & 0.9259 & 0.7611 & 0.7609 & 0.6932 & 0.8305 & 0.8940 & 0.7797 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.5936 & 0.3407 & 0.3480 & 0.3439 & 0.2994 & 0.3433 & 0.3628 & 0.3421 & 0.7991 & \\bfseries 0.8526 & 0.6649 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.9215 & 0.9600 & \\bfseries 0.9950 & 0.9391 & 0.9765 & 0.9349 & 0.9633 & 0.9368 & 0.9567 & 0.9447 & 0.9214 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries llama-65b} & \\bfseries bert-base-multilingual-cased & 0.7078 & 0.5300 & 0.8750 & 0.6477 & 0.3325 & 0.8211 & 0.5191 & 0.6043 & \\bfseries 0.9849 & 0.9764 & 0.4765 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & \\bfseries 0.7140 & 0.3303 & 0.3299 & 0.3348 & 0.3325 & 0.3383 & 0.3330 & 0.3356 & 0.6109 & 0.6025 & 0.3676 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.6015 & 0.4216 & 0.6322 & 0.6629 & 0.4170 & 0.7424 & 0.5012 & 0.4982 & \\bfseries 0.9395 & 0.9394 & 0.5690 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.5347 & 0.3326 & 0.6154 & 0.6470 & 0.3325 & 0.4964 & 0.3404 & 0.4847 & \\bfseries 0.9782 & 0.9359 & 0.4056 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.7629 & 0.4277 & 0.9848 & 0.8540 & 0.3824 & 0.7305 & 0.4145 & 0.5383 & \\bfseries 0.9883 & 0.9815 & 0.5622 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.6454 & 0.3311 & 0.3269 & 0.3341 & 0.3388 & 0.3375 & 0.3307 & 0.3356 & \\bfseries 0.9631 & 0.9309 & 0.3969 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.8858 & 0.5705 & 0.9865 & 0.9160 & 0.3705 & 0.5024 & 0.3674 & 0.5826 & \\bfseries 0.9933 & 0.9882 & 0.6314 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries opt-66b} & \\bfseries bert-base-multilingual-cased & 0.7593 & 0.6061 & 0.7673 & 0.6078 & 0.4720 & 0.6423 & 0.6428 & 0.6734 & \\bfseries 0.9340 & 0.9254 & 0.6408 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.5543 & 0.3326 & 0.3333 & 0.3356 & 0.3325 & 0.3394 & 0.3337 & 0.3375 & \\bfseries 0.7304 & 0.6932 & 0.3425 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.6817 & 0.3755 & 0.3814 & 0.3576 & 0.4726 & 0.3972 & 0.3388 & 0.3598 & \\bfseries 0.9085 & 0.8797 & 0.4361 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.9714 & 0.5123 & 0.5776 & 0.3999 & 0.3473 & 0.4262 & 0.6050 & 0.4165 & 0.9662 & \\bfseries 0.9814 & 0.7649 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.9393 & 0.7428 & 0.9833 & 0.4860 & 0.3413 & 0.3394 & 0.6255 & 0.3610 & 0.9848 & \\bfseries 0.9932 & 0.7645 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.4195 & 0.3266 & 0.3326 & 0.3348 & 0.3046 & 0.3386 & 0.3322 & 0.3375 & \\bfseries 0.9003 & 0.8830 & 0.3759 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.9546 & 0.8552 & \\bfseries 0.9917 & 0.5544 & 0.3453 & 0.5474 & 0.7734 & 0.6168 & 0.9848 & 0.9864 & 0.8530 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries opt-iml-max-1.3b} & \\bfseries bert-base-multilingual-cased & 0.9588 & 0.5420 & 0.8707 & 0.5073 & 0.3333 & 0.4417 & 0.5379 & 0.4153 & 0.9795 & \\bfseries 0.9828 & 0.8243 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.6770 & 0.3709 & 0.5542 & 0.4481 & 0.3669 & 0.5480 & 0.4653 & 0.5424 & 0.8379 & \\bfseries 0.8536 & 0.7043 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.8338 & 0.5143 & 0.4434 & 0.3979 & 0.3991 & 0.4127 & 0.4003 & 0.3891 & \\bfseries 0.9368 & 0.9188 & 0.5273 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.9640 & 0.6583 & 0.3597 & 0.5613 & 0.3750 & 0.3873 & 0.8374 & 0.4378 & \\bfseries 0.9949 & 0.9810 & 0.7678 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.8902 & 0.5114 & 0.9379 & 0.5767 & 0.3333 & 0.3926 & 0.6230 & 0.3984 & 0.9224 & \\bfseries 0.9707 & 0.8335 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.7226 & 0.3149 & 0.3979 & 0.3242 & 0.1294 & 0.3216 & 0.3521 & 0.3245 & \\bfseries 0.9333 & 0.9293 & 0.5404 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.9931 & 0.7275 & 0.9900 & 0.7551 & 0.3373 & 0.3944 & 0.8638 & 0.4432 & \\bfseries 0.9966 & 0.9948 & 0.9338 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries text-davinci-003} & \\bfseries bert-base-multilingual-cased & 0.7480 & 0.6998 & 0.6476 & 0.8087 & 0.7249 & 0.8167 & 0.7157 & 0.7645 & \\bfseries 0.9146 & 0.9079 & 0.7911 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.4395 & 0.3348 & 0.3326 & 0.3332 & \\bfseries 0.5357 & 0.3413 & 0.3306 & 0.3318 & 0.5136 & 0.4716 & 0.3935 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.7234 & 0.3333 & 0.3333 & 0.3296 & 0.3244 & 0.3264 & 0.3367 & 0.3245 & \\bfseries 0.8528 & 0.8408 & 0.3333 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.8798 & 0.6376 & 0.3552 & 0.8629 & 0.6831 & 0.8696 & 0.7987 & 0.7996 & \\bfseries 0.9315 & 0.8193 & 0.6410 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.8800 & 0.4399 & 0.6877 & 0.5837 & 0.3453 & 0.5184 & 0.4845 & 0.5132 & 0.9381 & \\bfseries 0.9598 & 0.7358 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.6924 & 0.3623 & 0.4165 & 0.3514 & 0.1657 & 0.3471 & 0.4236 & 0.3421 & \\bfseries 0.8814 & 0.8155 & 0.6138 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.8664 & 0.4440 & 0.7422 & 0.7117 & 0.3778 & 0.5679 & 0.5897 & 0.5700 & 0.9382 & \\bfseries 0.9497 & 0.7750 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries vicuna-13b} & \\bfseries bert-base-multilingual-cased & 0.5450 & 0.3623 & 0.5724 & 0.4549 & 0.3333 & 0.4751 & 0.3548 & 0.3721 & \\bfseries 0.9264 & 0.8401 & 0.3472 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.5084 & 0.3355 & 0.3376 & 0.3318 & 0.3333 & 0.3386 & 0.3306 & 0.3421 & \\bfseries 0.5535 & 0.5256 & 0.5498 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.5858 & 0.3787 & 0.4784 & 0.4744 & 0.5022 & 0.3811 & 0.3954 & 0.4326 & \\bfseries 0.8712 & 0.6147 & 0.4456 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.6846 & 0.3407 & 0.8659 & 0.8576 & 0.3722 & 0.6188 & 0.3663 & 0.5499 & \\bfseries 0.9548 & 0.8978 & 0.5443 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & \\bfseries 0.9883 & 0.7980 & 0.9381 & 0.9033 & 0.8136 & 0.9023 & 0.8760 & 0.8464 & 0.9565 & 0.9582 & 0.8594 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.5037 & 0.3326 & 0.3318 & 0.3356 & 0.3138 & 0.3394 & 0.3330 & 0.3382 & \\bfseries 0.8845 & 0.5946 & 0.6119 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & \\bfseries 0.9716 & 0.7526 & 0.9280 & 0.9391 & 0.8572 & 0.9298 & 0.6979 & 0.8416 & 0.9615 & 0.9278 & 0.8998 \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd004efc8b0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_60e04_row0_col0 {\n",
              "  background-color: #a8bedc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row0_col1 {\n",
              "  background-color: #cccfe5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row0_col2 {\n",
              "  background-color: #bdc8e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row0_col3 {\n",
              "  background-color: #c1cae2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row0_col4 {\n",
              "  background-color: #d2d3e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row0_col5 {\n",
              "  background-color: #c6cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row0_col6 {\n",
              "  background-color: #c8cde4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row0_col7 {\n",
              "  background-color: #cacee5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row0_col8 {\n",
              "  background-color: #8bb2d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row0_col9 {\n",
              "  background-color: #91b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row0_col10 {\n",
              "  background-color: #bcc7e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row1_col0 {\n",
              "  background-color: #93b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row1_col1 {\n",
              "  background-color: #b7c5df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row1_col2 {\n",
              "  background-color: #9ebad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row1_col3 {\n",
              "  background-color: #a4bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row1_col4 {\n",
              "  background-color: #c4cbe3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row1_col5, #T_60e04_row1_col6 {\n",
              "  background-color: #b0c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row1_col7 {\n",
              "  background-color: #b5c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row1_col8 {\n",
              "  background-color: #7eadd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row1_col9 {\n",
              "  background-color: #80aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row1_col10 {\n",
              "  background-color: #a9bfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row2_col0 {\n",
              "  background-color: #c0c9e2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row2_col1, #T_60e04_row2_col6 {\n",
              "  background-color: #e0dded;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row2_col2 {\n",
              "  background-color: #dddbec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row2_col3, #T_60e04_row2_col7 {\n",
              "  background-color: #dfddec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row2_col4 {\n",
              "  background-color: #e1dfed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row2_col5 {\n",
              "  background-color: #dcdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row2_col8 {\n",
              "  background-color: #9cb9d9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row2_col9 {\n",
              "  background-color: #a7bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_60e04_row2_col10 {\n",
              "  background-color: #d3d4e7;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_60e04\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_60e04_level0_col0\" class=\"col_heading level0 col0\" >ar</th>\n",
              "      <th id=\"T_60e04_level0_col1\" class=\"col_heading level0 col1\" >ca</th>\n",
              "      <th id=\"T_60e04_level0_col2\" class=\"col_heading level0 col2\" >cs</th>\n",
              "      <th id=\"T_60e04_level0_col3\" class=\"col_heading level0 col3\" >de</th>\n",
              "      <th id=\"T_60e04_level0_col4\" class=\"col_heading level0 col4\" >en</th>\n",
              "      <th id=\"T_60e04_level0_col5\" class=\"col_heading level0 col5\" >es</th>\n",
              "      <th id=\"T_60e04_level0_col6\" class=\"col_heading level0 col6\" >nl</th>\n",
              "      <th id=\"T_60e04_level0_col7\" class=\"col_heading level0 col7\" >pt</th>\n",
              "      <th id=\"T_60e04_level0_col8\" class=\"col_heading level0 col8\" >ru</th>\n",
              "      <th id=\"T_60e04_level0_col9\" class=\"col_heading level0 col9\" >uk</th>\n",
              "      <th id=\"T_60e04_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_60e04_level0_row0\" class=\"row_heading level0 row0\" >All Detectors Mean</th>\n",
              "      <td id=\"T_60e04_row0_col0\" class=\"data row0 col0\" >0.7421</td>\n",
              "      <td id=\"T_60e04_row0_col1\" class=\"data row0 col1\" >0.5274</td>\n",
              "      <td id=\"T_60e04_row0_col2\" class=\"data row0 col2\" >0.6171</td>\n",
              "      <td id=\"T_60e04_row0_col3\" class=\"data row0 col3\" >0.5914</td>\n",
              "      <td id=\"T_60e04_row0_col4\" class=\"data row0 col4\" >0.4795</td>\n",
              "      <td id=\"T_60e04_row0_col5\" class=\"data row0 col5\" >0.5614</td>\n",
              "      <td id=\"T_60e04_row0_col6\" class=\"data row0 col6\" >0.5524</td>\n",
              "      <td id=\"T_60e04_row0_col7\" class=\"data row0 col7\" >0.5369</td>\n",
              "      <td id=\"T_60e04_row0_col8\" class=\"data row0 col8\" >0.8870</td>\n",
              "      <td id=\"T_60e04_row0_col9\" class=\"data row0 col9\" >0.8557</td>\n",
              "      <td id=\"T_60e04_row0_col10\" class=\"data row0 col10\" >0.6183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_60e04_level0_row1\" class=\"row_heading level0 row1\" >Multilingual Base Models Mean</th>\n",
              "      <td id=\"T_60e04_row1_col0\" class=\"data row1 col0\" >0.8487</td>\n",
              "      <td id=\"T_60e04_row1_col1\" class=\"data row1 col1\" >0.6532</td>\n",
              "      <td id=\"T_60e04_row1_col2\" class=\"data row1 col2\" >0.7924</td>\n",
              "      <td id=\"T_60e04_row1_col3\" class=\"data row1 col3\" >0.7591</td>\n",
              "      <td id=\"T_60e04_row1_col4\" class=\"data row1 col4\" >0.5760</td>\n",
              "      <td id=\"T_60e04_row1_col5\" class=\"data row1 col5\" >0.6884</td>\n",
              "      <td id=\"T_60e04_row1_col6\" class=\"data row1 col6\" >0.6915</td>\n",
              "      <td id=\"T_60e04_row1_col7\" class=\"data row1 col7\" >0.6626</td>\n",
              "      <td id=\"T_60e04_row1_col8\" class=\"data row1 col8\" >0.9522</td>\n",
              "      <td id=\"T_60e04_row1_col9\" class=\"data row1 col9\" >0.9387</td>\n",
              "      <td id=\"T_60e04_row1_col10\" class=\"data row1 col10\" >0.7294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_60e04_level0_row2\" class=\"row_heading level0 row2\" >Monolingual Base Models Mean</th>\n",
              "      <td id=\"T_60e04_row2_col0\" class=\"data row2 col0\" >0.6001</td>\n",
              "      <td id=\"T_60e04_row2_col1\" class=\"data row2 col1\" >0.3596</td>\n",
              "      <td id=\"T_60e04_row2_col2\" class=\"data row2 col2\" >0.3835</td>\n",
              "      <td id=\"T_60e04_row2_col3\" class=\"data row2 col3\" >0.3677</td>\n",
              "      <td id=\"T_60e04_row2_col4\" class=\"data row2 col4\" >0.3508</td>\n",
              "      <td id=\"T_60e04_row2_col5\" class=\"data row2 col5\" >0.3920</td>\n",
              "      <td id=\"T_60e04_row2_col6\" class=\"data row2 col6\" >0.3669</td>\n",
              "      <td id=\"T_60e04_row2_col7\" class=\"data row2 col7\" >0.3692</td>\n",
              "      <td id=\"T_60e04_row2_col8\" class=\"data row2 col8\" >0.8000</td>\n",
              "      <td id=\"T_60e04_row2_col9\" class=\"data row2 col9\" >0.7451</td>\n",
              "      <td id=\"T_60e04_row2_col10\" class=\"data row2 col10\" >0.4701</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{lrrrrrrrrrrr}\n",
            " & \\bfseries ar & \\bfseries ca & \\bfseries cs & \\bfseries de & \\bfseries en & \\bfseries es & \\bfseries nl & \\bfseries pt & \\bfseries ru & \\bfseries uk & \\bfseries zh \\\\\n",
            "\\multicolumn{2}{r|}{\\bfseries All Detectors Mean} & {\\cellcolor[HTML]{A8BEDC}} \\textcolor{black}{0.7421} & {\\cellcolor[HTML]{CCCFE5}} \\textcolor{black}{0.5274} & {\\cellcolor[HTML]{BDC8E1}} \\textcolor{black}{0.6171} & {\\cellcolor[HTML]{C1CAE2}} \\textcolor{black}{0.5914} & {\\cellcolor[HTML]{D2D3E7}} \\textcolor{black}{0.4795} & {\\cellcolor[HTML]{C6CCE3}} \\textcolor{black}{0.5614} & {\\cellcolor[HTML]{C8CDE4}} \\textcolor{black}{0.5524} & {\\cellcolor[HTML]{CACEE5}} \\textcolor{black}{0.5369} & {\\cellcolor[HTML]{8BB2D4}} \\textcolor{black}{0.8870} & {\\cellcolor[HTML]{91B5D6}} \\textcolor{black}{0.8557} & {\\cellcolor[HTML]{BCC7E1}} \\textcolor{black}{0.6183} \\\\\n",
            "\\multicolumn{2}{r|}{\\bfseries Multilingual Base Models Mean} & {\\cellcolor[HTML]{93B5D6}} \\textcolor{black}{0.8487} & {\\cellcolor[HTML]{B7C5DF}} \\textcolor{black}{0.6532} & {\\cellcolor[HTML]{9EBAD9}} \\textcolor{black}{0.7924} & {\\cellcolor[HTML]{A4BCDA}} \\textcolor{black}{0.7591} & {\\cellcolor[HTML]{C4CBE3}} \\textcolor{black}{0.5760} & {\\cellcolor[HTML]{B0C2DE}} \\textcolor{black}{0.6884} & {\\cellcolor[HTML]{B0C2DE}} \\textcolor{black}{0.6915} & {\\cellcolor[HTML]{B5C4DF}} \\textcolor{black}{0.6626} & {\\cellcolor[HTML]{7EADD1}} \\textcolor{black}{0.9522} & {\\cellcolor[HTML]{80AED2}} \\textcolor{black}{0.9387} & {\\cellcolor[HTML]{A9BFDC}} \\textcolor{black}{0.7294} \\\\\n",
            "\\multicolumn{2}{r|}{\\bfseries Monolingual Base Models Mean} & {\\cellcolor[HTML]{C0C9E2}} \\textcolor{black}{0.6001} & {\\cellcolor[HTML]{E0DDED}} \\textcolor{black}{0.3596} & {\\cellcolor[HTML]{DDDBEC}} \\textcolor{black}{0.3835} & {\\cellcolor[HTML]{DFDDEC}} \\textcolor{black}{0.3677} & {\\cellcolor[HTML]{E1DFED}} \\textcolor{black}{0.3508} & {\\cellcolor[HTML]{DCDAEB}} \\textcolor{black}{0.3920} & {\\cellcolor[HTML]{E0DDED}} \\textcolor{black}{0.3669} & {\\cellcolor[HTML]{DFDDEC}} \\textcolor{black}{0.3692} & {\\cellcolor[HTML]{9CB9D9}} \\textcolor{black}{0.8000} & {\\cellcolor[HTML]{A7BDDB}} \\textcolor{black}{0.7451} & {\\cellcolor[HTML]{D3D4E7}} \\textcolor{black}{0.4701} \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp = results_all[['en', 'de', 'nl', 'es', 'pt', 'ca', 'cs', 'ru', 'uk','ar', 'zh']].corr().style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4)\n",
        "display(temp)\n",
        "print(temp.applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True))"
      ],
      "metadata": {
        "id": "EIwNAnutj_vQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "253bd11c-ed2e-493c-e7f7-c7ef8aaab8d1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd004f1d270>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_00c93_row0_col0, #T_00c93_row1_col1, #T_00c93_row2_col2, #T_00c93_row3_col3, #T_00c93_row4_col4, #T_00c93_row5_col5, #T_00c93_row6_col6, #T_00c93_row7_col7, #T_00c93_row8_col8, #T_00c93_row9_col9, #T_00c93_row10_col10 {\n",
              "  background-color: #73a9cf;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row0_col1, #T_00c93_row1_col0, #T_00c93_row4_col10, #T_00c93_row6_col7, #T_00c93_row7_col6, #T_00c93_row10_col4 {\n",
              "  background-color: #e0deed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row0_col2, #T_00c93_row2_col0 {\n",
              "  background-color: #d9d8ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row0_col3, #T_00c93_row3_col0 {\n",
              "  background-color: #f1ebf5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row0_col4, #T_00c93_row4_col0 {\n",
              "  background-color: #ede7f2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row0_col5, #T_00c93_row5_col0, #T_00c93_row5_col9, #T_00c93_row9_col5 {\n",
              "  background-color: #e7e3f0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row0_col6, #T_00c93_row0_col10, #T_00c93_row6_col0, #T_00c93_row10_col0 {\n",
              "  background-color: #f8f1f8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row0_col7, #T_00c93_row0_col8, #T_00c93_row0_col9, #T_00c93_row7_col0, #T_00c93_row8_col0, #T_00c93_row9_col0 {\n",
              "  background-color: #fff7fb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row1_col2, #T_00c93_row2_col1, #T_00c93_row8_col9, #T_00c93_row9_col8 {\n",
              "  background-color: #9fbad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row1_col3, #T_00c93_row3_col1 {\n",
              "  background-color: #acc0dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row1_col4, #T_00c93_row4_col1, #T_00c93_row7_col9, #T_00c93_row9_col7 {\n",
              "  background-color: #a2bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row1_col5, #T_00c93_row5_col1 {\n",
              "  background-color: #a7bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row1_col6, #T_00c93_row6_col1 {\n",
              "  background-color: #adc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row1_col7, #T_00c93_row1_col8, #T_00c93_row7_col1, #T_00c93_row8_col1 {\n",
              "  background-color: #e1dfed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row1_col9, #T_00c93_row9_col1 {\n",
              "  background-color: #dfddec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row1_col10, #T_00c93_row2_col10, #T_00c93_row10_col1, #T_00c93_row10_col2 {\n",
              "  background-color: #c8cde4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row2_col3, #T_00c93_row3_col2, #T_00c93_row9_col10, #T_00c93_row10_col9 {\n",
              "  background-color: #b1c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row2_col4, #T_00c93_row4_col2 {\n",
              "  background-color: #a5bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row2_col5, #T_00c93_row4_col5, #T_00c93_row5_col2, #T_00c93_row5_col4 {\n",
              "  background-color: #8fb4d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row2_col6, #T_00c93_row6_col2 {\n",
              "  background-color: #cacee5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row2_col7, #T_00c93_row7_col2 {\n",
              "  background-color: #f2ecf5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row2_col8, #T_00c93_row8_col2 {\n",
              "  background-color: #f3edf5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row2_col9, #T_00c93_row9_col2 {\n",
              "  background-color: #e5e1ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row3_col4, #T_00c93_row4_col3 {\n",
              "  background-color: #7dacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row3_col5, #T_00c93_row5_col3 {\n",
              "  background-color: #97b7d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row3_col6, #T_00c93_row4_col6, #T_00c93_row6_col3, #T_00c93_row6_col4 {\n",
              "  background-color: #d2d3e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row3_col7, #T_00c93_row7_col3 {\n",
              "  background-color: #fbf3f9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row3_col8, #T_00c93_row8_col3 {\n",
              "  background-color: #faf2f8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row3_col9, #T_00c93_row9_col3 {\n",
              "  background-color: #eee9f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row3_col10, #T_00c93_row10_col3 {\n",
              "  background-color: #e6e2ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row4_col7, #T_00c93_row4_col8, #T_00c93_row7_col4, #T_00c93_row8_col4 {\n",
              "  background-color: #f7f0f7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row4_col9, #T_00c93_row9_col4 {\n",
              "  background-color: #ece7f2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row5_col6, #T_00c93_row6_col5 {\n",
              "  background-color: #cccfe5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row5_col7, #T_00c93_row7_col5 {\n",
              "  background-color: #f4edf6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row5_col8, #T_00c93_row8_col5 {\n",
              "  background-color: #f4eef6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row5_col10, #T_00c93_row10_col5 {\n",
              "  background-color: #dad9ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row6_col8, #T_00c93_row6_col9, #T_00c93_row8_col6, #T_00c93_row9_col6 {\n",
              "  background-color: #d7d6e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row6_col10, #T_00c93_row10_col6 {\n",
              "  background-color: #bfc9e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row7_col8, #T_00c93_row8_col7 {\n",
              "  background-color: #81aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row7_col10, #T_00c93_row10_col7 {\n",
              "  background-color: #c5cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_00c93_row8_col10, #T_00c93_row10_col8 {\n",
              "  background-color: #c4cbe3;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_00c93\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_00c93_level0_col0\" class=\"col_heading level0 col0\" >en</th>\n",
              "      <th id=\"T_00c93_level0_col1\" class=\"col_heading level0 col1\" >de</th>\n",
              "      <th id=\"T_00c93_level0_col2\" class=\"col_heading level0 col2\" >nl</th>\n",
              "      <th id=\"T_00c93_level0_col3\" class=\"col_heading level0 col3\" >es</th>\n",
              "      <th id=\"T_00c93_level0_col4\" class=\"col_heading level0 col4\" >pt</th>\n",
              "      <th id=\"T_00c93_level0_col5\" class=\"col_heading level0 col5\" >ca</th>\n",
              "      <th id=\"T_00c93_level0_col6\" class=\"col_heading level0 col6\" >cs</th>\n",
              "      <th id=\"T_00c93_level0_col7\" class=\"col_heading level0 col7\" >ru</th>\n",
              "      <th id=\"T_00c93_level0_col8\" class=\"col_heading level0 col8\" >uk</th>\n",
              "      <th id=\"T_00c93_level0_col9\" class=\"col_heading level0 col9\" >ar</th>\n",
              "      <th id=\"T_00c93_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_00c93_level0_row0\" class=\"row_heading level0 row0\" >en</th>\n",
              "      <td id=\"T_00c93_row0_col0\" class=\"data row0 col0\" >1.0000</td>\n",
              "      <td id=\"T_00c93_row0_col1\" class=\"data row0 col1\" >0.3586</td>\n",
              "      <td id=\"T_00c93_row0_col2\" class=\"data row0 col2\" >0.4236</td>\n",
              "      <td id=\"T_00c93_row0_col3\" class=\"data row0 col3\" >0.1826</td>\n",
              "      <td id=\"T_00c93_row0_col4\" class=\"data row0 col4\" >0.2428</td>\n",
              "      <td id=\"T_00c93_row0_col5\" class=\"data row0 col5\" >0.2954</td>\n",
              "      <td id=\"T_00c93_row0_col6\" class=\"data row0 col6\" >0.0899</td>\n",
              "      <td id=\"T_00c93_row0_col7\" class=\"data row0 col7\" >-0.0791</td>\n",
              "      <td id=\"T_00c93_row0_col8\" class=\"data row0 col8\" >-0.1245</td>\n",
              "      <td id=\"T_00c93_row0_col9\" class=\"data row0 col9\" >-0.0030</td>\n",
              "      <td id=\"T_00c93_row0_col10\" class=\"data row0 col10\" >0.0994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_00c93_level0_row1\" class=\"row_heading level0 row1\" >de</th>\n",
              "      <td id=\"T_00c93_row1_col0\" class=\"data row1 col0\" >0.3586</td>\n",
              "      <td id=\"T_00c93_row1_col1\" class=\"data row1 col1\" >1.0000</td>\n",
              "      <td id=\"T_00c93_row1_col2\" class=\"data row1 col2\" >0.7830</td>\n",
              "      <td id=\"T_00c93_row1_col3\" class=\"data row1 col3\" >0.7144</td>\n",
              "      <td id=\"T_00c93_row1_col4\" class=\"data row1 col4\" >0.7724</td>\n",
              "      <td id=\"T_00c93_row1_col5\" class=\"data row1 col5\" >0.7467</td>\n",
              "      <td id=\"T_00c93_row1_col6\" class=\"data row1 col6\" >0.7077</td>\n",
              "      <td id=\"T_00c93_row1_col7\" class=\"data row1 col7\" >0.3471</td>\n",
              "      <td id=\"T_00c93_row1_col8\" class=\"data row1 col8\" >0.3500</td>\n",
              "      <td id=\"T_00c93_row1_col9\" class=\"data row1 col9\" >0.3736</td>\n",
              "      <td id=\"T_00c93_row1_col10\" class=\"data row1 col10\" >0.5491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_00c93_level0_row2\" class=\"row_heading level0 row2\" >nl</th>\n",
              "      <td id=\"T_00c93_row2_col0\" class=\"data row2 col0\" >0.4236</td>\n",
              "      <td id=\"T_00c93_row2_col1\" class=\"data row2 col1\" >0.7830</td>\n",
              "      <td id=\"T_00c93_row2_col2\" class=\"data row2 col2\" >1.0000</td>\n",
              "      <td id=\"T_00c93_row2_col3\" class=\"data row2 col3\" >0.6816</td>\n",
              "      <td id=\"T_00c93_row2_col4\" class=\"data row2 col4\" >0.7527</td>\n",
              "      <td id=\"T_00c93_row2_col5\" class=\"data row2 col5\" >0.8655</td>\n",
              "      <td id=\"T_00c93_row2_col6\" class=\"data row2 col6\" >0.5376</td>\n",
              "      <td id=\"T_00c93_row2_col7\" class=\"data row2 col7\" >0.1788</td>\n",
              "      <td id=\"T_00c93_row2_col8\" class=\"data row2 col8\" >0.1623</td>\n",
              "      <td id=\"T_00c93_row2_col9\" class=\"data row2 col9\" >0.3141</td>\n",
              "      <td id=\"T_00c93_row2_col10\" class=\"data row2 col10\" >0.5523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_00c93_level0_row3\" class=\"row_heading level0 row3\" >es</th>\n",
              "      <td id=\"T_00c93_row3_col0\" class=\"data row3 col0\" >0.1826</td>\n",
              "      <td id=\"T_00c93_row3_col1\" class=\"data row3 col1\" >0.7144</td>\n",
              "      <td id=\"T_00c93_row3_col2\" class=\"data row3 col2\" >0.6816</td>\n",
              "      <td id=\"T_00c93_row3_col3\" class=\"data row3 col3\" >1.0000</td>\n",
              "      <td id=\"T_00c93_row3_col4\" class=\"data row3 col4\" >0.9586</td>\n",
              "      <td id=\"T_00c93_row3_col5\" class=\"data row3 col5\" >0.8215</td>\n",
              "      <td id=\"T_00c93_row3_col6\" class=\"data row3 col6\" >0.4784</td>\n",
              "      <td id=\"T_00c93_row3_col7\" class=\"data row3 col7\" >0.0580</td>\n",
              "      <td id=\"T_00c93_row3_col8\" class=\"data row3 col8\" >0.0733</td>\n",
              "      <td id=\"T_00c93_row3_col9\" class=\"data row3 col9\" >0.2244</td>\n",
              "      <td id=\"T_00c93_row3_col10\" class=\"data row3 col10\" >0.3091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_00c93_level0_row4\" class=\"row_heading level0 row4\" >pt</th>\n",
              "      <td id=\"T_00c93_row4_col0\" class=\"data row4 col0\" >0.2428</td>\n",
              "      <td id=\"T_00c93_row4_col1\" class=\"data row4 col1\" >0.7724</td>\n",
              "      <td id=\"T_00c93_row4_col2\" class=\"data row4 col2\" >0.7527</td>\n",
              "      <td id=\"T_00c93_row4_col3\" class=\"data row4 col3\" >0.9586</td>\n",
              "      <td id=\"T_00c93_row4_col4\" class=\"data row4 col4\" >1.0000</td>\n",
              "      <td id=\"T_00c93_row4_col5\" class=\"data row4 col5\" >0.8666</td>\n",
              "      <td id=\"T_00c93_row4_col6\" class=\"data row4 col6\" >0.4834</td>\n",
              "      <td id=\"T_00c93_row4_col7\" class=\"data row4 col7\" >0.1055</td>\n",
              "      <td id=\"T_00c93_row4_col8\" class=\"data row4 col8\" >0.1045</td>\n",
              "      <td id=\"T_00c93_row4_col9\" class=\"data row4 col9\" >0.2535</td>\n",
              "      <td id=\"T_00c93_row4_col10\" class=\"data row4 col10\" >0.3519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_00c93_level0_row5\" class=\"row_heading level0 row5\" >ca</th>\n",
              "      <td id=\"T_00c93_row5_col0\" class=\"data row5 col0\" >0.2954</td>\n",
              "      <td id=\"T_00c93_row5_col1\" class=\"data row5 col1\" >0.7467</td>\n",
              "      <td id=\"T_00c93_row5_col2\" class=\"data row5 col2\" >0.8655</td>\n",
              "      <td id=\"T_00c93_row5_col3\" class=\"data row5 col3\" >0.8215</td>\n",
              "      <td id=\"T_00c93_row5_col4\" class=\"data row5 col4\" >0.8666</td>\n",
              "      <td id=\"T_00c93_row5_col5\" class=\"data row5 col5\" >1.0000</td>\n",
              "      <td id=\"T_00c93_row5_col6\" class=\"data row5 col6\" >0.5308</td>\n",
              "      <td id=\"T_00c93_row5_col7\" class=\"data row5 col7\" >0.1499</td>\n",
              "      <td id=\"T_00c93_row5_col8\" class=\"data row5 col8\" >0.1478</td>\n",
              "      <td id=\"T_00c93_row5_col9\" class=\"data row5 col9\" >0.3016</td>\n",
              "      <td id=\"T_00c93_row5_col10\" class=\"data row5 col10\" >0.4097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_00c93_level0_row6\" class=\"row_heading level0 row6\" >cs</th>\n",
              "      <td id=\"T_00c93_row6_col0\" class=\"data row6 col0\" >0.0899</td>\n",
              "      <td id=\"T_00c93_row6_col1\" class=\"data row6 col1\" >0.7077</td>\n",
              "      <td id=\"T_00c93_row6_col2\" class=\"data row6 col2\" >0.5376</td>\n",
              "      <td id=\"T_00c93_row6_col3\" class=\"data row6 col3\" >0.4784</td>\n",
              "      <td id=\"T_00c93_row6_col4\" class=\"data row6 col4\" >0.4834</td>\n",
              "      <td id=\"T_00c93_row6_col5\" class=\"data row6 col5\" >0.5308</td>\n",
              "      <td id=\"T_00c93_row6_col6\" class=\"data row6 col6\" >1.0000</td>\n",
              "      <td id=\"T_00c93_row6_col7\" class=\"data row6 col7\" >0.3570</td>\n",
              "      <td id=\"T_00c93_row6_col8\" class=\"data row6 col8\" >0.4382</td>\n",
              "      <td id=\"T_00c93_row6_col9\" class=\"data row6 col9\" >0.4438</td>\n",
              "      <td id=\"T_00c93_row6_col10\" class=\"data row6 col10\" >0.6073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_00c93_level0_row7\" class=\"row_heading level0 row7\" >ru</th>\n",
              "      <td id=\"T_00c93_row7_col0\" class=\"data row7 col0\" >-0.0791</td>\n",
              "      <td id=\"T_00c93_row7_col1\" class=\"data row7 col1\" >0.3471</td>\n",
              "      <td id=\"T_00c93_row7_col2\" class=\"data row7 col2\" >0.1788</td>\n",
              "      <td id=\"T_00c93_row7_col3\" class=\"data row7 col3\" >0.0580</td>\n",
              "      <td id=\"T_00c93_row7_col4\" class=\"data row7 col4\" >0.1055</td>\n",
              "      <td id=\"T_00c93_row7_col5\" class=\"data row7 col5\" >0.1499</td>\n",
              "      <td id=\"T_00c93_row7_col6\" class=\"data row7 col6\" >0.3570</td>\n",
              "      <td id=\"T_00c93_row7_col7\" class=\"data row7 col7\" >1.0000</td>\n",
              "      <td id=\"T_00c93_row7_col8\" class=\"data row7 col8\" >0.9317</td>\n",
              "      <td id=\"T_00c93_row7_col9\" class=\"data row7 col9\" >0.7726</td>\n",
              "      <td id=\"T_00c93_row7_col10\" class=\"data row7 col10\" >0.5683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_00c93_level0_row8\" class=\"row_heading level0 row8\" >uk</th>\n",
              "      <td id=\"T_00c93_row8_col0\" class=\"data row8 col0\" >-0.1245</td>\n",
              "      <td id=\"T_00c93_row8_col1\" class=\"data row8 col1\" >0.3500</td>\n",
              "      <td id=\"T_00c93_row8_col2\" class=\"data row8 col2\" >0.1623</td>\n",
              "      <td id=\"T_00c93_row8_col3\" class=\"data row8 col3\" >0.0733</td>\n",
              "      <td id=\"T_00c93_row8_col4\" class=\"data row8 col4\" >0.1045</td>\n",
              "      <td id=\"T_00c93_row8_col5\" class=\"data row8 col5\" >0.1478</td>\n",
              "      <td id=\"T_00c93_row8_col6\" class=\"data row8 col6\" >0.4382</td>\n",
              "      <td id=\"T_00c93_row8_col7\" class=\"data row8 col7\" >0.9317</td>\n",
              "      <td id=\"T_00c93_row8_col8\" class=\"data row8 col8\" >1.0000</td>\n",
              "      <td id=\"T_00c93_row8_col9\" class=\"data row8 col9\" >0.7883</td>\n",
              "      <td id=\"T_00c93_row8_col10\" class=\"data row8 col10\" >0.5773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_00c93_level0_row9\" class=\"row_heading level0 row9\" >ar</th>\n",
              "      <td id=\"T_00c93_row9_col0\" class=\"data row9 col0\" >-0.0030</td>\n",
              "      <td id=\"T_00c93_row9_col1\" class=\"data row9 col1\" >0.3736</td>\n",
              "      <td id=\"T_00c93_row9_col2\" class=\"data row9 col2\" >0.3141</td>\n",
              "      <td id=\"T_00c93_row9_col3\" class=\"data row9 col3\" >0.2244</td>\n",
              "      <td id=\"T_00c93_row9_col4\" class=\"data row9 col4\" >0.2535</td>\n",
              "      <td id=\"T_00c93_row9_col5\" class=\"data row9 col5\" >0.3016</td>\n",
              "      <td id=\"T_00c93_row9_col6\" class=\"data row9 col6\" >0.4438</td>\n",
              "      <td id=\"T_00c93_row9_col7\" class=\"data row9 col7\" >0.7726</td>\n",
              "      <td id=\"T_00c93_row9_col8\" class=\"data row9 col8\" >0.7883</td>\n",
              "      <td id=\"T_00c93_row9_col9\" class=\"data row9 col9\" >1.0000</td>\n",
              "      <td id=\"T_00c93_row9_col10\" class=\"data row9 col10\" >0.6829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_00c93_level0_row10\" class=\"row_heading level0 row10\" >zh</th>\n",
              "      <td id=\"T_00c93_row10_col0\" class=\"data row10 col0\" >0.0994</td>\n",
              "      <td id=\"T_00c93_row10_col1\" class=\"data row10 col1\" >0.5491</td>\n",
              "      <td id=\"T_00c93_row10_col2\" class=\"data row10 col2\" >0.5523</td>\n",
              "      <td id=\"T_00c93_row10_col3\" class=\"data row10 col3\" >0.3091</td>\n",
              "      <td id=\"T_00c93_row10_col4\" class=\"data row10 col4\" >0.3519</td>\n",
              "      <td id=\"T_00c93_row10_col5\" class=\"data row10 col5\" >0.4097</td>\n",
              "      <td id=\"T_00c93_row10_col6\" class=\"data row10 col6\" >0.6073</td>\n",
              "      <td id=\"T_00c93_row10_col7\" class=\"data row10 col7\" >0.5683</td>\n",
              "      <td id=\"T_00c93_row10_col8\" class=\"data row10 col8\" >0.5773</td>\n",
              "      <td id=\"T_00c93_row10_col9\" class=\"data row10 col9\" >0.6829</td>\n",
              "      <td id=\"T_00c93_row10_col10\" class=\"data row10 col10\" >1.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{lrrrrrrrrrrr}\n",
            " & \\bfseries en & \\bfseries de & \\bfseries nl & \\bfseries es & \\bfseries pt & \\bfseries ca & \\bfseries cs & \\bfseries ru & \\bfseries uk & \\bfseries ar & \\bfseries zh \\\\\n",
            "\\bfseries en & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{E0DEED}} \\color[HTML]{000000} 0.3586 & {\\cellcolor[HTML]{D9D8EA}} \\color[HTML]{000000} 0.4236 & {\\cellcolor[HTML]{F1EBF5}} \\color[HTML]{000000} 0.1826 & {\\cellcolor[HTML]{EDE7F2}} \\color[HTML]{000000} 0.2428 & {\\cellcolor[HTML]{E7E3F0}} \\color[HTML]{000000} 0.2954 & {\\cellcolor[HTML]{F8F1F8}} \\color[HTML]{000000} 0.0899 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.0791 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.1245 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.0030 & {\\cellcolor[HTML]{F8F1F8}} \\color[HTML]{000000} 0.0994 \\\\\n",
            "\\bfseries de & {\\cellcolor[HTML]{E0DEED}} \\color[HTML]{000000} 0.3586 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{9FBAD9}} \\color[HTML]{000000} 0.7830 & {\\cellcolor[HTML]{ACC0DD}} \\color[HTML]{000000} 0.7144 & {\\cellcolor[HTML]{A2BCDA}} \\color[HTML]{000000} 0.7724 & {\\cellcolor[HTML]{A7BDDB}} \\color[HTML]{000000} 0.7467 & {\\cellcolor[HTML]{ADC1DD}} \\color[HTML]{000000} 0.7077 & {\\cellcolor[HTML]{E1DFED}} \\color[HTML]{000000} 0.3471 & {\\cellcolor[HTML]{E1DFED}} \\color[HTML]{000000} 0.3500 & {\\cellcolor[HTML]{DFDDEC}} \\color[HTML]{000000} 0.3736 & {\\cellcolor[HTML]{C8CDE4}} \\color[HTML]{000000} 0.5491 \\\\\n",
            "\\bfseries nl & {\\cellcolor[HTML]{D9D8EA}} \\color[HTML]{000000} 0.4236 & {\\cellcolor[HTML]{9FBAD9}} \\color[HTML]{000000} 0.7830 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{B1C2DE}} \\color[HTML]{000000} 0.6816 & {\\cellcolor[HTML]{A5BDDB}} \\color[HTML]{000000} 0.7527 & {\\cellcolor[HTML]{8FB4D6}} \\color[HTML]{000000} 0.8655 & {\\cellcolor[HTML]{CACEE5}} \\color[HTML]{000000} 0.5376 & {\\cellcolor[HTML]{F2ECF5}} \\color[HTML]{000000} 0.1788 & {\\cellcolor[HTML]{F3EDF5}} \\color[HTML]{000000} 0.1623 & {\\cellcolor[HTML]{E5E1EF}} \\color[HTML]{000000} 0.3141 & {\\cellcolor[HTML]{C8CDE4}} \\color[HTML]{000000} 0.5523 \\\\\n",
            "\\bfseries es & {\\cellcolor[HTML]{F1EBF5}} \\color[HTML]{000000} 0.1826 & {\\cellcolor[HTML]{ACC0DD}} \\color[HTML]{000000} 0.7144 & {\\cellcolor[HTML]{B1C2DE}} \\color[HTML]{000000} 0.6816 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{7DACD1}} \\color[HTML]{000000} 0.9586 & {\\cellcolor[HTML]{97B7D7}} \\color[HTML]{000000} 0.8215 & {\\cellcolor[HTML]{D2D3E7}} \\color[HTML]{000000} 0.4784 & {\\cellcolor[HTML]{FBF3F9}} \\color[HTML]{000000} 0.0580 & {\\cellcolor[HTML]{FAF2F8}} \\color[HTML]{000000} 0.0733 & {\\cellcolor[HTML]{EEE9F3}} \\color[HTML]{000000} 0.2244 & {\\cellcolor[HTML]{E6E2EF}} \\color[HTML]{000000} 0.3091 \\\\\n",
            "\\bfseries pt & {\\cellcolor[HTML]{EDE7F2}} \\color[HTML]{000000} 0.2428 & {\\cellcolor[HTML]{A2BCDA}} \\color[HTML]{000000} 0.7724 & {\\cellcolor[HTML]{A5BDDB}} \\color[HTML]{000000} 0.7527 & {\\cellcolor[HTML]{7DACD1}} \\color[HTML]{000000} 0.9586 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{8FB4D6}} \\color[HTML]{000000} 0.8666 & {\\cellcolor[HTML]{D2D3E7}} \\color[HTML]{000000} 0.4834 & {\\cellcolor[HTML]{F7F0F7}} \\color[HTML]{000000} 0.1055 & {\\cellcolor[HTML]{F7F0F7}} \\color[HTML]{000000} 0.1045 & {\\cellcolor[HTML]{ECE7F2}} \\color[HTML]{000000} 0.2535 & {\\cellcolor[HTML]{E0DEED}} \\color[HTML]{000000} 0.3519 \\\\\n",
            "\\bfseries ca & {\\cellcolor[HTML]{E7E3F0}} \\color[HTML]{000000} 0.2954 & {\\cellcolor[HTML]{A7BDDB}} \\color[HTML]{000000} 0.7467 & {\\cellcolor[HTML]{8FB4D6}} \\color[HTML]{000000} 0.8655 & {\\cellcolor[HTML]{97B7D7}} \\color[HTML]{000000} 0.8215 & {\\cellcolor[HTML]{8FB4D6}} \\color[HTML]{000000} 0.8666 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{CCCFE5}} \\color[HTML]{000000} 0.5308 & {\\cellcolor[HTML]{F4EDF6}} \\color[HTML]{000000} 0.1499 & {\\cellcolor[HTML]{F4EEF6}} \\color[HTML]{000000} 0.1478 & {\\cellcolor[HTML]{E7E3F0}} \\color[HTML]{000000} 0.3016 & {\\cellcolor[HTML]{DAD9EA}} \\color[HTML]{000000} 0.4097 \\\\\n",
            "\\bfseries cs & {\\cellcolor[HTML]{F8F1F8}} \\color[HTML]{000000} 0.0899 & {\\cellcolor[HTML]{ADC1DD}} \\color[HTML]{000000} 0.7077 & {\\cellcolor[HTML]{CACEE5}} \\color[HTML]{000000} 0.5376 & {\\cellcolor[HTML]{D2D3E7}} \\color[HTML]{000000} 0.4784 & {\\cellcolor[HTML]{D2D3E7}} \\color[HTML]{000000} 0.4834 & {\\cellcolor[HTML]{CCCFE5}} \\color[HTML]{000000} 0.5308 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{E0DEED}} \\color[HTML]{000000} 0.3570 & {\\cellcolor[HTML]{D7D6E9}} \\color[HTML]{000000} 0.4382 & {\\cellcolor[HTML]{D7D6E9}} \\color[HTML]{000000} 0.4438 & {\\cellcolor[HTML]{BFC9E1}} \\color[HTML]{000000} 0.6073 \\\\\n",
            "\\bfseries ru & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.0791 & {\\cellcolor[HTML]{E1DFED}} \\color[HTML]{000000} 0.3471 & {\\cellcolor[HTML]{F2ECF5}} \\color[HTML]{000000} 0.1788 & {\\cellcolor[HTML]{FBF3F9}} \\color[HTML]{000000} 0.0580 & {\\cellcolor[HTML]{F7F0F7}} \\color[HTML]{000000} 0.1055 & {\\cellcolor[HTML]{F4EDF6}} \\color[HTML]{000000} 0.1499 & {\\cellcolor[HTML]{E0DEED}} \\color[HTML]{000000} 0.3570 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{81AED2}} \\color[HTML]{000000} 0.9317 & {\\cellcolor[HTML]{A2BCDA}} \\color[HTML]{000000} 0.7726 & {\\cellcolor[HTML]{C5CCE3}} \\color[HTML]{000000} 0.5683 \\\\\n",
            "\\bfseries uk & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.1245 & {\\cellcolor[HTML]{E1DFED}} \\color[HTML]{000000} 0.3500 & {\\cellcolor[HTML]{F3EDF5}} \\color[HTML]{000000} 0.1623 & {\\cellcolor[HTML]{FAF2F8}} \\color[HTML]{000000} 0.0733 & {\\cellcolor[HTML]{F7F0F7}} \\color[HTML]{000000} 0.1045 & {\\cellcolor[HTML]{F4EEF6}} \\color[HTML]{000000} 0.1478 & {\\cellcolor[HTML]{D7D6E9}} \\color[HTML]{000000} 0.4382 & {\\cellcolor[HTML]{81AED2}} \\color[HTML]{000000} 0.9317 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{9FBAD9}} \\color[HTML]{000000} 0.7883 & {\\cellcolor[HTML]{C4CBE3}} \\color[HTML]{000000} 0.5773 \\\\\n",
            "\\bfseries ar & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.0030 & {\\cellcolor[HTML]{DFDDEC}} \\color[HTML]{000000} 0.3736 & {\\cellcolor[HTML]{E5E1EF}} \\color[HTML]{000000} 0.3141 & {\\cellcolor[HTML]{EEE9F3}} \\color[HTML]{000000} 0.2244 & {\\cellcolor[HTML]{ECE7F2}} \\color[HTML]{000000} 0.2535 & {\\cellcolor[HTML]{E7E3F0}} \\color[HTML]{000000} 0.3016 & {\\cellcolor[HTML]{D7D6E9}} \\color[HTML]{000000} 0.4438 & {\\cellcolor[HTML]{A2BCDA}} \\color[HTML]{000000} 0.7726 & {\\cellcolor[HTML]{9FBAD9}} \\color[HTML]{000000} 0.7883 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{B1C2DE}} \\color[HTML]{000000} 0.6829 \\\\\n",
            "\\bfseries zh & {\\cellcolor[HTML]{F8F1F8}} \\color[HTML]{000000} 0.0994 & {\\cellcolor[HTML]{C8CDE4}} \\color[HTML]{000000} 0.5491 & {\\cellcolor[HTML]{C8CDE4}} \\color[HTML]{000000} 0.5523 & {\\cellcolor[HTML]{E6E2EF}} \\color[HTML]{000000} 0.3091 & {\\cellcolor[HTML]{E0DEED}} \\color[HTML]{000000} 0.3519 & {\\cellcolor[HTML]{DAD9EA}} \\color[HTML]{000000} 0.4097 & {\\cellcolor[HTML]{BFC9E1}} \\color[HTML]{000000} 0.6073 & {\\cellcolor[HTML]{C5CCE3}} \\color[HTML]{000000} 0.5683 & {\\cellcolor[HTML]{C4CBE3}} \\color[HTML]{000000} 0.5773 & {\\cellcolor[HTML]{B1C2DE}} \\color[HTML]{000000} 0.6829 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RQ3 Multilingual Generalization"
      ],
      "metadata": {
        "id": "0vMhiXehCEc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#How do detectors perform on individual language (only corresponding LLM machine data) when trained on all train languages per specified LLM?\n",
        "results_all = pd.DataFrame()\n",
        "for train_language in ['en', 'es', 'ru', 'all', 'en3']:\n",
        "  results = pd.DataFrame()\n",
        "  for test_language in ['ar', 'ca', 'cs', 'de', 'en', 'es', 'nl', 'pt', 'ru', 'uk', 'zh']:\n",
        "    temp = analyze_language_for_train_language_per_llm(test_results, train_language, test_language)\n",
        "    temp = temp[~temp['Train LLM'].str.contains('all')]\n",
        "    temp = temp[['Train Language', 'Train LLM', 'Model', 'Macro avg F1-score']]\n",
        "    temp = temp.sort_values(by=['Train Language', 'Train LLM', 'Model'])\n",
        "    temp = temp.set_index(['Train Language', 'Train LLM', 'Model'])\n",
        "    temp.rename(columns={'Macro avg F1-score': test_language}, inplace=True)\n",
        "    if len(results) > 0: temp = temp[test_language]\n",
        "    results = pd.concat([results, temp], copy=False, axis=1)\n",
        "  results_all = pd.concat([results_all, results], copy=False)"
      ],
      "metadata": {
        "id": "pqufi1juCJq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "342b37d0-86b9-49c6-dff8-18ead85e63c3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 324/324 [00:05<00:00, 56.00it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 88.28it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 89.70it/s]\n",
            "100%|██████████| 324/324 [00:05<00:00, 61.53it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 81.46it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 91.27it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 84.58it/s]\n",
            "100%|██████████| 324/324 [00:05<00:00, 58.97it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 88.70it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 90.52it/s]\n",
            "100%|██████████| 324/324 [00:05<00:00, 59.77it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 84.50it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 90.97it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 82.92it/s]\n",
            "100%|██████████| 324/324 [00:05<00:00, 60.13it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 93.96it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 91.10it/s] \n",
            "100%|██████████| 324/324 [00:05<00:00, 60.62it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 82.62it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 91.06it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 84.22it/s]\n",
            "100%|██████████| 324/324 [00:05<00:00, 59.69it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 90.22it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 89.59it/s] \n",
            "100%|██████████| 324/324 [00:05<00:00, 59.53it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 85.07it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 92.79it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 85.31it/s]\n",
            "100%|██████████| 324/324 [00:05<00:00, 59.73it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 91.12it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 89.74it/s] \n",
            "100%|██████████| 324/324 [00:05<00:00, 58.65it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 84.76it/s] \n",
            "100%|██████████| 324/324 [00:03<00:00, 90.53it/s]\n",
            "100%|██████████| 324/324 [00:04<00:00, 79.50it/s]\n",
            "100%|██████████| 324/324 [00:05<00:00, 61.91it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 91.78it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 92.36it/s]\n",
            "100%|██████████| 324/324 [00:05<00:00, 58.20it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 86.88it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 90.50it/s]\n",
            "100%|██████████| 324/324 [00:06<00:00, 49.34it/s]\n",
            "100%|██████████| 324/324 [00:05<00:00, 64.33it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 90.39it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 90.99it/s]\n",
            "100%|██████████| 324/324 [00:05<00:00, 56.19it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 90.15it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 91.11it/s]\n",
            "100%|██████████| 324/324 [00:04<00:00, 70.69it/s]\n",
            "100%|██████████| 324/324 [00:04<00:00, 69.07it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 91.59it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 91.24it/s]\n",
            "100%|██████████| 324/324 [00:05<00:00, 56.84it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 92.93it/s]\n",
            "100%|██████████| 324/324 [00:03<00:00, 90.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3min 47s, sys: 1.55 s, total: 3min 48s\n",
            "Wall time: 3min 53s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_mean = results_all.reset_index().drop(columns=['Train LLM', 'Model']).groupby(['Train Language']).agg('mean')\n",
        "temp_std = results_all.reset_index().drop(columns=['Train LLM', 'Model']).groupby(['Train Language']).agg('std')\n",
        "temp = temp_mean.copy()\n",
        "for col in temp_mean.columns:\n",
        "  temp[col] = [f\"{str('%.4f' % x)} (±{str('%.2f' % y)})\" for x,y in zip(temp_mean[col], temp_std[col])]\n",
        "sort_key = {'en': 0, 'es': 1, 'ru': 2, 'all': 3, 'en3': 4}\n",
        "temp = temp.sort_index(key=lambda x: x.map(sort_key))\n",
        "\n",
        "display(temp.style.apply(b_g,cmap='PuBu', axis=1))\n",
        "print(temp.style.apply(b_g,cmap='PuBu', axis=1).applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True))"
      ],
      "metadata": {
        "id": "LunUb7SXnrt0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "efc5e52d-9a92-4e57-b053-3572b1c4b3e3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001f06890>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_0df59_row0_col0 {\n",
              "  background-color: #fcf4fa;\n",
              "}\n",
              "#T_0df59_row0_col1, #T_0df59_row4_col5 {\n",
              "  background-color: #c8cde4;\n",
              "}\n",
              "#T_0df59_row0_col2 {\n",
              "  background-color: #dfddec;\n",
              "}\n",
              "#T_0df59_row0_col3 {\n",
              "  background-color: #bdc8e1;\n",
              "}\n",
              "#T_0df59_row0_col4 {\n",
              "  background-color: #04649d;\n",
              "}\n",
              "#T_0df59_row0_col5, #T_0df59_row0_col7 {\n",
              "  background-color: #d0d1e6;\n",
              "}\n",
              "#T_0df59_row0_col6 {\n",
              "  background-color: #b5c4df;\n",
              "}\n",
              "#T_0df59_row0_col8 {\n",
              "  background-color: #dcdaeb;\n",
              "}\n",
              "#T_0df59_row0_col9 {\n",
              "  background-color: #e2dfee;\n",
              "}\n",
              "#T_0df59_row0_col10, #T_0df59_row1_col4, #T_0df59_row2_col4, #T_0df59_row3_col10, #T_0df59_row4_col0 {\n",
              "  background-color: #fff7fb;\n",
              "}\n",
              "#T_0df59_row1_col0 {\n",
              "  background-color: #e6e2ef;\n",
              "}\n",
              "#T_0df59_row1_col1 {\n",
              "  background-color: #4897c4;\n",
              "}\n",
              "#T_0df59_row1_col2 {\n",
              "  background-color: #b3c3de;\n",
              "}\n",
              "#T_0df59_row1_col3 {\n",
              "  background-color: #83afd3;\n",
              "}\n",
              "#T_0df59_row1_col5 {\n",
              "  background-color: #056ba9;\n",
              "}\n",
              "#T_0df59_row1_col6 {\n",
              "  background-color: #93b5d6;\n",
              "}\n",
              "#T_0df59_row1_col7 {\n",
              "  background-color: #2786bb;\n",
              "}\n",
              "#T_0df59_row1_col8 {\n",
              "  background-color: #d5d5e8;\n",
              "}\n",
              "#T_0df59_row1_col9 {\n",
              "  background-color: #d6d6e9;\n",
              "}\n",
              "#T_0df59_row1_col10, #T_0df59_row4_col10 {\n",
              "  background-color: #f5eff6;\n",
              "}\n",
              "#T_0df59_row2_col0 {\n",
              "  background-color: #6ba5cd;\n",
              "}\n",
              "#T_0df59_row2_col1 {\n",
              "  background-color: #f1ebf4;\n",
              "}\n",
              "#T_0df59_row2_col2, #T_0df59_row2_col10 {\n",
              "  background-color: #c9cee4;\n",
              "}\n",
              "#T_0df59_row2_col3 {\n",
              "  background-color: #d7d6e9;\n",
              "}\n",
              "#T_0df59_row2_col5 {\n",
              "  background-color: #e4e1ef;\n",
              "}\n",
              "#T_0df59_row2_col6 {\n",
              "  background-color: #e8e4f0;\n",
              "}\n",
              "#T_0df59_row2_col7 {\n",
              "  background-color: #eee9f3;\n",
              "}\n",
              "#T_0df59_row2_col8 {\n",
              "  background-color: #0567a1;\n",
              "}\n",
              "#T_0df59_row2_col9 {\n",
              "  background-color: #0872b1;\n",
              "}\n",
              "#T_0df59_row3_col0 {\n",
              "  background-color: #d4d4e8;\n",
              "}\n",
              "#T_0df59_row3_col1 {\n",
              "  background-color: #549cc7;\n",
              "}\n",
              "#T_0df59_row3_col2, #T_0df59_row4_col3 {\n",
              "  background-color: #b0c2de;\n",
              "}\n",
              "#T_0df59_row3_col3, #T_0df59_row3_col9 {\n",
              "  background-color: #8bb2d4;\n",
              "}\n",
              "#T_0df59_row3_col4 {\n",
              "  background-color: #0771b1;\n",
              "}\n",
              "#T_0df59_row3_col5 {\n",
              "  background-color: #1077b4;\n",
              "}\n",
              "#T_0df59_row3_col6 {\n",
              "  background-color: #80aed2;\n",
              "}\n",
              "#T_0df59_row3_col7 {\n",
              "  background-color: #2c89bd;\n",
              "}\n",
              "#T_0df59_row3_col8 {\n",
              "  background-color: #69a5cc;\n",
              "}\n",
              "#T_0df59_row4_col1 {\n",
              "  background-color: #c2cbe2;\n",
              "}\n",
              "#T_0df59_row4_col2 {\n",
              "  background-color: #d2d3e7;\n",
              "}\n",
              "#T_0df59_row4_col4 {\n",
              "  background-color: #04639b;\n",
              "}\n",
              "#T_0df59_row4_col6 {\n",
              "  background-color: #b4c4df;\n",
              "}\n",
              "#T_0df59_row4_col7 {\n",
              "  background-color: #c6cce3;\n",
              "}\n",
              "#T_0df59_row4_col8 {\n",
              "  background-color: #e7e3f0;\n",
              "}\n",
              "#T_0df59_row4_col9 {\n",
              "  background-color: #e9e5f1;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_0df59\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_0df59_level0_col0\" class=\"col_heading level0 col0\" >ar</th>\n",
              "      <th id=\"T_0df59_level0_col1\" class=\"col_heading level0 col1\" >ca</th>\n",
              "      <th id=\"T_0df59_level0_col2\" class=\"col_heading level0 col2\" >cs</th>\n",
              "      <th id=\"T_0df59_level0_col3\" class=\"col_heading level0 col3\" >de</th>\n",
              "      <th id=\"T_0df59_level0_col4\" class=\"col_heading level0 col4\" >en</th>\n",
              "      <th id=\"T_0df59_level0_col5\" class=\"col_heading level0 col5\" >es</th>\n",
              "      <th id=\"T_0df59_level0_col6\" class=\"col_heading level0 col6\" >nl</th>\n",
              "      <th id=\"T_0df59_level0_col7\" class=\"col_heading level0 col7\" >pt</th>\n",
              "      <th id=\"T_0df59_level0_col8\" class=\"col_heading level0 col8\" >ru</th>\n",
              "      <th id=\"T_0df59_level0_col9\" class=\"col_heading level0 col9\" >uk</th>\n",
              "      <th id=\"T_0df59_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Train Language</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "      <th class=\"blank col7\" >&nbsp;</th>\n",
              "      <th class=\"blank col8\" >&nbsp;</th>\n",
              "      <th class=\"blank col9\" >&nbsp;</th>\n",
              "      <th class=\"blank col10\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_0df59_level0_row0\" class=\"row_heading level0 row0\" >en</th>\n",
              "      <td id=\"T_0df59_row0_col0\" class=\"data row0 col0\" >0.5016 (±0.17)</td>\n",
              "      <td id=\"T_0df59_row0_col1\" class=\"data row0 col1\" >0.6412 (±0.21)</td>\n",
              "      <td id=\"T_0df59_row0_col2\" class=\"data row0 col2\" >0.5909 (±0.19)</td>\n",
              "      <td id=\"T_0df59_row0_col3\" class=\"data row0 col3\" >0.6578 (±0.21)</td>\n",
              "      <td id=\"T_0df59_row0_col4\" class=\"data row0 col4\" >0.9361 (±0.06)</td>\n",
              "      <td id=\"T_0df59_row0_col5\" class=\"data row0 col5\" >0.6284 (±0.17)</td>\n",
              "      <td id=\"T_0df59_row0_col6\" class=\"data row0 col6\" >0.6703 (±0.21)</td>\n",
              "      <td id=\"T_0df59_row0_col7\" class=\"data row0 col7\" >0.6273 (±0.20)</td>\n",
              "      <td id=\"T_0df59_row0_col8\" class=\"data row0 col8\" >0.5976 (±0.18)</td>\n",
              "      <td id=\"T_0df59_row0_col9\" class=\"data row0 col9\" >0.5832 (±0.16)</td>\n",
              "      <td id=\"T_0df59_row0_col10\" class=\"data row0 col10\" >0.4902 (±0.14)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_0df59_level0_row1\" class=\"row_heading level0 row1\" >es</th>\n",
              "      <td id=\"T_0df59_row1_col0\" class=\"data row1 col0\" >0.6480 (±0.21)</td>\n",
              "      <td id=\"T_0df59_row1_col1\" class=\"data row1 col1\" >0.8413 (±0.12)</td>\n",
              "      <td id=\"T_0df59_row1_col2\" class=\"data row1 col2\" >0.7300 (±0.22)</td>\n",
              "      <td id=\"T_0df59_row1_col3\" class=\"data row1 col3\" >0.7850 (±0.19)</td>\n",
              "      <td id=\"T_0df59_row1_col4\" class=\"data row1 col4\" >0.5790 (±0.25)</td>\n",
              "      <td id=\"T_0df59_row1_col5\" class=\"data row1 col5\" >0.9259 (±0.06)</td>\n",
              "      <td id=\"T_0df59_row1_col6\" class=\"data row1 col6\" >0.7689 (±0.18)</td>\n",
              "      <td id=\"T_0df59_row1_col7\" class=\"data row1 col7\" >0.8749 (±0.09)</td>\n",
              "      <td id=\"T_0df59_row1_col8\" class=\"data row1 col8\" >0.6804 (±0.22)</td>\n",
              "      <td id=\"T_0df59_row1_col9\" class=\"data row1 col9\" >0.6800 (±0.21)</td>\n",
              "      <td id=\"T_0df59_row1_col10\" class=\"data row1 col10\" >0.6082 (±0.21)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_0df59_level0_row2\" class=\"row_heading level0 row2\" >ru</th>\n",
              "      <td id=\"T_0df59_row2_col0\" class=\"data row2 col0\" >0.7421 (±0.18)</td>\n",
              "      <td id=\"T_0df59_row2_col1\" class=\"data row2 col1\" >0.5274 (±0.21)</td>\n",
              "      <td id=\"T_0df59_row2_col2\" class=\"data row2 col2\" >0.6171 (±0.26)</td>\n",
              "      <td id=\"T_0df59_row2_col3\" class=\"data row2 col3\" >0.5914 (±0.24)</td>\n",
              "      <td id=\"T_0df59_row2_col4\" class=\"data row2 col4\" >0.4795 (±0.23)</td>\n",
              "      <td id=\"T_0df59_row2_col5\" class=\"data row2 col5\" >0.5614 (±0.23)</td>\n",
              "      <td id=\"T_0df59_row2_col6\" class=\"data row2 col6\" >0.5524 (±0.22)</td>\n",
              "      <td id=\"T_0df59_row2_col7\" class=\"data row2 col7\" >0.5369 (±0.21)</td>\n",
              "      <td id=\"T_0df59_row2_col8\" class=\"data row2 col8\" >0.8870 (±0.12)</td>\n",
              "      <td id=\"T_0df59_row2_col9\" class=\"data row2 col9\" >0.8557 (±0.14)</td>\n",
              "      <td id=\"T_0df59_row2_col10\" class=\"data row2 col10\" >0.6183 (±0.19)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_0df59_level0_row3\" class=\"row_heading level0 row3\" >all</th>\n",
              "      <td id=\"T_0df59_row3_col0\" class=\"data row3 col0\" >0.7463 (±0.19)</td>\n",
              "      <td id=\"T_0df59_row3_col1\" class=\"data row3 col1\" >0.8765 (±0.09)</td>\n",
              "      <td id=\"T_0df59_row3_col2\" class=\"data row3 col2\" >0.7918 (±0.22)</td>\n",
              "      <td id=\"T_0df59_row3_col3\" class=\"data row3 col3\" >0.8300 (±0.16)</td>\n",
              "      <td id=\"T_0df59_row3_col4\" class=\"data row3 col4\" >0.9472 (±0.04)</td>\n",
              "      <td id=\"T_0df59_row3_col5\" class=\"data row3 col5\" >0.9375 (±0.05)</td>\n",
              "      <td id=\"T_0df59_row3_col6\" class=\"data row3 col6\" >0.8407 (±0.13)</td>\n",
              "      <td id=\"T_0df59_row3_col7\" class=\"data row3 col7\" >0.9099 (±0.05)</td>\n",
              "      <td id=\"T_0df59_row3_col8\" class=\"data row3 col8\" >0.8592 (±0.20)</td>\n",
              "      <td id=\"T_0df59_row3_col9\" class=\"data row3 col9\" >0.8296 (±0.21)</td>\n",
              "      <td id=\"T_0df59_row3_col10\" class=\"data row3 col10\" >0.6558 (±0.18)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_0df59_level0_row4\" class=\"row_heading level0 row4\" >en3</th>\n",
              "      <td id=\"T_0df59_row4_col0\" class=\"data row4 col0\" >0.4931 (±0.20)</td>\n",
              "      <td id=\"T_0df59_row4_col1\" class=\"data row4 col1\" >0.6568 (±0.21)</td>\n",
              "      <td id=\"T_0df59_row4_col2\" class=\"data row4 col2\" >0.6270 (±0.21)</td>\n",
              "      <td id=\"T_0df59_row4_col3\" class=\"data row4 col3\" >0.6871 (±0.21)</td>\n",
              "      <td id=\"T_0df59_row4_col4\" class=\"data row4 col4\" >0.9529 (±0.06)</td>\n",
              "      <td id=\"T_0df59_row4_col5\" class=\"data row4 col5\" >0.6476 (±0.17)</td>\n",
              "      <td id=\"T_0df59_row4_col6\" class=\"data row4 col6\" >0.6800 (±0.22)</td>\n",
              "      <td id=\"T_0df59_row4_col7\" class=\"data row4 col7\" >0.6497 (±0.19)</td>\n",
              "      <td id=\"T_0df59_row4_col8\" class=\"data row4 col8\" >0.5759 (±0.20)</td>\n",
              "      <td id=\"T_0df59_row4_col9\" class=\"data row4 col9\" >0.5716 (±0.21)</td>\n",
              "      <td id=\"T_0df59_row4_col10\" class=\"data row4 col10\" >0.5299 (±0.16)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{llllllllllll}\n",
            " & \\bfseries ar & \\bfseries ca & \\bfseries cs & \\bfseries de & \\bfseries en & \\bfseries es & \\bfseries nl & \\bfseries pt & \\bfseries ru & \\bfseries uk & \\bfseries zh \\\\\n",
            "Train Language &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
            "\\bfseries en & {\\cellcolor[HTML]{FCF4FA}} 0.5016 (±0.17) & {\\cellcolor[HTML]{C8CDE4}} 0.6412 (±0.21) & {\\cellcolor[HTML]{DFDDEC}} 0.5909 (±0.19) & {\\cellcolor[HTML]{BDC8E1}} 0.6578 (±0.21) & {\\cellcolor[HTML]{04649D}} 0.9361 (±0.06) & {\\cellcolor[HTML]{D0D1E6}} 0.6284 (±0.17) & {\\cellcolor[HTML]{B5C4DF}} 0.6703 (±0.21) & {\\cellcolor[HTML]{D0D1E6}} 0.6273 (±0.20) & {\\cellcolor[HTML]{DCDAEB}} 0.5976 (±0.18) & {\\cellcolor[HTML]{E2DFEE}} 0.5832 (±0.16) & {\\cellcolor[HTML]{FFF7FB}} 0.4902 (±0.14) \\\\\n",
            "\\bfseries es & {\\cellcolor[HTML]{E6E2EF}} 0.6480 (±0.21) & {\\cellcolor[HTML]{4897C4}} 0.8413 (±0.12) & {\\cellcolor[HTML]{B3C3DE}} 0.7300 (±0.22) & {\\cellcolor[HTML]{83AFD3}} 0.7850 (±0.19) & {\\cellcolor[HTML]{FFF7FB}} 0.5790 (±0.25) & {\\cellcolor[HTML]{056BA9}} 0.9259 (±0.06) & {\\cellcolor[HTML]{93B5D6}} 0.7689 (±0.18) & {\\cellcolor[HTML]{2786BB}} 0.8749 (±0.09) & {\\cellcolor[HTML]{D5D5E8}} 0.6804 (±0.22) & {\\cellcolor[HTML]{D6D6E9}} 0.6800 (±0.21) & {\\cellcolor[HTML]{F5EFF6}} 0.6082 (±0.21) \\\\\n",
            "\\bfseries ru & {\\cellcolor[HTML]{6BA5CD}} 0.7421 (±0.18) & {\\cellcolor[HTML]{F1EBF4}} 0.5274 (±0.21) & {\\cellcolor[HTML]{C9CEE4}} 0.6171 (±0.26) & {\\cellcolor[HTML]{D7D6E9}} 0.5914 (±0.24) & {\\cellcolor[HTML]{FFF7FB}} 0.4795 (±0.23) & {\\cellcolor[HTML]{E4E1EF}} 0.5614 (±0.23) & {\\cellcolor[HTML]{E8E4F0}} 0.5524 (±0.22) & {\\cellcolor[HTML]{EEE9F3}} 0.5369 (±0.21) & {\\cellcolor[HTML]{0567A1}} 0.8870 (±0.12) & {\\cellcolor[HTML]{0872B1}} 0.8557 (±0.14) & {\\cellcolor[HTML]{C9CEE4}} 0.6183 (±0.19) \\\\\n",
            "\\bfseries all & {\\cellcolor[HTML]{D4D4E8}} 0.7463 (±0.19) & {\\cellcolor[HTML]{549CC7}} 0.8765 (±0.09) & {\\cellcolor[HTML]{B0C2DE}} 0.7918 (±0.22) & {\\cellcolor[HTML]{8BB2D4}} 0.8300 (±0.16) & {\\cellcolor[HTML]{0771B1}} 0.9472 (±0.04) & {\\cellcolor[HTML]{1077B4}} 0.9375 (±0.05) & {\\cellcolor[HTML]{80AED2}} 0.8407 (±0.13) & {\\cellcolor[HTML]{2C89BD}} 0.9099 (±0.05) & {\\cellcolor[HTML]{69A5CC}} 0.8592 (±0.20) & {\\cellcolor[HTML]{8BB2D4}} 0.8296 (±0.21) & {\\cellcolor[HTML]{FFF7FB}} 0.6558 (±0.18) \\\\\n",
            "\\bfseries en3 & {\\cellcolor[HTML]{FFF7FB}} 0.4931 (±0.20) & {\\cellcolor[HTML]{C2CBE2}} 0.6568 (±0.21) & {\\cellcolor[HTML]{D2D3E7}} 0.6270 (±0.21) & {\\cellcolor[HTML]{B0C2DE}} 0.6871 (±0.21) & {\\cellcolor[HTML]{04639B}} 0.9529 (±0.06) & {\\cellcolor[HTML]{C8CDE4}} 0.6476 (±0.17) & {\\cellcolor[HTML]{B4C4DF}} 0.6800 (±0.22) & {\\cellcolor[HTML]{C6CCE3}} 0.6497 (±0.19) & {\\cellcolor[HTML]{E7E3F0}} 0.5759 (±0.20) & {\\cellcolor[HTML]{E9E5F1}} 0.5716 (±0.21) & {\\cellcolor[HTML]{F5EFF6}} 0.5299 (±0.16) \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using the same scale (the same color representing the same value), i.e., not per row min-max\n",
        "temp = temp.style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, gmap=temp_mean, axis=None)\n",
        "display(temp)\n",
        "temp = temp.applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True)\n",
        "#little hack to properly format latex table\n",
        "print(temp.replace('\\\\color[HTML]{F1F1F1} ', '\\\\textcolor{white}{').replace('\\\\color[HTML]{000000} ', '\\\\textcolor{black}{').replace(') & {\\\\cellcolor', ')} & {\\\\cellcolor').replace(') \\\\', ')} \\\\'))"
      ],
      "metadata": {
        "id": "ayfR6qj9w3Lf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "ae4e2655-9639-41de-9706-9b4be6261472"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd004df17b0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_6e665_row0_col0 {\n",
              "  background-color: #d0d1e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row0_col1, #T_6e665_row1_col0, #T_6e665_row4_col5 {\n",
              "  background-color: #b8c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row0_col2, #T_6e665_row2_col3 {\n",
              "  background-color: #c1cae2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row0_col3, #T_6e665_row4_col1 {\n",
              "  background-color: #b5c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row0_col4 {\n",
              "  background-color: #81aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row0_col5, #T_6e665_row0_col7, #T_6e665_row4_col2 {\n",
              "  background-color: #bbc7e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row0_col6 {\n",
              "  background-color: #b4c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row0_col8 {\n",
              "  background-color: #c0c9e2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row0_col9, #T_6e665_row1_col4 {\n",
              "  background-color: #c2cbe2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row0_col10 {\n",
              "  background-color: #d2d2e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row1_col1, #T_6e665_row3_col6 {\n",
              "  background-color: #94b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row1_col2 {\n",
              "  background-color: #a9bfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row1_col3 {\n",
              "  background-color: #9fbad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row1_col5 {\n",
              "  background-color: #83afd3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row1_col6 {\n",
              "  background-color: #a2bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row1_col7 {\n",
              "  background-color: #8eb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row1_col8, #T_6e665_row1_col9, #T_6e665_row4_col3, #T_6e665_row4_col6 {\n",
              "  background-color: #b1c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row1_col10 {\n",
              "  background-color: #bfc9e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row2_col0 {\n",
              "  background-color: #a8bedc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row2_col1, #T_6e665_row4_col10 {\n",
              "  background-color: #cccfe5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row2_col2 {\n",
              "  background-color: #bdc8e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row2_col4 {\n",
              "  background-color: #d2d3e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row2_col5 {\n",
              "  background-color: #c6cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row2_col6 {\n",
              "  background-color: #c8cde4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row2_col7 {\n",
              "  background-color: #cacee5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row2_col8 {\n",
              "  background-color: #8bb2d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row2_col9, #T_6e665_row3_col8 {\n",
              "  background-color: #91b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row2_col10 {\n",
              "  background-color: #bcc7e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row3_col0 {\n",
              "  background-color: #a7bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row3_col1 {\n",
              "  background-color: #8cb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row3_col2 {\n",
              "  background-color: #9ebad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row3_col3, #T_6e665_row3_col9 {\n",
              "  background-color: #96b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row3_col4, #T_6e665_row4_col4 {\n",
              "  background-color: #7eadd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row3_col5 {\n",
              "  background-color: #80aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row3_col7 {\n",
              "  background-color: #86b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row3_col10, #T_6e665_row4_col7 {\n",
              "  background-color: #b7c5df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row4_col0 {\n",
              "  background-color: #d1d2e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_6e665_row4_col8, #T_6e665_row4_col9 {\n",
              "  background-color: #c4cbe3;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_6e665\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_6e665_level0_col0\" class=\"col_heading level0 col0\" >ar</th>\n",
              "      <th id=\"T_6e665_level0_col1\" class=\"col_heading level0 col1\" >ca</th>\n",
              "      <th id=\"T_6e665_level0_col2\" class=\"col_heading level0 col2\" >cs</th>\n",
              "      <th id=\"T_6e665_level0_col3\" class=\"col_heading level0 col3\" >de</th>\n",
              "      <th id=\"T_6e665_level0_col4\" class=\"col_heading level0 col4\" >en</th>\n",
              "      <th id=\"T_6e665_level0_col5\" class=\"col_heading level0 col5\" >es</th>\n",
              "      <th id=\"T_6e665_level0_col6\" class=\"col_heading level0 col6\" >nl</th>\n",
              "      <th id=\"T_6e665_level0_col7\" class=\"col_heading level0 col7\" >pt</th>\n",
              "      <th id=\"T_6e665_level0_col8\" class=\"col_heading level0 col8\" >ru</th>\n",
              "      <th id=\"T_6e665_level0_col9\" class=\"col_heading level0 col9\" >uk</th>\n",
              "      <th id=\"T_6e665_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Train Language</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "      <th class=\"blank col7\" >&nbsp;</th>\n",
              "      <th class=\"blank col8\" >&nbsp;</th>\n",
              "      <th class=\"blank col9\" >&nbsp;</th>\n",
              "      <th class=\"blank col10\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_6e665_level0_row0\" class=\"row_heading level0 row0\" >en</th>\n",
              "      <td id=\"T_6e665_row0_col0\" class=\"data row0 col0\" >0.5016 (±0.17)</td>\n",
              "      <td id=\"T_6e665_row0_col1\" class=\"data row0 col1\" >0.6412 (±0.21)</td>\n",
              "      <td id=\"T_6e665_row0_col2\" class=\"data row0 col2\" >0.5909 (±0.19)</td>\n",
              "      <td id=\"T_6e665_row0_col3\" class=\"data row0 col3\" >0.6578 (±0.21)</td>\n",
              "      <td id=\"T_6e665_row0_col4\" class=\"data row0 col4\" >0.9361 (±0.06)</td>\n",
              "      <td id=\"T_6e665_row0_col5\" class=\"data row0 col5\" >0.6284 (±0.17)</td>\n",
              "      <td id=\"T_6e665_row0_col6\" class=\"data row0 col6\" >0.6703 (±0.21)</td>\n",
              "      <td id=\"T_6e665_row0_col7\" class=\"data row0 col7\" >0.6273 (±0.20)</td>\n",
              "      <td id=\"T_6e665_row0_col8\" class=\"data row0 col8\" >0.5976 (±0.18)</td>\n",
              "      <td id=\"T_6e665_row0_col9\" class=\"data row0 col9\" >0.5832 (±0.16)</td>\n",
              "      <td id=\"T_6e665_row0_col10\" class=\"data row0 col10\" >0.4902 (±0.14)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_6e665_level0_row1\" class=\"row_heading level0 row1\" >es</th>\n",
              "      <td id=\"T_6e665_row1_col0\" class=\"data row1 col0\" >0.6480 (±0.21)</td>\n",
              "      <td id=\"T_6e665_row1_col1\" class=\"data row1 col1\" >0.8413 (±0.12)</td>\n",
              "      <td id=\"T_6e665_row1_col2\" class=\"data row1 col2\" >0.7300 (±0.22)</td>\n",
              "      <td id=\"T_6e665_row1_col3\" class=\"data row1 col3\" >0.7850 (±0.19)</td>\n",
              "      <td id=\"T_6e665_row1_col4\" class=\"data row1 col4\" >0.5790 (±0.25)</td>\n",
              "      <td id=\"T_6e665_row1_col5\" class=\"data row1 col5\" >0.9259 (±0.06)</td>\n",
              "      <td id=\"T_6e665_row1_col6\" class=\"data row1 col6\" >0.7689 (±0.18)</td>\n",
              "      <td id=\"T_6e665_row1_col7\" class=\"data row1 col7\" >0.8749 (±0.09)</td>\n",
              "      <td id=\"T_6e665_row1_col8\" class=\"data row1 col8\" >0.6804 (±0.22)</td>\n",
              "      <td id=\"T_6e665_row1_col9\" class=\"data row1 col9\" >0.6800 (±0.21)</td>\n",
              "      <td id=\"T_6e665_row1_col10\" class=\"data row1 col10\" >0.6082 (±0.21)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_6e665_level0_row2\" class=\"row_heading level0 row2\" >ru</th>\n",
              "      <td id=\"T_6e665_row2_col0\" class=\"data row2 col0\" >0.7421 (±0.18)</td>\n",
              "      <td id=\"T_6e665_row2_col1\" class=\"data row2 col1\" >0.5274 (±0.21)</td>\n",
              "      <td id=\"T_6e665_row2_col2\" class=\"data row2 col2\" >0.6171 (±0.26)</td>\n",
              "      <td id=\"T_6e665_row2_col3\" class=\"data row2 col3\" >0.5914 (±0.24)</td>\n",
              "      <td id=\"T_6e665_row2_col4\" class=\"data row2 col4\" >0.4795 (±0.23)</td>\n",
              "      <td id=\"T_6e665_row2_col5\" class=\"data row2 col5\" >0.5614 (±0.23)</td>\n",
              "      <td id=\"T_6e665_row2_col6\" class=\"data row2 col6\" >0.5524 (±0.22)</td>\n",
              "      <td id=\"T_6e665_row2_col7\" class=\"data row2 col7\" >0.5369 (±0.21)</td>\n",
              "      <td id=\"T_6e665_row2_col8\" class=\"data row2 col8\" >0.8870 (±0.12)</td>\n",
              "      <td id=\"T_6e665_row2_col9\" class=\"data row2 col9\" >0.8557 (±0.14)</td>\n",
              "      <td id=\"T_6e665_row2_col10\" class=\"data row2 col10\" >0.6183 (±0.19)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_6e665_level0_row3\" class=\"row_heading level0 row3\" >all</th>\n",
              "      <td id=\"T_6e665_row3_col0\" class=\"data row3 col0\" >0.7463 (±0.19)</td>\n",
              "      <td id=\"T_6e665_row3_col1\" class=\"data row3 col1\" >0.8765 (±0.09)</td>\n",
              "      <td id=\"T_6e665_row3_col2\" class=\"data row3 col2\" >0.7918 (±0.22)</td>\n",
              "      <td id=\"T_6e665_row3_col3\" class=\"data row3 col3\" >0.8300 (±0.16)</td>\n",
              "      <td id=\"T_6e665_row3_col4\" class=\"data row3 col4\" >0.9472 (±0.04)</td>\n",
              "      <td id=\"T_6e665_row3_col5\" class=\"data row3 col5\" >0.9375 (±0.05)</td>\n",
              "      <td id=\"T_6e665_row3_col6\" class=\"data row3 col6\" >0.8407 (±0.13)</td>\n",
              "      <td id=\"T_6e665_row3_col7\" class=\"data row3 col7\" >0.9099 (±0.05)</td>\n",
              "      <td id=\"T_6e665_row3_col8\" class=\"data row3 col8\" >0.8592 (±0.20)</td>\n",
              "      <td id=\"T_6e665_row3_col9\" class=\"data row3 col9\" >0.8296 (±0.21)</td>\n",
              "      <td id=\"T_6e665_row3_col10\" class=\"data row3 col10\" >0.6558 (±0.18)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_6e665_level0_row4\" class=\"row_heading level0 row4\" >en3</th>\n",
              "      <td id=\"T_6e665_row4_col0\" class=\"data row4 col0\" >0.4931 (±0.20)</td>\n",
              "      <td id=\"T_6e665_row4_col1\" class=\"data row4 col1\" >0.6568 (±0.21)</td>\n",
              "      <td id=\"T_6e665_row4_col2\" class=\"data row4 col2\" >0.6270 (±0.21)</td>\n",
              "      <td id=\"T_6e665_row4_col3\" class=\"data row4 col3\" >0.6871 (±0.21)</td>\n",
              "      <td id=\"T_6e665_row4_col4\" class=\"data row4 col4\" >0.9529 (±0.06)</td>\n",
              "      <td id=\"T_6e665_row4_col5\" class=\"data row4 col5\" >0.6476 (±0.17)</td>\n",
              "      <td id=\"T_6e665_row4_col6\" class=\"data row4 col6\" >0.6800 (±0.22)</td>\n",
              "      <td id=\"T_6e665_row4_col7\" class=\"data row4 col7\" >0.6497 (±0.19)</td>\n",
              "      <td id=\"T_6e665_row4_col8\" class=\"data row4 col8\" >0.5759 (±0.20)</td>\n",
              "      <td id=\"T_6e665_row4_col9\" class=\"data row4 col9\" >0.5716 (±0.21)</td>\n",
              "      <td id=\"T_6e665_row4_col10\" class=\"data row4 col10\" >0.5299 (±0.16)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{llllllllllll}\n",
            " & \\bfseries ar & \\bfseries ca & \\bfseries cs & \\bfseries de & \\bfseries en & \\bfseries es & \\bfseries nl & \\bfseries pt & \\bfseries ru & \\bfseries uk & \\bfseries zh \\\\\n",
            "Train Language &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
            "\\bfseries en & {\\cellcolor[HTML]{D0D1E6}} \\textcolor{black}{0.5016 (±0.17)} & {\\cellcolor[HTML]{B8C6E0}} \\textcolor{black}{0.6412 (±0.21)} & {\\cellcolor[HTML]{C1CAE2}} \\textcolor{black}{0.5909 (±0.19)} & {\\cellcolor[HTML]{B5C4DF}} \\textcolor{black}{0.6578 (±0.21)} & {\\cellcolor[HTML]{81AED2}} \\textcolor{black}{0.9361 (±0.06)} & {\\cellcolor[HTML]{BBC7E0}} \\textcolor{black}{0.6284 (±0.17)} & {\\cellcolor[HTML]{B4C4DF}} \\textcolor{black}{0.6703 (±0.21)} & {\\cellcolor[HTML]{BBC7E0}} \\textcolor{black}{0.6273 (±0.20)} & {\\cellcolor[HTML]{C0C9E2}} \\textcolor{black}{0.5976 (±0.18)} & {\\cellcolor[HTML]{C2CBE2}} \\textcolor{black}{0.5832 (±0.16)} & {\\cellcolor[HTML]{D2D2E7}} \\textcolor{black}{0.4902 (±0.14)} \\\\\n",
            "\\bfseries es & {\\cellcolor[HTML]{B8C6E0}} \\textcolor{black}{0.6480 (±0.21)} & {\\cellcolor[HTML]{94B6D7}} \\textcolor{black}{0.8413 (±0.12)} & {\\cellcolor[HTML]{A9BFDC}} \\textcolor{black}{0.7300 (±0.22)} & {\\cellcolor[HTML]{9FBAD9}} \\textcolor{black}{0.7850 (±0.19)} & {\\cellcolor[HTML]{C2CBE2}} \\textcolor{black}{0.5790 (±0.25)} & {\\cellcolor[HTML]{83AFD3}} \\textcolor{black}{0.9259 (±0.06)} & {\\cellcolor[HTML]{A2BCDA}} \\textcolor{black}{0.7689 (±0.18)} & {\\cellcolor[HTML]{8EB3D5}} \\textcolor{black}{0.8749 (±0.09)} & {\\cellcolor[HTML]{B1C2DE}} \\textcolor{black}{0.6804 (±0.22)} & {\\cellcolor[HTML]{B1C2DE}} \\textcolor{black}{0.6800 (±0.21)} & {\\cellcolor[HTML]{BFC9E1}} \\textcolor{black}{0.6082 (±0.21)} \\\\\n",
            "\\bfseries ru & {\\cellcolor[HTML]{A8BEDC}} \\textcolor{black}{0.7421 (±0.18)} & {\\cellcolor[HTML]{CCCFE5}} \\textcolor{black}{0.5274 (±0.21)} & {\\cellcolor[HTML]{BDC8E1}} \\textcolor{black}{0.6171 (±0.26)} & {\\cellcolor[HTML]{C1CAE2}} \\textcolor{black}{0.5914 (±0.24)} & {\\cellcolor[HTML]{D2D3E7}} \\textcolor{black}{0.4795 (±0.23)} & {\\cellcolor[HTML]{C6CCE3}} \\textcolor{black}{0.5614 (±0.23)} & {\\cellcolor[HTML]{C8CDE4}} \\textcolor{black}{0.5524 (±0.22)} & {\\cellcolor[HTML]{CACEE5}} \\textcolor{black}{0.5369 (±0.21)} & {\\cellcolor[HTML]{8BB2D4}} \\textcolor{black}{0.8870 (±0.12)} & {\\cellcolor[HTML]{91B5D6}} \\textcolor{black}{0.8557 (±0.14)} & {\\cellcolor[HTML]{BCC7E1}} \\textcolor{black}{0.6183 (±0.19)} \\\\\n",
            "\\bfseries all & {\\cellcolor[HTML]{A7BDDB}} \\textcolor{black}{0.7463 (±0.19)} & {\\cellcolor[HTML]{8CB3D5}} \\textcolor{black}{0.8765 (±0.09)} & {\\cellcolor[HTML]{9EBAD9}} \\textcolor{black}{0.7918 (±0.22)} & {\\cellcolor[HTML]{96B6D7}} \\textcolor{black}{0.8300 (±0.16)} & {\\cellcolor[HTML]{7EADD1}} \\textcolor{black}{0.9472 (±0.04)} & {\\cellcolor[HTML]{80AED2}} \\textcolor{black}{0.9375 (±0.05)} & {\\cellcolor[HTML]{94B6D7}} \\textcolor{black}{0.8407 (±0.13)} & {\\cellcolor[HTML]{86B0D3}} \\textcolor{black}{0.9099 (±0.05)} & {\\cellcolor[HTML]{91B5D6}} \\textcolor{black}{0.8592 (±0.20)} & {\\cellcolor[HTML]{96B6D7}} \\textcolor{black}{0.8296 (±0.21)} & {\\cellcolor[HTML]{B7C5DF}} \\textcolor{black}{0.6558 (±0.18)} \\\\\n",
            "\\bfseries en3 & {\\cellcolor[HTML]{D1D2E6}} \\textcolor{black}{0.4931 (±0.20)} & {\\cellcolor[HTML]{B5C4DF}} \\textcolor{black}{0.6568 (±0.21)} & {\\cellcolor[HTML]{BBC7E0}} \\textcolor{black}{0.6270 (±0.21)} & {\\cellcolor[HTML]{B1C2DE}} \\textcolor{black}{0.6871 (±0.21)} & {\\cellcolor[HTML]{7EADD1}} \\textcolor{black}{0.9529 (±0.06)} & {\\cellcolor[HTML]{B8C6E0}} \\textcolor{black}{0.6476 (±0.17)} & {\\cellcolor[HTML]{B1C2DE}} \\textcolor{black}{0.6800 (±0.22)} & {\\cellcolor[HTML]{B7C5DF}} \\textcolor{black}{0.6497 (±0.19)} & {\\cellcolor[HTML]{C4CBE3}} \\textcolor{black}{0.5759 (±0.20)} & {\\cellcolor[HTML]{C4CBE3}} \\textcolor{black}{0.5716 (±0.21)} & {\\cellcolor[HTML]{CCCFE5}} \\textcolor{black}{0.5299 (±0.16)} \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bg_cmap = 'PuBu'\n",
        "bg_vmin = 0.0\n",
        "bg_vmax = 2.0\n",
        "bg_text_color_threshold = 0"
      ],
      "metadata": {
        "id": "gcI_sR8Qbsxq"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Only multilingual detectors\n",
        "multilingual = ['mdeberta-v3-base', 'xlm-roberta-large', 'mGPT', 'bert-base-multilingual-cased']\n",
        "sorted_languages = ['en', 'de', 'nl', 'es', 'pt', 'ca', 'cs', 'ru', 'uk','ar', 'zh']\n",
        "\n",
        "results_all_multilingual = results_all.loc[[x in multilingual for x in results_all.reset_index().Model], :]\n",
        "temp_mean = results_all_multilingual.reset_index().drop(columns=['Train LLM', 'Model']).groupby(['Train Language']).agg('mean')\n",
        "temp_std = results_all_multilingual.reset_index().drop(columns=['Train LLM', 'Model']).groupby(['Train Language']).agg('std')\n",
        "temp = temp_mean.copy()\n",
        "for col in temp_mean.columns:\n",
        "  temp[col] = [f\"{str('%.4f' % x)} (±{str('%.2f' % y)})\" for x,y in zip(temp_mean[col], temp_std[col])]\n",
        "sort_key = {'en': 0, 'es': 1, 'ru': 2, 'all': 3, 'en3': 4}\n",
        "temp = temp.sort_index(key=lambda x: x.map(sort_key))\n",
        "\n",
        "temp = temp.style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, gmap=temp_mean, axis=None)\n",
        "display(temp)\n",
        "temp = temp.applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True)\n",
        "#little hack to properly format latex table\n",
        "print(temp.replace('\\\\color[HTML]{F1F1F1} ', '\\\\textcolor{white}{').replace('\\\\color[HTML]{000000} ', '\\\\textcolor{black}{').replace(') & {\\\\cellcolor', ')} & {\\\\cellcolor').replace(') \\\\', ')} \\\\').replace('\\\\bfseries all', '\\\\hline\\n\\\\bfseries all'))\n",
        "\n",
        "temp = results_all_multilingual[sorted_languages].corr().style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4)\n",
        "display(temp)\n",
        "print(temp.applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True))"
      ],
      "metadata": {
        "id": "2258p-rTw-nD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "78ec5640-81ba-45b4-ef33-ffda71bfe2d5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001e237f0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_4eeb5_row0_col0 {\n",
              "  background-color: #c9cee4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row0_col1, #T_4eeb5_row1_col4, #T_4eeb5_row2_col10, #T_4eeb5_row4_col2 {\n",
              "  background-color: #a9bfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row0_col2, #T_4eeb5_row0_col9 {\n",
              "  background-color: #b3c3de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row0_col3 {\n",
              "  background-color: #9ab8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row0_col4, #T_4eeb5_row3_col7 {\n",
              "  background-color: #83afd3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row0_col5 {\n",
              "  background-color: #afc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row0_col6, #T_4eeb5_row4_col7 {\n",
              "  background-color: #a5bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row0_col7 {\n",
              "  background-color: #a8bedc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row0_col8, #T_4eeb5_row4_col5, #T_4eeb5_row4_col9 {\n",
              "  background-color: #acc0dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row0_col10, #T_4eeb5_row4_col0 {\n",
              "  background-color: #c6cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row1_col0 {\n",
              "  background-color: #9fbad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row1_col1 {\n",
              "  background-color: #8eb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row1_col2 {\n",
              "  background-color: #9cb9d9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row1_col3, #T_4eeb5_row3_col6 {\n",
              "  background-color: #8cb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row1_col5, #T_4eeb5_row3_col5, #T_4eeb5_row3_col9 {\n",
              "  background-color: #81aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row1_col6 {\n",
              "  background-color: #99b8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row1_col7, #T_4eeb5_row3_col1 {\n",
              "  background-color: #89b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row1_col8 {\n",
              "  background-color: #94b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row1_col9 {\n",
              "  background-color: #96b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row1_col10 {\n",
              "  background-color: #abbfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row2_col0 {\n",
              "  background-color: #93b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row2_col1 {\n",
              "  background-color: #b7c5df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row2_col2 {\n",
              "  background-color: #9ebad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row2_col3 {\n",
              "  background-color: #a4bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row2_col4 {\n",
              "  background-color: #c4cbe3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row2_col5, #T_4eeb5_row2_col6 {\n",
              "  background-color: #b0c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row2_col7 {\n",
              "  background-color: #b5c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row2_col8 {\n",
              "  background-color: #7eadd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row2_col9, #T_4eeb5_row3_col4, #T_4eeb5_row4_col4 {\n",
              "  background-color: #80aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row3_col0 {\n",
              "  background-color: #91b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row3_col2 {\n",
              "  background-color: #8fb4d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row3_col3 {\n",
              "  background-color: #86b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row3_col8 {\n",
              "  background-color: #7dacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row3_col10 {\n",
              "  background-color: #a2bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row4_col1 {\n",
              "  background-color: #a7bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row4_col3 {\n",
              "  background-color: #97b7d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row4_col6 {\n",
              "  background-color: #a1bbda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row4_col8 {\n",
              "  background-color: #adc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4eeb5_row4_col10 {\n",
              "  background-color: #bdc8e1;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_4eeb5\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_4eeb5_level0_col0\" class=\"col_heading level0 col0\" >ar</th>\n",
              "      <th id=\"T_4eeb5_level0_col1\" class=\"col_heading level0 col1\" >ca</th>\n",
              "      <th id=\"T_4eeb5_level0_col2\" class=\"col_heading level0 col2\" >cs</th>\n",
              "      <th id=\"T_4eeb5_level0_col3\" class=\"col_heading level0 col3\" >de</th>\n",
              "      <th id=\"T_4eeb5_level0_col4\" class=\"col_heading level0 col4\" >en</th>\n",
              "      <th id=\"T_4eeb5_level0_col5\" class=\"col_heading level0 col5\" >es</th>\n",
              "      <th id=\"T_4eeb5_level0_col6\" class=\"col_heading level0 col6\" >nl</th>\n",
              "      <th id=\"T_4eeb5_level0_col7\" class=\"col_heading level0 col7\" >pt</th>\n",
              "      <th id=\"T_4eeb5_level0_col8\" class=\"col_heading level0 col8\" >ru</th>\n",
              "      <th id=\"T_4eeb5_level0_col9\" class=\"col_heading level0 col9\" >uk</th>\n",
              "      <th id=\"T_4eeb5_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Train Language</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "      <th class=\"blank col7\" >&nbsp;</th>\n",
              "      <th class=\"blank col8\" >&nbsp;</th>\n",
              "      <th class=\"blank col9\" >&nbsp;</th>\n",
              "      <th class=\"blank col10\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_4eeb5_level0_row0\" class=\"row_heading level0 row0\" >en</th>\n",
              "      <td id=\"T_4eeb5_row0_col0\" class=\"data row0 col0\" >0.5448 (±0.19)</td>\n",
              "      <td id=\"T_4eeb5_row0_col1\" class=\"data row0 col1\" >0.7335 (±0.21)</td>\n",
              "      <td id=\"T_4eeb5_row0_col2\" class=\"data row0 col2\" >0.6793 (±0.18)</td>\n",
              "      <td id=\"T_4eeb5_row0_col3\" class=\"data row0 col3\" >0.8104 (±0.11)</td>\n",
              "      <td id=\"T_4eeb5_row0_col4\" class=\"data row0 col4\" >0.9292 (±0.07)</td>\n",
              "      <td id=\"T_4eeb5_row0_col5\" class=\"data row0 col5\" >0.7018 (±0.13)</td>\n",
              "      <td id=\"T_4eeb5_row0_col6\" class=\"data row0 col6\" >0.7508 (±0.20)</td>\n",
              "      <td id=\"T_4eeb5_row0_col7\" class=\"data row0 col7\" >0.7362 (±0.15)</td>\n",
              "      <td id=\"T_4eeb5_row0_col8\" class=\"data row0 col8\" >0.7148 (±0.14)</td>\n",
              "      <td id=\"T_4eeb5_row0_col9\" class=\"data row0 col9\" >0.6746 (±0.14)</td>\n",
              "      <td id=\"T_4eeb5_row0_col10\" class=\"data row0 col10\" >0.5580 (±0.14)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4eeb5_level0_row1\" class=\"row_heading level0 row1\" >es</th>\n",
              "      <td id=\"T_4eeb5_row1_col0\" class=\"data row1 col0\" >0.7857 (±0.15)</td>\n",
              "      <td id=\"T_4eeb5_row1_col1\" class=\"data row1 col1\" >0.8747 (±0.08)</td>\n",
              "      <td id=\"T_4eeb5_row1_col2\" class=\"data row1 col2\" >0.8016 (±0.20)</td>\n",
              "      <td id=\"T_4eeb5_row1_col3\" class=\"data row1 col3\" >0.8812 (±0.09)</td>\n",
              "      <td id=\"T_4eeb5_row1_col4\" class=\"data row1 col4\" >0.7322 (±0.20)</td>\n",
              "      <td id=\"T_4eeb5_row1_col5\" class=\"data row1 col5\" >0.9314 (±0.06)</td>\n",
              "      <td id=\"T_4eeb5_row1_col6\" class=\"data row1 col6\" >0.8143 (±0.18)</td>\n",
              "      <td id=\"T_4eeb5_row1_col7\" class=\"data row1 col7\" >0.8944 (±0.09)</td>\n",
              "      <td id=\"T_4eeb5_row1_col8\" class=\"data row1 col8\" >0.8375 (±0.11)</td>\n",
              "      <td id=\"T_4eeb5_row1_col9\" class=\"data row1 col9\" >0.8299 (±0.13)</td>\n",
              "      <td id=\"T_4eeb5_row1_col10\" class=\"data row1 col10\" >0.7216 (±0.17)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4eeb5_level0_row2\" class=\"row_heading level0 row2\" >ru</th>\n",
              "      <td id=\"T_4eeb5_row2_col0\" class=\"data row2 col0\" >0.8487 (±0.14)</td>\n",
              "      <td id=\"T_4eeb5_row2_col1\" class=\"data row2 col1\" >0.6532 (±0.19)</td>\n",
              "      <td id=\"T_4eeb5_row2_col2\" class=\"data row2 col2\" >0.7924 (±0.21)</td>\n",
              "      <td id=\"T_4eeb5_row2_col3\" class=\"data row2 col3\" >0.7591 (±0.17)</td>\n",
              "      <td id=\"T_4eeb5_row2_col4\" class=\"data row2 col4\" >0.5760 (±0.25)</td>\n",
              "      <td id=\"T_4eeb5_row2_col5\" class=\"data row2 col5\" >0.6884 (±0.21)</td>\n",
              "      <td id=\"T_4eeb5_row2_col6\" class=\"data row2 col6\" >0.6915 (±0.20)</td>\n",
              "      <td id=\"T_4eeb5_row2_col7\" class=\"data row2 col7\" >0.6626 (±0.19)</td>\n",
              "      <td id=\"T_4eeb5_row2_col8\" class=\"data row2 col8\" >0.9522 (±0.03)</td>\n",
              "      <td id=\"T_4eeb5_row2_col9\" class=\"data row2 col9\" >0.9387 (±0.04)</td>\n",
              "      <td id=\"T_4eeb5_row2_col10\" class=\"data row2 col10\" >0.7294 (±0.15)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4eeb5_level0_row3\" class=\"row_heading level0 row3\" >all</th>\n",
              "      <td id=\"T_4eeb5_row3_col0\" class=\"data row3 col0\" >0.8537 (±0.10)</td>\n",
              "      <td id=\"T_4eeb5_row3_col1\" class=\"data row3 col1\" >0.8977 (±0.08)</td>\n",
              "      <td id=\"T_4eeb5_row3_col2\" class=\"data row3 col2\" >0.8604 (±0.20)</td>\n",
              "      <td id=\"T_4eeb5_row3_col3\" class=\"data row3 col3\" >0.9073 (±0.06)</td>\n",
              "      <td id=\"T_4eeb5_row3_col4\" class=\"data row3 col4\" >0.9420 (±0.04)</td>\n",
              "      <td id=\"T_4eeb5_row3_col5\" class=\"data row3 col5\" >0.9372 (±0.05)</td>\n",
              "      <td id=\"T_4eeb5_row3_col6\" class=\"data row3 col6\" >0.8808 (±0.12)</td>\n",
              "      <td id=\"T_4eeb5_row3_col7\" class=\"data row3 col7\" >0.9253 (±0.04)</td>\n",
              "      <td id=\"T_4eeb5_row3_col8\" class=\"data row3 col8\" >0.9560 (±0.03)</td>\n",
              "      <td id=\"T_4eeb5_row3_col9\" class=\"data row3 col9\" >0.9374 (±0.05)</td>\n",
              "      <td id=\"T_4eeb5_row3_col10\" class=\"data row3 col10\" >0.7659 (±0.12)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4eeb5_level0_row4\" class=\"row_heading level0 row4\" >en3</th>\n",
              "      <td id=\"T_4eeb5_row4_col0\" class=\"data row4 col0\" >0.5605 (±0.24)</td>\n",
              "      <td id=\"T_4eeb5_row4_col1\" class=\"data row4 col1\" >0.7484 (±0.18)</td>\n",
              "      <td id=\"T_4eeb5_row4_col2\" class=\"data row4 col2\" >0.7289 (±0.19)</td>\n",
              "      <td id=\"T_4eeb5_row4_col3\" class=\"data row4 col3\" >0.8244 (±0.10)</td>\n",
              "      <td id=\"T_4eeb5_row4_col4\" class=\"data row4 col4\" >0.9392 (±0.08)</td>\n",
              "      <td id=\"T_4eeb5_row4_col5\" class=\"data row4 col5\" >0.7156 (±0.11)</td>\n",
              "      <td id=\"T_4eeb5_row4_col6\" class=\"data row4 col6\" >0.7778 (±0.18)</td>\n",
              "      <td id=\"T_4eeb5_row4_col7\" class=\"data row4 col7\" >0.7508 (±0.14)</td>\n",
              "      <td id=\"T_4eeb5_row4_col8\" class=\"data row4 col8\" >0.7092 (±0.16)</td>\n",
              "      <td id=\"T_4eeb5_row4_col9\" class=\"data row4 col9\" >0.7118 (±0.16)</td>\n",
              "      <td id=\"T_4eeb5_row4_col10\" class=\"data row4 col10\" >0.6160 (±0.14)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{llllllllllll}\n",
            " & \\bfseries ar & \\bfseries ca & \\bfseries cs & \\bfseries de & \\bfseries en & \\bfseries es & \\bfseries nl & \\bfseries pt & \\bfseries ru & \\bfseries uk & \\bfseries zh \\\\\n",
            "Train Language &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
            "\\bfseries en & {\\cellcolor[HTML]{C9CEE4}} \\textcolor{black}{0.5448 (±0.19)} & {\\cellcolor[HTML]{A9BFDC}} \\textcolor{black}{0.7335 (±0.21)} & {\\cellcolor[HTML]{B3C3DE}} \\textcolor{black}{0.6793 (±0.18)} & {\\cellcolor[HTML]{9AB8D8}} \\textcolor{black}{0.8104 (±0.11)} & {\\cellcolor[HTML]{83AFD3}} \\textcolor{black}{0.9292 (±0.07)} & {\\cellcolor[HTML]{AFC1DD}} \\textcolor{black}{0.7018 (±0.13)} & {\\cellcolor[HTML]{A5BDDB}} \\textcolor{black}{0.7508 (±0.20)} & {\\cellcolor[HTML]{A8BEDC}} \\textcolor{black}{0.7362 (±0.15)} & {\\cellcolor[HTML]{ACC0DD}} \\textcolor{black}{0.7148 (±0.14)} & {\\cellcolor[HTML]{B3C3DE}} \\textcolor{black}{0.6746 (±0.14)} & {\\cellcolor[HTML]{C6CCE3}} \\textcolor{black}{0.5580 (±0.14)} \\\\\n",
            "\\bfseries es & {\\cellcolor[HTML]{9FBAD9}} \\textcolor{black}{0.7857 (±0.15)} & {\\cellcolor[HTML]{8EB3D5}} \\textcolor{black}{0.8747 (±0.08)} & {\\cellcolor[HTML]{9CB9D9}} \\textcolor{black}{0.8016 (±0.20)} & {\\cellcolor[HTML]{8CB3D5}} \\textcolor{black}{0.8812 (±0.09)} & {\\cellcolor[HTML]{A9BFDC}} \\textcolor{black}{0.7322 (±0.20)} & {\\cellcolor[HTML]{81AED2}} \\textcolor{black}{0.9314 (±0.06)} & {\\cellcolor[HTML]{99B8D8}} \\textcolor{black}{0.8143 (±0.18)} & {\\cellcolor[HTML]{89B1D4}} \\textcolor{black}{0.8944 (±0.09)} & {\\cellcolor[HTML]{94B6D7}} \\textcolor{black}{0.8375 (±0.11)} & {\\cellcolor[HTML]{96B6D7}} \\textcolor{black}{0.8299 (±0.13)} & {\\cellcolor[HTML]{ABBFDC}} \\textcolor{black}{0.7216 (±0.17)} \\\\\n",
            "\\bfseries ru & {\\cellcolor[HTML]{93B5D6}} \\textcolor{black}{0.8487 (±0.14)} & {\\cellcolor[HTML]{B7C5DF}} \\textcolor{black}{0.6532 (±0.19)} & {\\cellcolor[HTML]{9EBAD9}} \\textcolor{black}{0.7924 (±0.21)} & {\\cellcolor[HTML]{A4BCDA}} \\textcolor{black}{0.7591 (±0.17)} & {\\cellcolor[HTML]{C4CBE3}} \\textcolor{black}{0.5760 (±0.25)} & {\\cellcolor[HTML]{B0C2DE}} \\textcolor{black}{0.6884 (±0.21)} & {\\cellcolor[HTML]{B0C2DE}} \\textcolor{black}{0.6915 (±0.20)} & {\\cellcolor[HTML]{B5C4DF}} \\textcolor{black}{0.6626 (±0.19)} & {\\cellcolor[HTML]{7EADD1}} \\textcolor{black}{0.9522 (±0.03)} & {\\cellcolor[HTML]{80AED2}} \\textcolor{black}{0.9387 (±0.04)} & {\\cellcolor[HTML]{A9BFDC}} \\textcolor{black}{0.7294 (±0.15)} \\\\\n",
            "\\hline\n",
            "\\bfseries all & {\\cellcolor[HTML]{91B5D6}} \\textcolor{black}{0.8537 (±0.10)} & {\\cellcolor[HTML]{89B1D4}} \\textcolor{black}{0.8977 (±0.08)} & {\\cellcolor[HTML]{8FB4D6}} \\textcolor{black}{0.8604 (±0.20)} & {\\cellcolor[HTML]{86B0D3}} \\textcolor{black}{0.9073 (±0.06)} & {\\cellcolor[HTML]{80AED2}} \\textcolor{black}{0.9420 (±0.04)} & {\\cellcolor[HTML]{81AED2}} \\textcolor{black}{0.9372 (±0.05)} & {\\cellcolor[HTML]{8CB3D5}} \\textcolor{black}{0.8808 (±0.12)} & {\\cellcolor[HTML]{83AFD3}} \\textcolor{black}{0.9253 (±0.04)} & {\\cellcolor[HTML]{7DACD1}} \\textcolor{black}{0.9560 (±0.03)} & {\\cellcolor[HTML]{81AED2}} \\textcolor{black}{0.9374 (±0.05)} & {\\cellcolor[HTML]{A2BCDA}} \\textcolor{black}{0.7659 (±0.12)} \\\\\n",
            "\\bfseries en3 & {\\cellcolor[HTML]{C6CCE3}} \\textcolor{black}{0.5605 (±0.24)} & {\\cellcolor[HTML]{A7BDDB}} \\textcolor{black}{0.7484 (±0.18)} & {\\cellcolor[HTML]{A9BFDC}} \\textcolor{black}{0.7289 (±0.19)} & {\\cellcolor[HTML]{97B7D7}} \\textcolor{black}{0.8244 (±0.10)} & {\\cellcolor[HTML]{80AED2}} \\textcolor{black}{0.9392 (±0.08)} & {\\cellcolor[HTML]{ACC0DD}} \\textcolor{black}{0.7156 (±0.11)} & {\\cellcolor[HTML]{A1BBDA}} \\textcolor{black}{0.7778 (±0.18)} & {\\cellcolor[HTML]{A5BDDB}} \\textcolor{black}{0.7508 (±0.14)} & {\\cellcolor[HTML]{ADC1DD}} \\textcolor{black}{0.7092 (±0.16)} & {\\cellcolor[HTML]{ACC0DD}} \\textcolor{black}{0.7118 (±0.16)} & {\\cellcolor[HTML]{BDC8E1}} \\textcolor{black}{0.6160 (±0.14)} \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd0020e9240>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_8b2aa_row0_col0, #T_8b2aa_row1_col1, #T_8b2aa_row2_col2, #T_8b2aa_row3_col3, #T_8b2aa_row4_col4, #T_8b2aa_row5_col5, #T_8b2aa_row6_col6, #T_8b2aa_row7_col7, #T_8b2aa_row8_col8, #T_8b2aa_row9_col9, #T_8b2aa_row10_col10 {\n",
              "  background-color: #73a9cf;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row0_col1, #T_8b2aa_row0_col2, #T_8b2aa_row1_col0, #T_8b2aa_row2_col0, #T_8b2aa_row8_col10, #T_8b2aa_row10_col8 {\n",
              "  background-color: #cccfe5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row0_col3, #T_8b2aa_row3_col0, #T_8b2aa_row6_col7, #T_8b2aa_row7_col6 {\n",
              "  background-color: #dedcec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row0_col4, #T_8b2aa_row4_col0 {\n",
              "  background-color: #d1d2e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row0_col5, #T_8b2aa_row3_col7, #T_8b2aa_row5_col0, #T_8b2aa_row7_col3 {\n",
              "  background-color: #d9d8ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row0_col6, #T_8b2aa_row0_col7, #T_8b2aa_row0_col8, #T_8b2aa_row0_col9, #T_8b2aa_row6_col0, #T_8b2aa_row7_col0, #T_8b2aa_row8_col0, #T_8b2aa_row9_col0 {\n",
              "  background-color: #fff7fb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row0_col10, #T_8b2aa_row10_col0 {\n",
              "  background-color: #fcf4fa;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row1_col2, #T_8b2aa_row2_col1 {\n",
              "  background-color: #b3c3de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row1_col3, #T_8b2aa_row3_col1, #T_8b2aa_row8_col9, #T_8b2aa_row9_col8 {\n",
              "  background-color: #a5bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row1_col4, #T_8b2aa_row4_col1 {\n",
              "  background-color: #99b8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row1_col5, #T_8b2aa_row5_col1 {\n",
              "  background-color: #afc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row1_col6, #T_8b2aa_row6_col1 {\n",
              "  background-color: #ede7f2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row1_col7, #T_8b2aa_row2_col7, #T_8b2aa_row7_col1, #T_8b2aa_row7_col2 {\n",
              "  background-color: #e0dded;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row1_col8, #T_8b2aa_row8_col1 {\n",
              "  background-color: #ece7f2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row1_col9, #T_8b2aa_row1_col10, #T_8b2aa_row4_col8, #T_8b2aa_row5_col8, #T_8b2aa_row8_col4, #T_8b2aa_row8_col5, #T_8b2aa_row9_col1, #T_8b2aa_row10_col1 {\n",
              "  background-color: #e7e3f0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row2_col3, #T_8b2aa_row3_col2 {\n",
              "  background-color: #c2cbe2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row2_col4, #T_8b2aa_row4_col2 {\n",
              "  background-color: #abbfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row2_col5, #T_8b2aa_row4_col5, #T_8b2aa_row5_col2, #T_8b2aa_row5_col4 {\n",
              "  background-color: #93b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row2_col6, #T_8b2aa_row6_col2 {\n",
              "  background-color: #f8f1f8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row2_col8, #T_8b2aa_row3_col6, #T_8b2aa_row6_col3, #T_8b2aa_row8_col2 {\n",
              "  background-color: #efe9f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row2_col9, #T_8b2aa_row4_col7, #T_8b2aa_row4_col9, #T_8b2aa_row7_col4, #T_8b2aa_row9_col2, #T_8b2aa_row9_col4 {\n",
              "  background-color: #dddbec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row2_col10, #T_8b2aa_row7_col10, #T_8b2aa_row10_col2, #T_8b2aa_row10_col7 {\n",
              "  background-color: #ced0e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row3_col4, #T_8b2aa_row4_col3 {\n",
              "  background-color: #83afd3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row3_col5, #T_8b2aa_row5_col3 {\n",
              "  background-color: #a7bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row3_col8, #T_8b2aa_row8_col3 {\n",
              "  background-color: #e0deed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row3_col9, #T_8b2aa_row9_col3 {\n",
              "  background-color: #d5d5e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row3_col10, #T_8b2aa_row10_col3 {\n",
              "  background-color: #e5e1ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row4_col6, #T_8b2aa_row6_col4 {\n",
              "  background-color: #f4edf6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row4_col10, #T_8b2aa_row10_col4 {\n",
              "  background-color: #e6e2ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row5_col6, #T_8b2aa_row6_col5 {\n",
              "  background-color: #f0eaf4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row5_col7, #T_8b2aa_row7_col5 {\n",
              "  background-color: #dbdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row5_col9, #T_8b2aa_row9_col5 {\n",
              "  background-color: #d8d7e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row5_col10, #T_8b2aa_row6_col9, #T_8b2aa_row9_col6, #T_8b2aa_row10_col5 {\n",
              "  background-color: #dad9ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row6_col8, #T_8b2aa_row8_col6 {\n",
              "  background-color: #d3d4e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row6_col10, #T_8b2aa_row10_col6 {\n",
              "  background-color: #d2d3e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row7_col8, #T_8b2aa_row8_col7 {\n",
              "  background-color: #89b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row7_col9, #T_8b2aa_row9_col7 {\n",
              "  background-color: #a4bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_8b2aa_row9_col10, #T_8b2aa_row10_col9 {\n",
              "  background-color: #acc0dd;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_8b2aa\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_8b2aa_level0_col0\" class=\"col_heading level0 col0\" >en</th>\n",
              "      <th id=\"T_8b2aa_level0_col1\" class=\"col_heading level0 col1\" >de</th>\n",
              "      <th id=\"T_8b2aa_level0_col2\" class=\"col_heading level0 col2\" >nl</th>\n",
              "      <th id=\"T_8b2aa_level0_col3\" class=\"col_heading level0 col3\" >es</th>\n",
              "      <th id=\"T_8b2aa_level0_col4\" class=\"col_heading level0 col4\" >pt</th>\n",
              "      <th id=\"T_8b2aa_level0_col5\" class=\"col_heading level0 col5\" >ca</th>\n",
              "      <th id=\"T_8b2aa_level0_col6\" class=\"col_heading level0 col6\" >cs</th>\n",
              "      <th id=\"T_8b2aa_level0_col7\" class=\"col_heading level0 col7\" >ru</th>\n",
              "      <th id=\"T_8b2aa_level0_col8\" class=\"col_heading level0 col8\" >uk</th>\n",
              "      <th id=\"T_8b2aa_level0_col9\" class=\"col_heading level0 col9\" >ar</th>\n",
              "      <th id=\"T_8b2aa_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_8b2aa_level0_row0\" class=\"row_heading level0 row0\" >en</th>\n",
              "      <td id=\"T_8b2aa_row0_col0\" class=\"data row0 col0\" >1.0000</td>\n",
              "      <td id=\"T_8b2aa_row0_col1\" class=\"data row0 col1\" >0.5248</td>\n",
              "      <td id=\"T_8b2aa_row0_col2\" class=\"data row0 col2\" >0.5249</td>\n",
              "      <td id=\"T_8b2aa_row0_col3\" class=\"data row0 col3\" >0.3774</td>\n",
              "      <td id=\"T_8b2aa_row0_col4\" class=\"data row0 col4\" >0.4949</td>\n",
              "      <td id=\"T_8b2aa_row0_col5\" class=\"data row0 col5\" >0.4240</td>\n",
              "      <td id=\"T_8b2aa_row0_col6\" class=\"data row0 col6\" >-0.0870</td>\n",
              "      <td id=\"T_8b2aa_row0_col7\" class=\"data row0 col7\" >-0.1903</td>\n",
              "      <td id=\"T_8b2aa_row0_col8\" class=\"data row0 col8\" >-0.3074</td>\n",
              "      <td id=\"T_8b2aa_row0_col9\" class=\"data row0 col9\" >-0.1778</td>\n",
              "      <td id=\"T_8b2aa_row0_col10\" class=\"data row0 col10\" >0.0412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8b2aa_level0_row1\" class=\"row_heading level0 row1\" >de</th>\n",
              "      <td id=\"T_8b2aa_row1_col0\" class=\"data row1 col0\" >0.5248</td>\n",
              "      <td id=\"T_8b2aa_row1_col1\" class=\"data row1 col1\" >1.0000</td>\n",
              "      <td id=\"T_8b2aa_row1_col2\" class=\"data row1 col2\" >0.6765</td>\n",
              "      <td id=\"T_8b2aa_row1_col3\" class=\"data row1 col3\" >0.7511</td>\n",
              "      <td id=\"T_8b2aa_row1_col4\" class=\"data row1 col4\" >0.8161</td>\n",
              "      <td id=\"T_8b2aa_row1_col5\" class=\"data row1 col5\" >0.7022</td>\n",
              "      <td id=\"T_8b2aa_row1_col6\" class=\"data row1 col6\" >0.2435</td>\n",
              "      <td id=\"T_8b2aa_row1_col7\" class=\"data row1 col7\" >0.3652</td>\n",
              "      <td id=\"T_8b2aa_row1_col8\" class=\"data row1 col8\" >0.2556</td>\n",
              "      <td id=\"T_8b2aa_row1_col9\" class=\"data row1 col9\" >0.3026</td>\n",
              "      <td id=\"T_8b2aa_row1_col10\" class=\"data row1 col10\" >0.2963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8b2aa_level0_row2\" class=\"row_heading level0 row2\" >nl</th>\n",
              "      <td id=\"T_8b2aa_row2_col0\" class=\"data row2 col0\" >0.5249</td>\n",
              "      <td id=\"T_8b2aa_row2_col1\" class=\"data row2 col1\" >0.6765</td>\n",
              "      <td id=\"T_8b2aa_row2_col2\" class=\"data row2 col2\" >1.0000</td>\n",
              "      <td id=\"T_8b2aa_row2_col3\" class=\"data row2 col3\" >0.5792</td>\n",
              "      <td id=\"T_8b2aa_row2_col4\" class=\"data row2 col4\" >0.7243</td>\n",
              "      <td id=\"T_8b2aa_row2_col5\" class=\"data row2 col5\" >0.8492</td>\n",
              "      <td id=\"T_8b2aa_row2_col6\" class=\"data row2 col6\" >0.0985</td>\n",
              "      <td id=\"T_8b2aa_row2_col7\" class=\"data row2 col7\" >0.3641</td>\n",
              "      <td id=\"T_8b2aa_row2_col8\" class=\"data row2 col8\" >0.2168</td>\n",
              "      <td id=\"T_8b2aa_row2_col9\" class=\"data row2 col9\" >0.3890</td>\n",
              "      <td id=\"T_8b2aa_row2_col10\" class=\"data row2 col10\" >0.5100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8b2aa_level0_row3\" class=\"row_heading level0 row3\" >es</th>\n",
              "      <td id=\"T_8b2aa_row3_col0\" class=\"data row3 col0\" >0.3774</td>\n",
              "      <td id=\"T_8b2aa_row3_col1\" class=\"data row3 col1\" >0.7511</td>\n",
              "      <td id=\"T_8b2aa_row3_col2\" class=\"data row3 col2\" >0.5792</td>\n",
              "      <td id=\"T_8b2aa_row3_col3\" class=\"data row3 col3\" >1.0000</td>\n",
              "      <td id=\"T_8b2aa_row3_col4\" class=\"data row3 col4\" >0.9243</td>\n",
              "      <td id=\"T_8b2aa_row3_col5\" class=\"data row3 col5\" >0.7478</td>\n",
              "      <td id=\"T_8b2aa_row3_col6\" class=\"data row3 col6\" >0.2131</td>\n",
              "      <td id=\"T_8b2aa_row3_col7\" class=\"data row3 col7\" >0.4184</td>\n",
              "      <td id=\"T_8b2aa_row3_col8\" class=\"data row3 col8\" >0.3518</td>\n",
              "      <td id=\"T_8b2aa_row3_col9\" class=\"data row3 col9\" >0.4588</td>\n",
              "      <td id=\"T_8b2aa_row3_col10\" class=\"data row3 col10\" >0.3148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8b2aa_level0_row4\" class=\"row_heading level0 row4\" >pt</th>\n",
              "      <td id=\"T_8b2aa_row4_col0\" class=\"data row4 col0\" >0.4949</td>\n",
              "      <td id=\"T_8b2aa_row4_col1\" class=\"data row4 col1\" >0.8161</td>\n",
              "      <td id=\"T_8b2aa_row4_col2\" class=\"data row4 col2\" >0.7243</td>\n",
              "      <td id=\"T_8b2aa_row4_col3\" class=\"data row4 col3\" >0.9243</td>\n",
              "      <td id=\"T_8b2aa_row4_col4\" class=\"data row4 col4\" >1.0000</td>\n",
              "      <td id=\"T_8b2aa_row4_col5\" class=\"data row4 col5\" >0.8457</td>\n",
              "      <td id=\"T_8b2aa_row4_col6\" class=\"data row4 col6\" >0.1508</td>\n",
              "      <td id=\"T_8b2aa_row4_col7\" class=\"data row4 col7\" >0.3831</td>\n",
              "      <td id=\"T_8b2aa_row4_col8\" class=\"data row4 col8\" >0.2955</td>\n",
              "      <td id=\"T_8b2aa_row4_col9\" class=\"data row4 col9\" >0.3898</td>\n",
              "      <td id=\"T_8b2aa_row4_col10\" class=\"data row4 col10\" >0.3068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8b2aa_level0_row5\" class=\"row_heading level0 row5\" >ca</th>\n",
              "      <td id=\"T_8b2aa_row5_col0\" class=\"data row5 col0\" >0.4240</td>\n",
              "      <td id=\"T_8b2aa_row5_col1\" class=\"data row5 col1\" >0.7022</td>\n",
              "      <td id=\"T_8b2aa_row5_col2\" class=\"data row5 col2\" >0.8492</td>\n",
              "      <td id=\"T_8b2aa_row5_col3\" class=\"data row5 col3\" >0.7478</td>\n",
              "      <td id=\"T_8b2aa_row5_col4\" class=\"data row5 col4\" >0.8457</td>\n",
              "      <td id=\"T_8b2aa_row5_col5\" class=\"data row5 col5\" >1.0000</td>\n",
              "      <td id=\"T_8b2aa_row5_col6\" class=\"data row5 col6\" >0.2092</td>\n",
              "      <td id=\"T_8b2aa_row5_col7\" class=\"data row5 col7\" >0.4019</td>\n",
              "      <td id=\"T_8b2aa_row5_col8\" class=\"data row5 col8\" >0.2981</td>\n",
              "      <td id=\"T_8b2aa_row5_col9\" class=\"data row5 col9\" >0.4311</td>\n",
              "      <td id=\"T_8b2aa_row5_col10\" class=\"data row5 col10\" >0.4134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8b2aa_level0_row6\" class=\"row_heading level0 row6\" >cs</th>\n",
              "      <td id=\"T_8b2aa_row6_col0\" class=\"data row6 col0\" >-0.0870</td>\n",
              "      <td id=\"T_8b2aa_row6_col1\" class=\"data row6 col1\" >0.2435</td>\n",
              "      <td id=\"T_8b2aa_row6_col2\" class=\"data row6 col2\" >0.0985</td>\n",
              "      <td id=\"T_8b2aa_row6_col3\" class=\"data row6 col3\" >0.2131</td>\n",
              "      <td id=\"T_8b2aa_row6_col4\" class=\"data row6 col4\" >0.1508</td>\n",
              "      <td id=\"T_8b2aa_row6_col5\" class=\"data row6 col5\" >0.2092</td>\n",
              "      <td id=\"T_8b2aa_row6_col6\" class=\"data row6 col6\" >1.0000</td>\n",
              "      <td id=\"T_8b2aa_row6_col7\" class=\"data row6 col7\" >0.3783</td>\n",
              "      <td id=\"T_8b2aa_row6_col8\" class=\"data row6 col8\" >0.4723</td>\n",
              "      <td id=\"T_8b2aa_row6_col9\" class=\"data row6 col9\" >0.4136</td>\n",
              "      <td id=\"T_8b2aa_row6_col10\" class=\"data row6 col10\" >0.4772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8b2aa_level0_row7\" class=\"row_heading level0 row7\" >ru</th>\n",
              "      <td id=\"T_8b2aa_row7_col0\" class=\"data row7 col0\" >-0.1903</td>\n",
              "      <td id=\"T_8b2aa_row7_col1\" class=\"data row7 col1\" >0.3652</td>\n",
              "      <td id=\"T_8b2aa_row7_col2\" class=\"data row7 col2\" >0.3641</td>\n",
              "      <td id=\"T_8b2aa_row7_col3\" class=\"data row7 col3\" >0.4184</td>\n",
              "      <td id=\"T_8b2aa_row7_col4\" class=\"data row7 col4\" >0.3831</td>\n",
              "      <td id=\"T_8b2aa_row7_col5\" class=\"data row7 col5\" >0.4019</td>\n",
              "      <td id=\"T_8b2aa_row7_col6\" class=\"data row7 col6\" >0.3783</td>\n",
              "      <td id=\"T_8b2aa_row7_col7\" class=\"data row7 col7\" >1.0000</td>\n",
              "      <td id=\"T_8b2aa_row7_col8\" class=\"data row7 col8\" >0.8976</td>\n",
              "      <td id=\"T_8b2aa_row7_col9\" class=\"data row7 col9\" >0.7622</td>\n",
              "      <td id=\"T_8b2aa_row7_col10\" class=\"data row7 col10\" >0.5121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8b2aa_level0_row8\" class=\"row_heading level0 row8\" >uk</th>\n",
              "      <td id=\"T_8b2aa_row8_col0\" class=\"data row8 col0\" >-0.3074</td>\n",
              "      <td id=\"T_8b2aa_row8_col1\" class=\"data row8 col1\" >0.2556</td>\n",
              "      <td id=\"T_8b2aa_row8_col2\" class=\"data row8 col2\" >0.2168</td>\n",
              "      <td id=\"T_8b2aa_row8_col3\" class=\"data row8 col3\" >0.3518</td>\n",
              "      <td id=\"T_8b2aa_row8_col4\" class=\"data row8 col4\" >0.2955</td>\n",
              "      <td id=\"T_8b2aa_row8_col5\" class=\"data row8 col5\" >0.2981</td>\n",
              "      <td id=\"T_8b2aa_row8_col6\" class=\"data row8 col6\" >0.4723</td>\n",
              "      <td id=\"T_8b2aa_row8_col7\" class=\"data row8 col7\" >0.8976</td>\n",
              "      <td id=\"T_8b2aa_row8_col8\" class=\"data row8 col8\" >1.0000</td>\n",
              "      <td id=\"T_8b2aa_row8_col9\" class=\"data row8 col9\" >0.7574</td>\n",
              "      <td id=\"T_8b2aa_row8_col10\" class=\"data row8 col10\" >0.5292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8b2aa_level0_row9\" class=\"row_heading level0 row9\" >ar</th>\n",
              "      <td id=\"T_8b2aa_row9_col0\" class=\"data row9 col0\" >-0.1778</td>\n",
              "      <td id=\"T_8b2aa_row9_col1\" class=\"data row9 col1\" >0.3026</td>\n",
              "      <td id=\"T_8b2aa_row9_col2\" class=\"data row9 col2\" >0.3890</td>\n",
              "      <td id=\"T_8b2aa_row9_col3\" class=\"data row9 col3\" >0.4588</td>\n",
              "      <td id=\"T_8b2aa_row9_col4\" class=\"data row9 col4\" >0.3898</td>\n",
              "      <td id=\"T_8b2aa_row9_col5\" class=\"data row9 col5\" >0.4311</td>\n",
              "      <td id=\"T_8b2aa_row9_col6\" class=\"data row9 col6\" >0.4136</td>\n",
              "      <td id=\"T_8b2aa_row9_col7\" class=\"data row9 col7\" >0.7622</td>\n",
              "      <td id=\"T_8b2aa_row9_col8\" class=\"data row9 col8\" >0.7574</td>\n",
              "      <td id=\"T_8b2aa_row9_col9\" class=\"data row9 col9\" >1.0000</td>\n",
              "      <td id=\"T_8b2aa_row9_col10\" class=\"data row9 col10\" >0.7178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_8b2aa_level0_row10\" class=\"row_heading level0 row10\" >zh</th>\n",
              "      <td id=\"T_8b2aa_row10_col0\" class=\"data row10 col0\" >0.0412</td>\n",
              "      <td id=\"T_8b2aa_row10_col1\" class=\"data row10 col1\" >0.2963</td>\n",
              "      <td id=\"T_8b2aa_row10_col2\" class=\"data row10 col2\" >0.5100</td>\n",
              "      <td id=\"T_8b2aa_row10_col3\" class=\"data row10 col3\" >0.3148</td>\n",
              "      <td id=\"T_8b2aa_row10_col4\" class=\"data row10 col4\" >0.3068</td>\n",
              "      <td id=\"T_8b2aa_row10_col5\" class=\"data row10 col5\" >0.4134</td>\n",
              "      <td id=\"T_8b2aa_row10_col6\" class=\"data row10 col6\" >0.4772</td>\n",
              "      <td id=\"T_8b2aa_row10_col7\" class=\"data row10 col7\" >0.5121</td>\n",
              "      <td id=\"T_8b2aa_row10_col8\" class=\"data row10 col8\" >0.5292</td>\n",
              "      <td id=\"T_8b2aa_row10_col9\" class=\"data row10 col9\" >0.7178</td>\n",
              "      <td id=\"T_8b2aa_row10_col10\" class=\"data row10 col10\" >1.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{lrrrrrrrrrrr}\n",
            " & \\bfseries en & \\bfseries de & \\bfseries nl & \\bfseries es & \\bfseries pt & \\bfseries ca & \\bfseries cs & \\bfseries ru & \\bfseries uk & \\bfseries ar & \\bfseries zh \\\\\n",
            "\\bfseries en & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{CCCFE5}} \\color[HTML]{000000} 0.5248 & {\\cellcolor[HTML]{CCCFE5}} \\color[HTML]{000000} 0.5249 & {\\cellcolor[HTML]{DEDCEC}} \\color[HTML]{000000} 0.3774 & {\\cellcolor[HTML]{D1D2E6}} \\color[HTML]{000000} 0.4949 & {\\cellcolor[HTML]{D9D8EA}} \\color[HTML]{000000} 0.4240 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.0870 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.1903 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.3074 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.1778 & {\\cellcolor[HTML]{FCF4FA}} \\color[HTML]{000000} 0.0412 \\\\\n",
            "\\bfseries de & {\\cellcolor[HTML]{CCCFE5}} \\color[HTML]{000000} 0.5248 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{B3C3DE}} \\color[HTML]{000000} 0.6765 & {\\cellcolor[HTML]{A5BDDB}} \\color[HTML]{000000} 0.7511 & {\\cellcolor[HTML]{99B8D8}} \\color[HTML]{000000} 0.8161 & {\\cellcolor[HTML]{AFC1DD}} \\color[HTML]{000000} 0.7022 & {\\cellcolor[HTML]{EDE7F2}} \\color[HTML]{000000} 0.2435 & {\\cellcolor[HTML]{E0DDED}} \\color[HTML]{000000} 0.3652 & {\\cellcolor[HTML]{ECE7F2}} \\color[HTML]{000000} 0.2556 & {\\cellcolor[HTML]{E7E3F0}} \\color[HTML]{000000} 0.3026 & {\\cellcolor[HTML]{E7E3F0}} \\color[HTML]{000000} 0.2963 \\\\\n",
            "\\bfseries nl & {\\cellcolor[HTML]{CCCFE5}} \\color[HTML]{000000} 0.5249 & {\\cellcolor[HTML]{B3C3DE}} \\color[HTML]{000000} 0.6765 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{C2CBE2}} \\color[HTML]{000000} 0.5792 & {\\cellcolor[HTML]{ABBFDC}} \\color[HTML]{000000} 0.7243 & {\\cellcolor[HTML]{93B5D6}} \\color[HTML]{000000} 0.8492 & {\\cellcolor[HTML]{F8F1F8}} \\color[HTML]{000000} 0.0985 & {\\cellcolor[HTML]{E0DDED}} \\color[HTML]{000000} 0.3641 & {\\cellcolor[HTML]{EFE9F3}} \\color[HTML]{000000} 0.2168 & {\\cellcolor[HTML]{DDDBEC}} \\color[HTML]{000000} 0.3890 & {\\cellcolor[HTML]{CED0E6}} \\color[HTML]{000000} 0.5100 \\\\\n",
            "\\bfseries es & {\\cellcolor[HTML]{DEDCEC}} \\color[HTML]{000000} 0.3774 & {\\cellcolor[HTML]{A5BDDB}} \\color[HTML]{000000} 0.7511 & {\\cellcolor[HTML]{C2CBE2}} \\color[HTML]{000000} 0.5792 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{83AFD3}} \\color[HTML]{000000} 0.9243 & {\\cellcolor[HTML]{A7BDDB}} \\color[HTML]{000000} 0.7478 & {\\cellcolor[HTML]{EFE9F3}} \\color[HTML]{000000} 0.2131 & {\\cellcolor[HTML]{D9D8EA}} \\color[HTML]{000000} 0.4184 & {\\cellcolor[HTML]{E0DEED}} \\color[HTML]{000000} 0.3518 & {\\cellcolor[HTML]{D5D5E8}} \\color[HTML]{000000} 0.4588 & {\\cellcolor[HTML]{E5E1EF}} \\color[HTML]{000000} 0.3148 \\\\\n",
            "\\bfseries pt & {\\cellcolor[HTML]{D1D2E6}} \\color[HTML]{000000} 0.4949 & {\\cellcolor[HTML]{99B8D8}} \\color[HTML]{000000} 0.8161 & {\\cellcolor[HTML]{ABBFDC}} \\color[HTML]{000000} 0.7243 & {\\cellcolor[HTML]{83AFD3}} \\color[HTML]{000000} 0.9243 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{93B5D6}} \\color[HTML]{000000} 0.8457 & {\\cellcolor[HTML]{F4EDF6}} \\color[HTML]{000000} 0.1508 & {\\cellcolor[HTML]{DDDBEC}} \\color[HTML]{000000} 0.3831 & {\\cellcolor[HTML]{E7E3F0}} \\color[HTML]{000000} 0.2955 & {\\cellcolor[HTML]{DDDBEC}} \\color[HTML]{000000} 0.3898 & {\\cellcolor[HTML]{E6E2EF}} \\color[HTML]{000000} 0.3068 \\\\\n",
            "\\bfseries ca & {\\cellcolor[HTML]{D9D8EA}} \\color[HTML]{000000} 0.4240 & {\\cellcolor[HTML]{AFC1DD}} \\color[HTML]{000000} 0.7022 & {\\cellcolor[HTML]{93B5D6}} \\color[HTML]{000000} 0.8492 & {\\cellcolor[HTML]{A7BDDB}} \\color[HTML]{000000} 0.7478 & {\\cellcolor[HTML]{93B5D6}} \\color[HTML]{000000} 0.8457 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{F0EAF4}} \\color[HTML]{000000} 0.2092 & {\\cellcolor[HTML]{DBDAEB}} \\color[HTML]{000000} 0.4019 & {\\cellcolor[HTML]{E7E3F0}} \\color[HTML]{000000} 0.2981 & {\\cellcolor[HTML]{D8D7E9}} \\color[HTML]{000000} 0.4311 & {\\cellcolor[HTML]{DAD9EA}} \\color[HTML]{000000} 0.4134 \\\\\n",
            "\\bfseries cs & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.0870 & {\\cellcolor[HTML]{EDE7F2}} \\color[HTML]{000000} 0.2435 & {\\cellcolor[HTML]{F8F1F8}} \\color[HTML]{000000} 0.0985 & {\\cellcolor[HTML]{EFE9F3}} \\color[HTML]{000000} 0.2131 & {\\cellcolor[HTML]{F4EDF6}} \\color[HTML]{000000} 0.1508 & {\\cellcolor[HTML]{F0EAF4}} \\color[HTML]{000000} 0.2092 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{DEDCEC}} \\color[HTML]{000000} 0.3783 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4723 & {\\cellcolor[HTML]{DAD9EA}} \\color[HTML]{000000} 0.4136 & {\\cellcolor[HTML]{D2D3E7}} \\color[HTML]{000000} 0.4772 \\\\\n",
            "\\bfseries ru & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.1903 & {\\cellcolor[HTML]{E0DDED}} \\color[HTML]{000000} 0.3652 & {\\cellcolor[HTML]{E0DDED}} \\color[HTML]{000000} 0.3641 & {\\cellcolor[HTML]{D9D8EA}} \\color[HTML]{000000} 0.4184 & {\\cellcolor[HTML]{DDDBEC}} \\color[HTML]{000000} 0.3831 & {\\cellcolor[HTML]{DBDAEB}} \\color[HTML]{000000} 0.4019 & {\\cellcolor[HTML]{DEDCEC}} \\color[HTML]{000000} 0.3783 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{89B1D4}} \\color[HTML]{000000} 0.8976 & {\\cellcolor[HTML]{A4BCDA}} \\color[HTML]{000000} 0.7622 & {\\cellcolor[HTML]{CED0E6}} \\color[HTML]{000000} 0.5121 \\\\\n",
            "\\bfseries uk & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.3074 & {\\cellcolor[HTML]{ECE7F2}} \\color[HTML]{000000} 0.2556 & {\\cellcolor[HTML]{EFE9F3}} \\color[HTML]{000000} 0.2168 & {\\cellcolor[HTML]{E0DEED}} \\color[HTML]{000000} 0.3518 & {\\cellcolor[HTML]{E7E3F0}} \\color[HTML]{000000} 0.2955 & {\\cellcolor[HTML]{E7E3F0}} \\color[HTML]{000000} 0.2981 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4723 & {\\cellcolor[HTML]{89B1D4}} \\color[HTML]{000000} 0.8976 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{A5BDDB}} \\color[HTML]{000000} 0.7574 & {\\cellcolor[HTML]{CCCFE5}} \\color[HTML]{000000} 0.5292 \\\\\n",
            "\\bfseries ar & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.1778 & {\\cellcolor[HTML]{E7E3F0}} \\color[HTML]{000000} 0.3026 & {\\cellcolor[HTML]{DDDBEC}} \\color[HTML]{000000} 0.3890 & {\\cellcolor[HTML]{D5D5E8}} \\color[HTML]{000000} 0.4588 & {\\cellcolor[HTML]{DDDBEC}} \\color[HTML]{000000} 0.3898 & {\\cellcolor[HTML]{D8D7E9}} \\color[HTML]{000000} 0.4311 & {\\cellcolor[HTML]{DAD9EA}} \\color[HTML]{000000} 0.4136 & {\\cellcolor[HTML]{A4BCDA}} \\color[HTML]{000000} 0.7622 & {\\cellcolor[HTML]{A5BDDB}} \\color[HTML]{000000} 0.7574 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{ACC0DD}} \\color[HTML]{000000} 0.7178 \\\\\n",
            "\\bfseries zh & {\\cellcolor[HTML]{FCF4FA}} \\color[HTML]{000000} 0.0412 & {\\cellcolor[HTML]{E7E3F0}} \\color[HTML]{000000} 0.2963 & {\\cellcolor[HTML]{CED0E6}} \\color[HTML]{000000} 0.5100 & {\\cellcolor[HTML]{E5E1EF}} \\color[HTML]{000000} 0.3148 & {\\cellcolor[HTML]{E6E2EF}} \\color[HTML]{000000} 0.3068 & {\\cellcolor[HTML]{DAD9EA}} \\color[HTML]{000000} 0.4134 & {\\cellcolor[HTML]{D2D3E7}} \\color[HTML]{000000} 0.4772 & {\\cellcolor[HTML]{CED0E6}} \\color[HTML]{000000} 0.5121 & {\\cellcolor[HTML]{CCCFE5}} \\color[HTML]{000000} 0.5292 & {\\cellcolor[HTML]{ACC0DD}} \\color[HTML]{000000} 0.7178 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_all_multilingual"
      ],
      "metadata": {
        "id": "t5Ej6nv64VuB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8e8849a5-4019-4685-83b7-63374bdf49e0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                    ar  \\\n",
              "Train Language Train LLM        Model                                    \n",
              "en             alpaca-lora-30b  bert-base-multilingual-cased  0.537542   \n",
              "                                mGPT                          0.402388   \n",
              "                                mdeberta-v3-base              0.207984   \n",
              "                                xlm-roberta-large             0.447388   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.921465   \n",
              "                                mGPT                          0.700916   \n",
              "                                mdeberta-v3-base              0.787647   \n",
              "                                xlm-roberta-large             0.721820   \n",
              "               gpt-4            bert-base-multilingual-cased  0.841926   \n",
              "                                mGPT                          0.694497   \n",
              "                                mdeberta-v3-base              0.719857   \n",
              "                                xlm-roberta-large             0.565875   \n",
              "               llama-65b        bert-base-multilingual-cased  0.589577   \n",
              "                                mGPT                          0.384803   \n",
              "                                mdeberta-v3-base              0.511518   \n",
              "                                xlm-roberta-large             0.477913   \n",
              "               opt-66b          bert-base-multilingual-cased  0.455733   \n",
              "                                mGPT                          0.295934   \n",
              "                                mdeberta-v3-base              0.390341   \n",
              "                                xlm-roberta-large             0.490988   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.459169   \n",
              "                                mGPT                          0.338008   \n",
              "                                mdeberta-v3-base              0.793226   \n",
              "                                xlm-roberta-large             0.580995   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.764651   \n",
              "                                mGPT                          0.539324   \n",
              "                                mdeberta-v3-base              0.543187   \n",
              "                                xlm-roberta-large             0.503535   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.791310   \n",
              "                                mGPT                          0.408638   \n",
              "                                mdeberta-v3-base              0.178446   \n",
              "                                xlm-roberta-large             0.386417   \n",
              "es             alpaca-lora-30b  bert-base-multilingual-cased  0.601879   \n",
              "                                mGPT                          0.697994   \n",
              "                                mdeberta-v3-base              0.825526   \n",
              "                                xlm-roberta-large             0.699932   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.889624   \n",
              "                                mGPT                          0.894145   \n",
              "                                mdeberta-v3-base              0.928117   \n",
              "                                xlm-roberta-large             0.767102   \n",
              "               gpt-4            bert-base-multilingual-cased  0.876361   \n",
              "                                mGPT                          0.849110   \n",
              "                                mdeberta-v3-base              0.933106   \n",
              "                                xlm-roberta-large             0.512410   \n",
              "               llama-65b        bert-base-multilingual-cased  0.795561   \n",
              "                                mGPT                          0.591239   \n",
              "                                mdeberta-v3-base              0.913549   \n",
              "                                xlm-roberta-large             0.846051   \n",
              "               opt-66b          bert-base-multilingual-cased  0.737808   \n",
              "                                mGPT                          0.390615   \n",
              "                                mdeberta-v3-base              0.913856   \n",
              "                                xlm-roberta-large             0.605757   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.953707   \n",
              "                                mGPT                          0.943433   \n",
              "                                mdeberta-v3-base              0.993148   \n",
              "                                xlm-roberta-large             0.979411   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.731949   \n",
              "                                mGPT                          0.718775   \n",
              "                                mdeberta-v3-base              0.643584   \n",
              "                                xlm-roberta-large             0.799302   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.862826   \n",
              "                                mGPT                          0.723426   \n",
              "                                mdeberta-v3-base              0.678396   \n",
              "                                xlm-roberta-large             0.844734   \n",
              "ru             alpaca-lora-30b  bert-base-multilingual-cased  0.602643   \n",
              "                                mGPT                          0.563543   \n",
              "                                mdeberta-v3-base              0.971599   \n",
              "                                xlm-roberta-large             0.918018   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.941563   \n",
              "                                mGPT                          0.784573   \n",
              "                                mdeberta-v3-base              0.936497   \n",
              "                                xlm-roberta-large             0.933217   \n",
              "               gpt-4            bert-base-multilingual-cased  0.866685   \n",
              "                                mGPT                          0.897834   \n",
              "                                mdeberta-v3-base              0.934891   \n",
              "                                xlm-roberta-large             0.921514   \n",
              "               llama-65b        bert-base-multilingual-cased  0.707838   \n",
              "                                mGPT                          0.534740   \n",
              "                                mdeberta-v3-base              0.762908   \n",
              "                                xlm-roberta-large             0.885839   \n",
              "               opt-66b          bert-base-multilingual-cased  0.759294   \n",
              "                                mGPT                          0.971423   \n",
              "                                mdeberta-v3-base              0.939331   \n",
              "                                xlm-roberta-large             0.954589   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.958780   \n",
              "                                mGPT                          0.963964   \n",
              "                                mdeberta-v3-base              0.890215   \n",
              "                                xlm-roberta-large             0.993146   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.748031   \n",
              "                                mGPT                          0.879773   \n",
              "                                mdeberta-v3-base              0.880012   \n",
              "                                xlm-roberta-large             0.866381   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.545024   \n",
              "                                mGPT                          0.684569   \n",
              "                                mdeberta-v3-base              0.988313   \n",
              "                                xlm-roberta-large             0.971604   \n",
              "all            alpaca-lora-30b  bert-base-multilingual-cased  0.509603   \n",
              "                                mGPT                          0.695488   \n",
              "                                mdeberta-v3-base              0.843820   \n",
              "                                xlm-roberta-large             0.774189   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.848292   \n",
              "                                mGPT                          0.885649   \n",
              "                                mdeberta-v3-base              0.946576   \n",
              "                                xlm-roberta-large             0.928008   \n",
              "               gpt-4            bert-base-multilingual-cased  0.941559   \n",
              "                                mGPT                          0.891135   \n",
              "                                mdeberta-v3-base              0.938224   \n",
              "                                xlm-roberta-large             0.959932   \n",
              "               llama-65b        bert-base-multilingual-cased  0.782089   \n",
              "                                mGPT                          0.891892   \n",
              "                                mdeberta-v3-base              0.800609   \n",
              "                                xlm-roberta-large             0.744559   \n",
              "               opt-66b          bert-base-multilingual-cased  0.796683   \n",
              "                                mGPT                          0.906852   \n",
              "                                mdeberta-v3-base              0.891294   \n",
              "                                xlm-roberta-large             0.941006   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.950318   \n",
              "                                mGPT                          0.976027   \n",
              "                                mdeberta-v3-base              0.976027   \n",
              "                                xlm-roberta-large             0.986297   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.824558   \n",
              "                                mGPT                          0.846222   \n",
              "                                mdeberta-v3-base              0.759815   \n",
              "                                xlm-roberta-large             0.670553   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.829439   \n",
              "                                mGPT                          0.846662   \n",
              "                                mdeberta-v3-base              0.854263   \n",
              "                                xlm-roberta-large             0.880938   \n",
              "en3            alpaca-lora-30b  bert-base-multilingual-cased  0.495071   \n",
              "                                mGPT                          0.293049   \n",
              "                                mdeberta-v3-base              0.176374   \n",
              "                                xlm-roberta-large             0.330592   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.944893   \n",
              "                                mGPT                          0.678622   \n",
              "                                mdeberta-v3-base              0.814844   \n",
              "                                xlm-roberta-large             0.919866   \n",
              "               gpt-4            bert-base-multilingual-cased  0.834971   \n",
              "                                mGPT                          0.848069   \n",
              "                                mdeberta-v3-base              0.819887   \n",
              "                                xlm-roberta-large             0.877966   \n",
              "               llama-65b        bert-base-multilingual-cased  0.655237   \n",
              "                                mGPT                          0.336877   \n",
              "                                mdeberta-v3-base              0.747725   \n",
              "                                xlm-roberta-large             0.453154   \n",
              "               opt-66b          bert-base-multilingual-cased  0.412144   \n",
              "                                mGPT                          0.198296   \n",
              "                                mdeberta-v3-base              0.261175   \n",
              "                                xlm-roberta-large             0.418210   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.570502   \n",
              "                                mGPT                          0.260741   \n",
              "                                mdeberta-v3-base              0.730906   \n",
              "                                xlm-roberta-large             0.669366   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.785305   \n",
              "                                mGPT                          0.613629   \n",
              "                                mdeberta-v3-base              0.579101   \n",
              "                                xlm-roberta-large             0.389863   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.767944   \n",
              "                                mGPT                          0.388148   \n",
              "                                mdeberta-v3-base              0.257780   \n",
              "                                xlm-roberta-large             0.406231   \n",
              "\n",
              "                                                                    ca  \\\n",
              "Train Language Train LLM        Model                                    \n",
              "en             alpaca-lora-30b  bert-base-multilingual-cased  0.827078   \n",
              "                                mGPT                          0.808877   \n",
              "                                mdeberta-v3-base              0.859243   \n",
              "                                xlm-roberta-large             0.848775   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.890380   \n",
              "                                mGPT                          0.926614   \n",
              "                                mdeberta-v3-base              0.559878   \n",
              "                                xlm-roberta-large             0.973319   \n",
              "               gpt-4            bert-base-multilingual-cased  0.926529   \n",
              "                                mGPT                          0.788771   \n",
              "                                mdeberta-v3-base              0.973331   \n",
              "                                xlm-roberta-large             0.934778   \n",
              "               llama-65b        bert-base-multilingual-cased  0.606979   \n",
              "                                mGPT                          0.350849   \n",
              "                                mdeberta-v3-base              0.405992   \n",
              "                                xlm-roberta-large             0.489240   \n",
              "               opt-66b          bert-base-multilingual-cased  0.458569   \n",
              "                                mGPT                          0.504866   \n",
              "                                mdeberta-v3-base              0.518131   \n",
              "                                xlm-roberta-large             0.461322   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.517135   \n",
              "                                mGPT                          0.513427   \n",
              "                                mdeberta-v3-base              0.973103   \n",
              "                                xlm-roberta-large             0.856666   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.901368   \n",
              "                                mGPT                          0.919928   \n",
              "                                mdeberta-v3-base              0.639753   \n",
              "                                xlm-roberta-large             0.921334   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.885598   \n",
              "                                mGPT                          0.788775   \n",
              "                                mdeberta-v3-base              0.500000   \n",
              "                                xlm-roberta-large             0.941659   \n",
              "es             alpaca-lora-30b  bert-base-multilingual-cased  0.894965   \n",
              "                                mGPT                          0.863938   \n",
              "                                mdeberta-v3-base              0.733757   \n",
              "                                xlm-roberta-large             0.834861   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.936548   \n",
              "                                mGPT                          0.964984   \n",
              "                                mdeberta-v3-base              0.881723   \n",
              "                                xlm-roberta-large             0.954955   \n",
              "               gpt-4            bert-base-multilingual-cased  0.885192   \n",
              "                                mGPT                          0.870498   \n",
              "                                mdeberta-v3-base              0.889724   \n",
              "                                xlm-roberta-large             0.959946   \n",
              "               llama-65b        bert-base-multilingual-cased  0.917710   \n",
              "                                mGPT                          0.689859   \n",
              "                                mdeberta-v3-base              0.926183   \n",
              "                                xlm-roberta-large             0.830698   \n",
              "               opt-66b          bert-base-multilingual-cased  0.760815   \n",
              "                                mGPT                          0.729738   \n",
              "                                mdeberta-v3-base              0.850023   \n",
              "                                xlm-roberta-large             0.745234   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.969710   \n",
              "                                mGPT                          0.961329   \n",
              "                                mdeberta-v3-base              0.986553   \n",
              "                                xlm-roberta-large             0.991594   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.904712   \n",
              "                                mGPT                          0.802038   \n",
              "                                mdeberta-v3-base              0.761469   \n",
              "                                xlm-roberta-large             0.926402   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.914947   \n",
              "                                mGPT                          0.871649   \n",
              "                                mdeberta-v3-base              0.899356   \n",
              "                                xlm-roberta-large             0.878868   \n",
              "ru             alpaca-lora-30b  bert-base-multilingual-cased  0.632650   \n",
              "                                mGPT                          0.412312   \n",
              "                                mdeberta-v3-base              0.812253   \n",
              "                                xlm-roberta-large             0.898299   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.867992   \n",
              "                                mGPT                          0.885671   \n",
              "                                mdeberta-v3-base              0.838727   \n",
              "                                xlm-roberta-large             0.943227   \n",
              "               gpt-4            bert-base-multilingual-cased  0.706686   \n",
              "                                mGPT                          0.897758   \n",
              "                                mdeberta-v3-base              0.555606   \n",
              "                                xlm-roberta-large             0.959956   \n",
              "               llama-65b        bert-base-multilingual-cased  0.530048   \n",
              "                                mGPT                          0.332589   \n",
              "                                mdeberta-v3-base              0.427667   \n",
              "                                xlm-roberta-large             0.570539   \n",
              "               opt-66b          bert-base-multilingual-cased  0.606131   \n",
              "                                mGPT                          0.512291   \n",
              "                                mdeberta-v3-base              0.742819   \n",
              "                                xlm-roberta-large             0.855203   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.542001   \n",
              "                                mGPT                          0.658299   \n",
              "                                mdeberta-v3-base              0.511446   \n",
              "                                xlm-roberta-large             0.727486   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.699821   \n",
              "                                mGPT                          0.637560   \n",
              "                                mdeberta-v3-base              0.439919   \n",
              "                                xlm-roberta-large             0.443979   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.362297   \n",
              "                                mGPT                          0.340698   \n",
              "                                mdeberta-v3-base              0.797985   \n",
              "                                xlm-roberta-large             0.752577   \n",
              "all            alpaca-lora-30b  bert-base-multilingual-cased  0.913332   \n",
              "                                mGPT                          0.883328   \n",
              "                                mdeberta-v3-base              0.806245   \n",
              "                                xlm-roberta-large             0.928309   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.974999   \n",
              "                                mGPT                          0.919799   \n",
              "                                mdeberta-v3-base              0.871004   \n",
              "                                xlm-roberta-large             0.985000   \n",
              "               gpt-4            bert-base-multilingual-cased  0.956608   \n",
              "                                mGPT                          0.944988   \n",
              "                                mdeberta-v3-base              0.931459   \n",
              "                                xlm-roberta-large             0.973326   \n",
              "               llama-65b        bert-base-multilingual-cased  0.864468   \n",
              "                                mGPT                          0.885282   \n",
              "                                mdeberta-v3-base              0.888725   \n",
              "                                xlm-roberta-large             0.801332   \n",
              "               opt-66b          bert-base-multilingual-cased  0.721942   \n",
              "                                mGPT                          0.844007   \n",
              "                                mdeberta-v3-base              0.939626   \n",
              "                                xlm-roberta-large             0.951433   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.932673   \n",
              "                                mGPT                          0.959650   \n",
              "                                mdeberta-v3-base              0.988235   \n",
              "                                xlm-roberta-large             0.996638   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.958319   \n",
              "                                mGPT                          0.684687   \n",
              "                                mdeberta-v3-base              0.853584   \n",
              "                                xlm-roberta-large             0.793827   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.953291   \n",
              "                                mGPT                          0.862044   \n",
              "                                mdeberta-v3-base              0.807752   \n",
              "                                xlm-roberta-large             0.951656   \n",
              "en3            alpaca-lora-30b  bert-base-multilingual-cased  0.874293   \n",
              "                                mGPT                          0.762298   \n",
              "                                mdeberta-v3-base              0.687197   \n",
              "                                xlm-roberta-large             0.837792   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.917785   \n",
              "                                mGPT                          0.908333   \n",
              "                                mdeberta-v3-base              0.735275   \n",
              "                                xlm-roberta-large             0.979999   \n",
              "               gpt-4            bert-base-multilingual-cased  0.900893   \n",
              "                                mGPT                          0.929888   \n",
              "                                mdeberta-v3-base              0.724894   \n",
              "                                xlm-roberta-large             0.909319   \n",
              "               llama-65b        bert-base-multilingual-cased  0.641660   \n",
              "                                mGPT                          0.361560   \n",
              "                                mdeberta-v3-base              0.664557   \n",
              "                                xlm-roberta-large             0.407351   \n",
              "               opt-66b          bert-base-multilingual-cased  0.524086   \n",
              "                                mGPT                          0.589050   \n",
              "                                mdeberta-v3-base              0.431624   \n",
              "                                xlm-roberta-large             0.408885   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.637249   \n",
              "                                mGPT                          0.566327   \n",
              "                                mdeberta-v3-base              0.861843   \n",
              "                                xlm-roberta-large             0.739327   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.906366   \n",
              "                                mGPT                          0.893237   \n",
              "                                mdeberta-v3-base              0.767995   \n",
              "                                xlm-roberta-large             0.904195   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.882074   \n",
              "                                mGPT                          0.907860   \n",
              "                                mdeberta-v3-base              0.786145   \n",
              "                                xlm-roberta-large             0.900893   \n",
              "\n",
              "                                                                    cs  \\\n",
              "Train Language Train LLM        Model                                    \n",
              "en             alpaca-lora-30b  bert-base-multilingual-cased  0.854560   \n",
              "                                mGPT                          0.613177   \n",
              "                                mdeberta-v3-base              0.769121   \n",
              "                                xlm-roberta-large             0.871277   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.914981   \n",
              "                                mGPT                          0.337026   \n",
              "                                mdeberta-v3-base              0.780318   \n",
              "                                xlm-roberta-large             0.721150   \n",
              "               gpt-4            bert-base-multilingual-cased  0.759176   \n",
              "                                mGPT                          0.386602   \n",
              "                                mdeberta-v3-base              0.862618   \n",
              "                                xlm-roberta-large             0.498656   \n",
              "               llama-65b        bert-base-multilingual-cased  0.695458   \n",
              "                                mGPT                          0.689755   \n",
              "                                mdeberta-v3-base              0.830677   \n",
              "                                xlm-roberta-large             0.850697   \n",
              "               opt-66b          bert-base-multilingual-cased  0.610699   \n",
              "                                mGPT                          0.395400   \n",
              "                                mdeberta-v3-base              0.627795   \n",
              "                                xlm-roberta-large             0.549451   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.579223   \n",
              "                                mGPT                          0.641027   \n",
              "                                mdeberta-v3-base              1.000000   \n",
              "                                xlm-roberta-large             0.865734   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.853679   \n",
              "                                mGPT                          0.362297   \n",
              "                                mdeberta-v3-base              0.823919   \n",
              "                                xlm-roberta-large             0.696479   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.801202   \n",
              "                                mGPT                          0.504333   \n",
              "                                mdeberta-v3-base              0.501832   \n",
              "                                xlm-roberta-large             0.489440   \n",
              "es             alpaca-lora-30b  bert-base-multilingual-cased  0.810758   \n",
              "                                mGPT                          0.698896   \n",
              "                                mdeberta-v3-base              0.911024   \n",
              "                                xlm-roberta-large             0.900833   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.899715   \n",
              "                                mGPT                          0.355178   \n",
              "                                mdeberta-v3-base              0.966649   \n",
              "                                xlm-roberta-large             0.860860   \n",
              "               gpt-4            bert-base-multilingual-cased  0.929950   \n",
              "                                mGPT                          0.337026   \n",
              "                                mdeberta-v3-base              0.954909   \n",
              "                                xlm-roberta-large             0.568764   \n",
              "               llama-65b        bert-base-multilingual-cased  0.964457   \n",
              "                                mGPT                          0.860581   \n",
              "                                mdeberta-v3-base              0.988152   \n",
              "                                xlm-roberta-large             0.983077   \n",
              "               opt-66b          bert-base-multilingual-cased  0.644272   \n",
              "                                mGPT                          0.455011   \n",
              "                                mdeberta-v3-base              0.988333   \n",
              "                                xlm-roberta-large             0.876505   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.963200   \n",
              "                                mGPT                          0.920996   \n",
              "                                mdeberta-v3-base              0.993310   \n",
              "                                xlm-roberta-large             0.858342   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.883332   \n",
              "                                mGPT                          0.383186   \n",
              "                                mdeberta-v3-base              0.924799   \n",
              "                                xlm-roberta-large             0.948212   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.770478   \n",
              "                                mGPT                          0.542705   \n",
              "                                mdeberta-v3-base              0.838727   \n",
              "                                xlm-roberta-large             0.668109   \n",
              "ru             alpaca-lora-30b  bert-base-multilingual-cased  0.806734   \n",
              "                                mGPT                          0.672520   \n",
              "                                mdeberta-v3-base              0.984999   \n",
              "                                xlm-roberta-large             0.969973   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.904463   \n",
              "                                mGPT                          0.355178   \n",
              "                                mdeberta-v3-base              0.981664   \n",
              "                                xlm-roberta-large             0.988333   \n",
              "               gpt-4            bert-base-multilingual-cased  0.782835   \n",
              "                                mGPT                          0.358748   \n",
              "                                mdeberta-v3-base              0.878788   \n",
              "                                xlm-roberta-large             0.995000   \n",
              "               llama-65b        bert-base-multilingual-cased  0.875015   \n",
              "                                mGPT                          0.615361   \n",
              "                                mdeberta-v3-base              0.984771   \n",
              "                                xlm-roberta-large             0.986464   \n",
              "               opt-66b          bert-base-multilingual-cased  0.767321   \n",
              "                                mGPT                          0.577647   \n",
              "                                mdeberta-v3-base              0.983329   \n",
              "                                xlm-roberta-large             0.991666   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.870710   \n",
              "                                mGPT                          0.359651   \n",
              "                                mdeberta-v3-base              0.937863   \n",
              "                                xlm-roberta-large             0.989965   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.647573   \n",
              "                                mGPT                          0.355178   \n",
              "                                mdeberta-v3-base              0.687666   \n",
              "                                xlm-roberta-large             0.742229   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.572402   \n",
              "                                mGPT                          0.865946   \n",
              "                                mdeberta-v3-base              0.938098   \n",
              "                                xlm-roberta-large             0.927963   \n",
              "all            alpaca-lora-30b  bert-base-multilingual-cased  0.887860   \n",
              "                                mGPT                          0.648040   \n",
              "                                mdeberta-v3-base              0.890609   \n",
              "                                xlm-roberta-large             0.959972   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.963332   \n",
              "                                mGPT                          0.355178   \n",
              "                                mdeberta-v3-base              0.976666   \n",
              "                                xlm-roberta-large             0.988332   \n",
              "               gpt-4            bert-base-multilingual-cased  0.953308   \n",
              "                                mGPT                          0.340698   \n",
              "                                mdeberta-v3-base              0.988333   \n",
              "                                xlm-roberta-large             0.985000   \n",
              "               llama-65b        bert-base-multilingual-cased  0.928762   \n",
              "                                mGPT                          0.950910   \n",
              "                                mdeberta-v3-base              0.983078   \n",
              "                                xlm-roberta-large             0.937253   \n",
              "               opt-66b          bert-base-multilingual-cased  0.767449   \n",
              "                                mGPT                          0.801006   \n",
              "                                mdeberta-v3-base              1.000000   \n",
              "                                xlm-roberta-large             0.996667   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.959830   \n",
              "                                mGPT                          0.924281   \n",
              "                                mdeberta-v3-base              1.000000   \n",
              "                                xlm-roberta-large             1.000000   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.934904   \n",
              "                                mGPT                          0.333333   \n",
              "                                mdeberta-v3-base              0.951569   \n",
              "                                xlm-roberta-large             0.887007   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.896251   \n",
              "                                mGPT                          0.509959   \n",
              "                                mdeberta-v3-base              0.899057   \n",
              "                                xlm-roberta-large             0.934778   \n",
              "en3            alpaca-lora-30b  bert-base-multilingual-cased  0.859961   \n",
              "                                mGPT                          0.495798   \n",
              "                                mdeberta-v3-base              0.809567   \n",
              "                                xlm-roberta-large             0.878184   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.938332   \n",
              "                                mGPT                          0.333333   \n",
              "                                mdeberta-v3-base              0.815809   \n",
              "                                xlm-roberta-large             0.974997   \n",
              "               gpt-4            bert-base-multilingual-cased  0.919999   \n",
              "                                mGPT                          0.333333   \n",
              "                                mdeberta-v3-base              0.907612   \n",
              "                                xlm-roberta-large             0.956643   \n",
              "               llama-65b        bert-base-multilingual-cased  0.797071   \n",
              "                                mGPT                          0.776111   \n",
              "                                mdeberta-v3-base              0.927212   \n",
              "                                xlm-roberta-large             0.841352   \n",
              "               opt-66b          bert-base-multilingual-cased  0.684992   \n",
              "                                mGPT                          0.570750   \n",
              "                                mdeberta-v3-base              0.531647   \n",
              "                                xlm-roberta-large             0.602140   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.708583   \n",
              "                                mGPT                          0.573060   \n",
              "                                mdeberta-v3-base              0.921165   \n",
              "                                xlm-roberta-large             0.742273   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.881461   \n",
              "                                mGPT                          0.333333   \n",
              "                                mdeberta-v3-base              0.893975   \n",
              "                                xlm-roberta-large             0.526789   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.836601   \n",
              "                                mGPT                          0.488887   \n",
              "                                mdeberta-v3-base              0.698312   \n",
              "                                xlm-roberta-large             0.764844   \n",
              "\n",
              "                                                                    de  \\\n",
              "Train Language Train LLM        Model                                    \n",
              "en             alpaca-lora-30b  bert-base-multilingual-cased  0.891714   \n",
              "                                mGPT                          0.876305   \n",
              "                                mdeberta-v3-base              0.900331   \n",
              "                                xlm-roberta-large             0.932405   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.902026   \n",
              "                                mGPT                          0.907073   \n",
              "                                mdeberta-v3-base              0.805155   \n",
              "                                xlm-roberta-large             0.939164   \n",
              "               gpt-4            bert-base-multilingual-cased  0.816308   \n",
              "                                mGPT                          0.836477   \n",
              "                                mdeberta-v3-base              0.894392   \n",
              "                                xlm-roberta-large             0.892721   \n",
              "               llama-65b        bert-base-multilingual-cased  0.689542   \n",
              "                                mGPT                          0.711503   \n",
              "                                mdeberta-v3-base              0.716159   \n",
              "                                xlm-roberta-large             0.798271   \n",
              "               opt-66b          bert-base-multilingual-cased  0.588965   \n",
              "                                mGPT                          0.635131   \n",
              "                                mdeberta-v3-base              0.656829   \n",
              "                                xlm-roberta-large             0.673485   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.591575   \n",
              "                                mGPT                          0.745042   \n",
              "                                mdeberta-v3-base              0.960929   \n",
              "                                xlm-roberta-large             0.841577   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.855428   \n",
              "                                mGPT                          0.893556   \n",
              "                                mdeberta-v3-base              0.856322   \n",
              "                                xlm-roberta-large             0.891345   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.844550   \n",
              "                                mGPT                          0.851064   \n",
              "                                mdeberta-v3-base              0.629463   \n",
              "                                xlm-roberta-large             0.908579   \n",
              "es             alpaca-lora-30b  bert-base-multilingual-cased  0.876534   \n",
              "                                mGPT                          0.900098   \n",
              "                                mdeberta-v3-base              0.917144   \n",
              "                                xlm-roberta-large             0.949315   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.890037   \n",
              "                                mGPT                          0.930710   \n",
              "                                mdeberta-v3-base              0.957743   \n",
              "                                xlm-roberta-large             0.853948   \n",
              "               gpt-4            bert-base-multilingual-cased  0.880837   \n",
              "                                mGPT                          0.918737   \n",
              "                                mdeberta-v3-base              0.935623   \n",
              "                                xlm-roberta-large             0.801054   \n",
              "               llama-65b        bert-base-multilingual-cased  0.771126   \n",
              "                                mGPT                          0.754042   \n",
              "                                mdeberta-v3-base              0.960824   \n",
              "                                xlm-roberta-large             0.955682   \n",
              "               opt-66b          bert-base-multilingual-cased  0.587242   \n",
              "                                mGPT                          0.695724   \n",
              "                                mdeberta-v3-base              0.949266   \n",
              "                                xlm-roberta-large             0.810358   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.867572   \n",
              "                                mGPT                          0.993208   \n",
              "                                mdeberta-v3-base              0.871046   \n",
              "                                xlm-roberta-large             0.988115   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.860236   \n",
              "                                mGPT                          0.927360   \n",
              "                                mdeberta-v3-base              0.923908   \n",
              "                                xlm-roberta-large             0.950973   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.834088   \n",
              "                                mGPT                          0.920602   \n",
              "                                mdeberta-v3-base              0.905250   \n",
              "                                xlm-roberta-large             0.861400   \n",
              "ru             alpaca-lora-30b  bert-base-multilingual-cased  0.740883   \n",
              "                                mGPT                          0.871620   \n",
              "                                mdeberta-v3-base              0.940874   \n",
              "                                xlm-roberta-large             0.944230   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.853816   \n",
              "                                mGPT                          0.907050   \n",
              "                                mdeberta-v3-base              0.925553   \n",
              "                                xlm-roberta-large             0.939133   \n",
              "               gpt-4            bert-base-multilingual-cased  0.753498   \n",
              "                                mGPT                          0.918762   \n",
              "                                mdeberta-v3-base              0.922123   \n",
              "                                xlm-roberta-large             0.939120   \n",
              "               llama-65b        bert-base-multilingual-cased  0.647655   \n",
              "                                mGPT                          0.647033   \n",
              "                                mdeberta-v3-base              0.854020   \n",
              "                                xlm-roberta-large             0.916035   \n",
              "               opt-66b          bert-base-multilingual-cased  0.607818   \n",
              "                                mGPT                          0.399889   \n",
              "                                mdeberta-v3-base              0.486039   \n",
              "                                xlm-roberta-large             0.554446   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.507309   \n",
              "                                mGPT                          0.561295   \n",
              "                                mdeberta-v3-base              0.576746   \n",
              "                                xlm-roberta-large             0.755102   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.808713   \n",
              "                                mGPT                          0.862949   \n",
              "                                mdeberta-v3-base              0.583750   \n",
              "                                xlm-roberta-large             0.711712   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.454921   \n",
              "                                mGPT                          0.857633   \n",
              "                                mdeberta-v3-base              0.903297   \n",
              "                                xlm-roberta-large             0.939120   \n",
              "all            alpaca-lora-30b  bert-base-multilingual-cased  0.913839   \n",
              "                                mGPT                          0.928988   \n",
              "                                mdeberta-v3-base              0.872700   \n",
              "                                xlm-roberta-large             0.957769   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.939072   \n",
              "                                mGPT                          0.896361   \n",
              "                                mdeberta-v3-base              0.957769   \n",
              "                                xlm-roberta-large             0.942473   \n",
              "               gpt-4            bert-base-multilingual-cased  0.925458   \n",
              "                                mGPT                          0.901432   \n",
              "                                mdeberta-v3-base              0.979728   \n",
              "                                xlm-roberta-large             0.969595   \n",
              "               llama-65b        bert-base-multilingual-cased  0.786738   \n",
              "                                mGPT                          0.905754   \n",
              "                                mdeberta-v3-base              0.806141   \n",
              "                                xlm-roberta-large             0.882691   \n",
              "               opt-66b          bert-base-multilingual-cased  0.728228   \n",
              "                                mGPT                          0.818552   \n",
              "                                mdeberta-v3-base              0.897798   \n",
              "                                xlm-roberta-large             0.834821   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.838827   \n",
              "                                mGPT                          0.952422   \n",
              "                                mdeberta-v3-base              0.957543   \n",
              "                                xlm-roberta-large             0.993208   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.952694   \n",
              "                                mGPT                          0.972973   \n",
              "                                mdeberta-v3-base              0.911799   \n",
              "                                xlm-roberta-large             0.935667   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.890177   \n",
              "                                mGPT                          0.903105   \n",
              "                                mdeberta-v3-base              0.920361   \n",
              "                                xlm-roberta-large             0.959448   \n",
              "en3            alpaca-lora-30b  bert-base-multilingual-cased  0.905336   \n",
              "                                mGPT                          0.885029   \n",
              "                                mdeberta-v3-base              0.852901   \n",
              "                                xlm-roberta-large             0.942514   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.925675   \n",
              "                                mGPT                          0.904880   \n",
              "                                mdeberta-v3-base              0.878317   \n",
              "                                xlm-roberta-large             0.940817   \n",
              "               gpt-4            bert-base-multilingual-cased  0.930727   \n",
              "                                mGPT                          0.910430   \n",
              "                                mdeberta-v3-base              0.867904   \n",
              "                                xlm-roberta-large             0.964522   \n",
              "               llama-65b        bert-base-multilingual-cased  0.701859   \n",
              "                                mGPT                          0.715696   \n",
              "                                mdeberta-v3-base              0.756037   \n",
              "                                xlm-roberta-large             0.719821   \n",
              "               opt-66b          bert-base-multilingual-cased  0.678879   \n",
              "                                mGPT                          0.656421   \n",
              "                                mdeberta-v3-base              0.625698   \n",
              "                                xlm-roberta-large             0.675034   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.700335   \n",
              "                                mGPT                          0.636859   \n",
              "                                mdeberta-v3-base              0.815227   \n",
              "                                xlm-roberta-large             0.812958   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.895068   \n",
              "                                mGPT                          0.870103   \n",
              "                                mdeberta-v3-base              0.889506   \n",
              "                                xlm-roberta-large             0.901578   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.840988   \n",
              "                                mGPT                          0.825447   \n",
              "                                mdeberta-v3-base              0.822525   \n",
              "                                xlm-roberta-large             0.932370   \n",
              "\n",
              "                                                                    en  \\\n",
              "Train Language Train LLM        Model                                    \n",
              "en             alpaca-lora-30b  bert-base-multilingual-cased  0.956665   \n",
              "                                mGPT                          0.963891   \n",
              "                                mdeberta-v3-base              0.943947   \n",
              "                                xlm-roberta-large             0.980144   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.978339   \n",
              "                                mGPT                          0.989169   \n",
              "                                mdeberta-v3-base              0.717787   \n",
              "                                xlm-roberta-large             0.983754   \n",
              "               gpt-4            bert-base-multilingual-cased  0.976534   \n",
              "                                mGPT                          0.990975   \n",
              "                                mdeberta-v3-base              0.825513   \n",
              "                                xlm-roberta-large             0.989170   \n",
              "               llama-65b        bert-base-multilingual-cased  0.901658   \n",
              "                                mGPT                          0.903682   \n",
              "                                mdeberta-v3-base              0.900289   \n",
              "                                xlm-roberta-large             0.916509   \n",
              "               opt-66b          bert-base-multilingual-cased  0.859176   \n",
              "                                mGPT                          0.893075   \n",
              "                                mdeberta-v3-base              0.853764   \n",
              "                                xlm-roberta-large             0.882653   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.906132   \n",
              "                                mGPT                          0.958480   \n",
              "                                mdeberta-v3-base              0.925661   \n",
              "                                xlm-roberta-large             0.943910   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.969314   \n",
              "                                mGPT                          0.978335   \n",
              "                                mdeberta-v3-base              0.951251   \n",
              "                                xlm-roberta-large             0.985559   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.974726   \n",
              "                                mGPT                          0.976534   \n",
              "                                mdeberta-v3-base              0.777510   \n",
              "                                xlm-roberta-large             0.980144   \n",
              "es             alpaca-lora-30b  bert-base-multilingual-cased  0.751693   \n",
              "                                mGPT                          0.658906   \n",
              "                                mdeberta-v3-base              0.877062   \n",
              "                                xlm-roberta-large             0.929459   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.916880   \n",
              "                                mGPT                          0.839701   \n",
              "                                mdeberta-v3-base              0.936777   \n",
              "                                xlm-roberta-large             0.980144   \n",
              "               gpt-4            bert-base-multilingual-cased  0.730575   \n",
              "                                mGPT                          0.947633   \n",
              "                                mdeberta-v3-base              0.882561   \n",
              "                                xlm-roberta-large             0.972914   \n",
              "               llama-65b        bert-base-multilingual-cased  0.361111   \n",
              "                                mGPT                          0.350532   \n",
              "                                mdeberta-v3-base              0.625957   \n",
              "                                xlm-roberta-large             0.668031   \n",
              "               opt-66b          bert-base-multilingual-cased  0.470484   \n",
              "                                mGPT                          0.372230   \n",
              "                                mdeberta-v3-base              0.624038   \n",
              "                                xlm-roberta-large             0.444280   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.683951   \n",
              "                                mGPT                          0.404483   \n",
              "                                mdeberta-v3-base              0.586456   \n",
              "                                xlm-roberta-large             0.619244   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.760513   \n",
              "                                mGPT                          0.903856   \n",
              "                                mdeberta-v3-base              0.940433   \n",
              "                                xlm-roberta-large             0.942226   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.750260   \n",
              "                                mGPT                          0.673940   \n",
              "                                mdeberta-v3-base              0.916859   \n",
              "                                xlm-roberta-large             0.906107   \n",
              "ru             alpaca-lora-30b  bert-base-multilingual-cased  0.463444   \n",
              "                                mGPT                          0.527200   \n",
              "                                mdeberta-v3-base              0.854983   \n",
              "                                xlm-roberta-large             0.916940   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.781013   \n",
              "                                mGPT                          0.881669   \n",
              "                                mdeberta-v3-base              0.960288   \n",
              "                                xlm-roberta-large             0.967505   \n",
              "               gpt-4            bert-base-multilingual-cased  0.495834   \n",
              "                                mGPT                          0.871267   \n",
              "                                mdeberta-v3-base              0.925886   \n",
              "                                xlm-roberta-large             0.976532   \n",
              "               llama-65b        bert-base-multilingual-cased  0.332527   \n",
              "                                mGPT                          0.332527   \n",
              "                                mdeberta-v3-base              0.382439   \n",
              "                                xlm-roberta-large             0.370507   \n",
              "               opt-66b          bert-base-multilingual-cased  0.471954   \n",
              "                                mGPT                          0.347268   \n",
              "                                mdeberta-v3-base              0.341306   \n",
              "                                xlm-roberta-large             0.345255   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.333333   \n",
              "                                mGPT                          0.374969   \n",
              "                                mdeberta-v3-base              0.333333   \n",
              "                                xlm-roberta-large             0.337332   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.724933   \n",
              "                                mGPT                          0.683093   \n",
              "                                mdeberta-v3-base              0.345255   \n",
              "                                xlm-roberta-large             0.377834   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.333333   \n",
              "                                mGPT                          0.372237   \n",
              "                                mdeberta-v3-base              0.813637   \n",
              "                                xlm-roberta-large             0.857196   \n",
              "all            alpaca-lora-30b  bert-base-multilingual-cased  0.965698   \n",
              "                                mGPT                          0.958483   \n",
              "                                mdeberta-v3-base              0.907484   \n",
              "                                xlm-roberta-large             0.985559   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.981949   \n",
              "                                mGPT                          0.980143   \n",
              "                                mdeberta-v3-base              0.974728   \n",
              "                                xlm-roberta-large             0.990974   \n",
              "               gpt-4            bert-base-multilingual-cased  0.992780   \n",
              "                                mGPT                          0.992779   \n",
              "                                mdeberta-v3-base              0.945747   \n",
              "                                xlm-roberta-large             0.994585   \n",
              "               llama-65b        bert-base-multilingual-cased  0.907606   \n",
              "                                mGPT                          0.929281   \n",
              "                                mdeberta-v3-base              0.900354   \n",
              "                                xlm-roberta-large             0.898359   \n",
              "               opt-66b          bert-base-multilingual-cased  0.871580   \n",
              "                                mGPT                          0.904262   \n",
              "                                mdeberta-v3-base              0.847868   \n",
              "                                xlm-roberta-large             0.887941   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.920568   \n",
              "                                mGPT                          0.956678   \n",
              "                                mdeberta-v3-base              0.920511   \n",
              "                                xlm-roberta-large             0.905427   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.963876   \n",
              "                                mGPT                          0.980143   \n",
              "                                mdeberta-v3-base              0.896429   \n",
              "                                xlm-roberta-large             0.969302   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.960288   \n",
              "                                mGPT                          0.980139   \n",
              "                                mdeberta-v3-base              0.890658   \n",
              "                                xlm-roberta-large             0.981949   \n",
              "en3            alpaca-lora-30b  bert-base-multilingual-cased  0.983754   \n",
              "                                mGPT                          0.980143   \n",
              "                                mdeberta-v3-base              0.938548   \n",
              "                                xlm-roberta-large             0.987364   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.981949   \n",
              "                                mGPT                          0.987364   \n",
              "                                mdeberta-v3-base              0.925760   \n",
              "                                xlm-roberta-large             0.983752   \n",
              "               gpt-4            bert-base-multilingual-cased  0.996390   \n",
              "                                mGPT                          0.992780   \n",
              "                                mdeberta-v3-base              0.629051   \n",
              "                                xlm-roberta-large             0.994585   \n",
              "               llama-65b        bert-base-multilingual-cased  0.921855   \n",
              "                                mGPT                          0.914707   \n",
              "                                mdeberta-v3-base              0.895919   \n",
              "                                xlm-roberta-large             0.938366   \n",
              "               opt-66b          bert-base-multilingual-cased  0.907918   \n",
              "                                mGPT                          0.918517   \n",
              "                                mdeberta-v3-base              0.771954   \n",
              "                                xlm-roberta-large             0.920452   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.954856   \n",
              "                                mGPT                          0.972924   \n",
              "                                mdeberta-v3-base              0.972922   \n",
              "                                xlm-roberta-large             0.971110   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.969313   \n",
              "                                mGPT                          0.989169   \n",
              "                                mdeberta-v3-base              0.907435   \n",
              "                                xlm-roberta-large             0.985559   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.987365   \n",
              "                                mGPT                          0.990975   \n",
              "                                mdeberta-v3-base              0.792751   \n",
              "                                xlm-roberta-large             0.989170   \n",
              "\n",
              "                                                                    es  \\\n",
              "Train Language Train LLM        Model                                    \n",
              "en             alpaca-lora-30b  bert-base-multilingual-cased  0.751676   \n",
              "                                mGPT                          0.738976   \n",
              "                                mdeberta-v3-base              0.774384   \n",
              "                                xlm-roberta-large             0.631892   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.854525   \n",
              "                                mGPT                          0.850593   \n",
              "                                mdeberta-v3-base              0.602325   \n",
              "                                xlm-roberta-large             0.786081   \n",
              "               gpt-4            bert-base-multilingual-cased  0.826112   \n",
              "                                mGPT                          0.795370   \n",
              "                                mdeberta-v3-base              0.943480   \n",
              "                                xlm-roberta-large             0.859221   \n",
              "               llama-65b        bert-base-multilingual-cased  0.799832   \n",
              "                                mGPT                          0.524759   \n",
              "                                mdeberta-v3-base              0.607432   \n",
              "                                xlm-roberta-large             0.724079   \n",
              "               opt-66b          bert-base-multilingual-cased  0.539436   \n",
              "                                mGPT                          0.506420   \n",
              "                                mdeberta-v3-base              0.605320   \n",
              "                                xlm-roberta-large             0.561258   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.514289   \n",
              "                                mGPT                          0.528383   \n",
              "                                mdeberta-v3-base              0.902335   \n",
              "                                xlm-roberta-large             0.726786   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.774992   \n",
              "                                mGPT                          0.767779   \n",
              "                                mdeberta-v3-base              0.577358   \n",
              "                                xlm-roberta-large             0.771487   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.785887   \n",
              "                                mGPT                          0.663255   \n",
              "                                mdeberta-v3-base              0.475434   \n",
              "                                xlm-roberta-large             0.686643   \n",
              "es             alpaca-lora-30b  bert-base-multilingual-cased  0.909150   \n",
              "                                mGPT                          0.945195   \n",
              "                                mdeberta-v3-base              0.825619   \n",
              "                                xlm-roberta-large             0.928069   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.926344   \n",
              "                                mGPT                          0.969155   \n",
              "                                mdeberta-v3-base              0.957037   \n",
              "                                xlm-roberta-large             0.974282   \n",
              "               gpt-4            bert-base-multilingual-cased  0.927672   \n",
              "                                mGPT                          0.982867   \n",
              "                                mdeberta-v3-base              0.919452   \n",
              "                                xlm-roberta-large             0.981155   \n",
              "               llama-65b        bert-base-multilingual-cased  0.991388   \n",
              "                                mGPT                          0.967288   \n",
              "                                mdeberta-v3-base              0.986219   \n",
              "                                xlm-roberta-large             0.987945   \n",
              "               opt-66b          bert-base-multilingual-cased  0.765752   \n",
              "                                mGPT                          0.799220   \n",
              "                                mdeberta-v3-base              0.876242   \n",
              "                                xlm-roberta-large             0.812310   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.948185   \n",
              "                                mGPT                          0.987905   \n",
              "                                mdeberta-v3-base              0.982728   \n",
              "                                xlm-roberta-large             0.981002   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.931246   \n",
              "                                mGPT                          0.965721   \n",
              "                                mdeberta-v3-base              0.805888   \n",
              "                                xlm-roberta-large             0.977716   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.928081   \n",
              "                                mGPT                          0.967438   \n",
              "                                mdeberta-v3-base              0.951998   \n",
              "                                xlm-roberta-large             0.943445   \n",
              "ru             alpaca-lora-30b  bert-base-multilingual-cased  0.657224   \n",
              "                                mGPT                          0.771379   \n",
              "                                mdeberta-v3-base              0.888526   \n",
              "                                xlm-roberta-large             0.905788   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.875397   \n",
              "                                mGPT                          0.922780   \n",
              "                                mdeberta-v3-base              0.929596   \n",
              "                                xlm-roberta-large             0.943465   \n",
              "               gpt-4            bert-base-multilingual-cased  0.680546   \n",
              "                                mGPT                          0.938269   \n",
              "                                mdeberta-v3-base              0.761128   \n",
              "                                xlm-roberta-large             0.934855   \n",
              "               llama-65b        bert-base-multilingual-cased  0.821116   \n",
              "                                mGPT                          0.496365   \n",
              "                                mdeberta-v3-base              0.730500   \n",
              "                                xlm-roberta-large             0.502410   \n",
              "               opt-66b          bert-base-multilingual-cased  0.642337   \n",
              "                                mGPT                          0.426150   \n",
              "                                mdeberta-v3-base              0.339394   \n",
              "                                xlm-roberta-large             0.547415   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.441679   \n",
              "                                mGPT                          0.387302   \n",
              "                                mdeberta-v3-base              0.392633   \n",
              "                                xlm-roberta-large             0.394444   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.816685   \n",
              "                                mGPT                          0.869563   \n",
              "                                mdeberta-v3-base              0.518355   \n",
              "                                xlm-roberta-large             0.567945   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.475122   \n",
              "                                mGPT                          0.618774   \n",
              "                                mdeberta-v3-base              0.902333   \n",
              "                                xlm-roberta-large             0.929760   \n",
              "all            alpaca-lora-30b  bert-base-multilingual-cased  0.940064   \n",
              "                                mGPT                          0.931506   \n",
              "                                mdeberta-v3-base              0.864293   \n",
              "                                xlm-roberta-large             0.958900   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.977734   \n",
              "                                mGPT                          0.946918   \n",
              "                                mdeberta-v3-base              0.928041   \n",
              "                                xlm-roberta-large             0.984581   \n",
              "               gpt-4            bert-base-multilingual-cased  0.948512   \n",
              "                                mGPT                          0.964017   \n",
              "                                mdeberta-v3-base              0.950183   \n",
              "                                xlm-roberta-large             0.969142   \n",
              "               llama-65b        bert-base-multilingual-cased  0.987940   \n",
              "                                mGPT                          0.986227   \n",
              "                                mdeberta-v3-base              0.982770   \n",
              "                                xlm-roberta-large             0.991388   \n",
              "               opt-66b          bert-base-multilingual-cased  0.809685   \n",
              "                                mGPT                          0.862601   \n",
              "                                mdeberta-v3-base              0.884706   \n",
              "                                xlm-roberta-large             0.869471   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.921907   \n",
              "                                mGPT                          0.979273   \n",
              "                                mdeberta-v3-base              0.967184   \n",
              "                                xlm-roberta-large             0.968910   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.963975   \n",
              "                                mGPT                          0.974306   \n",
              "                                mdeberta-v3-base              0.844561   \n",
              "                                xlm-roberta-large             0.929794   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.948608   \n",
              "                                mGPT                          0.939827   \n",
              "                                mdeberta-v3-base              0.841004   \n",
              "                                xlm-roberta-large             0.970876   \n",
              "en3            alpaca-lora-30b  bert-base-multilingual-cased  0.761689   \n",
              "                                mGPT                          0.736009   \n",
              "                                mdeberta-v3-base              0.635581   \n",
              "                                xlm-roberta-large             0.698462   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.871843   \n",
              "                                mGPT                          0.887388   \n",
              "                                mdeberta-v3-base              0.702010   \n",
              "                                xlm-roberta-large             0.919230   \n",
              "               gpt-4            bert-base-multilingual-cased  0.732342   \n",
              "                                mGPT                          0.795138   \n",
              "                                mdeberta-v3-base              0.650597   \n",
              "                                xlm-roberta-large             0.828572   \n",
              "               llama-65b        bert-base-multilingual-cased  0.805686   \n",
              "                                mGPT                          0.551698   \n",
              "                                mdeberta-v3-base              0.866191   \n",
              "                                xlm-roberta-large             0.639744   \n",
              "               opt-66b          bert-base-multilingual-cased  0.555801   \n",
              "                                mGPT                          0.560484   \n",
              "                                mdeberta-v3-base              0.471266   \n",
              "                                xlm-roberta-large             0.587124   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.632731   \n",
              "                                mGPT                          0.588083   \n",
              "                                mdeberta-v3-base              0.772649   \n",
              "                                xlm-roberta-large             0.755262   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.798652   \n",
              "                                mGPT                          0.848392   \n",
              "                                mdeberta-v3-base              0.640090   \n",
              "                                xlm-roberta-large             0.832714   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.786081   \n",
              "                                mGPT                          0.696255   \n",
              "                                mdeberta-v3-base              0.614244   \n",
              "                                xlm-roberta-large             0.676076   \n",
              "\n",
              "                                                                    nl  \\\n",
              "Train Language Train LLM        Model                                    \n",
              "en             alpaca-lora-30b  bert-base-multilingual-cased  0.856264   \n",
              "                                mGPT                          0.879058   \n",
              "                                mdeberta-v3-base              0.897682   \n",
              "                                xlm-roberta-large             0.770589   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.934819   \n",
              "                                mGPT                          0.943226   \n",
              "                                mdeberta-v3-base              0.779041   \n",
              "                                xlm-roberta-large             0.922878   \n",
              "               gpt-4            bert-base-multilingual-cased  0.926544   \n",
              "                                mGPT                          0.833370   \n",
              "                                mdeberta-v3-base              0.933106   \n",
              "                                xlm-roberta-large             0.949913   \n",
              "               llama-65b        bert-base-multilingual-cased  0.577084   \n",
              "                                mGPT                          0.410240   \n",
              "                                mdeberta-v3-base              0.459106   \n",
              "                                xlm-roberta-large             0.436761   \n",
              "               opt-66b          bert-base-multilingual-cased  0.426829   \n",
              "                                mGPT                          0.494248   \n",
              "                                mdeberta-v3-base              0.492703   \n",
              "                                xlm-roberta-large             0.418667   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.513633   \n",
              "                                mGPT                          0.695527   \n",
              "                                mdeberta-v3-base              0.961500   \n",
              "                                xlm-roberta-large             0.768422   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.903165   \n",
              "                                mGPT                          0.902670   \n",
              "                                mdeberta-v3-base              0.759612   \n",
              "                                xlm-roberta-large             0.897266   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.936540   \n",
              "                                mGPT                          0.887594   \n",
              "                                mdeberta-v3-base              0.619061   \n",
              "                                xlm-roberta-large             0.840000   \n",
              "es             alpaca-lora-30b  bert-base-multilingual-cased  0.894101   \n",
              "                                mGPT                          0.708766   \n",
              "                                mdeberta-v3-base              0.803750   \n",
              "                                xlm-roberta-large             0.953177   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.923169   \n",
              "                                mGPT                          0.897390   \n",
              "                                mdeberta-v3-base              0.910947   \n",
              "                                xlm-roberta-large             0.961602   \n",
              "               gpt-4            bert-base-multilingual-cased  0.907483   \n",
              "                                mGPT                          0.924467   \n",
              "                                mdeberta-v3-base              0.933140   \n",
              "                                xlm-roberta-large             0.961602   \n",
              "               llama-65b        bert-base-multilingual-cased  0.666485   \n",
              "                                mGPT                          0.360353   \n",
              "                                mdeberta-v3-base              0.659130   \n",
              "                                xlm-roberta-large             0.458293   \n",
              "               opt-66b          bert-base-multilingual-cased  0.703752   \n",
              "                                mGPT                          0.386565   \n",
              "                                mdeberta-v3-base              0.853753   \n",
              "                                xlm-roberta-large             0.470239   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.969888   \n",
              "                                mGPT                          0.840959   \n",
              "                                mdeberta-v3-base              0.932942   \n",
              "                                xlm-roberta-large             0.979933   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.889246   \n",
              "                                mGPT                          0.938161   \n",
              "                                mdeberta-v3-base              0.906504   \n",
              "                                xlm-roberta-large             0.973283   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.899598   \n",
              "                                mGPT                          0.688021   \n",
              "                                mdeberta-v3-base              0.875332   \n",
              "                                xlm-roberta-large             0.826702   \n",
              "ru             alpaca-lora-30b  bert-base-multilingual-cased  0.670451   \n",
              "                                mGPT                          0.532546   \n",
              "                                mdeberta-v3-base              0.886782   \n",
              "                                xlm-roberta-large             0.943052   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.878277   \n",
              "                                mGPT                          0.922999   \n",
              "                                mdeberta-v3-base              0.914644   \n",
              "                                xlm-roberta-large             0.946552   \n",
              "               gpt-4            bert-base-multilingual-cased  0.751414   \n",
              "                                mGPT                          0.924571   \n",
              "                                mdeberta-v3-base              0.760885   \n",
              "                                xlm-roberta-large             0.963270   \n",
              "               llama-65b        bert-base-multilingual-cased  0.519080   \n",
              "                                mGPT                          0.340351   \n",
              "                                mdeberta-v3-base              0.414468   \n",
              "                                xlm-roberta-large             0.367375   \n",
              "               opt-66b          bert-base-multilingual-cased  0.642806   \n",
              "                                mGPT                          0.604990   \n",
              "                                mdeberta-v3-base              0.625486   \n",
              "                                xlm-roberta-large             0.773418   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.537946   \n",
              "                                mGPT                          0.837362   \n",
              "                                mdeberta-v3-base              0.622961   \n",
              "                                xlm-roberta-large             0.863783   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.715688   \n",
              "                                mGPT                          0.798742   \n",
              "                                mdeberta-v3-base              0.484494   \n",
              "                                xlm-roberta-large             0.589670   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.354810   \n",
              "                                mGPT                          0.366299   \n",
              "                                mdeberta-v3-base              0.875988   \n",
              "                                xlm-roberta-large             0.697895   \n",
              "all            alpaca-lora-30b  bert-base-multilingual-cased  0.916148   \n",
              "                                mGPT                          0.821486   \n",
              "                                mdeberta-v3-base              0.804425   \n",
              "                                xlm-roberta-large             0.948084   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.981635   \n",
              "                                mGPT                          0.956594   \n",
              "                                mdeberta-v3-base              0.897707   \n",
              "                                xlm-roberta-large             0.979967   \n",
              "               gpt-4            bert-base-multilingual-cased  0.968277   \n",
              "                                mGPT                          0.924571   \n",
              "                                mdeberta-v3-base              0.958217   \n",
              "                                xlm-roberta-large             0.981636   \n",
              "               llama-65b        bert-base-multilingual-cased  0.783536   \n",
              "                                mGPT                          0.719234   \n",
              "                                mdeberta-v3-base              0.645039   \n",
              "                                xlm-roberta-large             0.467301   \n",
              "               opt-66b          bert-base-multilingual-cased  0.688022   \n",
              "                                mGPT                          0.759110   \n",
              "                                mdeberta-v3-base              0.876344   \n",
              "                                xlm-roberta-large             0.895853   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.900615   \n",
              "                                mGPT                          0.956452   \n",
              "                                mdeberta-v3-base              0.973225   \n",
              "                                xlm-roberta-large             0.991639   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.961587   \n",
              "                                mGPT                          0.959930   \n",
              "                                mdeberta-v3-base              0.875486   \n",
              "                                xlm-roberta-large             0.964938   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.959933   \n",
              "                                mGPT                          0.890662   \n",
              "                                mdeberta-v3-base              0.859559   \n",
              "                                xlm-roberta-large             0.917866   \n",
              "en3            alpaca-lora-30b  bert-base-multilingual-cased  0.907913   \n",
              "                                mGPT                          0.859440   \n",
              "                                mdeberta-v3-base              0.788842   \n",
              "                                xlm-roberta-large             0.900725   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.949876   \n",
              "                                mGPT                          0.963271   \n",
              "                                mdeberta-v3-base              0.826702   \n",
              "                                xlm-roberta-large             0.954912   \n",
              "               gpt-4            bert-base-multilingual-cased  0.938175   \n",
              "                                mGPT                          0.931403   \n",
              "                                mdeberta-v3-base              0.802330   \n",
              "                                xlm-roberta-large             0.924821   \n",
              "               llama-65b        bert-base-multilingual-cased  0.637019   \n",
              "                                mGPT                          0.459106   \n",
              "                                mdeberta-v3-base              0.682070   \n",
              "                                xlm-roberta-large             0.407951   \n",
              "               opt-66b          bert-base-multilingual-cased  0.527395   \n",
              "                                mGPT                          0.582609   \n",
              "                                mdeberta-v3-base              0.418367   \n",
              "                                xlm-roberta-large             0.420474   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.670498   \n",
              "                                mGPT                          0.590092   \n",
              "                                mdeberta-v3-base              0.855827   \n",
              "                                xlm-roberta-large             0.619457   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.911471   \n",
              "                                mGPT                          0.938219   \n",
              "                                mdeberta-v3-base              0.874799   \n",
              "                                xlm-roberta-large             0.948226   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.938175   \n",
              "                                mGPT                          0.916054   \n",
              "                                mdeberta-v3-base              0.880938   \n",
              "                                xlm-roberta-large             0.861264   \n",
              "\n",
              "                                                                    pt  \\\n",
              "Train Language Train LLM        Model                                    \n",
              "en             alpaca-lora-30b  bert-base-multilingual-cased  0.805481   \n",
              "                                mGPT                          0.833282   \n",
              "                                mdeberta-v3-base              0.857008   \n",
              "                                xlm-roberta-large             0.712993   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.912430   \n",
              "                                mGPT                          0.914732   \n",
              "                                mdeberta-v3-base              0.678157   \n",
              "                                xlm-roberta-large             0.928099   \n",
              "               gpt-4            bert-base-multilingual-cased  0.870419   \n",
              "                                mGPT                          0.865765   \n",
              "                                mdeberta-v3-base              0.931852   \n",
              "                                xlm-roberta-large             0.926661   \n",
              "               llama-65b        bert-base-multilingual-cased  0.675244   \n",
              "                                mGPT                          0.483544   \n",
              "                                mdeberta-v3-base              0.541285   \n",
              "                                xlm-roberta-large             0.608223   \n",
              "               opt-66b          bert-base-multilingual-cased  0.563455   \n",
              "                                mGPT                          0.601702   \n",
              "                                mdeberta-v3-base              0.643090   \n",
              "                                xlm-roberta-large             0.589688   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.472087   \n",
              "                                mGPT                          0.521178   \n",
              "                                mdeberta-v3-base              0.902473   \n",
              "                                xlm-roberta-large             0.699108   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.840752   \n",
              "                                mGPT                          0.838866   \n",
              "                                mdeberta-v3-base              0.633527   \n",
              "                                xlm-roberta-large             0.823210   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.851997   \n",
              "                                mGPT                          0.807120   \n",
              "                                mdeberta-v3-base              0.507817   \n",
              "                                xlm-roberta-large             0.716308   \n",
              "es             alpaca-lora-30b  bert-base-multilingual-cased  0.894326   \n",
              "                                mGPT                          0.940350   \n",
              "                                mdeberta-v3-base              0.777745   \n",
              "                                xlm-roberta-large             0.930124   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.885839   \n",
              "                                mGPT                          0.955613   \n",
              "                                mdeberta-v3-base              0.908940   \n",
              "                                xlm-roberta-large             0.967608   \n",
              "               gpt-4            bert-base-multilingual-cased  0.867887   \n",
              "                                mGPT                          0.982956   \n",
              "                                mdeberta-v3-base              0.902705   \n",
              "                                xlm-roberta-large             0.967626   \n",
              "               llama-65b        bert-base-multilingual-cased  0.979375   \n",
              "                                mGPT                          0.950147   \n",
              "                                mdeberta-v3-base              0.977658   \n",
              "                                xlm-roberta-large             0.972497   \n",
              "               opt-66b          bert-base-multilingual-cased  0.766234   \n",
              "                                mGPT                          0.594937   \n",
              "                                mdeberta-v3-base              0.787190   \n",
              "                                xlm-roberta-large             0.633545   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.882016   \n",
              "                                mGPT                          0.981028   \n",
              "                                mdeberta-v3-base              0.982758   \n",
              "                                xlm-roberta-large             0.979310   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.863110   \n",
              "                                mGPT                          0.916500   \n",
              "                                mdeberta-v3-base              0.848353   \n",
              "                                xlm-roberta-large             0.935165   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.882343   \n",
              "                                mGPT                          0.916407   \n",
              "                                mdeberta-v3-base              0.895597   \n",
              "                                xlm-roberta-large             0.895936   \n",
              "ru             alpaca-lora-30b  bert-base-multilingual-cased  0.656725   \n",
              "                                mGPT                          0.791988   \n",
              "                                mdeberta-v3-base              0.819913   \n",
              "                                xlm-roberta-large             0.899260   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.837753   \n",
              "                                mGPT                          0.904259   \n",
              "                                mdeberta-v3-base              0.902296   \n",
              "                                xlm-roberta-large             0.914272   \n",
              "               gpt-4            bert-base-multilingual-cased  0.691832   \n",
              "                                mGPT                          0.926369   \n",
              "                                mdeberta-v3-base              0.693175   \n",
              "                                xlm-roberta-large             0.936803   \n",
              "               llama-65b        bert-base-multilingual-cased  0.604306   \n",
              "                                mGPT                          0.484719   \n",
              "                                mdeberta-v3-base              0.538322   \n",
              "                                xlm-roberta-large             0.582594   \n",
              "               opt-66b          bert-base-multilingual-cased  0.673385   \n",
              "                                mGPT                          0.416472   \n",
              "                                mdeberta-v3-base              0.361007   \n",
              "                                xlm-roberta-large             0.616838   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.415349   \n",
              "                                mGPT                          0.437818   \n",
              "                                mdeberta-v3-base              0.398365   \n",
              "                                xlm-roberta-large             0.443158   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.764468   \n",
              "                                mGPT                          0.799616   \n",
              "                                mdeberta-v3-base              0.513155   \n",
              "                                xlm-roberta-large             0.569955   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.372091   \n",
              "                                mGPT                          0.549896   \n",
              "                                mdeberta-v3-base              0.846442   \n",
              "                                xlm-roberta-large             0.841560   \n",
              "all            alpaca-lora-30b  bert-base-multilingual-cased  0.911300   \n",
              "                                mGPT                          0.933449   \n",
              "                                mdeberta-v3-base              0.836376   \n",
              "                                xlm-roberta-large             0.952183   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.950596   \n",
              "                                mGPT                          0.936968   \n",
              "                                mdeberta-v3-base              0.903930   \n",
              "                                xlm-roberta-large             0.972736   \n",
              "               gpt-4            bert-base-multilingual-cased  0.945291   \n",
              "                                mGPT                          0.948731   \n",
              "                                mdeberta-v3-base              0.919479   \n",
              "                                xlm-roberta-large             0.982950   \n",
              "               llama-65b        bert-base-multilingual-cased  0.944907   \n",
              "                                mGPT                          0.969059   \n",
              "                                mdeberta-v3-base              0.953522   \n",
              "                                xlm-roberta-large             0.944806   \n",
              "               opt-66b          bert-base-multilingual-cased  0.825600   \n",
              "                                mGPT                          0.801186   \n",
              "                                mdeberta-v3-base              0.864558   \n",
              "                                xlm-roberta-large             0.921224   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.922375   \n",
              "                                mGPT                          0.970687   \n",
              "                                mdeberta-v3-base              0.981028   \n",
              "                                xlm-roberta-large             0.970689   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.935083   \n",
              "                                mGPT                          0.943779   \n",
              "                                mdeberta-v3-base              0.889139   \n",
              "                                xlm-roberta-large             0.906194   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.942030   \n",
              "                                mGPT                          0.935210   \n",
              "                                mdeberta-v3-base              0.872052   \n",
              "                                xlm-roberta-large             0.921323   \n",
              "en3            alpaca-lora-30b  bert-base-multilingual-cased  0.857861   \n",
              "                                mGPT                          0.865471   \n",
              "                                mdeberta-v3-base              0.734008   \n",
              "                                xlm-roberta-large             0.787154   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.929827   \n",
              "                                mGPT                          0.928358   \n",
              "                                mdeberta-v3-base              0.783397   \n",
              "                                xlm-roberta-large             0.938500   \n",
              "               gpt-4            bert-base-multilingual-cased  0.838871   \n",
              "                                mGPT                          0.907192   \n",
              "                                mdeberta-v3-base              0.706365   \n",
              "                                xlm-roberta-large             0.880757   \n",
              "               llama-65b        bert-base-multilingual-cased  0.687980   \n",
              "                                mGPT                          0.561643   \n",
              "                                mdeberta-v3-base              0.717597   \n",
              "                                xlm-roberta-large             0.553549   \n",
              "               opt-66b          bert-base-multilingual-cased  0.637663   \n",
              "                                mGPT                          0.644910   \n",
              "                                mdeberta-v3-base              0.389667   \n",
              "                                xlm-roberta-large             0.593539   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.644652   \n",
              "                                mGPT                          0.495213   \n",
              "                                mdeberta-v3-base              0.739090   \n",
              "                                xlm-roberta-large             0.693278   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.863244   \n",
              "                                mGPT                          0.907865   \n",
              "                                mdeberta-v3-base              0.772539   \n",
              "                                xlm-roberta-large             0.887163   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.821761   \n",
              "                                mGPT                          0.804291   \n",
              "                                mdeberta-v3-base              0.706436   \n",
              "                                xlm-roberta-large             0.744415   \n",
              "\n",
              "                                                                    ru  \\\n",
              "Train Language Train LLM        Model                                    \n",
              "en             alpaca-lora-30b  bert-base-multilingual-cased  0.837419   \n",
              "                                mGPT                          0.816224   \n",
              "                                mdeberta-v3-base              0.798806   \n",
              "                                xlm-roberta-large             0.873298   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.918328   \n",
              "                                mGPT                          0.901331   \n",
              "                                mdeberta-v3-base              0.648425   \n",
              "                                xlm-roberta-large             0.883877   \n",
              "               gpt-4            bert-base-multilingual-cased  0.792921   \n",
              "                                mGPT                          0.800774   \n",
              "                                mdeberta-v3-base              0.839545   \n",
              "                                xlm-roberta-large             0.767590   \n",
              "               llama-65b        bert-base-multilingual-cased  0.703173   \n",
              "                                mGPT                          0.466942   \n",
              "                                mdeberta-v3-base              0.710270   \n",
              "                                xlm-roberta-large             0.689892   \n",
              "               opt-66b          bert-base-multilingual-cased  0.587605   \n",
              "                                mGPT                          0.441578   \n",
              "                                mdeberta-v3-base              0.579892   \n",
              "                                xlm-roberta-large             0.670640   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.549435   \n",
              "                                mGPT                          0.482872   \n",
              "                                mdeberta-v3-base              0.947054   \n",
              "                                xlm-roberta-large             0.748330   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.716103   \n",
              "                                mGPT                          0.771285   \n",
              "                                mdeberta-v3-base              0.696867   \n",
              "                                xlm-roberta-large             0.576880   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.830933   \n",
              "                                mGPT                          0.634704   \n",
              "                                mdeberta-v3-base              0.412107   \n",
              "                                xlm-roberta-large             0.778696   \n",
              "es             alpaca-lora-30b  bert-base-multilingual-cased  0.859986   \n",
              "                                mGPT                          0.836644   \n",
              "                                mdeberta-v3-base              0.931657   \n",
              "                                xlm-roberta-large             0.884512   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.894916   \n",
              "                                mGPT                          0.894236   \n",
              "                                mdeberta-v3-base              0.888962   \n",
              "                                xlm-roberta-large             0.909319   \n",
              "               gpt-4            bert-base-multilingual-cased  0.891630   \n",
              "                                mGPT                          0.901645   \n",
              "                                mdeberta-v3-base              0.854453   \n",
              "                                xlm-roberta-large             0.598705   \n",
              "               llama-65b        bert-base-multilingual-cased  0.822676   \n",
              "                                mGPT                          0.540075   \n",
              "                                mdeberta-v3-base              0.976510   \n",
              "                                xlm-roberta-large             0.941188   \n",
              "               opt-66b          bert-base-multilingual-cased  0.674798   \n",
              "                                mGPT                          0.571312   \n",
              "                                mdeberta-v3-base              0.969586   \n",
              "                                xlm-roberta-large             0.810581   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.902707   \n",
              "                                mGPT                          0.897030   \n",
              "                                mdeberta-v3-base              0.994877   \n",
              "                                xlm-roberta-large             0.757796   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.802661   \n",
              "                                mGPT                          0.857829   \n",
              "                                mdeberta-v3-base              0.859653   \n",
              "                                xlm-roberta-large             0.894400   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.822568   \n",
              "                                mGPT                          0.720465   \n",
              "                                mdeberta-v3-base              0.859492   \n",
              "                                xlm-roberta-large             0.777281   \n",
              "ru             alpaca-lora-30b  bert-base-multilingual-cased  0.909936   \n",
              "                                mGPT                          0.934978   \n",
              "                                mdeberta-v3-base              0.961667   \n",
              "                                xlm-roberta-large             0.943282   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.956666   \n",
              "                                mGPT                          0.953325   \n",
              "                                mdeberta-v3-base              0.928300   \n",
              "                                xlm-roberta-large             0.976663   \n",
              "               gpt-4            bert-base-multilingual-cased  0.933297   \n",
              "                                mGPT                          0.956649   \n",
              "                                mdeberta-v3-base              0.830468   \n",
              "                                xlm-roberta-large             0.956667   \n",
              "               llama-65b        bert-base-multilingual-cased  0.984894   \n",
              "                                mGPT                          0.978185   \n",
              "                                mdeberta-v3-base              0.988253   \n",
              "                                xlm-roberta-large             0.993287   \n",
              "               opt-66b          bert-base-multilingual-cased  0.933984   \n",
              "                                mGPT                          0.966202   \n",
              "                                mdeberta-v3-base              0.984788   \n",
              "                                xlm-roberta-large             0.984794   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.979482   \n",
              "                                mGPT                          0.994876   \n",
              "                                mdeberta-v3-base              0.922422   \n",
              "                                xlm-roberta-large             0.996585   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.914614   \n",
              "                                mGPT                          0.931540   \n",
              "                                mdeberta-v3-base              0.938054   \n",
              "                                xlm-roberta-large             0.938186   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.926421   \n",
              "                                mGPT                          0.954849   \n",
              "                                mdeberta-v3-base              0.956463   \n",
              "                                xlm-roberta-large             0.961481   \n",
              "all            alpaca-lora-30b  bert-base-multilingual-cased  0.919999   \n",
              "                                mGPT                          0.938332   \n",
              "                                mdeberta-v3-base              0.928324   \n",
              "                                xlm-roberta-large             0.936622   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.954994   \n",
              "                                mGPT                          0.938284   \n",
              "                                mdeberta-v3-base              0.923248   \n",
              "                                xlm-roberta-large             0.984999   \n",
              "               gpt-4            bert-base-multilingual-cased  0.939983   \n",
              "                                mGPT                          0.946666   \n",
              "                                mdeberta-v3-base              0.938295   \n",
              "                                xlm-roberta-large             0.973332   \n",
              "               llama-65b        bert-base-multilingual-cased  0.978180   \n",
              "                                mGPT                          0.979860   \n",
              "                                mdeberta-v3-base              0.989933   \n",
              "                                xlm-roberta-large             0.989932   \n",
              "               opt-66b          bert-base-multilingual-cased  0.916831   \n",
              "                                mGPT                          0.974662   \n",
              "                                mdeberta-v3-base              0.988175   \n",
              "                                xlm-roberta-large             0.974624   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.986342   \n",
              "                                mGPT                          0.998292   \n",
              "                                mdeberta-v3-base              1.000000   \n",
              "                                xlm-roberta-large             1.000000   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.909843   \n",
              "                                mGPT                          0.949885   \n",
              "                                mdeberta-v3-base              0.934889   \n",
              "                                xlm-roberta-large             0.904119   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.939775   \n",
              "                                mGPT                          0.941314   \n",
              "                                mdeberta-v3-base              0.936295   \n",
              "                                xlm-roberta-large             0.974913   \n",
              "en3            alpaca-lora-30b  bert-base-multilingual-cased  0.846113   \n",
              "                                mGPT                          0.820896   \n",
              "                                mdeberta-v3-base              0.733177   \n",
              "                                xlm-roberta-large             0.829411   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.910000   \n",
              "                                mGPT                          0.923248   \n",
              "                                mdeberta-v3-base              0.676881   \n",
              "                                xlm-roberta-large             0.951660   \n",
              "               gpt-4            bert-base-multilingual-cased  0.876579   \n",
              "                                mGPT                          0.879774   \n",
              "                                mdeberta-v3-base              0.779360   \n",
              "                                xlm-roberta-large             0.906563   \n",
              "               llama-65b        bert-base-multilingual-cased  0.701551   \n",
              "                                mGPT                          0.484584   \n",
              "                                mdeberta-v3-base              0.773435   \n",
              "                                xlm-roberta-large             0.637093   \n",
              "               opt-66b          bert-base-multilingual-cased  0.610414   \n",
              "                                mGPT                          0.446283   \n",
              "                                mdeberta-v3-base              0.550282   \n",
              "                                xlm-roberta-large             0.588398   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.550129   \n",
              "                                mGPT                          0.256258   \n",
              "                                mdeberta-v3-base              0.754218   \n",
              "                                xlm-roberta-large             0.568899   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.777323   \n",
              "                                mGPT                          0.693971   \n",
              "                                mdeberta-v3-base              0.734054   \n",
              "                                xlm-roberta-large             0.598078   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.833505   \n",
              "                                mGPT                          0.636147   \n",
              "                                mdeberta-v3-base              0.603137   \n",
              "                                xlm-roberta-large             0.761834   \n",
              "\n",
              "                                                                    uk  \\\n",
              "Train Language Train LLM        Model                                    \n",
              "en             alpaca-lora-30b  bert-base-multilingual-cased  0.809057   \n",
              "                                mGPT                          0.821046   \n",
              "                                mdeberta-v3-base              0.756265   \n",
              "                                xlm-roberta-large             0.831866   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.896247   \n",
              "                                mGPT                          0.907969   \n",
              "                                mdeberta-v3-base              0.764939   \n",
              "                                xlm-roberta-large             0.629046   \n",
              "               gpt-4            bert-base-multilingual-cased  0.710398   \n",
              "                                mGPT                          0.784926   \n",
              "                                mdeberta-v3-base              0.766841   \n",
              "                                xlm-roberta-large             0.456707   \n",
              "               llama-65b        bert-base-multilingual-cased  0.702042   \n",
              "                                mGPT                          0.477949   \n",
              "                                mdeberta-v3-base              0.669289   \n",
              "                                xlm-roberta-large             0.628996   \n",
              "               opt-66b          bert-base-multilingual-cased  0.559436   \n",
              "                                mGPT                          0.464481   \n",
              "                                mdeberta-v3-base              0.601623   \n",
              "                                xlm-roberta-large             0.696108   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.533343   \n",
              "                                mGPT                          0.492465   \n",
              "                                mdeberta-v3-base              0.956963   \n",
              "                                xlm-roberta-large             0.758673   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.665660   \n",
              "                                mGPT                          0.693195   \n",
              "                                mdeberta-v3-base              0.601455   \n",
              "                                xlm-roberta-large             0.473909   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.813668   \n",
              "                                mGPT                          0.708828   \n",
              "                                mdeberta-v3-base              0.463450   \n",
              "                                xlm-roberta-large             0.489713   \n",
              "es             alpaca-lora-30b  bert-base-multilingual-cased  0.818227   \n",
              "                                mGPT                          0.887960   \n",
              "                                mdeberta-v3-base              0.909553   \n",
              "                                xlm-roberta-large             0.827786   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.872153   \n",
              "                                mGPT                          0.919624   \n",
              "                                mdeberta-v3-base              0.912794   \n",
              "                                xlm-roberta-large             0.736821   \n",
              "               gpt-4            bert-base-multilingual-cased  0.879208   \n",
              "                                mGPT                          0.845394   \n",
              "                                mdeberta-v3-base              0.909577   \n",
              "                                xlm-roberta-large             0.399356   \n",
              "               llama-65b        bert-base-multilingual-cased  0.929002   \n",
              "                                mGPT                          0.834843   \n",
              "                                mdeberta-v3-base              0.984847   \n",
              "                                xlm-roberta-large             0.967987   \n",
              "               opt-66b          bert-base-multilingual-cased  0.754343   \n",
              "                                mGPT                          0.642585   \n",
              "                                mdeberta-v3-base              0.962696   \n",
              "                                xlm-roberta-large             0.840842   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.970712   \n",
              "                                mGPT                          0.984507   \n",
              "                                mdeberta-v3-base              0.993109   \n",
              "                                xlm-roberta-large             0.706033   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.740548   \n",
              "                                mGPT                          0.716456   \n",
              "                                mdeberta-v3-base              0.744615   \n",
              "                                xlm-roberta-large             0.792063   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.786864   \n",
              "                                mGPT                          0.789295   \n",
              "                                mdeberta-v3-base              0.856181   \n",
              "                                xlm-roberta-large             0.640005   \n",
              "ru             alpaca-lora-30b  bert-base-multilingual-cased  0.882879   \n",
              "                                mGPT                          0.912926   \n",
              "                                mdeberta-v3-base              0.961536   \n",
              "                                xlm-roberta-large             0.939718   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.933107   \n",
              "                                mGPT                          0.924428   \n",
              "                                mdeberta-v3-base              0.948157   \n",
              "                                xlm-roberta-large             0.961514   \n",
              "               gpt-4            bert-base-multilingual-cased  0.921399   \n",
              "                                mGPT                          0.856972   \n",
              "                                mdeberta-v3-base              0.894049   \n",
              "                                xlm-roberta-large             0.944703   \n",
              "               llama-65b        bert-base-multilingual-cased  0.976421   \n",
              "                                mGPT                          0.935939   \n",
              "                                mdeberta-v3-base              0.981477   \n",
              "                                xlm-roberta-large             0.988215   \n",
              "               opt-66b          bert-base-multilingual-cased  0.925416   \n",
              "                                mGPT                          0.981352   \n",
              "                                mdeberta-v3-base              0.993219   \n",
              "                                xlm-roberta-large             0.986441   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.982761   \n",
              "                                mGPT                          0.981029   \n",
              "                                mdeberta-v3-base              0.970651   \n",
              "                                xlm-roberta-large             0.994832   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.907869   \n",
              "                                mGPT                          0.819250   \n",
              "                                mdeberta-v3-base              0.959758   \n",
              "                                xlm-roberta-large             0.949698   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.840089   \n",
              "                                mGPT                          0.897842   \n",
              "                                mdeberta-v3-base              0.958152   \n",
              "                                xlm-roberta-large             0.927846   \n",
              "all            alpaca-lora-30b  bert-base-multilingual-cased  0.892832   \n",
              "                                mGPT                          0.912946   \n",
              "                                mdeberta-v3-base              0.933074   \n",
              "                                xlm-roberta-large             0.924572   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.931318   \n",
              "                                mGPT                          0.893877   \n",
              "                                mdeberta-v3-base              0.939775   \n",
              "                                xlm-roberta-large             0.946371   \n",
              "               gpt-4            bert-base-multilingual-cased  0.936452   \n",
              "                                mGPT                          0.921219   \n",
              "                                mdeberta-v3-base              0.949787   \n",
              "                                xlm-roberta-large             0.983277   \n",
              "               llama-65b        bert-base-multilingual-cased  0.978111   \n",
              "                                mGPT                          0.973061   \n",
              "                                mdeberta-v3-base              0.966299   \n",
              "                                xlm-roberta-large             0.979792   \n",
              "               opt-66b          bert-base-multilingual-cased  0.899965   \n",
              "                                mGPT                          0.972880   \n",
              "                                mdeberta-v3-base              0.983051   \n",
              "                                xlm-roberta-large             0.996610   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.984503   \n",
              "                                mGPT                          0.996555   \n",
              "                                mdeberta-v3-base              0.996555   \n",
              "                                xlm-roberta-large             0.998278   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.877524   \n",
              "                                mGPT                          0.839593   \n",
              "                                mdeberta-v3-base              0.924432   \n",
              "                                xlm-roberta-large             0.833569   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.892804   \n",
              "                                mGPT                          0.845123   \n",
              "                                mdeberta-v3-base              0.936409   \n",
              "                                xlm-roberta-large             0.956520   \n",
              "en3            alpaca-lora-30b  bert-base-multilingual-cased  0.814381   \n",
              "                                mGPT                          0.804347   \n",
              "                                mdeberta-v3-base              0.682376   \n",
              "                                xlm-roberta-large             0.852366   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.894599   \n",
              "                                mGPT                          0.886038   \n",
              "                                mdeberta-v3-base              0.734812   \n",
              "                                xlm-roberta-large             0.946450   \n",
              "               gpt-4            bert-base-multilingual-cased  0.866167   \n",
              "                                mGPT                          0.806716   \n",
              "                                mdeberta-v3-base              0.827100   \n",
              "                                xlm-roberta-large             0.929765   \n",
              "               llama-65b        bert-base-multilingual-cased  0.803135   \n",
              "                                mGPT                          0.513736   \n",
              "                                mdeberta-v3-base              0.853296   \n",
              "                                xlm-roberta-large             0.655976   \n",
              "               opt-66b          bert-base-multilingual-cased  0.647736   \n",
              "                                mGPT                          0.498090   \n",
              "                                mdeberta-v3-base              0.471574   \n",
              "                                xlm-roberta-large             0.622024   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.637987   \n",
              "                                mGPT                          0.253658   \n",
              "                                mdeberta-v3-base              0.847703   \n",
              "                                xlm-roberta-large             0.597704   \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.733287   \n",
              "                                mGPT                          0.568728   \n",
              "                                mdeberta-v3-base              0.612957   \n",
              "                                xlm-roberta-large             0.393205   \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.837781   \n",
              "                                mGPT                          0.721517   \n",
              "                                mdeberta-v3-base              0.676501   \n",
              "                                xlm-roberta-large             0.786622   \n",
              "\n",
              "                                                                    zh  \n",
              "Train Language Train LLM        Model                                   \n",
              "en             alpaca-lora-30b  bert-base-multilingual-cased  0.553690  \n",
              "                                mGPT                          0.462625  \n",
              "                                mdeberta-v3-base              0.309948  \n",
              "                                xlm-roberta-large             0.447399  \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.893291  \n",
              "                                mGPT                          0.561038  \n",
              "                                mdeberta-v3-base              0.673301  \n",
              "                                xlm-roberta-large             0.515536  \n",
              "               gpt-4            bert-base-multilingual-cased  0.836037  \n",
              "                                mGPT                          0.473412  \n",
              "                                mdeberta-v3-base              0.570567  \n",
              "                                xlm-roberta-large             0.358748  \n",
              "               llama-65b        bert-base-multilingual-cased  0.519355  \n",
              "                                mGPT                          0.523263  \n",
              "                                mdeberta-v3-base              0.438383  \n",
              "                                xlm-roberta-large             0.489185  \n",
              "               opt-66b          bert-base-multilingual-cased  0.504128  \n",
              "                                mGPT                          0.416126  \n",
              "                                mdeberta-v3-base              0.561681  \n",
              "                                xlm-roberta-large             0.535815  \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.439036  \n",
              "                                mGPT                          0.578835  \n",
              "                                mdeberta-v3-base              0.701631  \n",
              "                                xlm-roberta-large             0.623693  \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.840924  \n",
              "                                mGPT                          0.524630  \n",
              "                                mdeberta-v3-base              0.777948  \n",
              "                                xlm-roberta-large             0.437676  \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.666693  \n",
              "                                mGPT                          0.594102  \n",
              "                                mdeberta-v3-base              0.588810  \n",
              "                                xlm-roberta-large             0.437038  \n",
              "es             alpaca-lora-30b  bert-base-multilingual-cased  0.771335  \n",
              "                                mGPT                          0.569114  \n",
              "                                mdeberta-v3-base              0.795438  \n",
              "                                xlm-roberta-large             0.616377  \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.876469  \n",
              "                                mGPT                          0.678108  \n",
              "                                mdeberta-v3-base              0.941620  \n",
              "                                xlm-roberta-large             0.850401  \n",
              "               gpt-4            bert-base-multilingual-cased  0.879892  \n",
              "                                mGPT                          0.664903  \n",
              "                                mdeberta-v3-base              0.904510  \n",
              "                                xlm-roberta-large             0.400083  \n",
              "               llama-65b        bert-base-multilingual-cased  0.639578  \n",
              "                                mGPT                          0.373068  \n",
              "                                mdeberta-v3-base              0.353950  \n",
              "                                xlm-roberta-large             0.684155  \n",
              "               opt-66b          bert-base-multilingual-cased  0.822457  \n",
              "                                mGPT                          0.455412  \n",
              "                                mdeberta-v3-base              0.699728  \n",
              "                                xlm-roberta-large             0.538099  \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.952752  \n",
              "                                mGPT                          0.583843  \n",
              "                                mdeberta-v3-base              0.874422  \n",
              "                                xlm-roberta-large             0.818718  \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.839972  \n",
              "                                mGPT                          0.766293  \n",
              "                                mdeberta-v3-base              0.916644  \n",
              "                                xlm-roberta-large             0.876797  \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.851378  \n",
              "                                mGPT                          0.619095  \n",
              "                                mdeberta-v3-base              0.829064  \n",
              "                                xlm-roberta-large             0.647270  \n",
              "ru             alpaca-lora-30b  bert-base-multilingual-cased  0.669646  \n",
              "                                mGPT                          0.490640  \n",
              "                                mdeberta-v3-base              0.823733  \n",
              "                                xlm-roberta-large             0.852248  \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.866630  \n",
              "                                mGPT                          0.625838  \n",
              "                                mdeberta-v3-base              0.832907  \n",
              "                                xlm-roberta-large             0.904835  \n",
              "               gpt-4            bert-base-multilingual-cased  0.812230  \n",
              "                                mGPT                          0.707815  \n",
              "                                mdeberta-v3-base              0.779666  \n",
              "                                xlm-roberta-large             0.921368  \n",
              "               llama-65b        bert-base-multilingual-cased  0.476467  \n",
              "                                mGPT                          0.405639  \n",
              "                                mdeberta-v3-base              0.562238  \n",
              "                                xlm-roberta-large             0.631352  \n",
              "               opt-66b          bert-base-multilingual-cased  0.640839  \n",
              "                                mGPT                          0.764907  \n",
              "                                mdeberta-v3-base              0.764493  \n",
              "                                xlm-roberta-large             0.852985  \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.824275  \n",
              "                                mGPT                          0.767807  \n",
              "                                mdeberta-v3-base              0.833545  \n",
              "                                xlm-roberta-large             0.933835  \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.791094  \n",
              "                                mGPT                          0.641026  \n",
              "                                mdeberta-v3-base              0.735809  \n",
              "                                xlm-roberta-large             0.774999  \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.347210  \n",
              "                                mGPT                          0.544260  \n",
              "                                mdeberta-v3-base              0.859437  \n",
              "                                xlm-roberta-large             0.899810  \n",
              "all            alpaca-lora-30b  bert-base-multilingual-cased  0.747233  \n",
              "                                mGPT                          0.555541  \n",
              "                                mdeberta-v3-base              0.711756  \n",
              "                                xlm-roberta-large             0.819962  \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.914828  \n",
              "                                mGPT                          0.569309  \n",
              "                                mdeberta-v3-base              0.852572  \n",
              "                                xlm-roberta-large             0.938168  \n",
              "               gpt-4            bert-base-multilingual-cased  0.904807  \n",
              "                                mGPT                          0.723636  \n",
              "                                mdeberta-v3-base              0.885362  \n",
              "                                xlm-roberta-large             0.936548  \n",
              "               llama-65b        bert-base-multilingual-cased  0.658036  \n",
              "                                mGPT                          0.533715  \n",
              "                                mdeberta-v3-base              0.597399  \n",
              "                                xlm-roberta-large             0.599157  \n",
              "               opt-66b          bert-base-multilingual-cased  0.773006  \n",
              "                                mGPT                          0.692877  \n",
              "                                mdeberta-v3-base              0.807321  \n",
              "                                xlm-roberta-large             0.780601  \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.949368  \n",
              "                                mGPT                          0.768063  \n",
              "                                mdeberta-v3-base              0.799917  \n",
              "                                xlm-roberta-large             0.807778  \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.918315  \n",
              "                                mGPT                          0.844200  \n",
              "                                mdeberta-v3-base              0.933065  \n",
              "                                xlm-roberta-large             0.671403  \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.717279  \n",
              "                                mGPT                          0.652242  \n",
              "                                mdeberta-v3-base              0.701151  \n",
              "                                xlm-roberta-large             0.744186  \n",
              "en3            alpaca-lora-30b  bert-base-multilingual-cased  0.571886  \n",
              "                                mGPT                          0.426802  \n",
              "                                mdeberta-v3-base              0.417706  \n",
              "                                xlm-roberta-large             0.598436  \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased  0.908321  \n",
              "                                mGPT                          0.549381  \n",
              "                                mdeberta-v3-base              0.729492  \n",
              "                                xlm-roberta-large             0.796341  \n",
              "               gpt-4            bert-base-multilingual-cased  0.807944  \n",
              "                                mGPT                          0.657042  \n",
              "                                mdeberta-v3-base              0.693959  \n",
              "                                xlm-roberta-large             0.861447  \n",
              "               llama-65b        bert-base-multilingual-cased  0.565878  \n",
              "                                mGPT                          0.555561  \n",
              "                                mdeberta-v3-base              0.590222  \n",
              "                                xlm-roberta-large             0.448793  \n",
              "               opt-66b          bert-base-multilingual-cased  0.532262  \n",
              "                                mGPT                          0.446363  \n",
              "                                mdeberta-v3-base              0.530702  \n",
              "                                xlm-roberta-large             0.517499  \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased  0.518781  \n",
              "                                mGPT                          0.415553  \n",
              "                                mdeberta-v3-base              0.732787  \n",
              "                                xlm-roberta-large             0.616519  \n",
              "               text-davinci-003 bert-base-multilingual-cased  0.888281  \n",
              "                                mGPT                          0.590378  \n",
              "                                mdeberta-v3-base              0.719948  \n",
              "                                xlm-roberta-large             0.398708  \n",
              "               vicuna-13b       bert-base-multilingual-cased  0.637889  \n",
              "                                mGPT                          0.677753  \n",
              "                                mdeberta-v3-base              0.548784  \n",
              "                                xlm-roberta-large             0.759474  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-59b69970-6294-4772-9a63-b48605ddf30e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>ar</th>\n",
              "      <th>ca</th>\n",
              "      <th>cs</th>\n",
              "      <th>de</th>\n",
              "      <th>en</th>\n",
              "      <th>es</th>\n",
              "      <th>nl</th>\n",
              "      <th>pt</th>\n",
              "      <th>ru</th>\n",
              "      <th>uk</th>\n",
              "      <th>zh</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train Language</th>\n",
              "      <th>Train LLM</th>\n",
              "      <th>Model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"32\" valign=\"top\">en</th>\n",
              "      <th rowspan=\"4\" valign=\"top\">alpaca-lora-30b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.537542</td>\n",
              "      <td>0.827078</td>\n",
              "      <td>0.854560</td>\n",
              "      <td>0.891714</td>\n",
              "      <td>0.956665</td>\n",
              "      <td>0.751676</td>\n",
              "      <td>0.856264</td>\n",
              "      <td>0.805481</td>\n",
              "      <td>0.837419</td>\n",
              "      <td>0.809057</td>\n",
              "      <td>0.553690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.402388</td>\n",
              "      <td>0.808877</td>\n",
              "      <td>0.613177</td>\n",
              "      <td>0.876305</td>\n",
              "      <td>0.963891</td>\n",
              "      <td>0.738976</td>\n",
              "      <td>0.879058</td>\n",
              "      <td>0.833282</td>\n",
              "      <td>0.816224</td>\n",
              "      <td>0.821046</td>\n",
              "      <td>0.462625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.207984</td>\n",
              "      <td>0.859243</td>\n",
              "      <td>0.769121</td>\n",
              "      <td>0.900331</td>\n",
              "      <td>0.943947</td>\n",
              "      <td>0.774384</td>\n",
              "      <td>0.897682</td>\n",
              "      <td>0.857008</td>\n",
              "      <td>0.798806</td>\n",
              "      <td>0.756265</td>\n",
              "      <td>0.309948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.447388</td>\n",
              "      <td>0.848775</td>\n",
              "      <td>0.871277</td>\n",
              "      <td>0.932405</td>\n",
              "      <td>0.980144</td>\n",
              "      <td>0.631892</td>\n",
              "      <td>0.770589</td>\n",
              "      <td>0.712993</td>\n",
              "      <td>0.873298</td>\n",
              "      <td>0.831866</td>\n",
              "      <td>0.447399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">gpt-3.5-turbo</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.921465</td>\n",
              "      <td>0.890380</td>\n",
              "      <td>0.914981</td>\n",
              "      <td>0.902026</td>\n",
              "      <td>0.978339</td>\n",
              "      <td>0.854525</td>\n",
              "      <td>0.934819</td>\n",
              "      <td>0.912430</td>\n",
              "      <td>0.918328</td>\n",
              "      <td>0.896247</td>\n",
              "      <td>0.893291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.700916</td>\n",
              "      <td>0.926614</td>\n",
              "      <td>0.337026</td>\n",
              "      <td>0.907073</td>\n",
              "      <td>0.989169</td>\n",
              "      <td>0.850593</td>\n",
              "      <td>0.943226</td>\n",
              "      <td>0.914732</td>\n",
              "      <td>0.901331</td>\n",
              "      <td>0.907969</td>\n",
              "      <td>0.561038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.787647</td>\n",
              "      <td>0.559878</td>\n",
              "      <td>0.780318</td>\n",
              "      <td>0.805155</td>\n",
              "      <td>0.717787</td>\n",
              "      <td>0.602325</td>\n",
              "      <td>0.779041</td>\n",
              "      <td>0.678157</td>\n",
              "      <td>0.648425</td>\n",
              "      <td>0.764939</td>\n",
              "      <td>0.673301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.721820</td>\n",
              "      <td>0.973319</td>\n",
              "      <td>0.721150</td>\n",
              "      <td>0.939164</td>\n",
              "      <td>0.983754</td>\n",
              "      <td>0.786081</td>\n",
              "      <td>0.922878</td>\n",
              "      <td>0.928099</td>\n",
              "      <td>0.883877</td>\n",
              "      <td>0.629046</td>\n",
              "      <td>0.515536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">gpt-4</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.841926</td>\n",
              "      <td>0.926529</td>\n",
              "      <td>0.759176</td>\n",
              "      <td>0.816308</td>\n",
              "      <td>0.976534</td>\n",
              "      <td>0.826112</td>\n",
              "      <td>0.926544</td>\n",
              "      <td>0.870419</td>\n",
              "      <td>0.792921</td>\n",
              "      <td>0.710398</td>\n",
              "      <td>0.836037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.694497</td>\n",
              "      <td>0.788771</td>\n",
              "      <td>0.386602</td>\n",
              "      <td>0.836477</td>\n",
              "      <td>0.990975</td>\n",
              "      <td>0.795370</td>\n",
              "      <td>0.833370</td>\n",
              "      <td>0.865765</td>\n",
              "      <td>0.800774</td>\n",
              "      <td>0.784926</td>\n",
              "      <td>0.473412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.719857</td>\n",
              "      <td>0.973331</td>\n",
              "      <td>0.862618</td>\n",
              "      <td>0.894392</td>\n",
              "      <td>0.825513</td>\n",
              "      <td>0.943480</td>\n",
              "      <td>0.933106</td>\n",
              "      <td>0.931852</td>\n",
              "      <td>0.839545</td>\n",
              "      <td>0.766841</td>\n",
              "      <td>0.570567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.565875</td>\n",
              "      <td>0.934778</td>\n",
              "      <td>0.498656</td>\n",
              "      <td>0.892721</td>\n",
              "      <td>0.989170</td>\n",
              "      <td>0.859221</td>\n",
              "      <td>0.949913</td>\n",
              "      <td>0.926661</td>\n",
              "      <td>0.767590</td>\n",
              "      <td>0.456707</td>\n",
              "      <td>0.358748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">llama-65b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.589577</td>\n",
              "      <td>0.606979</td>\n",
              "      <td>0.695458</td>\n",
              "      <td>0.689542</td>\n",
              "      <td>0.901658</td>\n",
              "      <td>0.799832</td>\n",
              "      <td>0.577084</td>\n",
              "      <td>0.675244</td>\n",
              "      <td>0.703173</td>\n",
              "      <td>0.702042</td>\n",
              "      <td>0.519355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.384803</td>\n",
              "      <td>0.350849</td>\n",
              "      <td>0.689755</td>\n",
              "      <td>0.711503</td>\n",
              "      <td>0.903682</td>\n",
              "      <td>0.524759</td>\n",
              "      <td>0.410240</td>\n",
              "      <td>0.483544</td>\n",
              "      <td>0.466942</td>\n",
              "      <td>0.477949</td>\n",
              "      <td>0.523263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.511518</td>\n",
              "      <td>0.405992</td>\n",
              "      <td>0.830677</td>\n",
              "      <td>0.716159</td>\n",
              "      <td>0.900289</td>\n",
              "      <td>0.607432</td>\n",
              "      <td>0.459106</td>\n",
              "      <td>0.541285</td>\n",
              "      <td>0.710270</td>\n",
              "      <td>0.669289</td>\n",
              "      <td>0.438383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.477913</td>\n",
              "      <td>0.489240</td>\n",
              "      <td>0.850697</td>\n",
              "      <td>0.798271</td>\n",
              "      <td>0.916509</td>\n",
              "      <td>0.724079</td>\n",
              "      <td>0.436761</td>\n",
              "      <td>0.608223</td>\n",
              "      <td>0.689892</td>\n",
              "      <td>0.628996</td>\n",
              "      <td>0.489185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">opt-66b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.455733</td>\n",
              "      <td>0.458569</td>\n",
              "      <td>0.610699</td>\n",
              "      <td>0.588965</td>\n",
              "      <td>0.859176</td>\n",
              "      <td>0.539436</td>\n",
              "      <td>0.426829</td>\n",
              "      <td>0.563455</td>\n",
              "      <td>0.587605</td>\n",
              "      <td>0.559436</td>\n",
              "      <td>0.504128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.295934</td>\n",
              "      <td>0.504866</td>\n",
              "      <td>0.395400</td>\n",
              "      <td>0.635131</td>\n",
              "      <td>0.893075</td>\n",
              "      <td>0.506420</td>\n",
              "      <td>0.494248</td>\n",
              "      <td>0.601702</td>\n",
              "      <td>0.441578</td>\n",
              "      <td>0.464481</td>\n",
              "      <td>0.416126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.390341</td>\n",
              "      <td>0.518131</td>\n",
              "      <td>0.627795</td>\n",
              "      <td>0.656829</td>\n",
              "      <td>0.853764</td>\n",
              "      <td>0.605320</td>\n",
              "      <td>0.492703</td>\n",
              "      <td>0.643090</td>\n",
              "      <td>0.579892</td>\n",
              "      <td>0.601623</td>\n",
              "      <td>0.561681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.490988</td>\n",
              "      <td>0.461322</td>\n",
              "      <td>0.549451</td>\n",
              "      <td>0.673485</td>\n",
              "      <td>0.882653</td>\n",
              "      <td>0.561258</td>\n",
              "      <td>0.418667</td>\n",
              "      <td>0.589688</td>\n",
              "      <td>0.670640</td>\n",
              "      <td>0.696108</td>\n",
              "      <td>0.535815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">opt-iml-max-1.3b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.459169</td>\n",
              "      <td>0.517135</td>\n",
              "      <td>0.579223</td>\n",
              "      <td>0.591575</td>\n",
              "      <td>0.906132</td>\n",
              "      <td>0.514289</td>\n",
              "      <td>0.513633</td>\n",
              "      <td>0.472087</td>\n",
              "      <td>0.549435</td>\n",
              "      <td>0.533343</td>\n",
              "      <td>0.439036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.338008</td>\n",
              "      <td>0.513427</td>\n",
              "      <td>0.641027</td>\n",
              "      <td>0.745042</td>\n",
              "      <td>0.958480</td>\n",
              "      <td>0.528383</td>\n",
              "      <td>0.695527</td>\n",
              "      <td>0.521178</td>\n",
              "      <td>0.482872</td>\n",
              "      <td>0.492465</td>\n",
              "      <td>0.578835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.793226</td>\n",
              "      <td>0.973103</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.960929</td>\n",
              "      <td>0.925661</td>\n",
              "      <td>0.902335</td>\n",
              "      <td>0.961500</td>\n",
              "      <td>0.902473</td>\n",
              "      <td>0.947054</td>\n",
              "      <td>0.956963</td>\n",
              "      <td>0.701631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.580995</td>\n",
              "      <td>0.856666</td>\n",
              "      <td>0.865734</td>\n",
              "      <td>0.841577</td>\n",
              "      <td>0.943910</td>\n",
              "      <td>0.726786</td>\n",
              "      <td>0.768422</td>\n",
              "      <td>0.699108</td>\n",
              "      <td>0.748330</td>\n",
              "      <td>0.758673</td>\n",
              "      <td>0.623693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">text-davinci-003</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.764651</td>\n",
              "      <td>0.901368</td>\n",
              "      <td>0.853679</td>\n",
              "      <td>0.855428</td>\n",
              "      <td>0.969314</td>\n",
              "      <td>0.774992</td>\n",
              "      <td>0.903165</td>\n",
              "      <td>0.840752</td>\n",
              "      <td>0.716103</td>\n",
              "      <td>0.665660</td>\n",
              "      <td>0.840924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.539324</td>\n",
              "      <td>0.919928</td>\n",
              "      <td>0.362297</td>\n",
              "      <td>0.893556</td>\n",
              "      <td>0.978335</td>\n",
              "      <td>0.767779</td>\n",
              "      <td>0.902670</td>\n",
              "      <td>0.838866</td>\n",
              "      <td>0.771285</td>\n",
              "      <td>0.693195</td>\n",
              "      <td>0.524630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.543187</td>\n",
              "      <td>0.639753</td>\n",
              "      <td>0.823919</td>\n",
              "      <td>0.856322</td>\n",
              "      <td>0.951251</td>\n",
              "      <td>0.577358</td>\n",
              "      <td>0.759612</td>\n",
              "      <td>0.633527</td>\n",
              "      <td>0.696867</td>\n",
              "      <td>0.601455</td>\n",
              "      <td>0.777948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.503535</td>\n",
              "      <td>0.921334</td>\n",
              "      <td>0.696479</td>\n",
              "      <td>0.891345</td>\n",
              "      <td>0.985559</td>\n",
              "      <td>0.771487</td>\n",
              "      <td>0.897266</td>\n",
              "      <td>0.823210</td>\n",
              "      <td>0.576880</td>\n",
              "      <td>0.473909</td>\n",
              "      <td>0.437676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">vicuna-13b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.791310</td>\n",
              "      <td>0.885598</td>\n",
              "      <td>0.801202</td>\n",
              "      <td>0.844550</td>\n",
              "      <td>0.974726</td>\n",
              "      <td>0.785887</td>\n",
              "      <td>0.936540</td>\n",
              "      <td>0.851997</td>\n",
              "      <td>0.830933</td>\n",
              "      <td>0.813668</td>\n",
              "      <td>0.666693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.408638</td>\n",
              "      <td>0.788775</td>\n",
              "      <td>0.504333</td>\n",
              "      <td>0.851064</td>\n",
              "      <td>0.976534</td>\n",
              "      <td>0.663255</td>\n",
              "      <td>0.887594</td>\n",
              "      <td>0.807120</td>\n",
              "      <td>0.634704</td>\n",
              "      <td>0.708828</td>\n",
              "      <td>0.594102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.178446</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.501832</td>\n",
              "      <td>0.629463</td>\n",
              "      <td>0.777510</td>\n",
              "      <td>0.475434</td>\n",
              "      <td>0.619061</td>\n",
              "      <td>0.507817</td>\n",
              "      <td>0.412107</td>\n",
              "      <td>0.463450</td>\n",
              "      <td>0.588810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.386417</td>\n",
              "      <td>0.941659</td>\n",
              "      <td>0.489440</td>\n",
              "      <td>0.908579</td>\n",
              "      <td>0.980144</td>\n",
              "      <td>0.686643</td>\n",
              "      <td>0.840000</td>\n",
              "      <td>0.716308</td>\n",
              "      <td>0.778696</td>\n",
              "      <td>0.489713</td>\n",
              "      <td>0.437038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"32\" valign=\"top\">es</th>\n",
              "      <th rowspan=\"4\" valign=\"top\">alpaca-lora-30b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.601879</td>\n",
              "      <td>0.894965</td>\n",
              "      <td>0.810758</td>\n",
              "      <td>0.876534</td>\n",
              "      <td>0.751693</td>\n",
              "      <td>0.909150</td>\n",
              "      <td>0.894101</td>\n",
              "      <td>0.894326</td>\n",
              "      <td>0.859986</td>\n",
              "      <td>0.818227</td>\n",
              "      <td>0.771335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.697994</td>\n",
              "      <td>0.863938</td>\n",
              "      <td>0.698896</td>\n",
              "      <td>0.900098</td>\n",
              "      <td>0.658906</td>\n",
              "      <td>0.945195</td>\n",
              "      <td>0.708766</td>\n",
              "      <td>0.940350</td>\n",
              "      <td>0.836644</td>\n",
              "      <td>0.887960</td>\n",
              "      <td>0.569114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.825526</td>\n",
              "      <td>0.733757</td>\n",
              "      <td>0.911024</td>\n",
              "      <td>0.917144</td>\n",
              "      <td>0.877062</td>\n",
              "      <td>0.825619</td>\n",
              "      <td>0.803750</td>\n",
              "      <td>0.777745</td>\n",
              "      <td>0.931657</td>\n",
              "      <td>0.909553</td>\n",
              "      <td>0.795438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.699932</td>\n",
              "      <td>0.834861</td>\n",
              "      <td>0.900833</td>\n",
              "      <td>0.949315</td>\n",
              "      <td>0.929459</td>\n",
              "      <td>0.928069</td>\n",
              "      <td>0.953177</td>\n",
              "      <td>0.930124</td>\n",
              "      <td>0.884512</td>\n",
              "      <td>0.827786</td>\n",
              "      <td>0.616377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">gpt-3.5-turbo</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.889624</td>\n",
              "      <td>0.936548</td>\n",
              "      <td>0.899715</td>\n",
              "      <td>0.890037</td>\n",
              "      <td>0.916880</td>\n",
              "      <td>0.926344</td>\n",
              "      <td>0.923169</td>\n",
              "      <td>0.885839</td>\n",
              "      <td>0.894916</td>\n",
              "      <td>0.872153</td>\n",
              "      <td>0.876469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.894145</td>\n",
              "      <td>0.964984</td>\n",
              "      <td>0.355178</td>\n",
              "      <td>0.930710</td>\n",
              "      <td>0.839701</td>\n",
              "      <td>0.969155</td>\n",
              "      <td>0.897390</td>\n",
              "      <td>0.955613</td>\n",
              "      <td>0.894236</td>\n",
              "      <td>0.919624</td>\n",
              "      <td>0.678108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.928117</td>\n",
              "      <td>0.881723</td>\n",
              "      <td>0.966649</td>\n",
              "      <td>0.957743</td>\n",
              "      <td>0.936777</td>\n",
              "      <td>0.957037</td>\n",
              "      <td>0.910947</td>\n",
              "      <td>0.908940</td>\n",
              "      <td>0.888962</td>\n",
              "      <td>0.912794</td>\n",
              "      <td>0.941620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.767102</td>\n",
              "      <td>0.954955</td>\n",
              "      <td>0.860860</td>\n",
              "      <td>0.853948</td>\n",
              "      <td>0.980144</td>\n",
              "      <td>0.974282</td>\n",
              "      <td>0.961602</td>\n",
              "      <td>0.967608</td>\n",
              "      <td>0.909319</td>\n",
              "      <td>0.736821</td>\n",
              "      <td>0.850401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">gpt-4</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.876361</td>\n",
              "      <td>0.885192</td>\n",
              "      <td>0.929950</td>\n",
              "      <td>0.880837</td>\n",
              "      <td>0.730575</td>\n",
              "      <td>0.927672</td>\n",
              "      <td>0.907483</td>\n",
              "      <td>0.867887</td>\n",
              "      <td>0.891630</td>\n",
              "      <td>0.879208</td>\n",
              "      <td>0.879892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.849110</td>\n",
              "      <td>0.870498</td>\n",
              "      <td>0.337026</td>\n",
              "      <td>0.918737</td>\n",
              "      <td>0.947633</td>\n",
              "      <td>0.982867</td>\n",
              "      <td>0.924467</td>\n",
              "      <td>0.982956</td>\n",
              "      <td>0.901645</td>\n",
              "      <td>0.845394</td>\n",
              "      <td>0.664903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.933106</td>\n",
              "      <td>0.889724</td>\n",
              "      <td>0.954909</td>\n",
              "      <td>0.935623</td>\n",
              "      <td>0.882561</td>\n",
              "      <td>0.919452</td>\n",
              "      <td>0.933140</td>\n",
              "      <td>0.902705</td>\n",
              "      <td>0.854453</td>\n",
              "      <td>0.909577</td>\n",
              "      <td>0.904510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.512410</td>\n",
              "      <td>0.959946</td>\n",
              "      <td>0.568764</td>\n",
              "      <td>0.801054</td>\n",
              "      <td>0.972914</td>\n",
              "      <td>0.981155</td>\n",
              "      <td>0.961602</td>\n",
              "      <td>0.967626</td>\n",
              "      <td>0.598705</td>\n",
              "      <td>0.399356</td>\n",
              "      <td>0.400083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">llama-65b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.795561</td>\n",
              "      <td>0.917710</td>\n",
              "      <td>0.964457</td>\n",
              "      <td>0.771126</td>\n",
              "      <td>0.361111</td>\n",
              "      <td>0.991388</td>\n",
              "      <td>0.666485</td>\n",
              "      <td>0.979375</td>\n",
              "      <td>0.822676</td>\n",
              "      <td>0.929002</td>\n",
              "      <td>0.639578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.591239</td>\n",
              "      <td>0.689859</td>\n",
              "      <td>0.860581</td>\n",
              "      <td>0.754042</td>\n",
              "      <td>0.350532</td>\n",
              "      <td>0.967288</td>\n",
              "      <td>0.360353</td>\n",
              "      <td>0.950147</td>\n",
              "      <td>0.540075</td>\n",
              "      <td>0.834843</td>\n",
              "      <td>0.373068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.913549</td>\n",
              "      <td>0.926183</td>\n",
              "      <td>0.988152</td>\n",
              "      <td>0.960824</td>\n",
              "      <td>0.625957</td>\n",
              "      <td>0.986219</td>\n",
              "      <td>0.659130</td>\n",
              "      <td>0.977658</td>\n",
              "      <td>0.976510</td>\n",
              "      <td>0.984847</td>\n",
              "      <td>0.353950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.846051</td>\n",
              "      <td>0.830698</td>\n",
              "      <td>0.983077</td>\n",
              "      <td>0.955682</td>\n",
              "      <td>0.668031</td>\n",
              "      <td>0.987945</td>\n",
              "      <td>0.458293</td>\n",
              "      <td>0.972497</td>\n",
              "      <td>0.941188</td>\n",
              "      <td>0.967987</td>\n",
              "      <td>0.684155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">opt-66b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.737808</td>\n",
              "      <td>0.760815</td>\n",
              "      <td>0.644272</td>\n",
              "      <td>0.587242</td>\n",
              "      <td>0.470484</td>\n",
              "      <td>0.765752</td>\n",
              "      <td>0.703752</td>\n",
              "      <td>0.766234</td>\n",
              "      <td>0.674798</td>\n",
              "      <td>0.754343</td>\n",
              "      <td>0.822457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.390615</td>\n",
              "      <td>0.729738</td>\n",
              "      <td>0.455011</td>\n",
              "      <td>0.695724</td>\n",
              "      <td>0.372230</td>\n",
              "      <td>0.799220</td>\n",
              "      <td>0.386565</td>\n",
              "      <td>0.594937</td>\n",
              "      <td>0.571312</td>\n",
              "      <td>0.642585</td>\n",
              "      <td>0.455412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.913856</td>\n",
              "      <td>0.850023</td>\n",
              "      <td>0.988333</td>\n",
              "      <td>0.949266</td>\n",
              "      <td>0.624038</td>\n",
              "      <td>0.876242</td>\n",
              "      <td>0.853753</td>\n",
              "      <td>0.787190</td>\n",
              "      <td>0.969586</td>\n",
              "      <td>0.962696</td>\n",
              "      <td>0.699728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.605757</td>\n",
              "      <td>0.745234</td>\n",
              "      <td>0.876505</td>\n",
              "      <td>0.810358</td>\n",
              "      <td>0.444280</td>\n",
              "      <td>0.812310</td>\n",
              "      <td>0.470239</td>\n",
              "      <td>0.633545</td>\n",
              "      <td>0.810581</td>\n",
              "      <td>0.840842</td>\n",
              "      <td>0.538099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">opt-iml-max-1.3b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.953707</td>\n",
              "      <td>0.969710</td>\n",
              "      <td>0.963200</td>\n",
              "      <td>0.867572</td>\n",
              "      <td>0.683951</td>\n",
              "      <td>0.948185</td>\n",
              "      <td>0.969888</td>\n",
              "      <td>0.882016</td>\n",
              "      <td>0.902707</td>\n",
              "      <td>0.970712</td>\n",
              "      <td>0.952752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.943433</td>\n",
              "      <td>0.961329</td>\n",
              "      <td>0.920996</td>\n",
              "      <td>0.993208</td>\n",
              "      <td>0.404483</td>\n",
              "      <td>0.987905</td>\n",
              "      <td>0.840959</td>\n",
              "      <td>0.981028</td>\n",
              "      <td>0.897030</td>\n",
              "      <td>0.984507</td>\n",
              "      <td>0.583843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.993148</td>\n",
              "      <td>0.986553</td>\n",
              "      <td>0.993310</td>\n",
              "      <td>0.871046</td>\n",
              "      <td>0.586456</td>\n",
              "      <td>0.982728</td>\n",
              "      <td>0.932942</td>\n",
              "      <td>0.982758</td>\n",
              "      <td>0.994877</td>\n",
              "      <td>0.993109</td>\n",
              "      <td>0.874422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.979411</td>\n",
              "      <td>0.991594</td>\n",
              "      <td>0.858342</td>\n",
              "      <td>0.988115</td>\n",
              "      <td>0.619244</td>\n",
              "      <td>0.981002</td>\n",
              "      <td>0.979933</td>\n",
              "      <td>0.979310</td>\n",
              "      <td>0.757796</td>\n",
              "      <td>0.706033</td>\n",
              "      <td>0.818718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">text-davinci-003</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.731949</td>\n",
              "      <td>0.904712</td>\n",
              "      <td>0.883332</td>\n",
              "      <td>0.860236</td>\n",
              "      <td>0.760513</td>\n",
              "      <td>0.931246</td>\n",
              "      <td>0.889246</td>\n",
              "      <td>0.863110</td>\n",
              "      <td>0.802661</td>\n",
              "      <td>0.740548</td>\n",
              "      <td>0.839972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.718775</td>\n",
              "      <td>0.802038</td>\n",
              "      <td>0.383186</td>\n",
              "      <td>0.927360</td>\n",
              "      <td>0.903856</td>\n",
              "      <td>0.965721</td>\n",
              "      <td>0.938161</td>\n",
              "      <td>0.916500</td>\n",
              "      <td>0.857829</td>\n",
              "      <td>0.716456</td>\n",
              "      <td>0.766293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.643584</td>\n",
              "      <td>0.761469</td>\n",
              "      <td>0.924799</td>\n",
              "      <td>0.923908</td>\n",
              "      <td>0.940433</td>\n",
              "      <td>0.805888</td>\n",
              "      <td>0.906504</td>\n",
              "      <td>0.848353</td>\n",
              "      <td>0.859653</td>\n",
              "      <td>0.744615</td>\n",
              "      <td>0.916644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.799302</td>\n",
              "      <td>0.926402</td>\n",
              "      <td>0.948212</td>\n",
              "      <td>0.950973</td>\n",
              "      <td>0.942226</td>\n",
              "      <td>0.977716</td>\n",
              "      <td>0.973283</td>\n",
              "      <td>0.935165</td>\n",
              "      <td>0.894400</td>\n",
              "      <td>0.792063</td>\n",
              "      <td>0.876797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">vicuna-13b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.862826</td>\n",
              "      <td>0.914947</td>\n",
              "      <td>0.770478</td>\n",
              "      <td>0.834088</td>\n",
              "      <td>0.750260</td>\n",
              "      <td>0.928081</td>\n",
              "      <td>0.899598</td>\n",
              "      <td>0.882343</td>\n",
              "      <td>0.822568</td>\n",
              "      <td>0.786864</td>\n",
              "      <td>0.851378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.723426</td>\n",
              "      <td>0.871649</td>\n",
              "      <td>0.542705</td>\n",
              "      <td>0.920602</td>\n",
              "      <td>0.673940</td>\n",
              "      <td>0.967438</td>\n",
              "      <td>0.688021</td>\n",
              "      <td>0.916407</td>\n",
              "      <td>0.720465</td>\n",
              "      <td>0.789295</td>\n",
              "      <td>0.619095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.678396</td>\n",
              "      <td>0.899356</td>\n",
              "      <td>0.838727</td>\n",
              "      <td>0.905250</td>\n",
              "      <td>0.916859</td>\n",
              "      <td>0.951998</td>\n",
              "      <td>0.875332</td>\n",
              "      <td>0.895597</td>\n",
              "      <td>0.859492</td>\n",
              "      <td>0.856181</td>\n",
              "      <td>0.829064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.844734</td>\n",
              "      <td>0.878868</td>\n",
              "      <td>0.668109</td>\n",
              "      <td>0.861400</td>\n",
              "      <td>0.906107</td>\n",
              "      <td>0.943445</td>\n",
              "      <td>0.826702</td>\n",
              "      <td>0.895936</td>\n",
              "      <td>0.777281</td>\n",
              "      <td>0.640005</td>\n",
              "      <td>0.647270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"32\" valign=\"top\">ru</th>\n",
              "      <th rowspan=\"4\" valign=\"top\">alpaca-lora-30b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.602643</td>\n",
              "      <td>0.632650</td>\n",
              "      <td>0.806734</td>\n",
              "      <td>0.740883</td>\n",
              "      <td>0.463444</td>\n",
              "      <td>0.657224</td>\n",
              "      <td>0.670451</td>\n",
              "      <td>0.656725</td>\n",
              "      <td>0.909936</td>\n",
              "      <td>0.882879</td>\n",
              "      <td>0.669646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.563543</td>\n",
              "      <td>0.412312</td>\n",
              "      <td>0.672520</td>\n",
              "      <td>0.871620</td>\n",
              "      <td>0.527200</td>\n",
              "      <td>0.771379</td>\n",
              "      <td>0.532546</td>\n",
              "      <td>0.791988</td>\n",
              "      <td>0.934978</td>\n",
              "      <td>0.912926</td>\n",
              "      <td>0.490640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.971599</td>\n",
              "      <td>0.812253</td>\n",
              "      <td>0.984999</td>\n",
              "      <td>0.940874</td>\n",
              "      <td>0.854983</td>\n",
              "      <td>0.888526</td>\n",
              "      <td>0.886782</td>\n",
              "      <td>0.819913</td>\n",
              "      <td>0.961667</td>\n",
              "      <td>0.961536</td>\n",
              "      <td>0.823733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.918018</td>\n",
              "      <td>0.898299</td>\n",
              "      <td>0.969973</td>\n",
              "      <td>0.944230</td>\n",
              "      <td>0.916940</td>\n",
              "      <td>0.905788</td>\n",
              "      <td>0.943052</td>\n",
              "      <td>0.899260</td>\n",
              "      <td>0.943282</td>\n",
              "      <td>0.939718</td>\n",
              "      <td>0.852248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">gpt-3.5-turbo</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.941563</td>\n",
              "      <td>0.867992</td>\n",
              "      <td>0.904463</td>\n",
              "      <td>0.853816</td>\n",
              "      <td>0.781013</td>\n",
              "      <td>0.875397</td>\n",
              "      <td>0.878277</td>\n",
              "      <td>0.837753</td>\n",
              "      <td>0.956666</td>\n",
              "      <td>0.933107</td>\n",
              "      <td>0.866630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.784573</td>\n",
              "      <td>0.885671</td>\n",
              "      <td>0.355178</td>\n",
              "      <td>0.907050</td>\n",
              "      <td>0.881669</td>\n",
              "      <td>0.922780</td>\n",
              "      <td>0.922999</td>\n",
              "      <td>0.904259</td>\n",
              "      <td>0.953325</td>\n",
              "      <td>0.924428</td>\n",
              "      <td>0.625838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.936497</td>\n",
              "      <td>0.838727</td>\n",
              "      <td>0.981664</td>\n",
              "      <td>0.925553</td>\n",
              "      <td>0.960288</td>\n",
              "      <td>0.929596</td>\n",
              "      <td>0.914644</td>\n",
              "      <td>0.902296</td>\n",
              "      <td>0.928300</td>\n",
              "      <td>0.948157</td>\n",
              "      <td>0.832907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.933217</td>\n",
              "      <td>0.943227</td>\n",
              "      <td>0.988333</td>\n",
              "      <td>0.939133</td>\n",
              "      <td>0.967505</td>\n",
              "      <td>0.943465</td>\n",
              "      <td>0.946552</td>\n",
              "      <td>0.914272</td>\n",
              "      <td>0.976663</td>\n",
              "      <td>0.961514</td>\n",
              "      <td>0.904835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">gpt-4</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.866685</td>\n",
              "      <td>0.706686</td>\n",
              "      <td>0.782835</td>\n",
              "      <td>0.753498</td>\n",
              "      <td>0.495834</td>\n",
              "      <td>0.680546</td>\n",
              "      <td>0.751414</td>\n",
              "      <td>0.691832</td>\n",
              "      <td>0.933297</td>\n",
              "      <td>0.921399</td>\n",
              "      <td>0.812230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.897834</td>\n",
              "      <td>0.897758</td>\n",
              "      <td>0.358748</td>\n",
              "      <td>0.918762</td>\n",
              "      <td>0.871267</td>\n",
              "      <td>0.938269</td>\n",
              "      <td>0.924571</td>\n",
              "      <td>0.926369</td>\n",
              "      <td>0.956649</td>\n",
              "      <td>0.856972</td>\n",
              "      <td>0.707815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.934891</td>\n",
              "      <td>0.555606</td>\n",
              "      <td>0.878788</td>\n",
              "      <td>0.922123</td>\n",
              "      <td>0.925886</td>\n",
              "      <td>0.761128</td>\n",
              "      <td>0.760885</td>\n",
              "      <td>0.693175</td>\n",
              "      <td>0.830468</td>\n",
              "      <td>0.894049</td>\n",
              "      <td>0.779666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.921514</td>\n",
              "      <td>0.959956</td>\n",
              "      <td>0.995000</td>\n",
              "      <td>0.939120</td>\n",
              "      <td>0.976532</td>\n",
              "      <td>0.934855</td>\n",
              "      <td>0.963270</td>\n",
              "      <td>0.936803</td>\n",
              "      <td>0.956667</td>\n",
              "      <td>0.944703</td>\n",
              "      <td>0.921368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">llama-65b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.707838</td>\n",
              "      <td>0.530048</td>\n",
              "      <td>0.875015</td>\n",
              "      <td>0.647655</td>\n",
              "      <td>0.332527</td>\n",
              "      <td>0.821116</td>\n",
              "      <td>0.519080</td>\n",
              "      <td>0.604306</td>\n",
              "      <td>0.984894</td>\n",
              "      <td>0.976421</td>\n",
              "      <td>0.476467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.534740</td>\n",
              "      <td>0.332589</td>\n",
              "      <td>0.615361</td>\n",
              "      <td>0.647033</td>\n",
              "      <td>0.332527</td>\n",
              "      <td>0.496365</td>\n",
              "      <td>0.340351</td>\n",
              "      <td>0.484719</td>\n",
              "      <td>0.978185</td>\n",
              "      <td>0.935939</td>\n",
              "      <td>0.405639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.762908</td>\n",
              "      <td>0.427667</td>\n",
              "      <td>0.984771</td>\n",
              "      <td>0.854020</td>\n",
              "      <td>0.382439</td>\n",
              "      <td>0.730500</td>\n",
              "      <td>0.414468</td>\n",
              "      <td>0.538322</td>\n",
              "      <td>0.988253</td>\n",
              "      <td>0.981477</td>\n",
              "      <td>0.562238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.885839</td>\n",
              "      <td>0.570539</td>\n",
              "      <td>0.986464</td>\n",
              "      <td>0.916035</td>\n",
              "      <td>0.370507</td>\n",
              "      <td>0.502410</td>\n",
              "      <td>0.367375</td>\n",
              "      <td>0.582594</td>\n",
              "      <td>0.993287</td>\n",
              "      <td>0.988215</td>\n",
              "      <td>0.631352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">opt-66b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.759294</td>\n",
              "      <td>0.606131</td>\n",
              "      <td>0.767321</td>\n",
              "      <td>0.607818</td>\n",
              "      <td>0.471954</td>\n",
              "      <td>0.642337</td>\n",
              "      <td>0.642806</td>\n",
              "      <td>0.673385</td>\n",
              "      <td>0.933984</td>\n",
              "      <td>0.925416</td>\n",
              "      <td>0.640839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.971423</td>\n",
              "      <td>0.512291</td>\n",
              "      <td>0.577647</td>\n",
              "      <td>0.399889</td>\n",
              "      <td>0.347268</td>\n",
              "      <td>0.426150</td>\n",
              "      <td>0.604990</td>\n",
              "      <td>0.416472</td>\n",
              "      <td>0.966202</td>\n",
              "      <td>0.981352</td>\n",
              "      <td>0.764907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.939331</td>\n",
              "      <td>0.742819</td>\n",
              "      <td>0.983329</td>\n",
              "      <td>0.486039</td>\n",
              "      <td>0.341306</td>\n",
              "      <td>0.339394</td>\n",
              "      <td>0.625486</td>\n",
              "      <td>0.361007</td>\n",
              "      <td>0.984788</td>\n",
              "      <td>0.993219</td>\n",
              "      <td>0.764493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.954589</td>\n",
              "      <td>0.855203</td>\n",
              "      <td>0.991666</td>\n",
              "      <td>0.554446</td>\n",
              "      <td>0.345255</td>\n",
              "      <td>0.547415</td>\n",
              "      <td>0.773418</td>\n",
              "      <td>0.616838</td>\n",
              "      <td>0.984794</td>\n",
              "      <td>0.986441</td>\n",
              "      <td>0.852985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">opt-iml-max-1.3b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.958780</td>\n",
              "      <td>0.542001</td>\n",
              "      <td>0.870710</td>\n",
              "      <td>0.507309</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.441679</td>\n",
              "      <td>0.537946</td>\n",
              "      <td>0.415349</td>\n",
              "      <td>0.979482</td>\n",
              "      <td>0.982761</td>\n",
              "      <td>0.824275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.963964</td>\n",
              "      <td>0.658299</td>\n",
              "      <td>0.359651</td>\n",
              "      <td>0.561295</td>\n",
              "      <td>0.374969</td>\n",
              "      <td>0.387302</td>\n",
              "      <td>0.837362</td>\n",
              "      <td>0.437818</td>\n",
              "      <td>0.994876</td>\n",
              "      <td>0.981029</td>\n",
              "      <td>0.767807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.890215</td>\n",
              "      <td>0.511446</td>\n",
              "      <td>0.937863</td>\n",
              "      <td>0.576746</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.392633</td>\n",
              "      <td>0.622961</td>\n",
              "      <td>0.398365</td>\n",
              "      <td>0.922422</td>\n",
              "      <td>0.970651</td>\n",
              "      <td>0.833545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.993146</td>\n",
              "      <td>0.727486</td>\n",
              "      <td>0.989965</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.337332</td>\n",
              "      <td>0.394444</td>\n",
              "      <td>0.863783</td>\n",
              "      <td>0.443158</td>\n",
              "      <td>0.996585</td>\n",
              "      <td>0.994832</td>\n",
              "      <td>0.933835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">text-davinci-003</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.748031</td>\n",
              "      <td>0.699821</td>\n",
              "      <td>0.647573</td>\n",
              "      <td>0.808713</td>\n",
              "      <td>0.724933</td>\n",
              "      <td>0.816685</td>\n",
              "      <td>0.715688</td>\n",
              "      <td>0.764468</td>\n",
              "      <td>0.914614</td>\n",
              "      <td>0.907869</td>\n",
              "      <td>0.791094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.879773</td>\n",
              "      <td>0.637560</td>\n",
              "      <td>0.355178</td>\n",
              "      <td>0.862949</td>\n",
              "      <td>0.683093</td>\n",
              "      <td>0.869563</td>\n",
              "      <td>0.798742</td>\n",
              "      <td>0.799616</td>\n",
              "      <td>0.931540</td>\n",
              "      <td>0.819250</td>\n",
              "      <td>0.641026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.880012</td>\n",
              "      <td>0.439919</td>\n",
              "      <td>0.687666</td>\n",
              "      <td>0.583750</td>\n",
              "      <td>0.345255</td>\n",
              "      <td>0.518355</td>\n",
              "      <td>0.484494</td>\n",
              "      <td>0.513155</td>\n",
              "      <td>0.938054</td>\n",
              "      <td>0.959758</td>\n",
              "      <td>0.735809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.866381</td>\n",
              "      <td>0.443979</td>\n",
              "      <td>0.742229</td>\n",
              "      <td>0.711712</td>\n",
              "      <td>0.377834</td>\n",
              "      <td>0.567945</td>\n",
              "      <td>0.589670</td>\n",
              "      <td>0.569955</td>\n",
              "      <td>0.938186</td>\n",
              "      <td>0.949698</td>\n",
              "      <td>0.774999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">vicuna-13b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.545024</td>\n",
              "      <td>0.362297</td>\n",
              "      <td>0.572402</td>\n",
              "      <td>0.454921</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.475122</td>\n",
              "      <td>0.354810</td>\n",
              "      <td>0.372091</td>\n",
              "      <td>0.926421</td>\n",
              "      <td>0.840089</td>\n",
              "      <td>0.347210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.684569</td>\n",
              "      <td>0.340698</td>\n",
              "      <td>0.865946</td>\n",
              "      <td>0.857633</td>\n",
              "      <td>0.372237</td>\n",
              "      <td>0.618774</td>\n",
              "      <td>0.366299</td>\n",
              "      <td>0.549896</td>\n",
              "      <td>0.954849</td>\n",
              "      <td>0.897842</td>\n",
              "      <td>0.544260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.988313</td>\n",
              "      <td>0.797985</td>\n",
              "      <td>0.938098</td>\n",
              "      <td>0.903297</td>\n",
              "      <td>0.813637</td>\n",
              "      <td>0.902333</td>\n",
              "      <td>0.875988</td>\n",
              "      <td>0.846442</td>\n",
              "      <td>0.956463</td>\n",
              "      <td>0.958152</td>\n",
              "      <td>0.859437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.971604</td>\n",
              "      <td>0.752577</td>\n",
              "      <td>0.927963</td>\n",
              "      <td>0.939120</td>\n",
              "      <td>0.857196</td>\n",
              "      <td>0.929760</td>\n",
              "      <td>0.697895</td>\n",
              "      <td>0.841560</td>\n",
              "      <td>0.961481</td>\n",
              "      <td>0.927846</td>\n",
              "      <td>0.899810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"32\" valign=\"top\">all</th>\n",
              "      <th rowspan=\"4\" valign=\"top\">alpaca-lora-30b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.509603</td>\n",
              "      <td>0.913332</td>\n",
              "      <td>0.887860</td>\n",
              "      <td>0.913839</td>\n",
              "      <td>0.965698</td>\n",
              "      <td>0.940064</td>\n",
              "      <td>0.916148</td>\n",
              "      <td>0.911300</td>\n",
              "      <td>0.919999</td>\n",
              "      <td>0.892832</td>\n",
              "      <td>0.747233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.695488</td>\n",
              "      <td>0.883328</td>\n",
              "      <td>0.648040</td>\n",
              "      <td>0.928988</td>\n",
              "      <td>0.958483</td>\n",
              "      <td>0.931506</td>\n",
              "      <td>0.821486</td>\n",
              "      <td>0.933449</td>\n",
              "      <td>0.938332</td>\n",
              "      <td>0.912946</td>\n",
              "      <td>0.555541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.843820</td>\n",
              "      <td>0.806245</td>\n",
              "      <td>0.890609</td>\n",
              "      <td>0.872700</td>\n",
              "      <td>0.907484</td>\n",
              "      <td>0.864293</td>\n",
              "      <td>0.804425</td>\n",
              "      <td>0.836376</td>\n",
              "      <td>0.928324</td>\n",
              "      <td>0.933074</td>\n",
              "      <td>0.711756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.774189</td>\n",
              "      <td>0.928309</td>\n",
              "      <td>0.959972</td>\n",
              "      <td>0.957769</td>\n",
              "      <td>0.985559</td>\n",
              "      <td>0.958900</td>\n",
              "      <td>0.948084</td>\n",
              "      <td>0.952183</td>\n",
              "      <td>0.936622</td>\n",
              "      <td>0.924572</td>\n",
              "      <td>0.819962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">gpt-3.5-turbo</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.848292</td>\n",
              "      <td>0.974999</td>\n",
              "      <td>0.963332</td>\n",
              "      <td>0.939072</td>\n",
              "      <td>0.981949</td>\n",
              "      <td>0.977734</td>\n",
              "      <td>0.981635</td>\n",
              "      <td>0.950596</td>\n",
              "      <td>0.954994</td>\n",
              "      <td>0.931318</td>\n",
              "      <td>0.914828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.885649</td>\n",
              "      <td>0.919799</td>\n",
              "      <td>0.355178</td>\n",
              "      <td>0.896361</td>\n",
              "      <td>0.980143</td>\n",
              "      <td>0.946918</td>\n",
              "      <td>0.956594</td>\n",
              "      <td>0.936968</td>\n",
              "      <td>0.938284</td>\n",
              "      <td>0.893877</td>\n",
              "      <td>0.569309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.946576</td>\n",
              "      <td>0.871004</td>\n",
              "      <td>0.976666</td>\n",
              "      <td>0.957769</td>\n",
              "      <td>0.974728</td>\n",
              "      <td>0.928041</td>\n",
              "      <td>0.897707</td>\n",
              "      <td>0.903930</td>\n",
              "      <td>0.923248</td>\n",
              "      <td>0.939775</td>\n",
              "      <td>0.852572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.928008</td>\n",
              "      <td>0.985000</td>\n",
              "      <td>0.988332</td>\n",
              "      <td>0.942473</td>\n",
              "      <td>0.990974</td>\n",
              "      <td>0.984581</td>\n",
              "      <td>0.979967</td>\n",
              "      <td>0.972736</td>\n",
              "      <td>0.984999</td>\n",
              "      <td>0.946371</td>\n",
              "      <td>0.938168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">gpt-4</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.941559</td>\n",
              "      <td>0.956608</td>\n",
              "      <td>0.953308</td>\n",
              "      <td>0.925458</td>\n",
              "      <td>0.992780</td>\n",
              "      <td>0.948512</td>\n",
              "      <td>0.968277</td>\n",
              "      <td>0.945291</td>\n",
              "      <td>0.939983</td>\n",
              "      <td>0.936452</td>\n",
              "      <td>0.904807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.891135</td>\n",
              "      <td>0.944988</td>\n",
              "      <td>0.340698</td>\n",
              "      <td>0.901432</td>\n",
              "      <td>0.992779</td>\n",
              "      <td>0.964017</td>\n",
              "      <td>0.924571</td>\n",
              "      <td>0.948731</td>\n",
              "      <td>0.946666</td>\n",
              "      <td>0.921219</td>\n",
              "      <td>0.723636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.938224</td>\n",
              "      <td>0.931459</td>\n",
              "      <td>0.988333</td>\n",
              "      <td>0.979728</td>\n",
              "      <td>0.945747</td>\n",
              "      <td>0.950183</td>\n",
              "      <td>0.958217</td>\n",
              "      <td>0.919479</td>\n",
              "      <td>0.938295</td>\n",
              "      <td>0.949787</td>\n",
              "      <td>0.885362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.959932</td>\n",
              "      <td>0.973326</td>\n",
              "      <td>0.985000</td>\n",
              "      <td>0.969595</td>\n",
              "      <td>0.994585</td>\n",
              "      <td>0.969142</td>\n",
              "      <td>0.981636</td>\n",
              "      <td>0.982950</td>\n",
              "      <td>0.973332</td>\n",
              "      <td>0.983277</td>\n",
              "      <td>0.936548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">llama-65b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.782089</td>\n",
              "      <td>0.864468</td>\n",
              "      <td>0.928762</td>\n",
              "      <td>0.786738</td>\n",
              "      <td>0.907606</td>\n",
              "      <td>0.987940</td>\n",
              "      <td>0.783536</td>\n",
              "      <td>0.944907</td>\n",
              "      <td>0.978180</td>\n",
              "      <td>0.978111</td>\n",
              "      <td>0.658036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.885282</td>\n",
              "      <td>0.950910</td>\n",
              "      <td>0.905754</td>\n",
              "      <td>0.929281</td>\n",
              "      <td>0.986227</td>\n",
              "      <td>0.719234</td>\n",
              "      <td>0.969059</td>\n",
              "      <td>0.979860</td>\n",
              "      <td>0.973061</td>\n",
              "      <td>0.533715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.800609</td>\n",
              "      <td>0.888725</td>\n",
              "      <td>0.983078</td>\n",
              "      <td>0.806141</td>\n",
              "      <td>0.900354</td>\n",
              "      <td>0.982770</td>\n",
              "      <td>0.645039</td>\n",
              "      <td>0.953522</td>\n",
              "      <td>0.989933</td>\n",
              "      <td>0.966299</td>\n",
              "      <td>0.597399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.744559</td>\n",
              "      <td>0.801332</td>\n",
              "      <td>0.937253</td>\n",
              "      <td>0.882691</td>\n",
              "      <td>0.898359</td>\n",
              "      <td>0.991388</td>\n",
              "      <td>0.467301</td>\n",
              "      <td>0.944806</td>\n",
              "      <td>0.989932</td>\n",
              "      <td>0.979792</td>\n",
              "      <td>0.599157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">opt-66b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.796683</td>\n",
              "      <td>0.721942</td>\n",
              "      <td>0.767449</td>\n",
              "      <td>0.728228</td>\n",
              "      <td>0.871580</td>\n",
              "      <td>0.809685</td>\n",
              "      <td>0.688022</td>\n",
              "      <td>0.825600</td>\n",
              "      <td>0.916831</td>\n",
              "      <td>0.899965</td>\n",
              "      <td>0.773006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.906852</td>\n",
              "      <td>0.844007</td>\n",
              "      <td>0.801006</td>\n",
              "      <td>0.818552</td>\n",
              "      <td>0.904262</td>\n",
              "      <td>0.862601</td>\n",
              "      <td>0.759110</td>\n",
              "      <td>0.801186</td>\n",
              "      <td>0.974662</td>\n",
              "      <td>0.972880</td>\n",
              "      <td>0.692877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.891294</td>\n",
              "      <td>0.939626</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.897798</td>\n",
              "      <td>0.847868</td>\n",
              "      <td>0.884706</td>\n",
              "      <td>0.876344</td>\n",
              "      <td>0.864558</td>\n",
              "      <td>0.988175</td>\n",
              "      <td>0.983051</td>\n",
              "      <td>0.807321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.941006</td>\n",
              "      <td>0.951433</td>\n",
              "      <td>0.996667</td>\n",
              "      <td>0.834821</td>\n",
              "      <td>0.887941</td>\n",
              "      <td>0.869471</td>\n",
              "      <td>0.895853</td>\n",
              "      <td>0.921224</td>\n",
              "      <td>0.974624</td>\n",
              "      <td>0.996610</td>\n",
              "      <td>0.780601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">opt-iml-max-1.3b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.950318</td>\n",
              "      <td>0.932673</td>\n",
              "      <td>0.959830</td>\n",
              "      <td>0.838827</td>\n",
              "      <td>0.920568</td>\n",
              "      <td>0.921907</td>\n",
              "      <td>0.900615</td>\n",
              "      <td>0.922375</td>\n",
              "      <td>0.986342</td>\n",
              "      <td>0.984503</td>\n",
              "      <td>0.949368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.976027</td>\n",
              "      <td>0.959650</td>\n",
              "      <td>0.924281</td>\n",
              "      <td>0.952422</td>\n",
              "      <td>0.956678</td>\n",
              "      <td>0.979273</td>\n",
              "      <td>0.956452</td>\n",
              "      <td>0.970687</td>\n",
              "      <td>0.998292</td>\n",
              "      <td>0.996555</td>\n",
              "      <td>0.768063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.976027</td>\n",
              "      <td>0.988235</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.957543</td>\n",
              "      <td>0.920511</td>\n",
              "      <td>0.967184</td>\n",
              "      <td>0.973225</td>\n",
              "      <td>0.981028</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996555</td>\n",
              "      <td>0.799917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.986297</td>\n",
              "      <td>0.996638</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.993208</td>\n",
              "      <td>0.905427</td>\n",
              "      <td>0.968910</td>\n",
              "      <td>0.991639</td>\n",
              "      <td>0.970689</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998278</td>\n",
              "      <td>0.807778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">text-davinci-003</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.824558</td>\n",
              "      <td>0.958319</td>\n",
              "      <td>0.934904</td>\n",
              "      <td>0.952694</td>\n",
              "      <td>0.963876</td>\n",
              "      <td>0.963975</td>\n",
              "      <td>0.961587</td>\n",
              "      <td>0.935083</td>\n",
              "      <td>0.909843</td>\n",
              "      <td>0.877524</td>\n",
              "      <td>0.918315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.846222</td>\n",
              "      <td>0.684687</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.972973</td>\n",
              "      <td>0.980143</td>\n",
              "      <td>0.974306</td>\n",
              "      <td>0.959930</td>\n",
              "      <td>0.943779</td>\n",
              "      <td>0.949885</td>\n",
              "      <td>0.839593</td>\n",
              "      <td>0.844200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.759815</td>\n",
              "      <td>0.853584</td>\n",
              "      <td>0.951569</td>\n",
              "      <td>0.911799</td>\n",
              "      <td>0.896429</td>\n",
              "      <td>0.844561</td>\n",
              "      <td>0.875486</td>\n",
              "      <td>0.889139</td>\n",
              "      <td>0.934889</td>\n",
              "      <td>0.924432</td>\n",
              "      <td>0.933065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.670553</td>\n",
              "      <td>0.793827</td>\n",
              "      <td>0.887007</td>\n",
              "      <td>0.935667</td>\n",
              "      <td>0.969302</td>\n",
              "      <td>0.929794</td>\n",
              "      <td>0.964938</td>\n",
              "      <td>0.906194</td>\n",
              "      <td>0.904119</td>\n",
              "      <td>0.833569</td>\n",
              "      <td>0.671403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">vicuna-13b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.829439</td>\n",
              "      <td>0.953291</td>\n",
              "      <td>0.896251</td>\n",
              "      <td>0.890177</td>\n",
              "      <td>0.960288</td>\n",
              "      <td>0.948608</td>\n",
              "      <td>0.959933</td>\n",
              "      <td>0.942030</td>\n",
              "      <td>0.939775</td>\n",
              "      <td>0.892804</td>\n",
              "      <td>0.717279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.846662</td>\n",
              "      <td>0.862044</td>\n",
              "      <td>0.509959</td>\n",
              "      <td>0.903105</td>\n",
              "      <td>0.980139</td>\n",
              "      <td>0.939827</td>\n",
              "      <td>0.890662</td>\n",
              "      <td>0.935210</td>\n",
              "      <td>0.941314</td>\n",
              "      <td>0.845123</td>\n",
              "      <td>0.652242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.854263</td>\n",
              "      <td>0.807752</td>\n",
              "      <td>0.899057</td>\n",
              "      <td>0.920361</td>\n",
              "      <td>0.890658</td>\n",
              "      <td>0.841004</td>\n",
              "      <td>0.859559</td>\n",
              "      <td>0.872052</td>\n",
              "      <td>0.936295</td>\n",
              "      <td>0.936409</td>\n",
              "      <td>0.701151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.880938</td>\n",
              "      <td>0.951656</td>\n",
              "      <td>0.934778</td>\n",
              "      <td>0.959448</td>\n",
              "      <td>0.981949</td>\n",
              "      <td>0.970876</td>\n",
              "      <td>0.917866</td>\n",
              "      <td>0.921323</td>\n",
              "      <td>0.974913</td>\n",
              "      <td>0.956520</td>\n",
              "      <td>0.744186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"32\" valign=\"top\">en3</th>\n",
              "      <th rowspan=\"4\" valign=\"top\">alpaca-lora-30b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.495071</td>\n",
              "      <td>0.874293</td>\n",
              "      <td>0.859961</td>\n",
              "      <td>0.905336</td>\n",
              "      <td>0.983754</td>\n",
              "      <td>0.761689</td>\n",
              "      <td>0.907913</td>\n",
              "      <td>0.857861</td>\n",
              "      <td>0.846113</td>\n",
              "      <td>0.814381</td>\n",
              "      <td>0.571886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.293049</td>\n",
              "      <td>0.762298</td>\n",
              "      <td>0.495798</td>\n",
              "      <td>0.885029</td>\n",
              "      <td>0.980143</td>\n",
              "      <td>0.736009</td>\n",
              "      <td>0.859440</td>\n",
              "      <td>0.865471</td>\n",
              "      <td>0.820896</td>\n",
              "      <td>0.804347</td>\n",
              "      <td>0.426802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.176374</td>\n",
              "      <td>0.687197</td>\n",
              "      <td>0.809567</td>\n",
              "      <td>0.852901</td>\n",
              "      <td>0.938548</td>\n",
              "      <td>0.635581</td>\n",
              "      <td>0.788842</td>\n",
              "      <td>0.734008</td>\n",
              "      <td>0.733177</td>\n",
              "      <td>0.682376</td>\n",
              "      <td>0.417706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.330592</td>\n",
              "      <td>0.837792</td>\n",
              "      <td>0.878184</td>\n",
              "      <td>0.942514</td>\n",
              "      <td>0.987364</td>\n",
              "      <td>0.698462</td>\n",
              "      <td>0.900725</td>\n",
              "      <td>0.787154</td>\n",
              "      <td>0.829411</td>\n",
              "      <td>0.852366</td>\n",
              "      <td>0.598436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">gpt-3.5-turbo</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.944893</td>\n",
              "      <td>0.917785</td>\n",
              "      <td>0.938332</td>\n",
              "      <td>0.925675</td>\n",
              "      <td>0.981949</td>\n",
              "      <td>0.871843</td>\n",
              "      <td>0.949876</td>\n",
              "      <td>0.929827</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.894599</td>\n",
              "      <td>0.908321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.678622</td>\n",
              "      <td>0.908333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.904880</td>\n",
              "      <td>0.987364</td>\n",
              "      <td>0.887388</td>\n",
              "      <td>0.963271</td>\n",
              "      <td>0.928358</td>\n",
              "      <td>0.923248</td>\n",
              "      <td>0.886038</td>\n",
              "      <td>0.549381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.814844</td>\n",
              "      <td>0.735275</td>\n",
              "      <td>0.815809</td>\n",
              "      <td>0.878317</td>\n",
              "      <td>0.925760</td>\n",
              "      <td>0.702010</td>\n",
              "      <td>0.826702</td>\n",
              "      <td>0.783397</td>\n",
              "      <td>0.676881</td>\n",
              "      <td>0.734812</td>\n",
              "      <td>0.729492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.919866</td>\n",
              "      <td>0.979999</td>\n",
              "      <td>0.974997</td>\n",
              "      <td>0.940817</td>\n",
              "      <td>0.983752</td>\n",
              "      <td>0.919230</td>\n",
              "      <td>0.954912</td>\n",
              "      <td>0.938500</td>\n",
              "      <td>0.951660</td>\n",
              "      <td>0.946450</td>\n",
              "      <td>0.796341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">gpt-4</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.834971</td>\n",
              "      <td>0.900893</td>\n",
              "      <td>0.919999</td>\n",
              "      <td>0.930727</td>\n",
              "      <td>0.996390</td>\n",
              "      <td>0.732342</td>\n",
              "      <td>0.938175</td>\n",
              "      <td>0.838871</td>\n",
              "      <td>0.876579</td>\n",
              "      <td>0.866167</td>\n",
              "      <td>0.807944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.848069</td>\n",
              "      <td>0.929888</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.910430</td>\n",
              "      <td>0.992780</td>\n",
              "      <td>0.795138</td>\n",
              "      <td>0.931403</td>\n",
              "      <td>0.907192</td>\n",
              "      <td>0.879774</td>\n",
              "      <td>0.806716</td>\n",
              "      <td>0.657042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.819887</td>\n",
              "      <td>0.724894</td>\n",
              "      <td>0.907612</td>\n",
              "      <td>0.867904</td>\n",
              "      <td>0.629051</td>\n",
              "      <td>0.650597</td>\n",
              "      <td>0.802330</td>\n",
              "      <td>0.706365</td>\n",
              "      <td>0.779360</td>\n",
              "      <td>0.827100</td>\n",
              "      <td>0.693959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.877966</td>\n",
              "      <td>0.909319</td>\n",
              "      <td>0.956643</td>\n",
              "      <td>0.964522</td>\n",
              "      <td>0.994585</td>\n",
              "      <td>0.828572</td>\n",
              "      <td>0.924821</td>\n",
              "      <td>0.880757</td>\n",
              "      <td>0.906563</td>\n",
              "      <td>0.929765</td>\n",
              "      <td>0.861447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">llama-65b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.655237</td>\n",
              "      <td>0.641660</td>\n",
              "      <td>0.797071</td>\n",
              "      <td>0.701859</td>\n",
              "      <td>0.921855</td>\n",
              "      <td>0.805686</td>\n",
              "      <td>0.637019</td>\n",
              "      <td>0.687980</td>\n",
              "      <td>0.701551</td>\n",
              "      <td>0.803135</td>\n",
              "      <td>0.565878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.336877</td>\n",
              "      <td>0.361560</td>\n",
              "      <td>0.776111</td>\n",
              "      <td>0.715696</td>\n",
              "      <td>0.914707</td>\n",
              "      <td>0.551698</td>\n",
              "      <td>0.459106</td>\n",
              "      <td>0.561643</td>\n",
              "      <td>0.484584</td>\n",
              "      <td>0.513736</td>\n",
              "      <td>0.555561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.747725</td>\n",
              "      <td>0.664557</td>\n",
              "      <td>0.927212</td>\n",
              "      <td>0.756037</td>\n",
              "      <td>0.895919</td>\n",
              "      <td>0.866191</td>\n",
              "      <td>0.682070</td>\n",
              "      <td>0.717597</td>\n",
              "      <td>0.773435</td>\n",
              "      <td>0.853296</td>\n",
              "      <td>0.590222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.453154</td>\n",
              "      <td>0.407351</td>\n",
              "      <td>0.841352</td>\n",
              "      <td>0.719821</td>\n",
              "      <td>0.938366</td>\n",
              "      <td>0.639744</td>\n",
              "      <td>0.407951</td>\n",
              "      <td>0.553549</td>\n",
              "      <td>0.637093</td>\n",
              "      <td>0.655976</td>\n",
              "      <td>0.448793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">opt-66b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.412144</td>\n",
              "      <td>0.524086</td>\n",
              "      <td>0.684992</td>\n",
              "      <td>0.678879</td>\n",
              "      <td>0.907918</td>\n",
              "      <td>0.555801</td>\n",
              "      <td>0.527395</td>\n",
              "      <td>0.637663</td>\n",
              "      <td>0.610414</td>\n",
              "      <td>0.647736</td>\n",
              "      <td>0.532262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.198296</td>\n",
              "      <td>0.589050</td>\n",
              "      <td>0.570750</td>\n",
              "      <td>0.656421</td>\n",
              "      <td>0.918517</td>\n",
              "      <td>0.560484</td>\n",
              "      <td>0.582609</td>\n",
              "      <td>0.644910</td>\n",
              "      <td>0.446283</td>\n",
              "      <td>0.498090</td>\n",
              "      <td>0.446363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.261175</td>\n",
              "      <td>0.431624</td>\n",
              "      <td>0.531647</td>\n",
              "      <td>0.625698</td>\n",
              "      <td>0.771954</td>\n",
              "      <td>0.471266</td>\n",
              "      <td>0.418367</td>\n",
              "      <td>0.389667</td>\n",
              "      <td>0.550282</td>\n",
              "      <td>0.471574</td>\n",
              "      <td>0.530702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.418210</td>\n",
              "      <td>0.408885</td>\n",
              "      <td>0.602140</td>\n",
              "      <td>0.675034</td>\n",
              "      <td>0.920452</td>\n",
              "      <td>0.587124</td>\n",
              "      <td>0.420474</td>\n",
              "      <td>0.593539</td>\n",
              "      <td>0.588398</td>\n",
              "      <td>0.622024</td>\n",
              "      <td>0.517499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">opt-iml-max-1.3b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.570502</td>\n",
              "      <td>0.637249</td>\n",
              "      <td>0.708583</td>\n",
              "      <td>0.700335</td>\n",
              "      <td>0.954856</td>\n",
              "      <td>0.632731</td>\n",
              "      <td>0.670498</td>\n",
              "      <td>0.644652</td>\n",
              "      <td>0.550129</td>\n",
              "      <td>0.637987</td>\n",
              "      <td>0.518781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.260741</td>\n",
              "      <td>0.566327</td>\n",
              "      <td>0.573060</td>\n",
              "      <td>0.636859</td>\n",
              "      <td>0.972924</td>\n",
              "      <td>0.588083</td>\n",
              "      <td>0.590092</td>\n",
              "      <td>0.495213</td>\n",
              "      <td>0.256258</td>\n",
              "      <td>0.253658</td>\n",
              "      <td>0.415553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.730906</td>\n",
              "      <td>0.861843</td>\n",
              "      <td>0.921165</td>\n",
              "      <td>0.815227</td>\n",
              "      <td>0.972922</td>\n",
              "      <td>0.772649</td>\n",
              "      <td>0.855827</td>\n",
              "      <td>0.739090</td>\n",
              "      <td>0.754218</td>\n",
              "      <td>0.847703</td>\n",
              "      <td>0.732787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.669366</td>\n",
              "      <td>0.739327</td>\n",
              "      <td>0.742273</td>\n",
              "      <td>0.812958</td>\n",
              "      <td>0.971110</td>\n",
              "      <td>0.755262</td>\n",
              "      <td>0.619457</td>\n",
              "      <td>0.693278</td>\n",
              "      <td>0.568899</td>\n",
              "      <td>0.597704</td>\n",
              "      <td>0.616519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">text-davinci-003</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.785305</td>\n",
              "      <td>0.906366</td>\n",
              "      <td>0.881461</td>\n",
              "      <td>0.895068</td>\n",
              "      <td>0.969313</td>\n",
              "      <td>0.798652</td>\n",
              "      <td>0.911471</td>\n",
              "      <td>0.863244</td>\n",
              "      <td>0.777323</td>\n",
              "      <td>0.733287</td>\n",
              "      <td>0.888281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.613629</td>\n",
              "      <td>0.893237</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.870103</td>\n",
              "      <td>0.989169</td>\n",
              "      <td>0.848392</td>\n",
              "      <td>0.938219</td>\n",
              "      <td>0.907865</td>\n",
              "      <td>0.693971</td>\n",
              "      <td>0.568728</td>\n",
              "      <td>0.590378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.579101</td>\n",
              "      <td>0.767995</td>\n",
              "      <td>0.893975</td>\n",
              "      <td>0.889506</td>\n",
              "      <td>0.907435</td>\n",
              "      <td>0.640090</td>\n",
              "      <td>0.874799</td>\n",
              "      <td>0.772539</td>\n",
              "      <td>0.734054</td>\n",
              "      <td>0.612957</td>\n",
              "      <td>0.719948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.389863</td>\n",
              "      <td>0.904195</td>\n",
              "      <td>0.526789</td>\n",
              "      <td>0.901578</td>\n",
              "      <td>0.985559</td>\n",
              "      <td>0.832714</td>\n",
              "      <td>0.948226</td>\n",
              "      <td>0.887163</td>\n",
              "      <td>0.598078</td>\n",
              "      <td>0.393205</td>\n",
              "      <td>0.398708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">vicuna-13b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.767944</td>\n",
              "      <td>0.882074</td>\n",
              "      <td>0.836601</td>\n",
              "      <td>0.840988</td>\n",
              "      <td>0.987365</td>\n",
              "      <td>0.786081</td>\n",
              "      <td>0.938175</td>\n",
              "      <td>0.821761</td>\n",
              "      <td>0.833505</td>\n",
              "      <td>0.837781</td>\n",
              "      <td>0.637889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.388148</td>\n",
              "      <td>0.907860</td>\n",
              "      <td>0.488887</td>\n",
              "      <td>0.825447</td>\n",
              "      <td>0.990975</td>\n",
              "      <td>0.696255</td>\n",
              "      <td>0.916054</td>\n",
              "      <td>0.804291</td>\n",
              "      <td>0.636147</td>\n",
              "      <td>0.721517</td>\n",
              "      <td>0.677753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.257780</td>\n",
              "      <td>0.786145</td>\n",
              "      <td>0.698312</td>\n",
              "      <td>0.822525</td>\n",
              "      <td>0.792751</td>\n",
              "      <td>0.614244</td>\n",
              "      <td>0.880938</td>\n",
              "      <td>0.706436</td>\n",
              "      <td>0.603137</td>\n",
              "      <td>0.676501</td>\n",
              "      <td>0.548784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.406231</td>\n",
              "      <td>0.900893</td>\n",
              "      <td>0.764844</td>\n",
              "      <td>0.932370</td>\n",
              "      <td>0.989170</td>\n",
              "      <td>0.676076</td>\n",
              "      <td>0.861264</td>\n",
              "      <td>0.744415</td>\n",
              "      <td>0.761834</td>\n",
              "      <td>0.786622</td>\n",
              "      <td>0.759474</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59b69970-6294-4772-9a63-b48605ddf30e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-59b69970-6294-4772-9a63-b48605ddf30e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-59b69970-6294-4772-9a63-b48605ddf30e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9e6f65b1-c8ba-4f78-b58d-8b04cc5e0c4f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9e6f65b1-c8ba-4f78-b58d-8b04cc5e0c4f')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9e6f65b1-c8ba-4f78-b58d-8b04cc5e0c4f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Results just for models fine-tuned on English, on English vs Non-English\n",
        "temp = results_all.loc['en',:]\n",
        "means = pd.DataFrame()\n",
        "means = pd.concat([means, temp.agg('sum')], copy=False, axis=1)\n",
        "means.rename(columns={0:'All Detectors Mean'}, inplace=True)\n",
        "means = pd.concat([means, temp.loc[[x in multilingual for x in temp.reset_index().Model], :].agg('sum')], copy=False, axis=1)\n",
        "means.rename(columns={0:'Multilingual Base Models Mean'}, inplace=True)\n",
        "means = pd.concat([means, temp.loc[[x not in multilingual for x in temp.reset_index().Model], :].agg('sum')], copy=False, axis=1)\n",
        "means.rename(columns={0:'Monolingual Base Models Mean'}, inplace=True)\n",
        "means = means.T\n",
        "\n",
        "#!!!numbers need update for new detectors\n",
        "#en: 56, 32, 24\n",
        "#others: 560, 320, 24\n",
        "counts = pd.DataFrame([[56,32,24], [560,320,240]], index=['English', 'Non-English'], columns=['All Detectors Mean',\t'Multilingual Base Models Mean',\t'Monolingual Base Models Mean'])\n",
        "temp = means.T.reset_index()\n",
        "temp['others'] = ~temp['index'].str.contains('en')\n",
        "stat = temp.groupby('others').agg('sum')\n",
        "stat.rename(index={False:'English', True:'Non-English'}, inplace=True)\n",
        "stat.div(counts).T"
      ],
      "metadata": {
        "id": "7DWj2-D72pIO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "ee427e59-e08b-438f-daf3-c3cef52a5bde"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-e439636a3f9a>:18: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
            "  stat = temp.groupby('others').agg('sum')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "others                          English  Non-English\n",
              "All Detectors Mean             0.936122     0.598855\n",
              "Multilingual Base Models Mean  0.929195     0.690418\n",
              "Monolingual Base Models Mean   0.945359     0.476772"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4af2216f-42df-4947-a249-ca4fa7e7ebac\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>others</th>\n",
              "      <th>English</th>\n",
              "      <th>Non-English</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>All Detectors Mean</th>\n",
              "      <td>0.936122</td>\n",
              "      <td>0.598855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Multilingual Base Models Mean</th>\n",
              "      <td>0.929195</td>\n",
              "      <td>0.690418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Monolingual Base Models Mean</th>\n",
              "      <td>0.945359</td>\n",
              "      <td>0.476772</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4af2216f-42df-4947-a249-ca4fa7e7ebac')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4af2216f-42df-4947-a249-ca4fa7e7ebac button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4af2216f-42df-4947-a249-ca4fa7e7ebac');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e3b5747d-2e1f-482c-9863-5fef01cea67b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e3b5747d-2e1f-482c-9863-5fef01cea67b')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e3b5747d-2e1f-482c-9863-5fef01cea67b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Results just for models fine-tuned on all languages\n",
        "temp = results_all.loc['all',:]\n",
        "display(temp.style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4))\n",
        "print(temp.style.format(na_rep=0, precision=4).highlight_max(props='font-weight: bold;', axis=1).applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True))\n",
        "\n",
        "means = pd.DataFrame()\n",
        "means = pd.concat([means, temp.agg('mean')], copy=False, axis=1)\n",
        "means.rename(columns={0:'All Detectors Mean'}, inplace=True)\n",
        "means = pd.concat([means, temp.loc[[x in multilingual for x in temp.reset_index().Model], :].agg('mean')], copy=False, axis=1)\n",
        "means.rename(columns={0:'Multilingual Base Models Mean'}, inplace=True)\n",
        "means = pd.concat([means, temp.loc[[x not in multilingual for x in temp.reset_index().Model], :].agg('mean')], copy=False, axis=1)\n",
        "means.rename(columns={0:'Monolingual Base Models Mean'}, inplace=True)\n",
        "means = means.T.style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4)\n",
        "display(means)\n",
        "temp = means.applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True)\n",
        "#little hack to properly format latex table\n",
        "print(temp.replace('\\\\color[HTML]{F1F1F1} ', '\\\\textcolor{white}{').replace('\\\\color[HTML]{000000} ', '\\\\textcolor{black}{').replace(' & {\\\\cellcolor', '} & {\\\\cellcolor').replace(' \\\\\\\\', '} \\\\\\\\').replace('\\n\\\\bfseries', '\\n\\\\multicolumn{2}{r|}{\\\\bfseries').replace('zh} \\\\\\\\', 'zh \\\\\\\\'))"
      ],
      "metadata": {
        "id": "gkyyVabPnfso",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f65014ea-04f1-4434-e324-1d6e40ef5941"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001ce53c0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_455f9_row0_col0, #T_455f9_row52_col2 {\n",
              "  background-color: #ced0e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row0_col1, #T_455f9_row0_col3, #T_455f9_row0_col7, #T_455f9_row3_col9, #T_455f9_row4_col4, #T_455f9_row21_col4, #T_455f9_row30_col8, #T_455f9_row31_col0, #T_455f9_row33_col4, #T_455f9_row42_col8, #T_455f9_row46_col3, #T_455f9_row54_col3 {\n",
              "  background-color: #86b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row0_col2, #T_455f9_row3_col1, #T_455f9_row4_col2, #T_455f9_row10_col0, #T_455f9_row18_col10, #T_455f9_row24_col1, #T_455f9_row25_col1, #T_455f9_row30_col4, #T_455f9_row30_col9, #T_455f9_row32_col5, #T_455f9_row34_col4, #T_455f9_row46_col7, #T_455f9_row48_col2, #T_455f9_row49_col3 {\n",
              "  background-color: #8bb2d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row0_col4, #T_455f9_row7_col2, #T_455f9_row8_col5, #T_455f9_row14_col6, #T_455f9_row17_col5, #T_455f9_row19_col5, #T_455f9_row23_col1, #T_455f9_row25_col9, #T_455f9_row36_col5, #T_455f9_row39_col5, #T_455f9_row40_col4, #T_455f9_row42_col4, #T_455f9_row42_col5, #T_455f9_row42_col6, #T_455f9_row43_col5, #T_455f9_row48_col6 {\n",
              "  background-color: #7bacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row0_col5, #T_455f9_row3_col8, #T_455f9_row7_col3, #T_455f9_row9_col5, #T_455f9_row10_col8, #T_455f9_row11_col9, #T_455f9_row13_col3, #T_455f9_row13_col10, #T_455f9_row14_col0, #T_455f9_row14_col7, #T_455f9_row14_col8, #T_455f9_row17_col1, #T_455f9_row18_col0, #T_455f9_row18_col8, #T_455f9_row21_col7, #T_455f9_row22_col9, #T_455f9_row27_col7, #T_455f9_row32_col1, #T_455f9_row34_col0, #T_455f9_row36_col6, #T_455f9_row36_col7, #T_455f9_row40_col3, #T_455f9_row45_col7, #T_455f9_row49_col7, #T_455f9_row49_col8, #T_455f9_row52_col5, #T_455f9_row52_col8 {\n",
              "  background-color: #80aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row0_col6, #T_455f9_row0_col8, #T_455f9_row7_col10, #T_455f9_row9_col1, #T_455f9_row10_col1, #T_455f9_row17_col9, #T_455f9_row18_col7, #T_455f9_row22_col8, #T_455f9_row28_col8, #T_455f9_row34_col7, #T_455f9_row35_col4, #T_455f9_row37_col6, #T_455f9_row39_col4, #T_455f9_row40_col8, #T_455f9_row40_col9, #T_455f9_row42_col10, #T_455f9_row50_col7, #T_455f9_row53_col3, #T_455f9_row54_col1, #T_455f9_row55_col6, #T_455f9_row55_col7 {\n",
              "  background-color: #84b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row0_col9, #T_455f9_row2_col5, #T_455f9_row5_col7, #T_455f9_row10_col3, #T_455f9_row10_col9, #T_455f9_row11_col6, #T_455f9_row12_col8, #T_455f9_row16_col1, #T_455f9_row17_col0, #T_455f9_row19_col3, #T_455f9_row22_col4, #T_455f9_row24_col0, #T_455f9_row27_col4, #T_455f9_row29_col4, #T_455f9_row32_col0, #T_455f9_row32_col3, #T_455f9_row34_col6, #T_455f9_row46_col4, #T_455f9_row49_col2, #T_455f9_row49_col9, #T_455f9_row52_col6, #T_455f9_row53_col4, #T_455f9_row54_col6 {\n",
              "  background-color: #89b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row0_col10, #T_455f9_row1_col1, #T_455f9_row27_col0, #T_455f9_row51_col6, #T_455f9_row55_col10 {\n",
              "  background-color: #a7bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row1_col0, #T_455f9_row1_col8, #T_455f9_row16_col10, #T_455f9_row17_col2, #T_455f9_row29_col8, #T_455f9_row29_col9, #T_455f9_row36_col0, #T_455f9_row36_col8, #T_455f9_row36_col9, #T_455f9_row43_col3, #T_455f9_row50_col9 {\n",
              "  background-color: #e2dfee;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row1_col2, #T_455f9_row36_col10 {\n",
              "  background-color: #d7d6e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row1_col3 {\n",
              "  background-color: #d0d1e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row1_col4, #T_455f9_row6_col4, #T_455f9_row9_col4, #T_455f9_row13_col1, #T_455f9_row13_col2, #T_455f9_row13_col4, #T_455f9_row13_col5, #T_455f9_row13_col8, #T_455f9_row18_col2, #T_455f9_row20_col2, #T_455f9_row21_col5, #T_455f9_row22_col5, #T_455f9_row23_col5, #T_455f9_row24_col5, #T_455f9_row25_col8, #T_455f9_row26_col5, #T_455f9_row27_col5, #T_455f9_row27_col8, #T_455f9_row32_col8, #T_455f9_row35_col8, #T_455f9_row35_col9, #T_455f9_row39_col1, #T_455f9_row41_col0, #T_455f9_row41_col6, #T_455f9_row50_col4 {\n",
              "  background-color: #76aad0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row1_col5, #T_455f9_row6_col6, #T_455f9_row6_col7, #T_455f9_row7_col7, #T_455f9_row10_col5, #T_455f9_row11_col0, #T_455f9_row13_col9, #T_455f9_row14_col5, #T_455f9_row15_col7, #T_455f9_row17_col7, #T_455f9_row17_col8, #T_455f9_row18_col4, #T_455f9_row18_col5, #T_455f9_row18_col9, #T_455f9_row22_col3, #T_455f9_row23_col2, #T_455f9_row23_col3, #T_455f9_row24_col2, #T_455f9_row34_col1, #T_455f9_row35_col0, #T_455f9_row35_col10, #T_455f9_row38_col3, #T_455f9_row40_col1, #T_455f9_row42_col3, #T_455f9_row45_col8, #T_455f9_row46_col2, #T_455f9_row49_col5, #T_455f9_row55_col1 {\n",
              "  background-color: #7eadd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row1_col6, #T_455f9_row28_col2, #T_455f9_row28_col10, #T_455f9_row38_col10 {\n",
              "  background-color: #a2bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row1_col7, #T_455f9_row5_col8, #T_455f9_row27_col3, #T_455f9_row32_col6, #T_455f9_row33_col9, #T_455f9_row36_col2, #T_455f9_row36_col3, #T_455f9_row42_col9, #T_455f9_row46_col6, #T_455f9_row47_col3, #T_455f9_row47_col7, #T_455f9_row55_col0 {\n",
              "  background-color: #8cb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row1_col9, #T_455f9_row8_col2, #T_455f9_row8_col3, #T_455f9_row8_col8, #T_455f9_row8_col9, #T_455f9_row9_col10, #T_455f9_row15_col0, #T_455f9_row15_col2, #T_455f9_row15_col8, #T_455f9_row15_col9, #T_455f9_row15_col10, #T_455f9_row43_col0, #T_455f9_row43_col2, #T_455f9_row43_col8, #T_455f9_row43_col9, #T_455f9_row45_col2 {\n",
              "  background-color: #e3e0ee;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row1_col10 {\n",
              "  background-color: #dbdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row2_col0, #T_455f9_row2_col6, #T_455f9_row4_col1, #T_455f9_row25_col3, #T_455f9_row28_col5, #T_455f9_row30_col1, #T_455f9_row32_col10, #T_455f9_row41_col10, #T_455f9_row53_col1 {\n",
              "  background-color: #9ab8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row2_col1, #T_455f9_row8_col1, #T_455f9_row28_col0, #T_455f9_row29_col5, #T_455f9_row30_col5, #T_455f9_row33_col3, #T_455f9_row44_col6, #T_455f9_row48_col1 {\n",
              "  background-color: #9ebad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row2_col2, #T_455f9_row21_col10, #T_455f9_row23_col0, #T_455f9_row37_col10, #T_455f9_row40_col0, #T_455f9_row40_col10, #T_455f9_row51_col2 {\n",
              "  background-color: #b5c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row2_col3, #T_455f9_row6_col0, #T_455f9_row19_col6, #T_455f9_row34_col10 {\n",
              "  background-color: #a1bbda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row2_col4, #T_455f9_row7_col4, #T_455f9_row7_col5, #T_455f9_row7_col6, #T_455f9_row8_col4, #T_455f9_row10_col4, #T_455f9_row11_col2, #T_455f9_row13_col6, #T_455f9_row18_col3, #T_455f9_row19_col4, #T_455f9_row20_col6, #T_455f9_row20_col7, #T_455f9_row20_col9, #T_455f9_row21_col8, #T_455f9_row21_col9, #T_455f9_row24_col8, #T_455f9_row25_col2, #T_455f9_row25_col5, #T_455f9_row26_col1, #T_455f9_row26_col9, #T_455f9_row27_col9, #T_455f9_row32_col9, #T_455f9_row36_col4, #T_455f9_row37_col1, #T_455f9_row38_col5, #T_455f9_row39_col7, #T_455f9_row40_col2, #T_455f9_row40_col5, #T_455f9_row40_col6, #T_455f9_row43_col4, #T_455f9_row44_col4, #T_455f9_row45_col4, #T_455f9_row51_col4, #T_455f9_row52_col4, #T_455f9_row55_col4 {\n",
              "  background-color: #78abd0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row2_col7, #T_455f9_row19_col2, #T_455f9_row29_col7, #T_455f9_row34_col3, #T_455f9_row48_col9, #T_455f9_row49_col0, #T_455f9_row51_col7 {\n",
              "  background-color: #96b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row2_col8, #T_455f9_row4_col0, #T_455f9_row5_col2, #T_455f9_row5_col9, #T_455f9_row7_col0, #T_455f9_row9_col8, #T_455f9_row31_col1, #T_455f9_row32_col4, #T_455f9_row45_col0, #T_455f9_row45_col10, #T_455f9_row46_col5, #T_455f9_row47_col4, #T_455f9_row52_col0, #T_455f9_row52_col9 {\n",
              "  background-color: #93b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row2_col9, #T_455f9_row3_col6, #T_455f9_row15_col6, #T_455f9_row16_col9, #T_455f9_row28_col7, #T_455f9_row33_col6, #T_455f9_row42_col0 {\n",
              "  background-color: #97b7d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row2_col10, #T_455f9_row12_col10, #T_455f9_row29_col0 {\n",
              "  background-color: #d9d8ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row3_col0, #T_455f9_row5_col1, #T_455f9_row9_col9, #T_455f9_row16_col6, #T_455f9_row53_col10 {\n",
              "  background-color: #afc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row3_col2, #T_455f9_row25_col6 {\n",
              "  background-color: #b8c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row3_col3, #T_455f9_row4_col8, #T_455f9_row6_col1, #T_455f9_row6_col9, #T_455f9_row11_col5, #T_455f9_row11_col8, #T_455f9_row13_col0, #T_455f9_row14_col3, #T_455f9_row16_col5, #T_455f9_row17_col6, #T_455f9_row21_col2, #T_455f9_row22_col1, #T_455f9_row23_col6, #T_455f9_row24_col4, #T_455f9_row26_col4, #T_455f9_row35_col5, #T_455f9_row35_col7, #T_455f9_row36_col1, #T_455f9_row37_col2, #T_455f9_row37_col8, #T_455f9_row37_col9, #T_455f9_row38_col2, #T_455f9_row46_col9, #T_455f9_row47_col5, #T_455f9_row54_col7 {\n",
              "  background-color: #83afd3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row3_col4, #T_455f9_row6_col2, #T_455f9_row6_col3, #T_455f9_row6_col5, #T_455f9_row7_col8, #T_455f9_row10_col6, #T_455f9_row11_col3, #T_455f9_row14_col1, #T_455f9_row14_col2, #T_455f9_row18_col6, #T_455f9_row19_col7, #T_455f9_row20_col0, #T_455f9_row22_col7, #T_455f9_row23_col7, #T_455f9_row25_col7, #T_455f9_row26_col6, #T_455f9_row35_col2, #T_455f9_row37_col4, #T_455f9_row37_col5, #T_455f9_row38_col1, #T_455f9_row38_col4, #T_455f9_row38_col6, #T_455f9_row39_col3, #T_455f9_row42_col1, #T_455f9_row45_col6, #T_455f9_row49_col1, #T_455f9_row49_col4, #T_455f9_row49_col6, #T_455f9_row50_col5, #T_455f9_row54_col5, #T_455f9_row55_col3, #T_455f9_row55_col9 {\n",
              "  background-color: #7dacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row3_col5, #T_455f9_row3_col7, #T_455f9_row4_col9, #T_455f9_row5_col4, #T_455f9_row5_col5, #T_455f9_row6_col8, #T_455f9_row7_col9, #T_455f9_row10_col7, #T_455f9_row14_col9, #T_455f9_row15_col1, #T_455f9_row18_col1, #T_455f9_row20_col10, #T_455f9_row22_col2, #T_455f9_row27_col2, #T_455f9_row35_col1, #T_455f9_row40_col7, #T_455f9_row42_col2, #T_455f9_row42_col7, #T_455f9_row44_col1, #T_455f9_row44_col5, #T_455f9_row46_col8, #T_455f9_row46_col10, #T_455f9_row48_col3, #T_455f9_row48_col5, #T_455f9_row51_col5, #T_455f9_row52_col7, #T_455f9_row53_col8, #T_455f9_row53_col9, #T_455f9_row55_col2 {\n",
              "  background-color: #81aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row3_col10 {\n",
              "  background-color: #c6cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row4_col3, #T_455f9_row11_col1, #T_455f9_row16_col7, #T_455f9_row19_col1, #T_455f9_row28_col4, #T_455f9_row33_col7, #T_455f9_row34_col5, #T_455f9_row53_col7 {\n",
              "  background-color: #8eb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row4_col5, #T_455f9_row5_col3, #T_455f9_row5_col6, #T_455f9_row21_col1, #T_455f9_row26_col3, #T_455f9_row31_col5, #T_455f9_row32_col7, #T_455f9_row33_col2, #T_455f9_row33_col5, #T_455f9_row47_col2, #T_455f9_row47_col6, #T_455f9_row52_col1, #T_455f9_row53_col6, #T_455f9_row54_col2, #T_455f9_row54_col8 {\n",
              "  background-color: #8fb4d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row4_col6, #T_455f9_row5_col0, #T_455f9_row25_col0, #T_455f9_row27_col1, #T_455f9_row31_col2, #T_455f9_row31_col7, #T_455f9_row39_col10 {\n",
              "  background-color: #9cb9d9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row4_col7, #T_455f9_row35_col3, #T_455f9_row37_col3, #T_455f9_row43_col7, #T_455f9_row44_col9, #T_455f9_row45_col9, #T_455f9_row53_col5 {\n",
              "  background-color: #94b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row4_col10, #T_455f9_row30_col3, #T_455f9_row49_col10 {\n",
              "  background-color: #acc0dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row5_col10, #T_455f9_row17_col10, #T_455f9_row24_col6, #T_455f9_row28_col1, #T_455f9_row29_col6, #T_455f9_row54_col10 {\n",
              "  background-color: #abbfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row6_col10, #T_455f9_row31_col3, #T_455f9_row44_col7 {\n",
              "  background-color: #99b8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row7_col1, #T_455f9_row11_col4, #T_455f9_row12_col4, #T_455f9_row12_col5, #T_455f9_row12_col7, #T_455f9_row13_col7, #T_455f9_row15_col5, #T_455f9_row20_col1, #T_455f9_row20_col3, #T_455f9_row20_col5, #T_455f9_row20_col8, #T_455f9_row23_col8, #T_455f9_row23_col9, #T_455f9_row24_col7, #T_455f9_row24_col9, #T_455f9_row26_col2, #T_455f9_row26_col7, #T_455f9_row26_col8, #T_455f9_row31_col8, #T_455f9_row31_col9, #T_455f9_row34_col8, #T_455f9_row38_col0, #T_455f9_row38_col7, #T_455f9_row39_col0, #T_455f9_row39_col6, #T_455f9_row41_col5, #T_455f9_row41_col7, #T_455f9_row45_col3, #T_455f9_row45_col5, #T_455f9_row48_col4, #T_455f9_row54_col4, #T_455f9_row55_col5, #T_455f9_row55_col8 {\n",
              "  background-color: #79abd0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row8_col0 {\n",
              "  background-color: #e1dfed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row8_col6, #T_455f9_row50_col3 {\n",
              "  background-color: #d8d7e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row8_col7, #T_455f9_row9_col7, #T_455f9_row11_col10, #T_455f9_row16_col8, #T_455f9_row19_col8, #T_455f9_row37_col7, #T_455f9_row44_col8, #T_455f9_row46_col1, #T_455f9_row47_col8, #T_455f9_row47_col9, #T_455f9_row50_col1, #T_455f9_row51_col8, #T_455f9_row53_col0 {\n",
              "  background-color: #91b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row8_col10 {\n",
              "  background-color: #dddbec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row9_col0, #T_455f9_row28_col6, #T_455f9_row31_col10 {\n",
              "  background-color: #b0c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row9_col2, #T_455f9_row12_col0 {\n",
              "  background-color: #c9cee4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row9_col3, #T_455f9_row29_col2 {\n",
              "  background-color: #bbc7e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row9_col6, #T_455f9_row16_col0, #T_455f9_row29_col3, #T_455f9_row31_col6, #T_455f9_row44_col0, #T_455f9_row46_col0, #T_455f9_row54_col0 {\n",
              "  background-color: #a4bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row10_col2, #T_455f9_row15_col3, #T_455f9_row50_col0 {\n",
              "  background-color: #e0deed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row10_col10, #T_455f9_row12_col6 {\n",
              "  background-color: #c5cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row11_col7, #T_455f9_row14_col10, #T_455f9_row17_col3, #T_455f9_row22_col6, #T_455f9_row23_col4, #T_455f9_row24_col3, #T_455f9_row25_col4, #T_455f9_row28_col9, #T_455f9_row31_col4, #T_455f9_row33_col8, #T_455f9_row35_col6, #T_455f9_row41_col4, #T_455f9_row48_col7, #T_455f9_row48_col8, #T_455f9_row51_col1, #T_455f9_row52_col3, #T_455f9_row53_col2 {\n",
              "  background-color: #88b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row12_col1, #T_455f9_row33_col0 {\n",
              "  background-color: #b9c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row12_col2, #T_455f9_row16_col2, #T_455f9_row44_col2 {\n",
              "  background-color: #bcc7e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row12_col3, #T_455f9_row22_col0, #T_455f9_row30_col7, #T_455f9_row44_col3 {\n",
              "  background-color: #a8bedc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row12_col9, #T_455f9_row24_col10 {\n",
              "  background-color: #cacee5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row14_col4, #T_455f9_row15_col4, #T_455f9_row16_col4, #T_455f9_row17_col4, #T_455f9_row20_col4, #T_455f9_row34_col2, #T_455f9_row34_col9, #T_455f9_row38_col8, #T_455f9_row38_col9, #T_455f9_row39_col9, #T_455f9_row41_col1, #T_455f9_row41_col3, #T_455f9_row41_col9 {\n",
              "  background-color: #75a9cf;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row16_col3, #T_455f9_row23_col10, #T_455f9_row45_col1, #T_455f9_row47_col0, #T_455f9_row51_col3 {\n",
              "  background-color: #b1c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row19_col0, #T_455f9_row19_col9, #T_455f9_row21_col0, #T_455f9_row21_col3, #T_455f9_row21_col6, #T_455f9_row47_col1, #T_455f9_row54_col9 {\n",
              "  background-color: #9fbad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row19_col10 {\n",
              "  background-color: #cccfe5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row22_col10, #T_455f9_row26_col0, #T_455f9_row30_col2, #T_455f9_row30_col6 {\n",
              "  background-color: #b3c3de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row25_col10, #T_455f9_row27_col10, #T_455f9_row30_col10 {\n",
              "  background-color: #c0c9e2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row26_col10 {\n",
              "  background-color: #d6d6e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row27_col6 {\n",
              "  background-color: #d4d4e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row28_col3, #T_455f9_row29_col1 {\n",
              "  background-color: #a9bfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row29_col10, #T_455f9_row50_col8 {\n",
              "  background-color: #dfddec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row30_col0, #T_455f9_row51_col9 {\n",
              "  background-color: #bfc9e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row32_col2, #T_455f9_row39_col2, #T_455f9_row39_col8, #T_455f9_row41_col2, #T_455f9_row41_col8 {\n",
              "  background-color: #73a9cf;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row33_col1, #T_455f9_row33_col10, #T_455f9_row37_col0, #T_455f9_row48_col0, #T_455f9_row48_col10, #T_455f9_row50_col6 {\n",
              "  background-color: #b4c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row43_col1, #T_455f9_row51_col0 {\n",
              "  background-color: #a5bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row43_col6 {\n",
              "  background-color: #c4cbe3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row43_col10 {\n",
              "  background-color: #dedcec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row44_col10, #T_455f9_row50_col2 {\n",
              "  background-color: #dad9ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row47_col10 {\n",
              "  background-color: #bdc8e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row50_col10 {\n",
              "  background-color: #d1d2e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row51_col10 {\n",
              "  background-color: #d2d3e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_455f9_row52_col10 {\n",
              "  background-color: #b7c5df;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_455f9\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank\" >&nbsp;</th>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_455f9_level0_col0\" class=\"col_heading level0 col0\" >ar</th>\n",
              "      <th id=\"T_455f9_level0_col1\" class=\"col_heading level0 col1\" >ca</th>\n",
              "      <th id=\"T_455f9_level0_col2\" class=\"col_heading level0 col2\" >cs</th>\n",
              "      <th id=\"T_455f9_level0_col3\" class=\"col_heading level0 col3\" >de</th>\n",
              "      <th id=\"T_455f9_level0_col4\" class=\"col_heading level0 col4\" >en</th>\n",
              "      <th id=\"T_455f9_level0_col5\" class=\"col_heading level0 col5\" >es</th>\n",
              "      <th id=\"T_455f9_level0_col6\" class=\"col_heading level0 col6\" >nl</th>\n",
              "      <th id=\"T_455f9_level0_col7\" class=\"col_heading level0 col7\" >pt</th>\n",
              "      <th id=\"T_455f9_level0_col8\" class=\"col_heading level0 col8\" >ru</th>\n",
              "      <th id=\"T_455f9_level0_col9\" class=\"col_heading level0 col9\" >uk</th>\n",
              "      <th id=\"T_455f9_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Train LLM</th>\n",
              "      <th class=\"index_name level1\" >Model</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "      <th class=\"blank col7\" >&nbsp;</th>\n",
              "      <th class=\"blank col8\" >&nbsp;</th>\n",
              "      <th class=\"blank col9\" >&nbsp;</th>\n",
              "      <th class=\"blank col10\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"7\">alpaca-lora-30b</th>\n",
              "      <th id=\"T_455f9_level1_row0\" class=\"row_heading level1 row0\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_455f9_row0_col0\" class=\"data row0 col0\" >0.5096</td>\n",
              "      <td id=\"T_455f9_row0_col1\" class=\"data row0 col1\" >0.9133</td>\n",
              "      <td id=\"T_455f9_row0_col2\" class=\"data row0 col2\" >0.8879</td>\n",
              "      <td id=\"T_455f9_row0_col3\" class=\"data row0 col3\" >0.9138</td>\n",
              "      <td id=\"T_455f9_row0_col4\" class=\"data row0 col4\" >0.9657</td>\n",
              "      <td id=\"T_455f9_row0_col5\" class=\"data row0 col5\" >0.9401</td>\n",
              "      <td id=\"T_455f9_row0_col6\" class=\"data row0 col6\" >0.9161</td>\n",
              "      <td id=\"T_455f9_row0_col7\" class=\"data row0 col7\" >0.9113</td>\n",
              "      <td id=\"T_455f9_row0_col8\" class=\"data row0 col8\" >0.9200</td>\n",
              "      <td id=\"T_455f9_row0_col9\" class=\"data row0 col9\" >0.8928</td>\n",
              "      <td id=\"T_455f9_row0_col10\" class=\"data row0 col10\" >0.7472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row1\" class=\"row_heading level1 row1\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_455f9_row1_col0\" class=\"data row1 col0\" >0.3367</td>\n",
              "      <td id=\"T_455f9_row1_col1\" class=\"data row1 col1\" >0.7431</td>\n",
              "      <td id=\"T_455f9_row1_col2\" class=\"data row1 col2\" >0.4389</td>\n",
              "      <td id=\"T_455f9_row1_col3\" class=\"data row1 col3\" >0.5051</td>\n",
              "      <td id=\"T_455f9_row1_col4\" class=\"data row1 col4\" >0.9856</td>\n",
              "      <td id=\"T_455f9_row1_col5\" class=\"data row1 col5\" >0.9502</td>\n",
              "      <td id=\"T_455f9_row1_col6\" class=\"data row1 col6\" >0.7716</td>\n",
              "      <td id=\"T_455f9_row1_col7\" class=\"data row1 col7\" >0.8824</td>\n",
              "      <td id=\"T_455f9_row1_col8\" class=\"data row1 col8\" >0.3407</td>\n",
              "      <td id=\"T_455f9_row1_col9\" class=\"data row1 col9\" >0.3326</td>\n",
              "      <td id=\"T_455f9_row1_col10\" class=\"data row1 col10\" >0.3999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row2\" class=\"row_heading level1 row2\" >gpt2-medium</th>\n",
              "      <td id=\"T_455f9_row2_col0\" class=\"data row2 col0\" >0.8118</td>\n",
              "      <td id=\"T_455f9_row2_col1\" class=\"data row2 col1\" >0.7896</td>\n",
              "      <td id=\"T_455f9_row2_col2\" class=\"data row2 col2\" >0.6623</td>\n",
              "      <td id=\"T_455f9_row2_col3\" class=\"data row2 col3\" >0.7747</td>\n",
              "      <td id=\"T_455f9_row2_col4\" class=\"data row2 col4\" >0.9819</td>\n",
              "      <td id=\"T_455f9_row2_col5\" class=\"data row2 col5\" >0.8919</td>\n",
              "      <td id=\"T_455f9_row2_col6\" class=\"data row2 col6\" >0.8064</td>\n",
              "      <td id=\"T_455f9_row2_col7\" class=\"data row2 col7\" >0.8307</td>\n",
              "      <td id=\"T_455f9_row2_col8\" class=\"data row2 col8\" >0.8463</td>\n",
              "      <td id=\"T_455f9_row2_col9\" class=\"data row2 col9\" >0.8251</td>\n",
              "      <td id=\"T_455f9_row2_col10\" class=\"data row2 col10\" >0.4282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row3\" class=\"row_heading level1 row3\" >mGPT</th>\n",
              "      <td id=\"T_455f9_row3_col0\" class=\"data row3 col0\" >0.6955</td>\n",
              "      <td id=\"T_455f9_row3_col1\" class=\"data row3 col1\" >0.8833</td>\n",
              "      <td id=\"T_455f9_row3_col2\" class=\"data row3 col2\" >0.6480</td>\n",
              "      <td id=\"T_455f9_row3_col3\" class=\"data row3 col3\" >0.9290</td>\n",
              "      <td id=\"T_455f9_row3_col4\" class=\"data row3 col4\" >0.9585</td>\n",
              "      <td id=\"T_455f9_row3_col5\" class=\"data row3 col5\" >0.9315</td>\n",
              "      <td id=\"T_455f9_row3_col6\" class=\"data row3 col6\" >0.8215</td>\n",
              "      <td id=\"T_455f9_row3_col7\" class=\"data row3 col7\" >0.9334</td>\n",
              "      <td id=\"T_455f9_row3_col8\" class=\"data row3 col8\" >0.9383</td>\n",
              "      <td id=\"T_455f9_row3_col9\" class=\"data row3 col9\" >0.9129</td>\n",
              "      <td id=\"T_455f9_row3_col10\" class=\"data row3 col10\" >0.5555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row4\" class=\"row_heading level1 row4\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_455f9_row4_col0\" class=\"data row4 col0\" >0.8438</td>\n",
              "      <td id=\"T_455f9_row4_col1\" class=\"data row4 col1\" >0.8062</td>\n",
              "      <td id=\"T_455f9_row4_col2\" class=\"data row4 col2\" >0.8906</td>\n",
              "      <td id=\"T_455f9_row4_col3\" class=\"data row4 col3\" >0.8727</td>\n",
              "      <td id=\"T_455f9_row4_col4\" class=\"data row4 col4\" >0.9075</td>\n",
              "      <td id=\"T_455f9_row4_col5\" class=\"data row4 col5\" >0.8643</td>\n",
              "      <td id=\"T_455f9_row4_col6\" class=\"data row4 col6\" >0.8044</td>\n",
              "      <td id=\"T_455f9_row4_col7\" class=\"data row4 col7\" >0.8364</td>\n",
              "      <td id=\"T_455f9_row4_col8\" class=\"data row4 col8\" >0.9283</td>\n",
              "      <td id=\"T_455f9_row4_col9\" class=\"data row4 col9\" >0.9331</td>\n",
              "      <td id=\"T_455f9_row4_col10\" class=\"data row4 col10\" >0.7118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row5\" class=\"row_heading level1 row5\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_455f9_row5_col0\" class=\"data row5 col0\" >0.8040</td>\n",
              "      <td id=\"T_455f9_row5_col1\" class=\"data row5 col1\" >0.7010</td>\n",
              "      <td id=\"T_455f9_row5_col2\" class=\"data row5 col2\" >0.8484</td>\n",
              "      <td id=\"T_455f9_row5_col3\" class=\"data row5 col3\" >0.8629</td>\n",
              "      <td id=\"T_455f9_row5_col4\" class=\"data row5 col4\" >0.9348</td>\n",
              "      <td id=\"T_455f9_row5_col5\" class=\"data row5 col5\" >0.9363</td>\n",
              "      <td id=\"T_455f9_row5_col6\" class=\"data row5 col6\" >0.8641</td>\n",
              "      <td id=\"T_455f9_row5_col7\" class=\"data row5 col7\" >0.8969</td>\n",
              "      <td id=\"T_455f9_row5_col8\" class=\"data row5 col8\" >0.8817</td>\n",
              "      <td id=\"T_455f9_row5_col9\" class=\"data row5 col9\" >0.8487</td>\n",
              "      <td id=\"T_455f9_row5_col10\" class=\"data row5 col10\" >0.7252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row6\" class=\"row_heading level1 row6\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_455f9_row6_col0\" class=\"data row6 col0\" >0.7742</td>\n",
              "      <td id=\"T_455f9_row6_col1\" class=\"data row6 col1\" >0.9283</td>\n",
              "      <td id=\"T_455f9_row6_col2\" class=\"data row6 col2\" >0.9600</td>\n",
              "      <td id=\"T_455f9_row6_col3\" class=\"data row6 col3\" >0.9578</td>\n",
              "      <td id=\"T_455f9_row6_col4\" class=\"data row6 col4\" >0.9856</td>\n",
              "      <td id=\"T_455f9_row6_col5\" class=\"data row6 col5\" >0.9589</td>\n",
              "      <td id=\"T_455f9_row6_col6\" class=\"data row6 col6\" >0.9481</td>\n",
              "      <td id=\"T_455f9_row6_col7\" class=\"data row6 col7\" >0.9522</td>\n",
              "      <td id=\"T_455f9_row6_col8\" class=\"data row6 col8\" >0.9366</td>\n",
              "      <td id=\"T_455f9_row6_col9\" class=\"data row6 col9\" >0.9246</td>\n",
              "      <td id=\"T_455f9_row6_col10\" class=\"data row6 col10\" >0.8200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level0_row7\" class=\"row_heading level0 row7\" rowspan=\"7\">gpt-3.5-turbo</th>\n",
              "      <th id=\"T_455f9_level1_row7\" class=\"row_heading level1 row7\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_455f9_row7_col0\" class=\"data row7 col0\" >0.8483</td>\n",
              "      <td id=\"T_455f9_row7_col1\" class=\"data row7 col1\" >0.9750</td>\n",
              "      <td id=\"T_455f9_row7_col2\" class=\"data row7 col2\" >0.9633</td>\n",
              "      <td id=\"T_455f9_row7_col3\" class=\"data row7 col3\" >0.9391</td>\n",
              "      <td id=\"T_455f9_row7_col4\" class=\"data row7 col4\" >0.9819</td>\n",
              "      <td id=\"T_455f9_row7_col5\" class=\"data row7 col5\" >0.9777</td>\n",
              "      <td id=\"T_455f9_row7_col6\" class=\"data row7 col6\" >0.9816</td>\n",
              "      <td id=\"T_455f9_row7_col7\" class=\"data row7 col7\" >0.9506</td>\n",
              "      <td id=\"T_455f9_row7_col8\" class=\"data row7 col8\" >0.9550</td>\n",
              "      <td id=\"T_455f9_row7_col9\" class=\"data row7 col9\" >0.9313</td>\n",
              "      <td id=\"T_455f9_row7_col10\" class=\"data row7 col10\" >0.9148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row8\" class=\"row_heading level1 row8\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_455f9_row8_col0\" class=\"data row8 col0\" >0.3501</td>\n",
              "      <td id=\"T_455f9_row8_col1\" class=\"data row8 col1\" >0.7954</td>\n",
              "      <td id=\"T_455f9_row8_col2\" class=\"data row8 col2\" >0.3333</td>\n",
              "      <td id=\"T_455f9_row8_col3\" class=\"data row8 col3\" >0.3340</td>\n",
              "      <td id=\"T_455f9_row8_col4\" class=\"data row8 col4\" >0.9819</td>\n",
              "      <td id=\"T_455f9_row8_col5\" class=\"data row8 col5\" >0.9657</td>\n",
              "      <td id=\"T_455f9_row8_col6\" class=\"data row8 col6\" >0.4354</td>\n",
              "      <td id=\"T_455f9_row8_col7\" class=\"data row8 col7\" >0.8583</td>\n",
              "      <td id=\"T_455f9_row8_col8\" class=\"data row8 col8\" >0.3326</td>\n",
              "      <td id=\"T_455f9_row8_col9\" class=\"data row8 col9\" >0.3326</td>\n",
              "      <td id=\"T_455f9_row8_col10\" class=\"data row8 col10\" >0.3901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row9\" class=\"row_heading level1 row9\" >gpt2-medium</th>\n",
              "      <td id=\"T_455f9_row9_col0\" class=\"data row9 col0\" >0.6911</td>\n",
              "      <td id=\"T_455f9_row9_col1\" class=\"data row9 col1\" >0.9180</td>\n",
              "      <td id=\"T_455f9_row9_col2\" class=\"data row9 col2\" >0.5441</td>\n",
              "      <td id=\"T_455f9_row9_col3\" class=\"data row9 col3\" >0.6283</td>\n",
              "      <td id=\"T_455f9_row9_col4\" class=\"data row9 col4\" >0.9874</td>\n",
              "      <td id=\"T_455f9_row9_col5\" class=\"data row9 col5\" >0.9452</td>\n",
              "      <td id=\"T_455f9_row9_col6\" class=\"data row9 col6\" >0.7647</td>\n",
              "      <td id=\"T_455f9_row9_col7\" class=\"data row9 col7\" >0.8586</td>\n",
              "      <td id=\"T_455f9_row9_col8\" class=\"data row9 col8\" >0.8480</td>\n",
              "      <td id=\"T_455f9_row9_col9\" class=\"data row9 col9\" >0.6995</td>\n",
              "      <td id=\"T_455f9_row9_col10\" class=\"data row9 col10\" >0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row10\" class=\"row_heading level1 row10\" >mGPT</th>\n",
              "      <td id=\"T_455f9_row10_col0\" class=\"data row10 col0\" >0.8856</td>\n",
              "      <td id=\"T_455f9_row10_col1\" class=\"data row10 col1\" >0.9198</td>\n",
              "      <td id=\"T_455f9_row10_col2\" class=\"data row10 col2\" >0.3552</td>\n",
              "      <td id=\"T_455f9_row10_col3\" class=\"data row10 col3\" >0.8964</td>\n",
              "      <td id=\"T_455f9_row10_col4\" class=\"data row10 col4\" >0.9801</td>\n",
              "      <td id=\"T_455f9_row10_col5\" class=\"data row10 col5\" >0.9469</td>\n",
              "      <td id=\"T_455f9_row10_col6\" class=\"data row10 col6\" >0.9566</td>\n",
              "      <td id=\"T_455f9_row10_col7\" class=\"data row10 col7\" >0.9370</td>\n",
              "      <td id=\"T_455f9_row10_col8\" class=\"data row10 col8\" >0.9383</td>\n",
              "      <td id=\"T_455f9_row10_col9\" class=\"data row10 col9\" >0.8939</td>\n",
              "      <td id=\"T_455f9_row10_col10\" class=\"data row10 col10\" >0.5693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row11\" class=\"row_heading level1 row11\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_455f9_row11_col0\" class=\"data row11 col0\" >0.9466</td>\n",
              "      <td id=\"T_455f9_row11_col1\" class=\"data row11 col1\" >0.8710</td>\n",
              "      <td id=\"T_455f9_row11_col2\" class=\"data row11 col2\" >0.9767</td>\n",
              "      <td id=\"T_455f9_row11_col3\" class=\"data row11 col3\" >0.9578</td>\n",
              "      <td id=\"T_455f9_row11_col4\" class=\"data row11 col4\" >0.9747</td>\n",
              "      <td id=\"T_455f9_row11_col5\" class=\"data row11 col5\" >0.9280</td>\n",
              "      <td id=\"T_455f9_row11_col6\" class=\"data row11 col6\" >0.8977</td>\n",
              "      <td id=\"T_455f9_row11_col7\" class=\"data row11 col7\" >0.9039</td>\n",
              "      <td id=\"T_455f9_row11_col8\" class=\"data row11 col8\" >0.9232</td>\n",
              "      <td id=\"T_455f9_row11_col9\" class=\"data row11 col9\" >0.9398</td>\n",
              "      <td id=\"T_455f9_row11_col10\" class=\"data row11 col10\" >0.8526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row12\" class=\"row_heading level1 row12\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_455f9_row12_col0\" class=\"data row12 col0\" >0.5451</td>\n",
              "      <td id=\"T_455f9_row12_col1\" class=\"data row12 col1\" >0.6366</td>\n",
              "      <td id=\"T_455f9_row12_col2\" class=\"data row12 col2\" >0.6189</td>\n",
              "      <td id=\"T_455f9_row12_col3\" class=\"data row12 col3\" >0.7365</td>\n",
              "      <td id=\"T_455f9_row12_col4\" class=\"data row12 col4\" >0.9765</td>\n",
              "      <td id=\"T_455f9_row12_col5\" class=\"data row12 col5\" >0.9726</td>\n",
              "      <td id=\"T_455f9_row12_col6\" class=\"data row12 col6\" >0.5633</td>\n",
              "      <td id=\"T_455f9_row12_col7\" class=\"data row12 col7\" >0.9710</td>\n",
              "      <td id=\"T_455f9_row12_col8\" class=\"data row12 col8\" >0.8933</td>\n",
              "      <td id=\"T_455f9_row12_col9\" class=\"data row12 col9\" >0.5315</td>\n",
              "      <td id=\"T_455f9_row12_col10\" class=\"data row12 col10\" >0.4295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row13\" class=\"row_heading level1 row13\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_455f9_row13_col0\" class=\"data row13 col0\" >0.9280</td>\n",
              "      <td id=\"T_455f9_row13_col1\" class=\"data row13 col1\" >0.9850</td>\n",
              "      <td id=\"T_455f9_row13_col2\" class=\"data row13 col2\" >0.9883</td>\n",
              "      <td id=\"T_455f9_row13_col3\" class=\"data row13 col3\" >0.9425</td>\n",
              "      <td id=\"T_455f9_row13_col4\" class=\"data row13 col4\" >0.9910</td>\n",
              "      <td id=\"T_455f9_row13_col5\" class=\"data row13 col5\" >0.9846</td>\n",
              "      <td id=\"T_455f9_row13_col6\" class=\"data row13 col6\" >0.9800</td>\n",
              "      <td id=\"T_455f9_row13_col7\" class=\"data row13 col7\" >0.9727</td>\n",
              "      <td id=\"T_455f9_row13_col8\" class=\"data row13 col8\" >0.9850</td>\n",
              "      <td id=\"T_455f9_row13_col9\" class=\"data row13 col9\" >0.9464</td>\n",
              "      <td id=\"T_455f9_row13_col10\" class=\"data row13 col10\" >0.9382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level0_row14\" class=\"row_heading level0 row14\" rowspan=\"7\">gpt-4</th>\n",
              "      <th id=\"T_455f9_level1_row14\" class=\"row_heading level1 row14\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_455f9_row14_col0\" class=\"data row14 col0\" >0.9416</td>\n",
              "      <td id=\"T_455f9_row14_col1\" class=\"data row14 col1\" >0.9566</td>\n",
              "      <td id=\"T_455f9_row14_col2\" class=\"data row14 col2\" >0.9533</td>\n",
              "      <td id=\"T_455f9_row14_col3\" class=\"data row14 col3\" >0.9255</td>\n",
              "      <td id=\"T_455f9_row14_col4\" class=\"data row14 col4\" >0.9928</td>\n",
              "      <td id=\"T_455f9_row14_col5\" class=\"data row14 col5\" >0.9485</td>\n",
              "      <td id=\"T_455f9_row14_col6\" class=\"data row14 col6\" >0.9683</td>\n",
              "      <td id=\"T_455f9_row14_col7\" class=\"data row14 col7\" >0.9453</td>\n",
              "      <td id=\"T_455f9_row14_col8\" class=\"data row14 col8\" >0.9400</td>\n",
              "      <td id=\"T_455f9_row14_col9\" class=\"data row14 col9\" >0.9365</td>\n",
              "      <td id=\"T_455f9_row14_col10\" class=\"data row14 col10\" >0.9048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row15\" class=\"row_heading level1 row15\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_455f9_row15_col0\" class=\"data row15 col0\" >0.3330</td>\n",
              "      <td id=\"T_455f9_row15_col1\" class=\"data row15 col1\" >0.9297</td>\n",
              "      <td id=\"T_455f9_row15_col2\" class=\"data row15 col2\" >0.3333</td>\n",
              "      <td id=\"T_455f9_row15_col3\" class=\"data row15 col3\" >0.3593</td>\n",
              "      <td id=\"T_455f9_row15_col4\" class=\"data row15 col4\" >0.9946</td>\n",
              "      <td id=\"T_455f9_row15_col5\" class=\"data row15 col5\" >0.9726</td>\n",
              "      <td id=\"T_455f9_row15_col6\" class=\"data row15 col6\" >0.8213</td>\n",
              "      <td id=\"T_455f9_row15_col7\" class=\"data row15 col7\" >0.9472</td>\n",
              "      <td id=\"T_455f9_row15_col8\" class=\"data row15 col8\" >0.3333</td>\n",
              "      <td id=\"T_455f9_row15_col9\" class=\"data row15 col9\" >0.3326</td>\n",
              "      <td id=\"T_455f9_row15_col10\" class=\"data row15 col10\" >0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row16\" class=\"row_heading level1 row16\" >gpt2-medium</th>\n",
              "      <td id=\"T_455f9_row16_col0\" class=\"data row16 col0\" >0.7644</td>\n",
              "      <td id=\"T_455f9_row16_col1\" class=\"data row16 col1\" >0.8923</td>\n",
              "      <td id=\"T_455f9_row16_col2\" class=\"data row16 col2\" >0.6236</td>\n",
              "      <td id=\"T_455f9_row16_col3\" class=\"data row16 col3\" >0.6851</td>\n",
              "      <td id=\"T_455f9_row16_col4\" class=\"data row16 col4\" >0.9982</td>\n",
              "      <td id=\"T_455f9_row16_col5\" class=\"data row16 col5\" >0.9279</td>\n",
              "      <td id=\"T_455f9_row16_col6\" class=\"data row16 col6\" >0.7007</td>\n",
              "      <td id=\"T_455f9_row16_col7\" class=\"data row16 col7\" >0.8714</td>\n",
              "      <td id=\"T_455f9_row16_col8\" class=\"data row16 col8\" >0.8516</td>\n",
              "      <td id=\"T_455f9_row16_col9\" class=\"data row16 col9\" >0.8235</td>\n",
              "      <td id=\"T_455f9_row16_col10\" class=\"data row16 col10\" >0.3392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row17\" class=\"row_heading level1 row17\" >mGPT</th>\n",
              "      <td id=\"T_455f9_row17_col0\" class=\"data row17 col0\" >0.8911</td>\n",
              "      <td id=\"T_455f9_row17_col1\" class=\"data row17 col1\" >0.9450</td>\n",
              "      <td id=\"T_455f9_row17_col2\" class=\"data row17 col2\" >0.3407</td>\n",
              "      <td id=\"T_455f9_row17_col3\" class=\"data row17 col3\" >0.9014</td>\n",
              "      <td id=\"T_455f9_row17_col4\" class=\"data row17 col4\" >0.9928</td>\n",
              "      <td id=\"T_455f9_row17_col5\" class=\"data row17 col5\" >0.9640</td>\n",
              "      <td id=\"T_455f9_row17_col6\" class=\"data row17 col6\" >0.9246</td>\n",
              "      <td id=\"T_455f9_row17_col7\" class=\"data row17 col7\" >0.9487</td>\n",
              "      <td id=\"T_455f9_row17_col8\" class=\"data row17 col8\" >0.9467</td>\n",
              "      <td id=\"T_455f9_row17_col9\" class=\"data row17 col9\" >0.9212</td>\n",
              "      <td id=\"T_455f9_row17_col10\" class=\"data row17 col10\" >0.7236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row18\" class=\"row_heading level1 row18\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_455f9_row18_col0\" class=\"data row18 col0\" >0.9382</td>\n",
              "      <td id=\"T_455f9_row18_col1\" class=\"data row18 col1\" >0.9315</td>\n",
              "      <td id=\"T_455f9_row18_col2\" class=\"data row18 col2\" >0.9883</td>\n",
              "      <td id=\"T_455f9_row18_col3\" class=\"data row18 col3\" >0.9797</td>\n",
              "      <td id=\"T_455f9_row18_col4\" class=\"data row18 col4\" >0.9457</td>\n",
              "      <td id=\"T_455f9_row18_col5\" class=\"data row18 col5\" >0.9502</td>\n",
              "      <td id=\"T_455f9_row18_col6\" class=\"data row18 col6\" >0.9582</td>\n",
              "      <td id=\"T_455f9_row18_col7\" class=\"data row18 col7\" >0.9195</td>\n",
              "      <td id=\"T_455f9_row18_col8\" class=\"data row18 col8\" >0.9383</td>\n",
              "      <td id=\"T_455f9_row18_col9\" class=\"data row18 col9\" >0.9498</td>\n",
              "      <td id=\"T_455f9_row18_col10\" class=\"data row18 col10\" >0.8854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row19\" class=\"row_heading level1 row19\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_455f9_row19_col0\" class=\"data row19 col0\" >0.7820</td>\n",
              "      <td id=\"T_455f9_row19_col1\" class=\"data row19 col1\" >0.8696</td>\n",
              "      <td id=\"T_455f9_row19_col2\" class=\"data row19 col2\" >0.8308</td>\n",
              "      <td id=\"T_455f9_row19_col3\" class=\"data row19 col3\" >0.8932</td>\n",
              "      <td id=\"T_455f9_row19_col4\" class=\"data row19 col4\" >0.9838</td>\n",
              "      <td id=\"T_455f9_row19_col5\" class=\"data row19 col5\" >0.9657</td>\n",
              "      <td id=\"T_455f9_row19_col6\" class=\"data row19 col6\" >0.7753</td>\n",
              "      <td id=\"T_455f9_row19_col7\" class=\"data row19 col7\" >0.9590</td>\n",
              "      <td id=\"T_455f9_row19_col8\" class=\"data row19 col8\" >0.8548</td>\n",
              "      <td id=\"T_455f9_row19_col9\" class=\"data row19 col9\" >0.7828</td>\n",
              "      <td id=\"T_455f9_row19_col10\" class=\"data row19 col10\" >0.5290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row20\" class=\"row_heading level1 row20\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_455f9_row20_col0\" class=\"data row20 col0\" >0.9599</td>\n",
              "      <td id=\"T_455f9_row20_col1\" class=\"data row20 col1\" >0.9733</td>\n",
              "      <td id=\"T_455f9_row20_col2\" class=\"data row20 col2\" >0.9850</td>\n",
              "      <td id=\"T_455f9_row20_col3\" class=\"data row20 col3\" >0.9696</td>\n",
              "      <td id=\"T_455f9_row20_col4\" class=\"data row20 col4\" >0.9946</td>\n",
              "      <td id=\"T_455f9_row20_col5\" class=\"data row20 col5\" >0.9691</td>\n",
              "      <td id=\"T_455f9_row20_col6\" class=\"data row20 col6\" >0.9816</td>\n",
              "      <td id=\"T_455f9_row20_col7\" class=\"data row20 col7\" >0.9829</td>\n",
              "      <td id=\"T_455f9_row20_col8\" class=\"data row20 col8\" >0.9733</td>\n",
              "      <td id=\"T_455f9_row20_col9\" class=\"data row20 col9\" >0.9833</td>\n",
              "      <td id=\"T_455f9_row20_col10\" class=\"data row20 col10\" >0.9365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level0_row21\" class=\"row_heading level0 row21\" rowspan=\"7\">llama-65b</th>\n",
              "      <th id=\"T_455f9_level1_row21\" class=\"row_heading level1 row21\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_455f9_row21_col0\" class=\"data row21 col0\" >0.7821</td>\n",
              "      <td id=\"T_455f9_row21_col1\" class=\"data row21 col1\" >0.8645</td>\n",
              "      <td id=\"T_455f9_row21_col2\" class=\"data row21 col2\" >0.9288</td>\n",
              "      <td id=\"T_455f9_row21_col3\" class=\"data row21 col3\" >0.7867</td>\n",
              "      <td id=\"T_455f9_row21_col4\" class=\"data row21 col4\" >0.9076</td>\n",
              "      <td id=\"T_455f9_row21_col5\" class=\"data row21 col5\" >0.9879</td>\n",
              "      <td id=\"T_455f9_row21_col6\" class=\"data row21 col6\" >0.7835</td>\n",
              "      <td id=\"T_455f9_row21_col7\" class=\"data row21 col7\" >0.9449</td>\n",
              "      <td id=\"T_455f9_row21_col8\" class=\"data row21 col8\" >0.9782</td>\n",
              "      <td id=\"T_455f9_row21_col9\" class=\"data row21 col9\" >0.9781</td>\n",
              "      <td id=\"T_455f9_row21_col10\" class=\"data row21 col10\" >0.6580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row22\" class=\"row_heading level1 row22\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_455f9_row22_col0\" class=\"data row22 col0\" >0.7392</td>\n",
              "      <td id=\"T_455f9_row22_col1\" class=\"data row22 col1\" >0.9281</td>\n",
              "      <td id=\"T_455f9_row22_col2\" class=\"data row22 col2\" >0.9320</td>\n",
              "      <td id=\"T_455f9_row22_col3\" class=\"data row22 col3\" >0.9455</td>\n",
              "      <td id=\"T_455f9_row22_col4\" class=\"data row22 col4\" >0.8964</td>\n",
              "      <td id=\"T_455f9_row22_col5\" class=\"data row22 col5\" >0.9862</td>\n",
              "      <td id=\"T_455f9_row22_col6\" class=\"data row22 col6\" >0.8995</td>\n",
              "      <td id=\"T_455f9_row22_col7\" class=\"data row22 col7\" >0.9570</td>\n",
              "      <td id=\"T_455f9_row22_col8\" class=\"data row22 col8\" >0.9189</td>\n",
              "      <td id=\"T_455f9_row22_col9\" class=\"data row22 col9\" >0.9443</td>\n",
              "      <td id=\"T_455f9_row22_col10\" class=\"data row22 col10\" >0.6784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row23\" class=\"row_heading level1 row23\" >gpt2-medium</th>\n",
              "      <td id=\"T_455f9_row23_col0\" class=\"data row23 col0\" >0.6580</td>\n",
              "      <td id=\"T_455f9_row23_col1\" class=\"data row23 col1\" >0.9632</td>\n",
              "      <td id=\"T_455f9_row23_col2\" class=\"data row23 col2\" >0.9491</td>\n",
              "      <td id=\"T_455f9_row23_col3\" class=\"data row23 col3\" >0.9490</td>\n",
              "      <td id=\"T_455f9_row23_col4\" class=\"data row23 col4\" >0.9016</td>\n",
              "      <td id=\"T_455f9_row23_col5\" class=\"data row23 col5\" >0.9845</td>\n",
              "      <td id=\"T_455f9_row23_col6\" class=\"data row23 col6\" >0.9279</td>\n",
              "      <td id=\"T_455f9_row23_col7\" class=\"data row23 col7\" >0.9605</td>\n",
              "      <td id=\"T_455f9_row23_col8\" class=\"data row23 col8\" >0.9765</td>\n",
              "      <td id=\"T_455f9_row23_col9\" class=\"data row23 col9\" >0.9697</td>\n",
              "      <td id=\"T_455f9_row23_col10\" class=\"data row23 col10\" >0.6849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row24\" class=\"row_heading level1 row24\" >mGPT</th>\n",
              "      <td id=\"T_455f9_row24_col0\" class=\"data row24 col0\" >0.8919</td>\n",
              "      <td id=\"T_455f9_row24_col1\" class=\"data row24 col1\" >0.8853</td>\n",
              "      <td id=\"T_455f9_row24_col2\" class=\"data row24 col2\" >0.9509</td>\n",
              "      <td id=\"T_455f9_row24_col3\" class=\"data row24 col3\" >0.9058</td>\n",
              "      <td id=\"T_455f9_row24_col4\" class=\"data row24 col4\" >0.9293</td>\n",
              "      <td id=\"T_455f9_row24_col5\" class=\"data row24 col5\" >0.9862</td>\n",
              "      <td id=\"T_455f9_row24_col6\" class=\"data row24 col6\" >0.7192</td>\n",
              "      <td id=\"T_455f9_row24_col7\" class=\"data row24 col7\" >0.9691</td>\n",
              "      <td id=\"T_455f9_row24_col8\" class=\"data row24 col8\" >0.9799</td>\n",
              "      <td id=\"T_455f9_row24_col9\" class=\"data row24 col9\" >0.9731</td>\n",
              "      <td id=\"T_455f9_row24_col10\" class=\"data row24 col10\" >0.5337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row25\" class=\"row_heading level1 row25\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_455f9_row25_col0\" class=\"data row25 col0\" >0.8006</td>\n",
              "      <td id=\"T_455f9_row25_col1\" class=\"data row25 col1\" >0.8887</td>\n",
              "      <td id=\"T_455f9_row25_col2\" class=\"data row25 col2\" >0.9831</td>\n",
              "      <td id=\"T_455f9_row25_col3\" class=\"data row25 col3\" >0.8061</td>\n",
              "      <td id=\"T_455f9_row25_col4\" class=\"data row25 col4\" >0.9004</td>\n",
              "      <td id=\"T_455f9_row25_col5\" class=\"data row25 col5\" >0.9828</td>\n",
              "      <td id=\"T_455f9_row25_col6\" class=\"data row25 col6\" >0.6450</td>\n",
              "      <td id=\"T_455f9_row25_col7\" class=\"data row25 col7\" >0.9535</td>\n",
              "      <td id=\"T_455f9_row25_col8\" class=\"data row25 col8\" >0.9899</td>\n",
              "      <td id=\"T_455f9_row25_col9\" class=\"data row25 col9\" >0.9663</td>\n",
              "      <td id=\"T_455f9_row25_col10\" class=\"data row25 col10\" >0.5974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row26\" class=\"row_heading level1 row26\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_455f9_row26_col0\" class=\"data row26 col0\" >0.6731</td>\n",
              "      <td id=\"T_455f9_row26_col1\" class=\"data row26 col1\" >0.9766</td>\n",
              "      <td id=\"T_455f9_row26_col2\" class=\"data row26 col2\" >0.9763</td>\n",
              "      <td id=\"T_455f9_row26_col3\" class=\"data row26 col3\" >0.8597</td>\n",
              "      <td id=\"T_455f9_row26_col4\" class=\"data row26 col4\" >0.9239</td>\n",
              "      <td id=\"T_455f9_row26_col5\" class=\"data row26 col5\" >0.9862</td>\n",
              "      <td id=\"T_455f9_row26_col6\" class=\"data row26 col6\" >0.9598</td>\n",
              "      <td id=\"T_455f9_row26_col7\" class=\"data row26 col7\" >0.9690</td>\n",
              "      <td id=\"T_455f9_row26_col8\" class=\"data row26 col8\" >0.9714</td>\n",
              "      <td id=\"T_455f9_row26_col9\" class=\"data row26 col9\" >0.9781</td>\n",
              "      <td id=\"T_455f9_row26_col10\" class=\"data row26 col10\" >0.4521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row27\" class=\"row_heading level1 row27\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_455f9_row27_col0\" class=\"data row27 col0\" >0.7446</td>\n",
              "      <td id=\"T_455f9_row27_col1\" class=\"data row27 col1\" >0.8013</td>\n",
              "      <td id=\"T_455f9_row27_col2\" class=\"data row27 col2\" >0.9373</td>\n",
              "      <td id=\"T_455f9_row27_col3\" class=\"data row27 col3\" >0.8827</td>\n",
              "      <td id=\"T_455f9_row27_col4\" class=\"data row27 col4\" >0.8984</td>\n",
              "      <td id=\"T_455f9_row27_col5\" class=\"data row27 col5\" >0.9914</td>\n",
              "      <td id=\"T_455f9_row27_col6\" class=\"data row27 col6\" >0.4673</td>\n",
              "      <td id=\"T_455f9_row27_col7\" class=\"data row27 col7\" >0.9448</td>\n",
              "      <td id=\"T_455f9_row27_col8\" class=\"data row27 col8\" >0.9899</td>\n",
              "      <td id=\"T_455f9_row27_col9\" class=\"data row27 col9\" >0.9798</td>\n",
              "      <td id=\"T_455f9_row27_col10\" class=\"data row27 col10\" >0.5992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level0_row28\" class=\"row_heading level0 row28\" rowspan=\"7\">opt-66b</th>\n",
              "      <th id=\"T_455f9_level1_row28\" class=\"row_heading level1 row28\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_455f9_row28_col0\" class=\"data row28 col0\" >0.7967</td>\n",
              "      <td id=\"T_455f9_row28_col1\" class=\"data row28 col1\" >0.7219</td>\n",
              "      <td id=\"T_455f9_row28_col2\" class=\"data row28 col2\" >0.7674</td>\n",
              "      <td id=\"T_455f9_row28_col3\" class=\"data row28 col3\" >0.7282</td>\n",
              "      <td id=\"T_455f9_row28_col4\" class=\"data row28 col4\" >0.8716</td>\n",
              "      <td id=\"T_455f9_row28_col5\" class=\"data row28 col5\" >0.8097</td>\n",
              "      <td id=\"T_455f9_row28_col6\" class=\"data row28 col6\" >0.6880</td>\n",
              "      <td id=\"T_455f9_row28_col7\" class=\"data row28 col7\" >0.8256</td>\n",
              "      <td id=\"T_455f9_row28_col8\" class=\"data row28 col8\" >0.9168</td>\n",
              "      <td id=\"T_455f9_row28_col9\" class=\"data row28 col9\" >0.9000</td>\n",
              "      <td id=\"T_455f9_row28_col10\" class=\"data row28 col10\" >0.7730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row29\" class=\"row_heading level1 row29\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_455f9_row29_col0\" class=\"data row29 col0\" >0.4285</td>\n",
              "      <td id=\"T_455f9_row29_col1\" class=\"data row29 col1\" >0.7333</td>\n",
              "      <td id=\"T_455f9_row29_col2\" class=\"data row29 col2\" >0.6311</td>\n",
              "      <td id=\"T_455f9_row29_col3\" class=\"data row29 col3\" >0.7610</td>\n",
              "      <td id=\"T_455f9_row29_col4\" class=\"data row29 col4\" >0.8950</td>\n",
              "      <td id=\"T_455f9_row29_col5\" class=\"data row29 col5\" >0.7945</td>\n",
              "      <td id=\"T_455f9_row29_col6\" class=\"data row29 col6\" >0.7222</td>\n",
              "      <td id=\"T_455f9_row29_col7\" class=\"data row29 col7\" >0.8359</td>\n",
              "      <td id=\"T_455f9_row29_col8\" class=\"data row29 col8\" >0.3401</td>\n",
              "      <td id=\"T_455f9_row29_col9\" class=\"data row29 col9\" >0.3394</td>\n",
              "      <td id=\"T_455f9_row29_col10\" class=\"data row29 col10\" >0.3731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row30\" class=\"row_heading level1 row30\" >gpt2-medium</th>\n",
              "      <td id=\"T_455f9_row30_col0\" class=\"data row30 col0\" >0.6023</td>\n",
              "      <td id=\"T_455f9_row30_col1\" class=\"data row30 col1\" >0.8078</td>\n",
              "      <td id=\"T_455f9_row30_col2\" class=\"data row30 col2\" >0.6790</td>\n",
              "      <td id=\"T_455f9_row30_col3\" class=\"data row30 col3\" >0.7160</td>\n",
              "      <td id=\"T_455f9_row30_col4\" class=\"data row30 col4\" >0.8877</td>\n",
              "      <td id=\"T_455f9_row30_col5\" class=\"data row30 col5\" >0.7941</td>\n",
              "      <td id=\"T_455f9_row30_col6\" class=\"data row30 col6\" >0.6774</td>\n",
              "      <td id=\"T_455f9_row30_col7\" class=\"data row30 col7\" >0.7390</td>\n",
              "      <td id=\"T_455f9_row30_col8\" class=\"data row30 col8\" >0.9070</td>\n",
              "      <td id=\"T_455f9_row30_col9\" class=\"data row30 col9\" >0.8897</td>\n",
              "      <td id=\"T_455f9_row30_col10\" class=\"data row30 col10\" >0.5969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row31\" class=\"row_heading level1 row31\" >mGPT</th>\n",
              "      <td id=\"T_455f9_row31_col0\" class=\"data row31 col0\" >0.9069</td>\n",
              "      <td id=\"T_455f9_row31_col1\" class=\"data row31 col1\" >0.8440</td>\n",
              "      <td id=\"T_455f9_row31_col2\" class=\"data row31 col2\" >0.8010</td>\n",
              "      <td id=\"T_455f9_row31_col3\" class=\"data row31 col3\" >0.8186</td>\n",
              "      <td id=\"T_455f9_row31_col4\" class=\"data row31 col4\" >0.9043</td>\n",
              "      <td id=\"T_455f9_row31_col5\" class=\"data row31 col5\" >0.8626</td>\n",
              "      <td id=\"T_455f9_row31_col6\" class=\"data row31 col6\" >0.7591</td>\n",
              "      <td id=\"T_455f9_row31_col7\" class=\"data row31 col7\" >0.8012</td>\n",
              "      <td id=\"T_455f9_row31_col8\" class=\"data row31 col8\" >0.9747</td>\n",
              "      <td id=\"T_455f9_row31_col9\" class=\"data row31 col9\" >0.9729</td>\n",
              "      <td id=\"T_455f9_row31_col10\" class=\"data row31 col10\" >0.6929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row32\" class=\"row_heading level1 row32\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_455f9_row32_col0\" class=\"data row32 col0\" >0.8913</td>\n",
              "      <td id=\"T_455f9_row32_col1\" class=\"data row32 col1\" >0.9396</td>\n",
              "      <td id=\"T_455f9_row32_col2\" class=\"data row32 col2\" >1.0000</td>\n",
              "      <td id=\"T_455f9_row32_col3\" class=\"data row32 col3\" >0.8978</td>\n",
              "      <td id=\"T_455f9_row32_col4\" class=\"data row32 col4\" >0.8479</td>\n",
              "      <td id=\"T_455f9_row32_col5\" class=\"data row32 col5\" >0.8847</td>\n",
              "      <td id=\"T_455f9_row32_col6\" class=\"data row32 col6\" >0.8763</td>\n",
              "      <td id=\"T_455f9_row32_col7\" class=\"data row32 col7\" >0.8646</td>\n",
              "      <td id=\"T_455f9_row32_col8\" class=\"data row32 col8\" >0.9882</td>\n",
              "      <td id=\"T_455f9_row32_col9\" class=\"data row32 col9\" >0.9831</td>\n",
              "      <td id=\"T_455f9_row32_col10\" class=\"data row32 col10\" >0.8073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row33\" class=\"row_heading level1 row33\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_455f9_row33_col0\" class=\"data row33 col0\" >0.6401</td>\n",
              "      <td id=\"T_455f9_row33_col1\" class=\"data row33 col1\" >0.6683</td>\n",
              "      <td id=\"T_455f9_row33_col2\" class=\"data row33 col2\" >0.8647</td>\n",
              "      <td id=\"T_455f9_row33_col3\" class=\"data row33 col3\" >0.7966</td>\n",
              "      <td id=\"T_455f9_row33_col4\" class=\"data row33 col4\" >0.9097</td>\n",
              "      <td id=\"T_455f9_row33_col5\" class=\"data row33 col5\" >0.8645</td>\n",
              "      <td id=\"T_455f9_row33_col6\" class=\"data row33 col6\" >0.8275</td>\n",
              "      <td id=\"T_455f9_row33_col7\" class=\"data row33 col7\" >0.8683</td>\n",
              "      <td id=\"T_455f9_row33_col8\" class=\"data row33 col8\" >0.8985</td>\n",
              "      <td id=\"T_455f9_row33_col9\" class=\"data row33 col9\" >0.8812</td>\n",
              "      <td id=\"T_455f9_row33_col10\" class=\"data row33 col10\" >0.6668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row34\" class=\"row_heading level1 row34\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_455f9_row34_col0\" class=\"data row34 col0\" >0.9410</td>\n",
              "      <td id=\"T_455f9_row34_col1\" class=\"data row34 col1\" >0.9514</td>\n",
              "      <td id=\"T_455f9_row34_col2\" class=\"data row34 col2\" >0.9967</td>\n",
              "      <td id=\"T_455f9_row34_col3\" class=\"data row34 col3\" >0.8348</td>\n",
              "      <td id=\"T_455f9_row34_col4\" class=\"data row34 col4\" >0.8879</td>\n",
              "      <td id=\"T_455f9_row34_col5\" class=\"data row34 col5\" >0.8695</td>\n",
              "      <td id=\"T_455f9_row34_col6\" class=\"data row34 col6\" >0.8959</td>\n",
              "      <td id=\"T_455f9_row34_col7\" class=\"data row34 col7\" >0.9212</td>\n",
              "      <td id=\"T_455f9_row34_col8\" class=\"data row34 col8\" >0.9746</td>\n",
              "      <td id=\"T_455f9_row34_col9\" class=\"data row34 col9\" >0.9966</td>\n",
              "      <td id=\"T_455f9_row34_col10\" class=\"data row34 col10\" >0.7806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level0_row35\" class=\"row_heading level0 row35\" rowspan=\"7\">opt-iml-max-1.3b</th>\n",
              "      <th id=\"T_455f9_level1_row35\" class=\"row_heading level1 row35\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_455f9_row35_col0\" class=\"data row35 col0\" >0.9503</td>\n",
              "      <td id=\"T_455f9_row35_col1\" class=\"data row35 col1\" >0.9327</td>\n",
              "      <td id=\"T_455f9_row35_col2\" class=\"data row35 col2\" >0.9598</td>\n",
              "      <td id=\"T_455f9_row35_col3\" class=\"data row35 col3\" >0.8388</td>\n",
              "      <td id=\"T_455f9_row35_col4\" class=\"data row35 col4\" >0.9206</td>\n",
              "      <td id=\"T_455f9_row35_col5\" class=\"data row35 col5\" >0.9219</td>\n",
              "      <td id=\"T_455f9_row35_col6\" class=\"data row35 col6\" >0.9006</td>\n",
              "      <td id=\"T_455f9_row35_col7\" class=\"data row35 col7\" >0.9224</td>\n",
              "      <td id=\"T_455f9_row35_col8\" class=\"data row35 col8\" >0.9863</td>\n",
              "      <td id=\"T_455f9_row35_col9\" class=\"data row35 col9\" >0.9845</td>\n",
              "      <td id=\"T_455f9_row35_col10\" class=\"data row35 col10\" >0.9494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row36\" class=\"row_heading level1 row36\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_455f9_row36_col0\" class=\"data row36 col0\" >0.3386</td>\n",
              "      <td id=\"T_455f9_row36_col1\" class=\"data row36 col1\" >0.9243</td>\n",
              "      <td id=\"T_455f9_row36_col2\" class=\"data row36 col2\" >0.8798</td>\n",
              "      <td id=\"T_455f9_row36_col3\" class=\"data row36 col3\" >0.8805</td>\n",
              "      <td id=\"T_455f9_row36_col4\" class=\"data row36 col4\" >0.9801</td>\n",
              "      <td id=\"T_455f9_row36_col5\" class=\"data row36 col5\" >0.9672</td>\n",
              "      <td id=\"T_455f9_row36_col6\" class=\"data row36 col6\" >0.9381</td>\n",
              "      <td id=\"T_455f9_row36_col7\" class=\"data row36 col7\" >0.9378</td>\n",
              "      <td id=\"T_455f9_row36_col8\" class=\"data row36 col8\" >0.3425</td>\n",
              "      <td id=\"T_455f9_row36_col9\" class=\"data row36 col9\" >0.3429</td>\n",
              "      <td id=\"T_455f9_row36_col10\" class=\"data row36 col10\" >0.4383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row37\" class=\"row_heading level1 row37\" >gpt2-medium</th>\n",
              "      <td id=\"T_455f9_row37_col0\" class=\"data row37 col0\" >0.6676</td>\n",
              "      <td id=\"T_455f9_row37_col1\" class=\"data row37 col1\" >0.9815</td>\n",
              "      <td id=\"T_455f9_row37_col2\" class=\"data row37 col2\" >0.9295</td>\n",
              "      <td id=\"T_455f9_row37_col3\" class=\"data row37 col3\" >0.8375</td>\n",
              "      <td id=\"T_455f9_row37_col4\" class=\"data row37 col4\" >0.9548</td>\n",
              "      <td id=\"T_455f9_row37_col5\" class=\"data row37 col5\" >0.9585</td>\n",
              "      <td id=\"T_455f9_row37_col6\" class=\"data row37 col6\" >0.9159</td>\n",
              "      <td id=\"T_455f9_row37_col7\" class=\"data row37 col7\" >0.8588</td>\n",
              "      <td id=\"T_455f9_row37_col8\" class=\"data row37 col8\" >0.9281</td>\n",
              "      <td id=\"T_455f9_row37_col9\" class=\"data row37 col9\" >0.9255</td>\n",
              "      <td id=\"T_455f9_row37_col10\" class=\"data row37 col10\" >0.6572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row38\" class=\"row_heading level1 row38\" >mGPT</th>\n",
              "      <td id=\"T_455f9_row38_col0\" class=\"data row38 col0\" >0.9760</td>\n",
              "      <td id=\"T_455f9_row38_col1\" class=\"data row38 col1\" >0.9597</td>\n",
              "      <td id=\"T_455f9_row38_col2\" class=\"data row38 col2\" >0.9243</td>\n",
              "      <td id=\"T_455f9_row38_col3\" class=\"data row38 col3\" >0.9524</td>\n",
              "      <td id=\"T_455f9_row38_col4\" class=\"data row38 col4\" >0.9567</td>\n",
              "      <td id=\"T_455f9_row38_col5\" class=\"data row38 col5\" >0.9793</td>\n",
              "      <td id=\"T_455f9_row38_col6\" class=\"data row38 col6\" >0.9565</td>\n",
              "      <td id=\"T_455f9_row38_col7\" class=\"data row38 col7\" >0.9707</td>\n",
              "      <td id=\"T_455f9_row38_col8\" class=\"data row38 col8\" >0.9983</td>\n",
              "      <td id=\"T_455f9_row38_col9\" class=\"data row38 col9\" >0.9966</td>\n",
              "      <td id=\"T_455f9_row38_col10\" class=\"data row38 col10\" >0.7681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row39\" class=\"row_heading level1 row39\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_455f9_row39_col0\" class=\"data row39 col0\" >0.9760</td>\n",
              "      <td id=\"T_455f9_row39_col1\" class=\"data row39 col1\" >0.9882</td>\n",
              "      <td id=\"T_455f9_row39_col2\" class=\"data row39 col2\" >1.0000</td>\n",
              "      <td id=\"T_455f9_row39_col3\" class=\"data row39 col3\" >0.9575</td>\n",
              "      <td id=\"T_455f9_row39_col4\" class=\"data row39 col4\" >0.9205</td>\n",
              "      <td id=\"T_455f9_row39_col5\" class=\"data row39 col5\" >0.9672</td>\n",
              "      <td id=\"T_455f9_row39_col6\" class=\"data row39 col6\" >0.9732</td>\n",
              "      <td id=\"T_455f9_row39_col7\" class=\"data row39 col7\" >0.9810</td>\n",
              "      <td id=\"T_455f9_row39_col8\" class=\"data row39 col8\" >1.0000</td>\n",
              "      <td id=\"T_455f9_row39_col9\" class=\"data row39 col9\" >0.9966</td>\n",
              "      <td id=\"T_455f9_row39_col10\" class=\"data row39 col10\" >0.7999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row40\" class=\"row_heading level1 row40\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_455f9_row40_col0\" class=\"data row40 col0\" >0.6580</td>\n",
              "      <td id=\"T_455f9_row40_col1\" class=\"data row40 col1\" >0.9511</td>\n",
              "      <td id=\"T_455f9_row40_col2\" class=\"data row40 col2\" >0.9783</td>\n",
              "      <td id=\"T_455f9_row40_col3\" class=\"data row40 col3\" >0.9405</td>\n",
              "      <td id=\"T_455f9_row40_col4\" class=\"data row40 col4\" >0.9675</td>\n",
              "      <td id=\"T_455f9_row40_col5\" class=\"data row40 col5\" >0.9810</td>\n",
              "      <td id=\"T_455f9_row40_col6\" class=\"data row40 col6\" >0.9799</td>\n",
              "      <td id=\"T_455f9_row40_col7\" class=\"data row40 col7\" >0.9326</td>\n",
              "      <td id=\"T_455f9_row40_col8\" class=\"data row40 col8\" >0.9215</td>\n",
              "      <td id=\"T_455f9_row40_col9\" class=\"data row40 col9\" >0.9191</td>\n",
              "      <td id=\"T_455f9_row40_col10\" class=\"data row40 col10\" >0.6601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row41\" class=\"row_heading level1 row41\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_455f9_row41_col0\" class=\"data row41 col0\" >0.9863</td>\n",
              "      <td id=\"T_455f9_row41_col1\" class=\"data row41 col1\" >0.9966</td>\n",
              "      <td id=\"T_455f9_row41_col2\" class=\"data row41 col2\" >1.0000</td>\n",
              "      <td id=\"T_455f9_row41_col3\" class=\"data row41 col3\" >0.9932</td>\n",
              "      <td id=\"T_455f9_row41_col4\" class=\"data row41 col4\" >0.9054</td>\n",
              "      <td id=\"T_455f9_row41_col5\" class=\"data row41 col5\" >0.9689</td>\n",
              "      <td id=\"T_455f9_row41_col6\" class=\"data row41 col6\" >0.9916</td>\n",
              "      <td id=\"T_455f9_row41_col7\" class=\"data row41 col7\" >0.9707</td>\n",
              "      <td id=\"T_455f9_row41_col8\" class=\"data row41 col8\" >1.0000</td>\n",
              "      <td id=\"T_455f9_row41_col9\" class=\"data row41 col9\" >0.9983</td>\n",
              "      <td id=\"T_455f9_row41_col10\" class=\"data row41 col10\" >0.8078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level0_row42\" class=\"row_heading level0 row42\" rowspan=\"7\">text-davinci-003</th>\n",
              "      <th id=\"T_455f9_level1_row42\" class=\"row_heading level1 row42\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_455f9_row42_col0\" class=\"data row42 col0\" >0.8246</td>\n",
              "      <td id=\"T_455f9_row42_col1\" class=\"data row42 col1\" >0.9583</td>\n",
              "      <td id=\"T_455f9_row42_col2\" class=\"data row42 col2\" >0.9349</td>\n",
              "      <td id=\"T_455f9_row42_col3\" class=\"data row42 col3\" >0.9527</td>\n",
              "      <td id=\"T_455f9_row42_col4\" class=\"data row42 col4\" >0.9639</td>\n",
              "      <td id=\"T_455f9_row42_col5\" class=\"data row42 col5\" >0.9640</td>\n",
              "      <td id=\"T_455f9_row42_col6\" class=\"data row42 col6\" >0.9616</td>\n",
              "      <td id=\"T_455f9_row42_col7\" class=\"data row42 col7\" >0.9351</td>\n",
              "      <td id=\"T_455f9_row42_col8\" class=\"data row42 col8\" >0.9098</td>\n",
              "      <td id=\"T_455f9_row42_col9\" class=\"data row42 col9\" >0.8775</td>\n",
              "      <td id=\"T_455f9_row42_col10\" class=\"data row42 col10\" >0.9183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row43\" class=\"row_heading level1 row43\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_455f9_row43_col0\" class=\"data row43 col0\" >0.3330</td>\n",
              "      <td id=\"T_455f9_row43_col1\" class=\"data row43 col1\" >0.7572</td>\n",
              "      <td id=\"T_455f9_row43_col2\" class=\"data row43 col2\" >0.3333</td>\n",
              "      <td id=\"T_455f9_row43_col3\" class=\"data row43 col3\" >0.3377</td>\n",
              "      <td id=\"T_455f9_row43_col4\" class=\"data row43 col4\" >0.9838</td>\n",
              "      <td id=\"T_455f9_row43_col5\" class=\"data row43 col5\" >0.9623</td>\n",
              "      <td id=\"T_455f9_row43_col6\" class=\"data row43 col6\" >0.5772</td>\n",
              "      <td id=\"T_455f9_row43_col7\" class=\"data row43 col7\" >0.8433</td>\n",
              "      <td id=\"T_455f9_row43_col8\" class=\"data row43 col8\" >0.3337</td>\n",
              "      <td id=\"T_455f9_row43_col9\" class=\"data row43 col9\" >0.3330</td>\n",
              "      <td id=\"T_455f9_row43_col10\" class=\"data row43 col10\" >0.3781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row44\" class=\"row_heading level1 row44\" >gpt2-medium</th>\n",
              "      <td id=\"T_455f9_row44_col0\" class=\"data row44 col0\" >0.7628</td>\n",
              "      <td id=\"T_455f9_row44_col1\" class=\"data row44 col1\" >0.9299</td>\n",
              "      <td id=\"T_455f9_row44_col2\" class=\"data row44 col2\" >0.6175</td>\n",
              "      <td id=\"T_455f9_row44_col3\" class=\"data row44 col3\" >0.7370</td>\n",
              "      <td id=\"T_455f9_row44_col4\" class=\"data row44 col4\" >0.9819</td>\n",
              "      <td id=\"T_455f9_row44_col5\" class=\"data row44 col5\" >0.9365</td>\n",
              "      <td id=\"T_455f9_row44_col6\" class=\"data row44 col6\" >0.7891</td>\n",
              "      <td id=\"T_455f9_row44_col7\" class=\"data row44 col7\" >0.8164</td>\n",
              "      <td id=\"T_455f9_row44_col8\" class=\"data row44 col8\" >0.8564</td>\n",
              "      <td id=\"T_455f9_row44_col9\" class=\"data row44 col9\" >0.8406</td>\n",
              "      <td id=\"T_455f9_row44_col10\" class=\"data row44 col10\" >0.4081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row45\" class=\"row_heading level1 row45\" >mGPT</th>\n",
              "      <td id=\"T_455f9_row45_col0\" class=\"data row45 col0\" >0.8462</td>\n",
              "      <td id=\"T_455f9_row45_col1\" class=\"data row45 col1\" >0.6847</td>\n",
              "      <td id=\"T_455f9_row45_col2\" class=\"data row45 col2\" >0.3333</td>\n",
              "      <td id=\"T_455f9_row45_col3\" class=\"data row45 col3\" >0.9730</td>\n",
              "      <td id=\"T_455f9_row45_col4\" class=\"data row45 col4\" >0.9801</td>\n",
              "      <td id=\"T_455f9_row45_col5\" class=\"data row45 col5\" >0.9743</td>\n",
              "      <td id=\"T_455f9_row45_col6\" class=\"data row45 col6\" >0.9599</td>\n",
              "      <td id=\"T_455f9_row45_col7\" class=\"data row45 col7\" >0.9438</td>\n",
              "      <td id=\"T_455f9_row45_col8\" class=\"data row45 col8\" >0.9499</td>\n",
              "      <td id=\"T_455f9_row45_col9\" class=\"data row45 col9\" >0.8396</td>\n",
              "      <td id=\"T_455f9_row45_col10\" class=\"data row45 col10\" >0.8442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row46\" class=\"row_heading level1 row46\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_455f9_row46_col0\" class=\"data row46 col0\" >0.7598</td>\n",
              "      <td id=\"T_455f9_row46_col1\" class=\"data row46 col1\" >0.8536</td>\n",
              "      <td id=\"T_455f9_row46_col2\" class=\"data row46 col2\" >0.9516</td>\n",
              "      <td id=\"T_455f9_row46_col3\" class=\"data row46 col3\" >0.9118</td>\n",
              "      <td id=\"T_455f9_row46_col4\" class=\"data row46 col4\" >0.8964</td>\n",
              "      <td id=\"T_455f9_row46_col5\" class=\"data row46 col5\" >0.8446</td>\n",
              "      <td id=\"T_455f9_row46_col6\" class=\"data row46 col6\" >0.8755</td>\n",
              "      <td id=\"T_455f9_row46_col7\" class=\"data row46 col7\" >0.8891</td>\n",
              "      <td id=\"T_455f9_row46_col8\" class=\"data row46 col8\" >0.9349</td>\n",
              "      <td id=\"T_455f9_row46_col9\" class=\"data row46 col9\" >0.9244</td>\n",
              "      <td id=\"T_455f9_row46_col10\" class=\"data row46 col10\" >0.9331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row47\" class=\"row_heading level1 row47\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_455f9_row47_col0\" class=\"data row47 col0\" >0.6837</td>\n",
              "      <td id=\"T_455f9_row47_col1\" class=\"data row47 col1\" >0.7831</td>\n",
              "      <td id=\"T_455f9_row47_col2\" class=\"data row47 col2\" >0.8645</td>\n",
              "      <td id=\"T_455f9_row47_col3\" class=\"data row47 col3\" >0.8792</td>\n",
              "      <td id=\"T_455f9_row47_col4\" class=\"data row47 col4\" >0.8488</td>\n",
              "      <td id=\"T_455f9_row47_col5\" class=\"data row47 col5\" >0.9257</td>\n",
              "      <td id=\"T_455f9_row47_col6\" class=\"data row47 col6\" >0.8654</td>\n",
              "      <td id=\"T_455f9_row47_col7\" class=\"data row47 col7\" >0.8806</td>\n",
              "      <td id=\"T_455f9_row47_col8\" class=\"data row47 col8\" >0.8556</td>\n",
              "      <td id=\"T_455f9_row47_col9\" class=\"data row47 col9\" >0.8590</td>\n",
              "      <td id=\"T_455f9_row47_col10\" class=\"data row47 col10\" >0.6139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row48\" class=\"row_heading level1 row48\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_455f9_row48_col0\" class=\"data row48 col0\" >0.6706</td>\n",
              "      <td id=\"T_455f9_row48_col1\" class=\"data row48 col1\" >0.7938</td>\n",
              "      <td id=\"T_455f9_row48_col2\" class=\"data row48 col2\" >0.8870</td>\n",
              "      <td id=\"T_455f9_row48_col3\" class=\"data row48 col3\" >0.9357</td>\n",
              "      <td id=\"T_455f9_row48_col4\" class=\"data row48 col4\" >0.9693</td>\n",
              "      <td id=\"T_455f9_row48_col5\" class=\"data row48 col5\" >0.9298</td>\n",
              "      <td id=\"T_455f9_row48_col6\" class=\"data row48 col6\" >0.9649</td>\n",
              "      <td id=\"T_455f9_row48_col7\" class=\"data row48 col7\" >0.9062</td>\n",
              "      <td id=\"T_455f9_row48_col8\" class=\"data row48 col8\" >0.9041</td>\n",
              "      <td id=\"T_455f9_row48_col9\" class=\"data row48 col9\" >0.8336</td>\n",
              "      <td id=\"T_455f9_row48_col10\" class=\"data row48 col10\" >0.6714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level0_row49\" class=\"row_heading level0 row49\" rowspan=\"7\">vicuna-13b</th>\n",
              "      <th id=\"T_455f9_level1_row49\" class=\"row_heading level1 row49\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_455f9_row49_col0\" class=\"data row49 col0\" >0.8294</td>\n",
              "      <td id=\"T_455f9_row49_col1\" class=\"data row49 col1\" >0.9533</td>\n",
              "      <td id=\"T_455f9_row49_col2\" class=\"data row49 col2\" >0.8963</td>\n",
              "      <td id=\"T_455f9_row49_col3\" class=\"data row49 col3\" >0.8902</td>\n",
              "      <td id=\"T_455f9_row49_col4\" class=\"data row49 col4\" >0.9603</td>\n",
              "      <td id=\"T_455f9_row49_col5\" class=\"data row49 col5\" >0.9486</td>\n",
              "      <td id=\"T_455f9_row49_col6\" class=\"data row49 col6\" >0.9599</td>\n",
              "      <td id=\"T_455f9_row49_col7\" class=\"data row49 col7\" >0.9420</td>\n",
              "      <td id=\"T_455f9_row49_col8\" class=\"data row49 col8\" >0.9398</td>\n",
              "      <td id=\"T_455f9_row49_col9\" class=\"data row49 col9\" >0.8928</td>\n",
              "      <td id=\"T_455f9_row49_col10\" class=\"data row49 col10\" >0.7173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row50\" class=\"row_heading level1 row50\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_455f9_row50_col0\" class=\"data row50 col0\" >0.3548</td>\n",
              "      <td id=\"T_455f9_row50_col1\" class=\"data row50 col1\" >0.8538</td>\n",
              "      <td id=\"T_455f9_row50_col2\" class=\"data row50 col2\" >0.4133</td>\n",
              "      <td id=\"T_455f9_row50_col3\" class=\"data row50 col3\" >0.4349</td>\n",
              "      <td id=\"T_455f9_row50_col4\" class=\"data row50 col4\" >0.9874</td>\n",
              "      <td id=\"T_455f9_row50_col5\" class=\"data row50 col5\" >0.9536</td>\n",
              "      <td id=\"T_455f9_row50_col6\" class=\"data row50 col6\" >0.6708</td>\n",
              "      <td id=\"T_455f9_row50_col7\" class=\"data row50 col7\" >0.9148</td>\n",
              "      <td id=\"T_455f9_row50_col8\" class=\"data row50 col8\" >0.3703</td>\n",
              "      <td id=\"T_455f9_row50_col9\" class=\"data row50 col9\" >0.3400</td>\n",
              "      <td id=\"T_455f9_row50_col10\" class=\"data row50 col10\" >0.4983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row51\" class=\"row_heading level1 row51\" >gpt2-medium</th>\n",
              "      <td id=\"T_455f9_row51_col0\" class=\"data row51 col0\" >0.7526</td>\n",
              "      <td id=\"T_455f9_row51_col1\" class=\"data row51 col1\" >0.9010</td>\n",
              "      <td id=\"T_455f9_row51_col2\" class=\"data row51 col2\" >0.6593</td>\n",
              "      <td id=\"T_455f9_row51_col3\" class=\"data row51 col3\" >0.6846</td>\n",
              "      <td id=\"T_455f9_row51_col4\" class=\"data row51 col4\" >0.9838</td>\n",
              "      <td id=\"T_455f9_row51_col5\" class=\"data row51 col5\" >0.9331</td>\n",
              "      <td id=\"T_455f9_row51_col6\" class=\"data row51 col6\" >0.7428</td>\n",
              "      <td id=\"T_455f9_row51_col7\" class=\"data row51 col7\" >0.8293</td>\n",
              "      <td id=\"T_455f9_row51_col8\" class=\"data row51 col8\" >0.8517</td>\n",
              "      <td id=\"T_455f9_row51_col9\" class=\"data row51 col9\" >0.6033</td>\n",
              "      <td id=\"T_455f9_row51_col10\" class=\"data row51 col10\" >0.4810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row52\" class=\"row_heading level1 row52\" >mGPT</th>\n",
              "      <td id=\"T_455f9_row52_col0\" class=\"data row52 col0\" >0.8467</td>\n",
              "      <td id=\"T_455f9_row52_col1\" class=\"data row52 col1\" >0.8620</td>\n",
              "      <td id=\"T_455f9_row52_col2\" class=\"data row52 col2\" >0.5100</td>\n",
              "      <td id=\"T_455f9_row52_col3\" class=\"data row52 col3\" >0.9031</td>\n",
              "      <td id=\"T_455f9_row52_col4\" class=\"data row52 col4\" >0.9801</td>\n",
              "      <td id=\"T_455f9_row52_col5\" class=\"data row52 col5\" >0.9398</td>\n",
              "      <td id=\"T_455f9_row52_col6\" class=\"data row52 col6\" >0.8907</td>\n",
              "      <td id=\"T_455f9_row52_col7\" class=\"data row52 col7\" >0.9352</td>\n",
              "      <td id=\"T_455f9_row52_col8\" class=\"data row52 col8\" >0.9413</td>\n",
              "      <td id=\"T_455f9_row52_col9\" class=\"data row52 col9\" >0.8451</td>\n",
              "      <td id=\"T_455f9_row52_col10\" class=\"data row52 col10\" >0.6522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row53\" class=\"row_heading level1 row53\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_455f9_row53_col0\" class=\"data row53 col0\" >0.8543</td>\n",
              "      <td id=\"T_455f9_row53_col1\" class=\"data row53 col1\" >0.8078</td>\n",
              "      <td id=\"T_455f9_row53_col2\" class=\"data row53 col2\" >0.8991</td>\n",
              "      <td id=\"T_455f9_row53_col3\" class=\"data row53 col3\" >0.9204</td>\n",
              "      <td id=\"T_455f9_row53_col4\" class=\"data row53 col4\" >0.8907</td>\n",
              "      <td id=\"T_455f9_row53_col5\" class=\"data row53 col5\" >0.8410</td>\n",
              "      <td id=\"T_455f9_row53_col6\" class=\"data row53 col6\" >0.8596</td>\n",
              "      <td id=\"T_455f9_row53_col7\" class=\"data row53 col7\" >0.8721</td>\n",
              "      <td id=\"T_455f9_row53_col8\" class=\"data row53 col8\" >0.9363</td>\n",
              "      <td id=\"T_455f9_row53_col9\" class=\"data row53 col9\" >0.9364</td>\n",
              "      <td id=\"T_455f9_row53_col10\" class=\"data row53 col10\" >0.7012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row54\" class=\"row_heading level1 row54\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_455f9_row54_col0\" class=\"data row54 col0\" >0.7635</td>\n",
              "      <td id=\"T_455f9_row54_col1\" class=\"data row54 col1\" >0.9214</td>\n",
              "      <td id=\"T_455f9_row54_col2\" class=\"data row54 col2\" >0.8653</td>\n",
              "      <td id=\"T_455f9_row54_col3\" class=\"data row54 col3\" >0.9070</td>\n",
              "      <td id=\"T_455f9_row54_col4\" class=\"data row54 col4\" >0.9729</td>\n",
              "      <td id=\"T_455f9_row54_col5\" class=\"data row54 col5\" >0.9570</td>\n",
              "      <td id=\"T_455f9_row54_col6\" class=\"data row54 col6\" >0.8976</td>\n",
              "      <td id=\"T_455f9_row54_col7\" class=\"data row54 col7\" >0.9281</td>\n",
              "      <td id=\"T_455f9_row54_col8\" class=\"data row54 col8\" >0.8671</td>\n",
              "      <td id=\"T_455f9_row54_col9\" class=\"data row54 col9\" >0.7857</td>\n",
              "      <td id=\"T_455f9_row54_col10\" class=\"data row54 col10\" >0.7200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_455f9_level1_row55\" class=\"row_heading level1 row55\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_455f9_row55_col0\" class=\"data row55 col0\" >0.8809</td>\n",
              "      <td id=\"T_455f9_row55_col1\" class=\"data row55 col1\" >0.9517</td>\n",
              "      <td id=\"T_455f9_row55_col2\" class=\"data row55 col2\" >0.9348</td>\n",
              "      <td id=\"T_455f9_row55_col3\" class=\"data row55 col3\" >0.9594</td>\n",
              "      <td id=\"T_455f9_row55_col4\" class=\"data row55 col4\" >0.9819</td>\n",
              "      <td id=\"T_455f9_row55_col5\" class=\"data row55 col5\" >0.9709</td>\n",
              "      <td id=\"T_455f9_row55_col6\" class=\"data row55 col6\" >0.9179</td>\n",
              "      <td id=\"T_455f9_row55_col7\" class=\"data row55 col7\" >0.9213</td>\n",
              "      <td id=\"T_455f9_row55_col8\" class=\"data row55 col8\" >0.9749</td>\n",
              "      <td id=\"T_455f9_row55_col9\" class=\"data row55 col9\" >0.9565</td>\n",
              "      <td id=\"T_455f9_row55_col10\" class=\"data row55 col10\" >0.7442</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{llrrrrrrrrrrr}\n",
            " &  & \\bfseries ar & \\bfseries ca & \\bfseries cs & \\bfseries de & \\bfseries en & \\bfseries es & \\bfseries nl & \\bfseries pt & \\bfseries ru & \\bfseries uk & \\bfseries zh \\\\\n",
            "Train LLM & Model &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries alpaca-lora-30b} & \\bfseries bert-base-multilingual-cased & 0.5096 & 0.9133 & 0.8879 & 0.9138 & \\bfseries 0.9657 & 0.9401 & 0.9161 & 0.9113 & 0.9200 & 0.8928 & 0.7472 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.3367 & 0.7431 & 0.4389 & 0.5051 & \\bfseries 0.9856 & 0.9502 & 0.7716 & 0.8824 & 0.3407 & 0.3326 & 0.3999 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.8118 & 0.7896 & 0.6623 & 0.7747 & \\bfseries 0.9819 & 0.8919 & 0.8064 & 0.8307 & 0.8463 & 0.8251 & 0.4282 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.6955 & 0.8833 & 0.6480 & 0.9290 & \\bfseries 0.9585 & 0.9315 & 0.8215 & 0.9334 & 0.9383 & 0.9129 & 0.5555 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.8438 & 0.8062 & 0.8906 & 0.8727 & 0.9075 & 0.8643 & 0.8044 & 0.8364 & 0.9283 & \\bfseries 0.9331 & 0.7118 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.8040 & 0.7010 & 0.8484 & 0.8629 & 0.9348 & \\bfseries 0.9363 & 0.8641 & 0.8969 & 0.8817 & 0.8487 & 0.7252 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.7742 & 0.9283 & 0.9600 & 0.9578 & \\bfseries 0.9856 & 0.9589 & 0.9481 & 0.9522 & 0.9366 & 0.9246 & 0.8200 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries gpt-3.5-turbo} & \\bfseries bert-base-multilingual-cased & 0.8483 & 0.9750 & 0.9633 & 0.9391 & \\bfseries 0.9819 & 0.9777 & 0.9816 & 0.9506 & 0.9550 & 0.9313 & 0.9148 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.3501 & 0.7954 & 0.3333 & 0.3340 & \\bfseries 0.9819 & 0.9657 & 0.4354 & 0.8583 & 0.3326 & 0.3326 & 0.3901 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.6911 & 0.9180 & 0.5441 & 0.6283 & \\bfseries 0.9874 & 0.9452 & 0.7647 & 0.8586 & 0.8480 & 0.6995 & 0.3333 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.8856 & 0.9198 & 0.3552 & 0.8964 & \\bfseries 0.9801 & 0.9469 & 0.9566 & 0.9370 & 0.9383 & 0.8939 & 0.5693 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.9466 & 0.8710 & \\bfseries 0.9767 & 0.9578 & 0.9747 & 0.9280 & 0.8977 & 0.9039 & 0.9232 & 0.9398 & 0.8526 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.5451 & 0.6366 & 0.6189 & 0.7365 & \\bfseries 0.9765 & 0.9726 & 0.5633 & 0.9710 & 0.8933 & 0.5315 & 0.4295 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.9280 & 0.9850 & 0.9883 & 0.9425 & \\bfseries 0.9910 & 0.9846 & 0.9800 & 0.9727 & 0.9850 & 0.9464 & 0.9382 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries gpt-4} & \\bfseries bert-base-multilingual-cased & 0.9416 & 0.9566 & 0.9533 & 0.9255 & \\bfseries 0.9928 & 0.9485 & 0.9683 & 0.9453 & 0.9400 & 0.9365 & 0.9048 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.3330 & 0.9297 & 0.3333 & 0.3593 & \\bfseries 0.9946 & 0.9726 & 0.8213 & 0.9472 & 0.3333 & 0.3326 & 0.3333 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.7644 & 0.8923 & 0.6236 & 0.6851 & \\bfseries 0.9982 & 0.9279 & 0.7007 & 0.8714 & 0.8516 & 0.8235 & 0.3392 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.8911 & 0.9450 & 0.3407 & 0.9014 & \\bfseries 0.9928 & 0.9640 & 0.9246 & 0.9487 & 0.9467 & 0.9212 & 0.7236 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.9382 & 0.9315 & \\bfseries 0.9883 & 0.9797 & 0.9457 & 0.9502 & 0.9582 & 0.9195 & 0.9383 & 0.9498 & 0.8854 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.7820 & 0.8696 & 0.8308 & 0.8932 & \\bfseries 0.9838 & 0.9657 & 0.7753 & 0.9590 & 0.8548 & 0.7828 & 0.5290 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.9599 & 0.9733 & 0.9850 & 0.9696 & \\bfseries 0.9946 & 0.9691 & 0.9816 & 0.9829 & 0.9733 & 0.9833 & 0.9365 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries llama-65b} & \\bfseries bert-base-multilingual-cased & 0.7821 & 0.8645 & 0.9288 & 0.7867 & 0.9076 & \\bfseries 0.9879 & 0.7835 & 0.9449 & 0.9782 & 0.9781 & 0.6580 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.7392 & 0.9281 & 0.9320 & 0.9455 & 0.8964 & \\bfseries 0.9862 & 0.8995 & 0.9570 & 0.9189 & 0.9443 & 0.6784 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.6580 & 0.9632 & 0.9491 & 0.9490 & 0.9016 & \\bfseries 0.9845 & 0.9279 & 0.9605 & 0.9765 & 0.9697 & 0.6849 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.8919 & 0.8853 & 0.9509 & 0.9058 & 0.9293 & \\bfseries 0.9862 & 0.7192 & 0.9691 & 0.9799 & 0.9731 & 0.5337 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.8006 & 0.8887 & 0.9831 & 0.8061 & 0.9004 & 0.9828 & 0.6450 & 0.9535 & \\bfseries 0.9899 & 0.9663 & 0.5974 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.6731 & 0.9766 & 0.9763 & 0.8597 & 0.9239 & \\bfseries 0.9862 & 0.9598 & 0.9690 & 0.9714 & 0.9781 & 0.4521 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.7446 & 0.8013 & 0.9373 & 0.8827 & 0.8984 & \\bfseries 0.9914 & 0.4673 & 0.9448 & 0.9899 & 0.9798 & 0.5992 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries opt-66b} & \\bfseries bert-base-multilingual-cased & 0.7967 & 0.7219 & 0.7674 & 0.7282 & 0.8716 & 0.8097 & 0.6880 & 0.8256 & \\bfseries 0.9168 & 0.9000 & 0.7730 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.4285 & 0.7333 & 0.6311 & 0.7610 & \\bfseries 0.8950 & 0.7945 & 0.7222 & 0.8359 & 0.3401 & 0.3394 & 0.3731 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.6023 & 0.8078 & 0.6790 & 0.7160 & 0.8877 & 0.7941 & 0.6774 & 0.7390 & \\bfseries 0.9070 & 0.8897 & 0.5969 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.9069 & 0.8440 & 0.8010 & 0.8186 & 0.9043 & 0.8626 & 0.7591 & 0.8012 & \\bfseries 0.9747 & 0.9729 & 0.6929 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.8913 & 0.9396 & \\bfseries 1.0000 & 0.8978 & 0.8479 & 0.8847 & 0.8763 & 0.8646 & 0.9882 & 0.9831 & 0.8073 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.6401 & 0.6683 & 0.8647 & 0.7966 & \\bfseries 0.9097 & 0.8645 & 0.8275 & 0.8683 & 0.8985 & 0.8812 & 0.6668 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.9410 & 0.9514 & \\bfseries 0.9967 & 0.8348 & 0.8879 & 0.8695 & 0.8959 & 0.9212 & 0.9746 & 0.9966 & 0.7806 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries opt-iml-max-1.3b} & \\bfseries bert-base-multilingual-cased & 0.9503 & 0.9327 & 0.9598 & 0.8388 & 0.9206 & 0.9219 & 0.9006 & 0.9224 & \\bfseries 0.9863 & 0.9845 & 0.9494 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.3386 & 0.9243 & 0.8798 & 0.8805 & \\bfseries 0.9801 & 0.9672 & 0.9381 & 0.9378 & 0.3425 & 0.3429 & 0.4383 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.6676 & \\bfseries 0.9815 & 0.9295 & 0.8375 & 0.9548 & 0.9585 & 0.9159 & 0.8588 & 0.9281 & 0.9255 & 0.6572 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.9760 & 0.9597 & 0.9243 & 0.9524 & 0.9567 & 0.9793 & 0.9565 & 0.9707 & \\bfseries 0.9983 & 0.9966 & 0.7681 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.9760 & 0.9882 & \\bfseries 1.0000 & 0.9575 & 0.9205 & 0.9672 & 0.9732 & 0.9810 & \\bfseries 1.0000 & 0.9966 & 0.7999 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.6580 & 0.9511 & 0.9783 & 0.9405 & 0.9675 & \\bfseries 0.9810 & 0.9799 & 0.9326 & 0.9215 & 0.9191 & 0.6601 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.9863 & 0.9966 & \\bfseries 1.0000 & 0.9932 & 0.9054 & 0.9689 & 0.9916 & 0.9707 & \\bfseries 1.0000 & 0.9983 & 0.8078 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries text-davinci-003} & \\bfseries bert-base-multilingual-cased & 0.8246 & 0.9583 & 0.9349 & 0.9527 & 0.9639 & \\bfseries 0.9640 & 0.9616 & 0.9351 & 0.9098 & 0.8775 & 0.9183 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.3330 & 0.7572 & 0.3333 & 0.3377 & \\bfseries 0.9838 & 0.9623 & 0.5772 & 0.8433 & 0.3337 & 0.3330 & 0.3781 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.7628 & 0.9299 & 0.6175 & 0.7370 & \\bfseries 0.9819 & 0.9365 & 0.7891 & 0.8164 & 0.8564 & 0.8406 & 0.4081 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.8462 & 0.6847 & 0.3333 & 0.9730 & \\bfseries 0.9801 & 0.9743 & 0.9599 & 0.9438 & 0.9499 & 0.8396 & 0.8442 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.7598 & 0.8536 & \\bfseries 0.9516 & 0.9118 & 0.8964 & 0.8446 & 0.8755 & 0.8891 & 0.9349 & 0.9244 & 0.9331 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.6837 & 0.7831 & 0.8645 & 0.8792 & 0.8488 & \\bfseries 0.9257 & 0.8654 & 0.8806 & 0.8556 & 0.8590 & 0.6139 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.6706 & 0.7938 & 0.8870 & 0.9357 & \\bfseries 0.9693 & 0.9298 & 0.9649 & 0.9062 & 0.9041 & 0.8336 & 0.6714 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries vicuna-13b} & \\bfseries bert-base-multilingual-cased & 0.8294 & 0.9533 & 0.8963 & 0.8902 & \\bfseries 0.9603 & 0.9486 & 0.9599 & 0.9420 & 0.9398 & 0.8928 & 0.7173 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.3548 & 0.8538 & 0.4133 & 0.4349 & \\bfseries 0.9874 & 0.9536 & 0.6708 & 0.9148 & 0.3703 & 0.3400 & 0.4983 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.7526 & 0.9010 & 0.6593 & 0.6846 & \\bfseries 0.9838 & 0.9331 & 0.7428 & 0.8293 & 0.8517 & 0.6033 & 0.4810 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.8467 & 0.8620 & 0.5100 & 0.9031 & \\bfseries 0.9801 & 0.9398 & 0.8907 & 0.9352 & 0.9413 & 0.8451 & 0.6522 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.8543 & 0.8078 & 0.8991 & 0.9204 & 0.8907 & 0.8410 & 0.8596 & 0.8721 & 0.9363 & \\bfseries 0.9364 & 0.7012 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.7635 & 0.9214 & 0.8653 & 0.9070 & \\bfseries 0.9729 & 0.9570 & 0.8976 & 0.9281 & 0.8671 & 0.7857 & 0.7200 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.8809 & 0.9517 & 0.9348 & 0.9594 & \\bfseries 0.9819 & 0.9709 & 0.9179 & 0.9213 & 0.9749 & 0.9565 & 0.7442 \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001e237f0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_b1daa_row0_col0 {\n",
              "  background-color: #a7bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row0_col1, #T_b1daa_row1_col6 {\n",
              "  background-color: #8cb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row0_col2 {\n",
              "  background-color: #9ebad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row0_col3, #T_b1daa_row0_col9 {\n",
              "  background-color: #96b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row0_col4 {\n",
              "  background-color: #7eadd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row0_col5, #T_b1daa_row1_col4, #T_b1daa_row2_col5 {\n",
              "  background-color: #80aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row0_col6 {\n",
              "  background-color: #94b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row0_col7, #T_b1daa_row1_col3 {\n",
              "  background-color: #86b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row0_col8, #T_b1daa_row1_col0 {\n",
              "  background-color: #91b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row0_col10 {\n",
              "  background-color: #b7c5df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row1_col1 {\n",
              "  background-color: #89b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row1_col2 {\n",
              "  background-color: #8fb4d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row1_col5, #T_b1daa_row1_col9 {\n",
              "  background-color: #81aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row1_col7 {\n",
              "  background-color: #83afd3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row1_col8, #T_b1daa_row2_col4 {\n",
              "  background-color: #7dacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row1_col10 {\n",
              "  background-color: #a2bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row2_col0 {\n",
              "  background-color: #bfc9e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row2_col1 {\n",
              "  background-color: #93b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row2_col2 {\n",
              "  background-color: #afc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row2_col3, #T_b1daa_row2_col8 {\n",
              "  background-color: #a9bfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row2_col6 {\n",
              "  background-color: #9fbad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row2_col7 {\n",
              "  background-color: #8bb2d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row2_col9 {\n",
              "  background-color: #b1c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_b1daa_row2_col10 {\n",
              "  background-color: #ced0e6;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_b1daa\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_b1daa_level0_col0\" class=\"col_heading level0 col0\" >ar</th>\n",
              "      <th id=\"T_b1daa_level0_col1\" class=\"col_heading level0 col1\" >ca</th>\n",
              "      <th id=\"T_b1daa_level0_col2\" class=\"col_heading level0 col2\" >cs</th>\n",
              "      <th id=\"T_b1daa_level0_col3\" class=\"col_heading level0 col3\" >de</th>\n",
              "      <th id=\"T_b1daa_level0_col4\" class=\"col_heading level0 col4\" >en</th>\n",
              "      <th id=\"T_b1daa_level0_col5\" class=\"col_heading level0 col5\" >es</th>\n",
              "      <th id=\"T_b1daa_level0_col6\" class=\"col_heading level0 col6\" >nl</th>\n",
              "      <th id=\"T_b1daa_level0_col7\" class=\"col_heading level0 col7\" >pt</th>\n",
              "      <th id=\"T_b1daa_level0_col8\" class=\"col_heading level0 col8\" >ru</th>\n",
              "      <th id=\"T_b1daa_level0_col9\" class=\"col_heading level0 col9\" >uk</th>\n",
              "      <th id=\"T_b1daa_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_b1daa_level0_row0\" class=\"row_heading level0 row0\" >All Detectors Mean</th>\n",
              "      <td id=\"T_b1daa_row0_col0\" class=\"data row0 col0\" >0.7463</td>\n",
              "      <td id=\"T_b1daa_row0_col1\" class=\"data row0 col1\" >0.8765</td>\n",
              "      <td id=\"T_b1daa_row0_col2\" class=\"data row0 col2\" >0.7918</td>\n",
              "      <td id=\"T_b1daa_row0_col3\" class=\"data row0 col3\" >0.8300</td>\n",
              "      <td id=\"T_b1daa_row0_col4\" class=\"data row0 col4\" >0.9472</td>\n",
              "      <td id=\"T_b1daa_row0_col5\" class=\"data row0 col5\" >0.9375</td>\n",
              "      <td id=\"T_b1daa_row0_col6\" class=\"data row0 col6\" >0.8407</td>\n",
              "      <td id=\"T_b1daa_row0_col7\" class=\"data row0 col7\" >0.9099</td>\n",
              "      <td id=\"T_b1daa_row0_col8\" class=\"data row0 col8\" >0.8592</td>\n",
              "      <td id=\"T_b1daa_row0_col9\" class=\"data row0 col9\" >0.8296</td>\n",
              "      <td id=\"T_b1daa_row0_col10\" class=\"data row0 col10\" >0.6558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_b1daa_level0_row1\" class=\"row_heading level0 row1\" >Multilingual Base Models Mean</th>\n",
              "      <td id=\"T_b1daa_row1_col0\" class=\"data row1 col0\" >0.8537</td>\n",
              "      <td id=\"T_b1daa_row1_col1\" class=\"data row1 col1\" >0.8977</td>\n",
              "      <td id=\"T_b1daa_row1_col2\" class=\"data row1 col2\" >0.8604</td>\n",
              "      <td id=\"T_b1daa_row1_col3\" class=\"data row1 col3\" >0.9073</td>\n",
              "      <td id=\"T_b1daa_row1_col4\" class=\"data row1 col4\" >0.9420</td>\n",
              "      <td id=\"T_b1daa_row1_col5\" class=\"data row1 col5\" >0.9372</td>\n",
              "      <td id=\"T_b1daa_row1_col6\" class=\"data row1 col6\" >0.8808</td>\n",
              "      <td id=\"T_b1daa_row1_col7\" class=\"data row1 col7\" >0.9253</td>\n",
              "      <td id=\"T_b1daa_row1_col8\" class=\"data row1 col8\" >0.9560</td>\n",
              "      <td id=\"T_b1daa_row1_col9\" class=\"data row1 col9\" >0.9374</td>\n",
              "      <td id=\"T_b1daa_row1_col10\" class=\"data row1 col10\" >0.7659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_b1daa_level0_row2\" class=\"row_heading level0 row2\" >Monolingual Base Models Mean</th>\n",
              "      <td id=\"T_b1daa_row2_col0\" class=\"data row2 col0\" >0.6031</td>\n",
              "      <td id=\"T_b1daa_row2_col1\" class=\"data row2 col1\" >0.8482</td>\n",
              "      <td id=\"T_b1daa_row2_col2\" class=\"data row2 col2\" >0.7003</td>\n",
              "      <td id=\"T_b1daa_row2_col3\" class=\"data row2 col3\" >0.7269</td>\n",
              "      <td id=\"T_b1daa_row2_col4\" class=\"data row2 col4\" >0.9542</td>\n",
              "      <td id=\"T_b1daa_row2_col5\" class=\"data row2 col5\" >0.9380</td>\n",
              "      <td id=\"T_b1daa_row2_col6\" class=\"data row2 col6\" >0.7872</td>\n",
              "      <td id=\"T_b1daa_row2_col7\" class=\"data row2 col7\" >0.8895</td>\n",
              "      <td id=\"T_b1daa_row2_col8\" class=\"data row2 col8\" >0.7301</td>\n",
              "      <td id=\"T_b1daa_row2_col9\" class=\"data row2 col9\" >0.6859</td>\n",
              "      <td id=\"T_b1daa_row2_col10\" class=\"data row2 col10\" >0.5090</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{lrrrrrrrrrrr}\n",
            " & \\bfseries ar & \\bfseries ca & \\bfseries cs & \\bfseries de & \\bfseries en & \\bfseries es & \\bfseries nl & \\bfseries pt & \\bfseries ru & \\bfseries uk & \\bfseries zh \\\\\n",
            "\\multicolumn{2}{r|}{\\bfseries All Detectors Mean} & {\\cellcolor[HTML]{A7BDDB}} \\textcolor{black}{0.7463} & {\\cellcolor[HTML]{8CB3D5}} \\textcolor{black}{0.8765} & {\\cellcolor[HTML]{9EBAD9}} \\textcolor{black}{0.7918} & {\\cellcolor[HTML]{96B6D7}} \\textcolor{black}{0.8300} & {\\cellcolor[HTML]{7EADD1}} \\textcolor{black}{0.9472} & {\\cellcolor[HTML]{80AED2}} \\textcolor{black}{0.9375} & {\\cellcolor[HTML]{94B6D7}} \\textcolor{black}{0.8407} & {\\cellcolor[HTML]{86B0D3}} \\textcolor{black}{0.9099} & {\\cellcolor[HTML]{91B5D6}} \\textcolor{black}{0.8592} & {\\cellcolor[HTML]{96B6D7}} \\textcolor{black}{0.8296} & {\\cellcolor[HTML]{B7C5DF}} \\textcolor{black}{0.6558} \\\\\n",
            "\\multicolumn{2}{r|}{\\bfseries Multilingual Base Models Mean} & {\\cellcolor[HTML]{91B5D6}} \\textcolor{black}{0.8537} & {\\cellcolor[HTML]{89B1D4}} \\textcolor{black}{0.8977} & {\\cellcolor[HTML]{8FB4D6}} \\textcolor{black}{0.8604} & {\\cellcolor[HTML]{86B0D3}} \\textcolor{black}{0.9073} & {\\cellcolor[HTML]{80AED2}} \\textcolor{black}{0.9420} & {\\cellcolor[HTML]{81AED2}} \\textcolor{black}{0.9372} & {\\cellcolor[HTML]{8CB3D5}} \\textcolor{black}{0.8808} & {\\cellcolor[HTML]{83AFD3}} \\textcolor{black}{0.9253} & {\\cellcolor[HTML]{7DACD1}} \\textcolor{black}{0.9560} & {\\cellcolor[HTML]{81AED2}} \\textcolor{black}{0.9374} & {\\cellcolor[HTML]{A2BCDA}} \\textcolor{black}{0.7659} \\\\\n",
            "\\multicolumn{2}{r|}{\\bfseries Monolingual Base Models Mean} & {\\cellcolor[HTML]{BFC9E1}} \\textcolor{black}{0.6031} & {\\cellcolor[HTML]{93B5D6}} \\textcolor{black}{0.8482} & {\\cellcolor[HTML]{AFC1DD}} \\textcolor{black}{0.7003} & {\\cellcolor[HTML]{A9BFDC}} \\textcolor{black}{0.7269} & {\\cellcolor[HTML]{7DACD1}} \\textcolor{black}{0.9542} & {\\cellcolor[HTML]{80AED2}} \\textcolor{black}{0.9380} & {\\cellcolor[HTML]{9FBAD9}} \\textcolor{black}{0.7872} & {\\cellcolor[HTML]{8BB2D4}} \\textcolor{black}{0.8895} & {\\cellcolor[HTML]{A9BFDC}} \\textcolor{black}{0.7301} & {\\cellcolor[HTML]{B1C2DE}} \\textcolor{black}{0.6859} & {\\cellcolor[HTML]{CED0E6}} \\textcolor{black}{0.5090} \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Results just for English fine-tuned models with 3x more samples available\n",
        "temp = results_all.loc['en3',:]\n",
        "display(temp.style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4))\n",
        "print(temp.style.format(na_rep=0, precision=4).highlight_max(props='font-weight: bold;', axis=1).applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True))\n",
        "\n",
        "means = pd.DataFrame()\n",
        "means = pd.concat([means, temp.agg('mean')], copy=False, axis=1)\n",
        "means.rename(columns={0:'All Detectors Mean'}, inplace=True)\n",
        "means = pd.concat([means, temp.loc[[x in multilingual for x in temp.reset_index().Model], :].agg('mean')], copy=False, axis=1)\n",
        "means.rename(columns={0:'Multilingual Base Models Mean'}, inplace=True)\n",
        "means = pd.concat([means, temp.loc[[x not in multilingual for x in temp.reset_index().Model], :].agg('mean')], copy=False, axis=1)\n",
        "means.rename(columns={0:'Monolingual Base Models Mean'}, inplace=True)\n",
        "means = means.T.style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4)\n",
        "display(means)\n",
        "temp = means.applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True)\n",
        "#little hack to properly format latex table\n",
        "print(temp.replace('\\\\color[HTML]{F1F1F1} ', '\\\\textcolor{white}{').replace('\\\\color[HTML]{000000} ', '\\\\textcolor{black}{').replace(' & {\\\\cellcolor', '} & {\\\\cellcolor').replace(' \\\\\\\\', '} \\\\\\\\').replace('\\n\\\\bfseries', '\\n\\\\multicolumn{2}{r|}{\\\\bfseries').replace('zh} \\\\\\\\', 'zh \\\\\\\\'))"
      ],
      "metadata": {
        "id": "9S8HP6JmSUKm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "382dd399-51f5-421c-afdd-d5ed0f41bf83"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd004df00a0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_4ea45_row0_col0, #T_4ea45_row3_col2, #T_4ea45_row5_col1, #T_4ea45_row30_col1, #T_4ea45_row31_col9, #T_4ea45_row37_col10, #T_4ea45_row38_col7 {\n",
              "  background-color: #d1d2e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row0_col1, #T_4ea45_row7_col5, #T_4ea45_row18_col3, #T_4ea45_row45_col3, #T_4ea45_row46_col6, #T_4ea45_row47_col3 {\n",
              "  background-color: #8eb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row0_col2, #T_4ea45_row3_col6, #T_4ea45_row3_col7, #T_4ea45_row12_col6, #T_4ea45_row14_col9, #T_4ea45_row20_col10, #T_4ea45_row25_col5, #T_4ea45_row39_col1, #T_4ea45_row42_col7, #T_4ea45_row55_col6 {\n",
              "  background-color: #8fb4d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row0_col3, #T_4ea45_row6_col6, #T_4ea45_row10_col3, #T_4ea45_row14_col1, #T_4ea45_row48_col1, #T_4ea45_row48_col3, #T_4ea45_row55_col1 {\n",
              "  background-color: #88b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row0_col4, #T_4ea45_row3_col4, #T_4ea45_row7_col4, #T_4ea45_row13_col1, #T_4ea45_row13_col4, #T_4ea45_row37_col4, #T_4ea45_row50_col4 {\n",
              "  background-color: #78abd0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row0_col5, #T_4ea45_row3_col1, #T_4ea45_row47_col1, #T_4ea45_row55_col2, #T_4ea45_row55_col8, #T_4ea45_row55_col10 {\n",
              "  background-color: #a4bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row0_col6, #T_4ea45_row7_col8, #T_4ea45_row7_col10, #T_4ea45_row10_col1, #T_4ea45_row17_col3, #T_4ea45_row17_col7, #T_4ea45_row18_col2, #T_4ea45_row20_col1, #T_4ea45_row20_col8, #T_4ea45_row28_col4, #T_4ea45_row40_col6, #T_4ea45_row42_col1, #T_4ea45_row42_col6, #T_4ea45_row45_col7, #T_4ea45_row46_col4, #T_4ea45_row52_col1 {\n",
              "  background-color: #86b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row0_col7, #T_4ea45_row4_col3, #T_4ea45_row6_col9, #T_4ea45_row25_col9, #T_4ea45_row39_col6 {\n",
              "  background-color: #91b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row0_col8, #T_4ea45_row12_col2, #T_4ea45_row17_col0, #T_4ea45_row36_col6, #T_4ea45_row36_col7, #T_4ea45_row39_col9, #T_4ea45_row45_col5 {\n",
              "  background-color: #93b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row0_col9, #T_4ea45_row11_col0, #T_4ea45_row11_col2, #T_4ea45_row18_col0, #T_4ea45_row39_col3, #T_4ea45_row41_col3, #T_4ea45_row47_col2 {\n",
              "  background-color: #99b8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row0_col10, #T_4ea45_row31_col2, #T_4ea45_row35_col0, #T_4ea45_row38_col2, #T_4ea45_row47_col0 {\n",
              "  background-color: #c4cbe3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row1_col0, #T_4ea45_row1_col3, #T_4ea45_row2_col7, #T_4ea45_row15_col8, #T_4ea45_row16_col0, #T_4ea45_row26_col6, #T_4ea45_row37_col8, #T_4ea45_row51_col6, #T_4ea45_row54_col7 {\n",
              "  background-color: #dedcec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row1_col1, #T_4ea45_row2_col2, #T_4ea45_row8_col6, #T_4ea45_row23_col3, #T_4ea45_row24_col6, #T_4ea45_row27_col0, #T_4ea45_row40_col9 {\n",
              "  background-color: #d5d5e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row1_col2, #T_4ea45_row1_col6, #T_4ea45_row2_col1, #T_4ea45_row19_col7, #T_4ea45_row30_col0, #T_4ea45_row51_col3, #T_4ea45_row51_col9, #T_4ea45_row54_col5 {\n",
              "  background-color: #d7d6e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row1_col4, #T_4ea45_row6_col4, #T_4ea45_row8_col4, #T_4ea45_row9_col4, #T_4ea45_row10_col4, #T_4ea45_row15_col4, #T_4ea45_row19_col4, #T_4ea45_row40_col4, #T_4ea45_row43_col4, #T_4ea45_row44_col4, #T_4ea45_row45_col4, #T_4ea45_row48_col4, #T_4ea45_row49_col4, #T_4ea45_row51_col4, #T_4ea45_row52_col4, #T_4ea45_row55_col4 {\n",
              "  background-color: #76aad0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row1_col5, #T_4ea45_row28_col10, #T_4ea45_row32_col2, #T_4ea45_row50_col10 {\n",
              "  background-color: #cacee5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row1_col7, #T_4ea45_row5_col2 {\n",
              "  background-color: #d4d4e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row1_col8, #T_4ea45_row8_col2, #T_4ea45_row9_col1, #T_4ea45_row9_col2, #T_4ea45_row9_col6, #T_4ea45_row15_col2, #T_4ea45_row15_col9, #T_4ea45_row15_col10, #T_4ea45_row16_col2, #T_4ea45_row16_col3, #T_4ea45_row16_col10, #T_4ea45_row22_col1, #T_4ea45_row22_col7, #T_4ea45_row24_col0, #T_4ea45_row26_col3, #T_4ea45_row26_col7, #T_4ea45_row29_col8, #T_4ea45_row33_col10, #T_4ea45_row40_col8 {\n",
              "  background-color: #e2dfee;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row1_col9, #T_4ea45_row6_col0, #T_4ea45_row9_col5, #T_4ea45_row9_col7, #T_4ea45_row9_col10, #T_4ea45_row10_col2, #T_4ea45_row17_col2, #T_4ea45_row22_col0, #T_4ea45_row22_col2, #T_4ea45_row22_col6, #T_4ea45_row22_col8, #T_4ea45_row22_col9, #T_4ea45_row22_col10, #T_4ea45_row26_col0, #T_4ea45_row26_col8, #T_4ea45_row26_col10, #T_4ea45_row29_col9, #T_4ea45_row29_col10, #T_4ea45_row33_col0, #T_4ea45_row33_col8, #T_4ea45_row36_col10, #T_4ea45_row43_col1, #T_4ea45_row43_col2, #T_4ea45_row43_col3, #T_4ea45_row43_col5, #T_4ea45_row43_col6, #T_4ea45_row43_col7, #T_4ea45_row43_col10, #T_4ea45_row44_col0, #T_4ea45_row44_col1, #T_4ea45_row44_col2, #T_4ea45_row44_col3, #T_4ea45_row44_col6, #T_4ea45_row44_col8, #T_4ea45_row44_col9, #T_4ea45_row44_col10, #T_4ea45_row45_col2, #T_4ea45_row54_col0, #T_4ea45_row54_col8, #T_4ea45_row54_col9 {\n",
              "  background-color: #e3e0ee;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row1_col10, #T_4ea45_row15_col0, #T_4ea45_row15_col3, #T_4ea45_row30_col10, #T_4ea45_row32_col5, #T_4ea45_row32_col9, #T_4ea45_row33_col2, #T_4ea45_row50_col8 {\n",
              "  background-color: #d3d4e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row2_col0, #T_4ea45_row9_col8, #T_4ea45_row12_col9, #T_4ea45_row24_col1, #T_4ea45_row26_col1 {\n",
              "  background-color: #e0dded;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row2_col3, #T_4ea45_row8_col1, #T_4ea45_row8_col8, #T_4ea45_row19_col9, #T_4ea45_row43_col0, #T_4ea45_row48_col10, #T_4ea45_row50_col0, #T_4ea45_row55_col0 {\n",
              "  background-color: #dbdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row2_col4, #T_4ea45_row14_col4, #T_4ea45_row16_col4, #T_4ea45_row17_col4, #T_4ea45_row20_col4, #T_4ea45_row36_col4, #T_4ea45_row47_col4 {\n",
              "  background-color: #75a9cf;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row2_col5, #T_4ea45_row27_col1, #T_4ea45_row27_col6, #T_4ea45_row28_col0, #T_4ea45_row33_col7, #T_4ea45_row34_col1, #T_4ea45_row43_col9, #T_4ea45_row51_col8, #T_4ea45_row54_col2 {\n",
              "  background-color: #dad9ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row2_col6, #T_4ea45_row5_col9, #T_4ea45_row5_col10, #T_4ea45_row32_col1, #T_4ea45_row50_col9 {\n",
              "  background-color: #d8d7e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row2_col8, #T_4ea45_row23_col8, #T_4ea45_row32_col7, #T_4ea45_row33_col9, #T_4ea45_row48_col0, #T_4ea45_row51_col10, #T_4ea45_row52_col0 {\n",
              "  background-color: #dddbec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row2_col9, #T_4ea45_row2_col10, #T_4ea45_row3_col10, #T_4ea45_row4_col10, #T_4ea45_row5_col3, #T_4ea45_row8_col0, #T_4ea45_row23_col7, #T_4ea45_row23_col10, #T_4ea45_row30_col2, #T_4ea45_row30_col6, #T_4ea45_row32_col6, #T_4ea45_row33_col5, #T_4ea45_row34_col0, #T_4ea45_row34_col6, #T_4ea45_row38_col10 {\n",
              "  background-color: #d9d8ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row3_col0 {\n",
              "  background-color: #e7e3f0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row3_col3, #T_4ea45_row10_col5, #T_4ea45_row10_col9, #T_4ea45_row30_col4, #T_4ea45_row36_col5, #T_4ea45_row40_col1, #T_4ea45_row42_col10, #T_4ea45_row46_col3, #T_4ea45_row48_col7 {\n",
              "  background-color: #8bb2d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row3_col5, #T_4ea45_row11_col1, #T_4ea45_row11_col9, #T_4ea45_row36_col3, #T_4ea45_row39_col7, #T_4ea45_row41_col1 {\n",
              "  background-color: #a8bedc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row3_col8, #T_4ea45_row11_col6, #T_4ea45_row18_col9, #T_4ea45_row49_col7, #T_4ea45_row52_col3, #T_4ea45_row53_col3 {\n",
              "  background-color: #97b7d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row3_col9, #T_4ea45_row18_col6, #T_4ea45_row21_col2, #T_4ea45_row21_col9, #T_4ea45_row42_col5, #T_4ea45_row52_col7 {\n",
              "  background-color: #9cb9d9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row4_col0 {\n",
              "  background-color: #f2ecf5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row4_col1, #T_4ea45_row4_col9, #T_4ea45_row25_col6, #T_4ea45_row28_col2, #T_4ea45_row40_col7 {\n",
              "  background-color: #b1c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row4_col2, #T_4ea45_row14_col10, #T_4ea45_row17_col9, #T_4ea45_row21_col5 {\n",
              "  background-color: #9ab8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row4_col4, #T_4ea45_row6_col3, #T_4ea45_row7_col0, #T_4ea45_row7_col2, #T_4ea45_row13_col3, #T_4ea45_row13_col7, #T_4ea45_row14_col6, #T_4ea45_row23_col4, #T_4ea45_row27_col4, #T_4ea45_row45_col6, #T_4ea45_row49_col6 {\n",
              "  background-color: #80aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row4_col5, #T_4ea45_row21_col6, #T_4ea45_row27_col5, #T_4ea45_row27_col8, #T_4ea45_row28_col7, #T_4ea45_row35_col1, #T_4ea45_row35_col9, #T_4ea45_row36_col1, #T_4ea45_row38_col3, #T_4ea45_row46_col5, #T_4ea45_row49_col10, #T_4ea45_row52_col8 {\n",
              "  background-color: #b9c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row4_col6, #T_4ea45_row6_col7, #T_4ea45_row11_col7, #T_4ea45_row15_col7, #T_4ea45_row42_col0, #T_4ea45_row49_col5, #T_4ea45_row53_col1, #T_4ea45_row55_col9 {\n",
              "  background-color: #9fbad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row4_col7, #T_4ea45_row4_col8, #T_4ea45_row11_col10, #T_4ea45_row12_col7, #T_4ea45_row14_col5, #T_4ea45_row39_col0, #T_4ea45_row39_col10, #T_4ea45_row42_col9, #T_4ea45_row46_col8, #T_4ea45_row47_col6 {\n",
              "  background-color: #a9bfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row5_col0, #T_4ea45_row9_col3, #T_4ea45_row16_col9, #T_4ea45_row23_col0, #T_4ea45_row26_col5, #T_4ea45_row54_col10 {\n",
              "  background-color: #e1dfed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row5_col4, #T_4ea45_row12_col4, #T_4ea45_row13_col2, #T_4ea45_row38_col4, #T_4ea45_row39_col4, #T_4ea45_row41_col4, #T_4ea45_row42_col4 {\n",
              "  background-color: #79abd0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row5_col5, #T_4ea45_row8_col9, #T_4ea45_row23_col1, #T_4ea45_row23_col6, #T_4ea45_row23_col9, #T_4ea45_row29_col0, #T_4ea45_row37_col9, #T_4ea45_row48_col9 {\n",
              "  background-color: #dcdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row5_col6, #T_4ea45_row17_col10, #T_4ea45_row29_col6, #T_4ea45_row31_col3 {\n",
              "  background-color: #b5c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row5_col7, #T_4ea45_row9_col0, #T_4ea45_row16_col8, #T_4ea45_row37_col0, #T_4ea45_row40_col10, #T_4ea45_row43_col8, #T_4ea45_row47_col9, #T_4ea45_row51_col1 {\n",
              "  background-color: #dfddec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row5_col8, #T_4ea45_row8_col3, #T_4ea45_row9_col9, #T_4ea45_row16_col1, #T_4ea45_row16_col5, #T_4ea45_row16_col6, #T_4ea45_row16_col7, #T_4ea45_row22_col3, #T_4ea45_row22_col5, #T_4ea45_row26_col2, #T_4ea45_row26_col9 {\n",
              "  background-color: #e0deed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row6_col1, #T_4ea45_row12_col3, #T_4ea45_row14_col7, #T_4ea45_row27_col2, #T_4ea45_row49_col2, #T_4ea45_row49_col3, #T_4ea45_row49_col9 {\n",
              "  background-color: #94b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row6_col2, #T_4ea45_row11_col3, #T_4ea45_row14_col8, #T_4ea45_row17_col8, #T_4ea45_row20_col0, #T_4ea45_row20_col7, #T_4ea45_row42_col2, #T_4ea45_row49_col1, #T_4ea45_row53_col6 {\n",
              "  background-color: #8cb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row6_col5, #T_4ea45_row11_col5, #T_4ea45_row21_col3, #T_4ea45_row21_col8, #T_4ea45_row29_col3, #T_4ea45_row29_col7, #T_4ea45_row35_col3, #T_4ea45_row52_col5, #T_4ea45_row53_col2, #T_4ea45_row54_col6 {\n",
              "  background-color: #afc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row6_col8, #T_4ea45_row14_col0, #T_4ea45_row15_col5, #T_4ea45_row20_col5, #T_4ea45_row37_col1, #T_4ea45_row48_col5, #T_4ea45_row49_col8 {\n",
              "  background-color: #96b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row6_col10, #T_4ea45_row30_col3, #T_4ea45_row37_col2, #T_4ea45_row37_col6, #T_4ea45_row41_col9, #T_4ea45_row48_col8 {\n",
              "  background-color: #c0c9e2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row7_col1, #T_4ea45_row12_col1, #T_4ea45_row13_col0, #T_4ea45_row13_col5, #T_4ea45_row14_col2, #T_4ea45_row21_col4, #T_4ea45_row24_col4, #T_4ea45_row31_col4, #T_4ea45_row34_col4, #T_4ea45_row39_col2, #T_4ea45_row52_col6 {\n",
              "  background-color: #84b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row7_col3, #T_4ea45_row10_col7, #T_4ea45_row10_col8, #T_4ea45_row11_col4, #T_4ea45_row20_col6, #T_4ea45_row25_col2 {\n",
              "  background-color: #83afd3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row7_col6, #T_4ea45_row13_col8, #T_4ea45_row13_col9, #T_4ea45_row26_col4, #T_4ea45_row48_col6, #T_4ea45_row54_col4 {\n",
              "  background-color: #7eadd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row7_col7, #T_4ea45_row14_col3, #T_4ea45_row17_col1, #T_4ea45_row17_col6, #T_4ea45_row20_col9, #T_4ea45_row22_col4, #T_4ea45_row29_col4, #T_4ea45_row33_col4, #T_4ea45_row55_col3 {\n",
              "  background-color: #81aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row7_col9, #T_4ea45_row25_col4, #T_4ea45_row42_col3, #T_4ea45_row45_col1, #T_4ea45_row46_col2 {\n",
              "  background-color: #89b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row8_col5, #T_4ea45_row10_col0, #T_4ea45_row11_col8, #T_4ea45_row12_col0, #T_4ea45_row19_col10, #T_4ea45_row28_col3, #T_4ea45_row29_col5, #T_4ea45_row33_col1, #T_4ea45_row34_col3, #T_4ea45_row40_col3, #T_4ea45_row52_col10, #T_4ea45_row53_col9, #T_4ea45_row55_col5 {\n",
              "  background-color: #b3c3de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row8_col7, #T_4ea45_row24_col7, #T_4ea45_row24_col10, #T_4ea45_row28_col5, #T_4ea45_row31_col5 {\n",
              "  background-color: #c6cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row8_col10 {\n",
              "  background-color: #e5e1ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row10_col6, #T_4ea45_row20_col3 {\n",
              "  background-color: #7bacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row10_col10, #T_4ea45_row23_col2, #T_4ea45_row24_col5, #T_4ea45_row27_col7, #T_4ea45_row30_col9, #T_4ea45_row32_col8, #T_4ea45_row35_col8, #T_4ea45_row50_col2, #T_4ea45_row53_col10 {\n",
              "  background-color: #c8cde4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row12_col5, #T_4ea45_row18_col8, #T_4ea45_row19_col6, #T_4ea45_row24_col2, #T_4ea45_row42_col8, #T_4ea45_row50_col5 {\n",
              "  background-color: #a1bbda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row12_col8, #T_4ea45_row25_col1, #T_4ea45_row33_col6, #T_4ea45_row35_col6, #T_4ea45_row41_col0 {\n",
              "  background-color: #b4c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row12_col10, #T_4ea45_row25_col10, #T_4ea45_row31_col1, #T_4ea45_row34_col5, #T_4ea45_row34_col7, #T_4ea45_row34_col8, #T_4ea45_row38_col5, #T_4ea45_row38_col6, #T_4ea45_row45_col10 {\n",
              "  background-color: #c1cae2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row13_col6, #T_4ea45_row20_col2, #T_4ea45_row35_col4 {\n",
              "  background-color: #7dacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row13_col10, #T_4ea45_row17_col5, #T_4ea45_row40_col2, #T_4ea45_row47_col5, #T_4ea45_row53_col4 {\n",
              "  background-color: #9ebad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row15_col1, #T_4ea45_row34_col9, #T_4ea45_row36_col2, #T_4ea45_row41_col6 {\n",
              "  background-color: #bcc7e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row15_col6, #T_4ea45_row21_col10, #T_4ea45_row38_col1, #T_4ea45_row41_col8, #T_4ea45_row45_col9 {\n",
              "  background-color: #c5cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row18_col1, #T_4ea45_row27_col3, #T_4ea45_row46_col10, #T_4ea45_row52_col9 {\n",
              "  background-color: #abbfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row18_col4, #T_4ea45_row32_col3, #T_4ea45_row35_col5 {\n",
              "  background-color: #bbc7e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row18_col5, #T_4ea45_row21_col0, #T_4ea45_row27_col9, #T_4ea45_row37_col7 {\n",
              "  background-color: #b7c5df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row18_col7, #T_4ea45_row35_col2, #T_4ea45_row53_col7 {\n",
              "  background-color: #adc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row18_col10, #T_4ea45_row19_col1, #T_4ea45_row21_col7, #T_4ea45_row29_col1, #T_4ea45_row41_col7, #T_4ea45_row45_col8, #T_4ea45_row47_col10 {\n",
              "  background-color: #b0c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row19_col0, #T_4ea45_row24_col9, #T_4ea45_row51_col0 {\n",
              "  background-color: #ced0e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row19_col2, #T_4ea45_row30_col5 {\n",
              "  background-color: #c9cee4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row19_col3, #T_4ea45_row34_col2, #T_4ea45_row53_col8 {\n",
              "  background-color: #bfc9e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row19_col5, #T_4ea45_row19_col8, #T_4ea45_row24_col8, #T_4ea45_row30_col8, #T_4ea45_row52_col2, #T_4ea45_row54_col1 {\n",
              "  background-color: #d2d2e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row21_col1, #T_4ea45_row28_col9, #T_4ea45_row29_col2, #T_4ea45_row30_col7, #T_4ea45_row31_col7, #T_4ea45_row35_col7, #T_4ea45_row37_col3, #T_4ea45_row50_col7 {\n",
              "  background-color: #b8c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row23_col5, #T_4ea45_row31_col6, #T_4ea45_row46_col0, #T_4ea45_row51_col5 {\n",
              "  background-color: #c2cbe2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row24_col3, #T_4ea45_row25_col7 {\n",
              "  background-color: #acc0dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row25_col0, #T_4ea45_row37_col5, #T_4ea45_row41_col2, #T_4ea45_row55_col7 {\n",
              "  background-color: #a7bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row25_col3, #T_4ea45_row39_col8, #T_4ea45_row41_col5 {\n",
              "  background-color: #a5bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row25_col8, #T_4ea45_row32_col4, #T_4ea45_row39_col5, #T_4ea45_row40_col5, #T_4ea45_row46_col1, #T_4ea45_row46_col7, #T_4ea45_row47_col7, #T_4ea45_row49_col0 {\n",
              "  background-color: #a2bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row27_col10, #T_4ea45_row31_col8, #T_4ea45_row31_col10, #T_4ea45_row33_col3, #T_4ea45_row51_col2, #T_4ea45_row51_col7 {\n",
              "  background-color: #d6d6e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row28_col1, #T_4ea45_row28_col6, #T_4ea45_row32_col10, #T_4ea45_row48_col2 {\n",
              "  background-color: #cccfe5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row28_col8, #T_4ea45_row41_col10, #T_4ea45_row45_col0, #T_4ea45_row46_col9, #T_4ea45_row47_col8, #T_4ea45_row50_col6, #T_4ea45_row53_col5 {\n",
              "  background-color: #bdc8e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row31_col0 {\n",
              "  background-color: #f0eaf4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row32_col0, #T_4ea45_row38_col0 {\n",
              "  background-color: #ebe6f2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row34_col10, #T_4ea45_row35_col10, #T_4ea45_row50_col3 {\n",
              "  background-color: #cdd0e5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row36_col0, #T_4ea45_row36_col8, #T_4ea45_row36_col9, #T_4ea45_row40_col0, #T_4ea45_row44_col5, #T_4ea45_row44_col7 {\n",
              "  background-color: #e4e1ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row38_col8, #T_4ea45_row38_col9, #T_4ea45_row53_col0 {\n",
              "  background-color: #ece7f2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row50_col1 {\n",
              "  background-color: #d0d1e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4ea45_row54_col3 {\n",
              "  background-color: #d2d3e7;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_4ea45\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank\" >&nbsp;</th>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_4ea45_level0_col0\" class=\"col_heading level0 col0\" >ar</th>\n",
              "      <th id=\"T_4ea45_level0_col1\" class=\"col_heading level0 col1\" >ca</th>\n",
              "      <th id=\"T_4ea45_level0_col2\" class=\"col_heading level0 col2\" >cs</th>\n",
              "      <th id=\"T_4ea45_level0_col3\" class=\"col_heading level0 col3\" >de</th>\n",
              "      <th id=\"T_4ea45_level0_col4\" class=\"col_heading level0 col4\" >en</th>\n",
              "      <th id=\"T_4ea45_level0_col5\" class=\"col_heading level0 col5\" >es</th>\n",
              "      <th id=\"T_4ea45_level0_col6\" class=\"col_heading level0 col6\" >nl</th>\n",
              "      <th id=\"T_4ea45_level0_col7\" class=\"col_heading level0 col7\" >pt</th>\n",
              "      <th id=\"T_4ea45_level0_col8\" class=\"col_heading level0 col8\" >ru</th>\n",
              "      <th id=\"T_4ea45_level0_col9\" class=\"col_heading level0 col9\" >uk</th>\n",
              "      <th id=\"T_4ea45_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >Train LLM</th>\n",
              "      <th class=\"index_name level1\" >Model</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "      <th class=\"blank col7\" >&nbsp;</th>\n",
              "      <th class=\"blank col8\" >&nbsp;</th>\n",
              "      <th class=\"blank col9\" >&nbsp;</th>\n",
              "      <th class=\"blank col10\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"7\">alpaca-lora-30b</th>\n",
              "      <th id=\"T_4ea45_level1_row0\" class=\"row_heading level1 row0\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_4ea45_row0_col0\" class=\"data row0 col0\" >0.4951</td>\n",
              "      <td id=\"T_4ea45_row0_col1\" class=\"data row0 col1\" >0.8743</td>\n",
              "      <td id=\"T_4ea45_row0_col2\" class=\"data row0 col2\" >0.8600</td>\n",
              "      <td id=\"T_4ea45_row0_col3\" class=\"data row0 col3\" >0.9053</td>\n",
              "      <td id=\"T_4ea45_row0_col4\" class=\"data row0 col4\" >0.9838</td>\n",
              "      <td id=\"T_4ea45_row0_col5\" class=\"data row0 col5\" >0.7617</td>\n",
              "      <td id=\"T_4ea45_row0_col6\" class=\"data row0 col6\" >0.9079</td>\n",
              "      <td id=\"T_4ea45_row0_col7\" class=\"data row0 col7\" >0.8579</td>\n",
              "      <td id=\"T_4ea45_row0_col8\" class=\"data row0 col8\" >0.8461</td>\n",
              "      <td id=\"T_4ea45_row0_col9\" class=\"data row0 col9\" >0.8144</td>\n",
              "      <td id=\"T_4ea45_row0_col10\" class=\"data row0 col10\" >0.5719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row1\" class=\"row_heading level1 row1\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_4ea45_row1_col0\" class=\"data row1 col0\" >0.3770</td>\n",
              "      <td id=\"T_4ea45_row1_col1\" class=\"data row1 col1\" >0.4539</td>\n",
              "      <td id=\"T_4ea45_row1_col2\" class=\"data row1 col2\" >0.4389</td>\n",
              "      <td id=\"T_4ea45_row1_col3\" class=\"data row1 col3\" >0.3827</td>\n",
              "      <td id=\"T_4ea45_row1_col4\" class=\"data row1 col4\" >0.9892</td>\n",
              "      <td id=\"T_4ea45_row1_col5\" class=\"data row1 col5\" >0.5344</td>\n",
              "      <td id=\"T_4ea45_row1_col6\" class=\"data row1 col6\" >0.4379</td>\n",
              "      <td id=\"T_4ea45_row1_col7\" class=\"data row1 col7\" >0.4684</td>\n",
              "      <td id=\"T_4ea45_row1_col8\" class=\"data row1 col8\" >0.3363</td>\n",
              "      <td id=\"T_4ea45_row1_col9\" class=\"data row1 col9\" >0.3333</td>\n",
              "      <td id=\"T_4ea45_row1_col10\" class=\"data row1 col10\" >0.4711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row2\" class=\"row_heading level1 row2\" >gpt2-medium</th>\n",
              "      <td id=\"T_4ea45_row2_col0\" class=\"data row2 col0\" >0.3656</td>\n",
              "      <td id=\"T_4ea45_row2_col1\" class=\"data row2 col1\" >0.4420</td>\n",
              "      <td id=\"T_4ea45_row2_col2\" class=\"data row2 col2\" >0.4544</td>\n",
              "      <td id=\"T_4ea45_row2_col3\" class=\"data row2 col3\" >0.4038</td>\n",
              "      <td id=\"T_4ea45_row2_col4\" class=\"data row2 col4\" >0.9928</td>\n",
              "      <td id=\"T_4ea45_row2_col5\" class=\"data row2 col5\" >0.4063</td>\n",
              "      <td id=\"T_4ea45_row2_col6\" class=\"data row2 col6\" >0.4297</td>\n",
              "      <td id=\"T_4ea45_row2_col7\" class=\"data row2 col7\" >0.3814</td>\n",
              "      <td id=\"T_4ea45_row2_col8\" class=\"data row2 col8\" >0.3854</td>\n",
              "      <td id=\"T_4ea45_row2_col9\" class=\"data row2 col9\" >0.4195</td>\n",
              "      <td id=\"T_4ea45_row2_col10\" class=\"data row2 col10\" >0.4141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row3\" class=\"row_heading level1 row3\" >mGPT</th>\n",
              "      <td id=\"T_4ea45_row3_col0\" class=\"data row3 col0\" >0.2930</td>\n",
              "      <td id=\"T_4ea45_row3_col1\" class=\"data row3 col1\" >0.7623</td>\n",
              "      <td id=\"T_4ea45_row3_col2\" class=\"data row3 col2\" >0.4958</td>\n",
              "      <td id=\"T_4ea45_row3_col3\" class=\"data row3 col3\" >0.8850</td>\n",
              "      <td id=\"T_4ea45_row3_col4\" class=\"data row3 col4\" >0.9801</td>\n",
              "      <td id=\"T_4ea45_row3_col5\" class=\"data row3 col5\" >0.7360</td>\n",
              "      <td id=\"T_4ea45_row3_col6\" class=\"data row3 col6\" >0.8594</td>\n",
              "      <td id=\"T_4ea45_row3_col7\" class=\"data row3 col7\" >0.8655</td>\n",
              "      <td id=\"T_4ea45_row3_col8\" class=\"data row3 col8\" >0.8209</td>\n",
              "      <td id=\"T_4ea45_row3_col9\" class=\"data row3 col9\" >0.8043</td>\n",
              "      <td id=\"T_4ea45_row3_col10\" class=\"data row3 col10\" >0.4268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row4\" class=\"row_heading level1 row4\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_4ea45_row4_col0\" class=\"data row4 col0\" >0.1764</td>\n",
              "      <td id=\"T_4ea45_row4_col1\" class=\"data row4 col1\" >0.6872</td>\n",
              "      <td id=\"T_4ea45_row4_col2\" class=\"data row4 col2\" >0.8096</td>\n",
              "      <td id=\"T_4ea45_row4_col3\" class=\"data row4 col3\" >0.8529</td>\n",
              "      <td id=\"T_4ea45_row4_col4\" class=\"data row4 col4\" >0.9385</td>\n",
              "      <td id=\"T_4ea45_row4_col5\" class=\"data row4 col5\" >0.6356</td>\n",
              "      <td id=\"T_4ea45_row4_col6\" class=\"data row4 col6\" >0.7888</td>\n",
              "      <td id=\"T_4ea45_row4_col7\" class=\"data row4 col7\" >0.7340</td>\n",
              "      <td id=\"T_4ea45_row4_col8\" class=\"data row4 col8\" >0.7332</td>\n",
              "      <td id=\"T_4ea45_row4_col9\" class=\"data row4 col9\" >0.6824</td>\n",
              "      <td id=\"T_4ea45_row4_col10\" class=\"data row4 col10\" >0.4177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row5\" class=\"row_heading level1 row5\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_4ea45_row5_col0\" class=\"data row5 col0\" >0.3448</td>\n",
              "      <td id=\"T_4ea45_row5_col1\" class=\"data row5 col1\" >0.4963</td>\n",
              "      <td id=\"T_4ea45_row5_col2\" class=\"data row5 col2\" >0.4613</td>\n",
              "      <td id=\"T_4ea45_row5_col3\" class=\"data row5 col3\" >0.4216</td>\n",
              "      <td id=\"T_4ea45_row5_col4\" class=\"data row5 col4\" >0.9711</td>\n",
              "      <td id=\"T_4ea45_row5_col5\" class=\"data row5 col5\" >0.3945</td>\n",
              "      <td id=\"T_4ea45_row5_col6\" class=\"data row5 col6\" >0.6572</td>\n",
              "      <td id=\"T_4ea45_row5_col7\" class=\"data row5 col7\" >0.3721</td>\n",
              "      <td id=\"T_4ea45_row5_col8\" class=\"data row5 col8\" >0.3571</td>\n",
              "      <td id=\"T_4ea45_row5_col9\" class=\"data row5 col9\" >0.4340</td>\n",
              "      <td id=\"T_4ea45_row5_col10\" class=\"data row5 col10\" >0.4346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row6\" class=\"row_heading level1 row6\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_4ea45_row6_col0\" class=\"data row6 col0\" >0.3306</td>\n",
              "      <td id=\"T_4ea45_row6_col1\" class=\"data row6 col1\" >0.8378</td>\n",
              "      <td id=\"T_4ea45_row6_col2\" class=\"data row6 col2\" >0.8782</td>\n",
              "      <td id=\"T_4ea45_row6_col3\" class=\"data row6 col3\" >0.9425</td>\n",
              "      <td id=\"T_4ea45_row6_col4\" class=\"data row6 col4\" >0.9874</td>\n",
              "      <td id=\"T_4ea45_row6_col5\" class=\"data row6 col5\" >0.6985</td>\n",
              "      <td id=\"T_4ea45_row6_col6\" class=\"data row6 col6\" >0.9007</td>\n",
              "      <td id=\"T_4ea45_row6_col7\" class=\"data row6 col7\" >0.7872</td>\n",
              "      <td id=\"T_4ea45_row6_col8\" class=\"data row6 col8\" >0.8294</td>\n",
              "      <td id=\"T_4ea45_row6_col9\" class=\"data row6 col9\" >0.8524</td>\n",
              "      <td id=\"T_4ea45_row6_col10\" class=\"data row6 col10\" >0.5984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level0_row7\" class=\"row_heading level0 row7\" rowspan=\"7\">gpt-3.5-turbo</th>\n",
              "      <th id=\"T_4ea45_level1_row7\" class=\"row_heading level1 row7\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_4ea45_row7_col0\" class=\"data row7 col0\" >0.9449</td>\n",
              "      <td id=\"T_4ea45_row7_col1\" class=\"data row7 col1\" >0.9178</td>\n",
              "      <td id=\"T_4ea45_row7_col2\" class=\"data row7 col2\" >0.9383</td>\n",
              "      <td id=\"T_4ea45_row7_col3\" class=\"data row7 col3\" >0.9257</td>\n",
              "      <td id=\"T_4ea45_row7_col4\" class=\"data row7 col4\" >0.9819</td>\n",
              "      <td id=\"T_4ea45_row7_col5\" class=\"data row7 col5\" >0.8718</td>\n",
              "      <td id=\"T_4ea45_row7_col6\" class=\"data row7 col6\" >0.9499</td>\n",
              "      <td id=\"T_4ea45_row7_col7\" class=\"data row7 col7\" >0.9298</td>\n",
              "      <td id=\"T_4ea45_row7_col8\" class=\"data row7 col8\" >0.9100</td>\n",
              "      <td id=\"T_4ea45_row7_col9\" class=\"data row7 col9\" >0.8946</td>\n",
              "      <td id=\"T_4ea45_row7_col10\" class=\"data row7 col10\" >0.9083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row8\" class=\"row_heading level1 row8\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_4ea45_row8_col0\" class=\"data row8 col0\" >0.4179</td>\n",
              "      <td id=\"T_4ea45_row8_col1\" class=\"data row8 col1\" >0.4001</td>\n",
              "      <td id=\"T_4ea45_row8_col2\" class=\"data row8 col2\" >0.3407</td>\n",
              "      <td id=\"T_4ea45_row8_col3\" class=\"data row8 col3\" >0.3524</td>\n",
              "      <td id=\"T_4ea45_row8_col4\" class=\"data row8 col4\" >0.9856</td>\n",
              "      <td id=\"T_4ea45_row8_col5\" class=\"data row8 col5\" >0.6747</td>\n",
              "      <td id=\"T_4ea45_row8_col6\" class=\"data row8 col6\" >0.4557</td>\n",
              "      <td id=\"T_4ea45_row8_col7\" class=\"data row8 col7\" >0.5561</td>\n",
              "      <td id=\"T_4ea45_row8_col8\" class=\"data row8 col8\" >0.4058</td>\n",
              "      <td id=\"T_4ea45_row8_col9\" class=\"data row8 col9\" >0.3972</td>\n",
              "      <td id=\"T_4ea45_row8_col10\" class=\"data row8 col10\" >0.3132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row9\" class=\"row_heading level1 row9\" >gpt2-medium</th>\n",
              "      <td id=\"T_4ea45_row9_col0\" class=\"data row9 col0\" >0.3690</td>\n",
              "      <td id=\"T_4ea45_row9_col1\" class=\"data row9 col1\" >0.3407</td>\n",
              "      <td id=\"T_4ea45_row9_col2\" class=\"data row9 col2\" >0.3370</td>\n",
              "      <td id=\"T_4ea45_row9_col3\" class=\"data row9 col3\" >0.3450</td>\n",
              "      <td id=\"T_4ea45_row9_col4\" class=\"data row9 col4\" >0.9910</td>\n",
              "      <td id=\"T_4ea45_row9_col5\" class=\"data row9 col5\" >0.3301</td>\n",
              "      <td id=\"T_4ea45_row9_col6\" class=\"data row9 col6\" >0.3432</td>\n",
              "      <td id=\"T_4ea45_row9_col7\" class=\"data row9 col7\" >0.3313</td>\n",
              "      <td id=\"T_4ea45_row9_col8\" class=\"data row9 col8\" >0.3658</td>\n",
              "      <td id=\"T_4ea45_row9_col9\" class=\"data row9 col9\" >0.3539</td>\n",
              "      <td id=\"T_4ea45_row9_col10\" class=\"data row9 col10\" >0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row10\" class=\"row_heading level1 row10\" >mGPT</th>\n",
              "      <td id=\"T_4ea45_row10_col0\" class=\"data row10 col0\" >0.6786</td>\n",
              "      <td id=\"T_4ea45_row10_col1\" class=\"data row10 col1\" >0.9083</td>\n",
              "      <td id=\"T_4ea45_row10_col2\" class=\"data row10 col2\" >0.3333</td>\n",
              "      <td id=\"T_4ea45_row10_col3\" class=\"data row10 col3\" >0.9049</td>\n",
              "      <td id=\"T_4ea45_row10_col4\" class=\"data row10 col4\" >0.9874</td>\n",
              "      <td id=\"T_4ea45_row10_col5\" class=\"data row10 col5\" >0.8874</td>\n",
              "      <td id=\"T_4ea45_row10_col6\" class=\"data row10 col6\" >0.9633</td>\n",
              "      <td id=\"T_4ea45_row10_col7\" class=\"data row10 col7\" >0.9284</td>\n",
              "      <td id=\"T_4ea45_row10_col8\" class=\"data row10 col8\" >0.9232</td>\n",
              "      <td id=\"T_4ea45_row10_col9\" class=\"data row10 col9\" >0.8860</td>\n",
              "      <td id=\"T_4ea45_row10_col10\" class=\"data row10 col10\" >0.5494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row11\" class=\"row_heading level1 row11\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_4ea45_row11_col0\" class=\"data row11 col0\" >0.8148</td>\n",
              "      <td id=\"T_4ea45_row11_col1\" class=\"data row11 col1\" >0.7353</td>\n",
              "      <td id=\"T_4ea45_row11_col2\" class=\"data row11 col2\" >0.8158</td>\n",
              "      <td id=\"T_4ea45_row11_col3\" class=\"data row11 col3\" >0.8783</td>\n",
              "      <td id=\"T_4ea45_row11_col4\" class=\"data row11 col4\" >0.9258</td>\n",
              "      <td id=\"T_4ea45_row11_col5\" class=\"data row11 col5\" >0.7020</td>\n",
              "      <td id=\"T_4ea45_row11_col6\" class=\"data row11 col6\" >0.8267</td>\n",
              "      <td id=\"T_4ea45_row11_col7\" class=\"data row11 col7\" >0.7834</td>\n",
              "      <td id=\"T_4ea45_row11_col8\" class=\"data row11 col8\" >0.6769</td>\n",
              "      <td id=\"T_4ea45_row11_col9\" class=\"data row11 col9\" >0.7348</td>\n",
              "      <td id=\"T_4ea45_row11_col10\" class=\"data row11 col10\" >0.7295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row12\" class=\"row_heading level1 row12\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_4ea45_row12_col0\" class=\"data row12 col0\" >0.6747</td>\n",
              "      <td id=\"T_4ea45_row12_col1\" class=\"data row12 col1\" >0.9182</td>\n",
              "      <td id=\"T_4ea45_row12_col2\" class=\"data row12 col2\" >0.8488</td>\n",
              "      <td id=\"T_4ea45_row12_col3\" class=\"data row12 col3\" >0.8416</td>\n",
              "      <td id=\"T_4ea45_row12_col4\" class=\"data row12 col4\" >0.9693</td>\n",
              "      <td id=\"T_4ea45_row12_col5\" class=\"data row12 col5\" >0.7811</td>\n",
              "      <td id=\"T_4ea45_row12_col6\" class=\"data row12 col6\" >0.8637</td>\n",
              "      <td id=\"T_4ea45_row12_col7\" class=\"data row12 col7\" >0.7284</td>\n",
              "      <td id=\"T_4ea45_row12_col8\" class=\"data row12 col8\" >0.6661</td>\n",
              "      <td id=\"T_4ea45_row12_col9\" class=\"data row12 col9\" >0.3600</td>\n",
              "      <td id=\"T_4ea45_row12_col10\" class=\"data row12 col10\" >0.5888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row13\" class=\"row_heading level1 row13\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_4ea45_row13_col0\" class=\"data row13 col0\" >0.9199</td>\n",
              "      <td id=\"T_4ea45_row13_col1\" class=\"data row13 col1\" >0.9800</td>\n",
              "      <td id=\"T_4ea45_row13_col2\" class=\"data row13 col2\" >0.9750</td>\n",
              "      <td id=\"T_4ea45_row13_col3\" class=\"data row13 col3\" >0.9408</td>\n",
              "      <td id=\"T_4ea45_row13_col4\" class=\"data row13 col4\" >0.9838</td>\n",
              "      <td id=\"T_4ea45_row13_col5\" class=\"data row13 col5\" >0.9192</td>\n",
              "      <td id=\"T_4ea45_row13_col6\" class=\"data row13 col6\" >0.9549</td>\n",
              "      <td id=\"T_4ea45_row13_col7\" class=\"data row13 col7\" >0.9385</td>\n",
              "      <td id=\"T_4ea45_row13_col8\" class=\"data row13 col8\" >0.9517</td>\n",
              "      <td id=\"T_4ea45_row13_col9\" class=\"data row13 col9\" >0.9464</td>\n",
              "      <td id=\"T_4ea45_row13_col10\" class=\"data row13 col10\" >0.7963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level0_row14\" class=\"row_heading level0 row14\" rowspan=\"7\">gpt-4</th>\n",
              "      <th id=\"T_4ea45_level1_row14\" class=\"row_heading level1 row14\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_4ea45_row14_col0\" class=\"data row14 col0\" >0.8350</td>\n",
              "      <td id=\"T_4ea45_row14_col1\" class=\"data row14 col1\" >0.9009</td>\n",
              "      <td id=\"T_4ea45_row14_col2\" class=\"data row14 col2\" >0.9200</td>\n",
              "      <td id=\"T_4ea45_row14_col3\" class=\"data row14 col3\" >0.9307</td>\n",
              "      <td id=\"T_4ea45_row14_col4\" class=\"data row14 col4\" >0.9964</td>\n",
              "      <td id=\"T_4ea45_row14_col5\" class=\"data row14 col5\" >0.7323</td>\n",
              "      <td id=\"T_4ea45_row14_col6\" class=\"data row14 col6\" >0.9382</td>\n",
              "      <td id=\"T_4ea45_row14_col7\" class=\"data row14 col7\" >0.8389</td>\n",
              "      <td id=\"T_4ea45_row14_col8\" class=\"data row14 col8\" >0.8766</td>\n",
              "      <td id=\"T_4ea45_row14_col9\" class=\"data row14 col9\" >0.8662</td>\n",
              "      <td id=\"T_4ea45_row14_col10\" class=\"data row14 col10\" >0.8079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row15\" class=\"row_heading level1 row15\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_4ea45_row15_col0\" class=\"data row15 col0\" >0.4747</td>\n",
              "      <td id=\"T_4ea45_row15_col1\" class=\"data row15 col1\" >0.6222</td>\n",
              "      <td id=\"T_4ea45_row15_col2\" class=\"data row15 col2\" >0.3363</td>\n",
              "      <td id=\"T_4ea45_row15_col3\" class=\"data row15 col3\" >0.4691</td>\n",
              "      <td id=\"T_4ea45_row15_col4\" class=\"data row15 col4\" >0.9910</td>\n",
              "      <td id=\"T_4ea45_row15_col5\" class=\"data row15 col5\" >0.8320</td>\n",
              "      <td id=\"T_4ea45_row15_col6\" class=\"data row15 col6\" >0.5694</td>\n",
              "      <td id=\"T_4ea45_row15_col7\" class=\"data row15 col7\" >0.7838</td>\n",
              "      <td id=\"T_4ea45_row15_col8\" class=\"data row15 col8\" >0.3804</td>\n",
              "      <td id=\"T_4ea45_row15_col9\" class=\"data row15 col9\" >0.3378</td>\n",
              "      <td id=\"T_4ea45_row15_col10\" class=\"data row15 col10\" >0.3370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row16\" class=\"row_heading level1 row16\" >gpt2-medium</th>\n",
              "      <td id=\"T_4ea45_row16_col0\" class=\"data row16 col0\" >0.3759</td>\n",
              "      <td id=\"T_4ea45_row16_col1\" class=\"data row16 col1\" >0.3516</td>\n",
              "      <td id=\"T_4ea45_row16_col2\" class=\"data row16 col2\" >0.3370</td>\n",
              "      <td id=\"T_4ea45_row16_col3\" class=\"data row16 col3\" >0.3413</td>\n",
              "      <td id=\"T_4ea45_row16_col4\" class=\"data row16 col4\" >0.9982</td>\n",
              "      <td id=\"T_4ea45_row16_col5\" class=\"data row16 col5\" >0.3553</td>\n",
              "      <td id=\"T_4ea45_row16_col6\" class=\"data row16 col6\" >0.3540</td>\n",
              "      <td id=\"T_4ea45_row16_col7\" class=\"data row16 col7\" >0.3565</td>\n",
              "      <td id=\"T_4ea45_row16_col8\" class=\"data row16 col8\" >0.3725</td>\n",
              "      <td id=\"T_4ea45_row16_col9\" class=\"data row16 col9\" >0.3468</td>\n",
              "      <td id=\"T_4ea45_row16_col10\" class=\"data row16 col10\" >0.3407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row17\" class=\"row_heading level1 row17\" >mGPT</th>\n",
              "      <td id=\"T_4ea45_row17_col0\" class=\"data row17 col0\" >0.8481</td>\n",
              "      <td id=\"T_4ea45_row17_col1\" class=\"data row17 col1\" >0.9299</td>\n",
              "      <td id=\"T_4ea45_row17_col2\" class=\"data row17 col2\" >0.3333</td>\n",
              "      <td id=\"T_4ea45_row17_col3\" class=\"data row17 col3\" >0.9104</td>\n",
              "      <td id=\"T_4ea45_row17_col4\" class=\"data row17 col4\" >0.9928</td>\n",
              "      <td id=\"T_4ea45_row17_col5\" class=\"data row17 col5\" >0.7951</td>\n",
              "      <td id=\"T_4ea45_row17_col6\" class=\"data row17 col6\" >0.9314</td>\n",
              "      <td id=\"T_4ea45_row17_col7\" class=\"data row17 col7\" >0.9072</td>\n",
              "      <td id=\"T_4ea45_row17_col8\" class=\"data row17 col8\" >0.8798</td>\n",
              "      <td id=\"T_4ea45_row17_col9\" class=\"data row17 col9\" >0.8067</td>\n",
              "      <td id=\"T_4ea45_row17_col10\" class=\"data row17 col10\" >0.6570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row18\" class=\"row_heading level1 row18\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_4ea45_row18_col0\" class=\"data row18 col0\" >0.8199</td>\n",
              "      <td id=\"T_4ea45_row18_col1\" class=\"data row18 col1\" >0.7249</td>\n",
              "      <td id=\"T_4ea45_row18_col2\" class=\"data row18 col2\" >0.9076</td>\n",
              "      <td id=\"T_4ea45_row18_col3\" class=\"data row18 col3\" >0.8679</td>\n",
              "      <td id=\"T_4ea45_row18_col4\" class=\"data row18 col4\" >0.6291</td>\n",
              "      <td id=\"T_4ea45_row18_col5\" class=\"data row18 col5\" >0.6506</td>\n",
              "      <td id=\"T_4ea45_row18_col6\" class=\"data row18 col6\" >0.8023</td>\n",
              "      <td id=\"T_4ea45_row18_col7\" class=\"data row18 col7\" >0.7064</td>\n",
              "      <td id=\"T_4ea45_row18_col8\" class=\"data row18 col8\" >0.7794</td>\n",
              "      <td id=\"T_4ea45_row18_col9\" class=\"data row18 col9\" >0.8271</td>\n",
              "      <td id=\"T_4ea45_row18_col10\" class=\"data row18 col10\" >0.6940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row19\" class=\"row_heading level1 row19\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_4ea45_row19_col0\" class=\"data row19 col0\" >0.5115</td>\n",
              "      <td id=\"T_4ea45_row19_col1\" class=\"data row19 col1\" >0.6923</td>\n",
              "      <td id=\"T_4ea45_row19_col2\" class=\"data row19 col2\" >0.5428</td>\n",
              "      <td id=\"T_4ea45_row19_col3\" class=\"data row19 col3\" >0.6090</td>\n",
              "      <td id=\"T_4ea45_row19_col4\" class=\"data row19 col4\" >0.9892</td>\n",
              "      <td id=\"T_4ea45_row19_col5\" class=\"data row19 col5\" >0.4870</td>\n",
              "      <td id=\"T_4ea45_row19_col6\" class=\"data row19 col6\" >0.7771</td>\n",
              "      <td id=\"T_4ea45_row19_col7\" class=\"data row19 col7\" >0.4381</td>\n",
              "      <td id=\"T_4ea45_row19_col8\" class=\"data row19 col8\" >0.4911</td>\n",
              "      <td id=\"T_4ea45_row19_col9\" class=\"data row19 col9\" >0.4030</td>\n",
              "      <td id=\"T_4ea45_row19_col10\" class=\"data row19 col10\" >0.6774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row20\" class=\"row_heading level1 row20\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_4ea45_row20_col0\" class=\"data row20 col0\" >0.8780</td>\n",
              "      <td id=\"T_4ea45_row20_col1\" class=\"data row20 col1\" >0.9093</td>\n",
              "      <td id=\"T_4ea45_row20_col2\" class=\"data row20 col2\" >0.9566</td>\n",
              "      <td id=\"T_4ea45_row20_col3\" class=\"data row20 col3\" >0.9645</td>\n",
              "      <td id=\"T_4ea45_row20_col4\" class=\"data row20 col4\" >0.9946</td>\n",
              "      <td id=\"T_4ea45_row20_col5\" class=\"data row20 col5\" >0.8286</td>\n",
              "      <td id=\"T_4ea45_row20_col6\" class=\"data row20 col6\" >0.9248</td>\n",
              "      <td id=\"T_4ea45_row20_col7\" class=\"data row20 col7\" >0.8808</td>\n",
              "      <td id=\"T_4ea45_row20_col8\" class=\"data row20 col8\" >0.9066</td>\n",
              "      <td id=\"T_4ea45_row20_col9\" class=\"data row20 col9\" >0.9298</td>\n",
              "      <td id=\"T_4ea45_row20_col10\" class=\"data row20 col10\" >0.8614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level0_row21\" class=\"row_heading level0 row21\" rowspan=\"7\">llama-65b</th>\n",
              "      <th id=\"T_4ea45_level1_row21\" class=\"row_heading level1 row21\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_4ea45_row21_col0\" class=\"data row21 col0\" >0.6552</td>\n",
              "      <td id=\"T_4ea45_row21_col1\" class=\"data row21 col1\" >0.6417</td>\n",
              "      <td id=\"T_4ea45_row21_col2\" class=\"data row21 col2\" >0.7971</td>\n",
              "      <td id=\"T_4ea45_row21_col3\" class=\"data row21 col3\" >0.7019</td>\n",
              "      <td id=\"T_4ea45_row21_col4\" class=\"data row21 col4\" >0.9219</td>\n",
              "      <td id=\"T_4ea45_row21_col5\" class=\"data row21 col5\" >0.8057</td>\n",
              "      <td id=\"T_4ea45_row21_col6\" class=\"data row21 col6\" >0.6370</td>\n",
              "      <td id=\"T_4ea45_row21_col7\" class=\"data row21 col7\" >0.6880</td>\n",
              "      <td id=\"T_4ea45_row21_col8\" class=\"data row21 col8\" >0.7016</td>\n",
              "      <td id=\"T_4ea45_row21_col9\" class=\"data row21 col9\" >0.8031</td>\n",
              "      <td id=\"T_4ea45_row21_col10\" class=\"data row21 col10\" >0.5659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row22\" class=\"row_heading level1 row22\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_4ea45_row22_col0\" class=\"data row22 col0\" >0.3311</td>\n",
              "      <td id=\"T_4ea45_row22_col1\" class=\"data row22 col1\" >0.3363</td>\n",
              "      <td id=\"T_4ea45_row22_col2\" class=\"data row22 col2\" >0.3336</td>\n",
              "      <td id=\"T_4ea45_row22_col3\" class=\"data row22 col3\" >0.3536</td>\n",
              "      <td id=\"T_4ea45_row22_col4\" class=\"data row22 col4\" >0.9365</td>\n",
              "      <td id=\"T_4ea45_row22_col5\" class=\"data row22 col5\" >0.3537</td>\n",
              "      <td id=\"T_4ea45_row22_col6\" class=\"data row22 col6\" >0.3330</td>\n",
              "      <td id=\"T_4ea45_row22_col7\" class=\"data row22 col7\" >0.3395</td>\n",
              "      <td id=\"T_4ea45_row22_col8\" class=\"data row22 col8\" >0.3311</td>\n",
              "      <td id=\"T_4ea45_row22_col9\" class=\"data row22 col9\" >0.3326</td>\n",
              "      <td id=\"T_4ea45_row22_col10\" class=\"data row22 col10\" >0.3318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row23\" class=\"row_heading level1 row23\" >gpt2-medium</th>\n",
              "      <td id=\"T_4ea45_row23_col0\" class=\"data row23 col0\" >0.3472</td>\n",
              "      <td id=\"T_4ea45_row23_col1\" class=\"data row23 col1\" >0.3979</td>\n",
              "      <td id=\"T_4ea45_row23_col2\" class=\"data row23 col2\" >0.5536</td>\n",
              "      <td id=\"T_4ea45_row23_col3\" class=\"data row23 col3\" >0.4564</td>\n",
              "      <td id=\"T_4ea45_row23_col4\" class=\"data row23 col4\" >0.9384</td>\n",
              "      <td id=\"T_4ea45_row23_col5\" class=\"data row23 col5\" >0.5798</td>\n",
              "      <td id=\"T_4ea45_row23_col6\" class=\"data row23 col6\" >0.3912</td>\n",
              "      <td id=\"T_4ea45_row23_col7\" class=\"data row23 col7\" >0.4192</td>\n",
              "      <td id=\"T_4ea45_row23_col8\" class=\"data row23 col8\" >0.3844</td>\n",
              "      <td id=\"T_4ea45_row23_col9\" class=\"data row23 col9\" >0.3912</td>\n",
              "      <td id=\"T_4ea45_row23_col10\" class=\"data row23 col10\" >0.4145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row24\" class=\"row_heading level1 row24\" >mGPT</th>\n",
              "      <td id=\"T_4ea45_row24_col0\" class=\"data row24 col0\" >0.3369</td>\n",
              "      <td id=\"T_4ea45_row24_col1\" class=\"data row24 col1\" >0.3616</td>\n",
              "      <td id=\"T_4ea45_row24_col2\" class=\"data row24 col2\" >0.7761</td>\n",
              "      <td id=\"T_4ea45_row24_col3\" class=\"data row24 col3\" >0.7157</td>\n",
              "      <td id=\"T_4ea45_row24_col4\" class=\"data row24 col4\" >0.9147</td>\n",
              "      <td id=\"T_4ea45_row24_col5\" class=\"data row24 col5\" >0.5517</td>\n",
              "      <td id=\"T_4ea45_row24_col6\" class=\"data row24 col6\" >0.4591</td>\n",
              "      <td id=\"T_4ea45_row24_col7\" class=\"data row24 col7\" >0.5616</td>\n",
              "      <td id=\"T_4ea45_row24_col8\" class=\"data row24 col8\" >0.4846</td>\n",
              "      <td id=\"T_4ea45_row24_col9\" class=\"data row24 col9\" >0.5137</td>\n",
              "      <td id=\"T_4ea45_row24_col10\" class=\"data row24 col10\" >0.5556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row25\" class=\"row_heading level1 row25\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_4ea45_row25_col0\" class=\"data row25 col0\" >0.7477</td>\n",
              "      <td id=\"T_4ea45_row25_col1\" class=\"data row25 col1\" >0.6646</td>\n",
              "      <td id=\"T_4ea45_row25_col2\" class=\"data row25 col2\" >0.9272</td>\n",
              "      <td id=\"T_4ea45_row25_col3\" class=\"data row25 col3\" >0.7560</td>\n",
              "      <td id=\"T_4ea45_row25_col4\" class=\"data row25 col4\" >0.8959</td>\n",
              "      <td id=\"T_4ea45_row25_col5\" class=\"data row25 col5\" >0.8662</td>\n",
              "      <td id=\"T_4ea45_row25_col6\" class=\"data row25 col6\" >0.6821</td>\n",
              "      <td id=\"T_4ea45_row25_col7\" class=\"data row25 col7\" >0.7176</td>\n",
              "      <td id=\"T_4ea45_row25_col8\" class=\"data row25 col8\" >0.7734</td>\n",
              "      <td id=\"T_4ea45_row25_col9\" class=\"data row25 col9\" >0.8533</td>\n",
              "      <td id=\"T_4ea45_row25_col10\" class=\"data row25 col10\" >0.5902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row26\" class=\"row_heading level1 row26\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_4ea45_row26_col0\" class=\"data row26 col0\" >0.3311</td>\n",
              "      <td id=\"T_4ea45_row26_col1\" class=\"data row26 col1\" >0.3607</td>\n",
              "      <td id=\"T_4ea45_row26_col2\" class=\"data row26 col2\" >0.3545</td>\n",
              "      <td id=\"T_4ea45_row26_col3\" class=\"data row26 col3\" >0.3408</td>\n",
              "      <td id=\"T_4ea45_row26_col4\" class=\"data row26 col4\" >0.9457</td>\n",
              "      <td id=\"T_4ea45_row26_col5\" class=\"data row26 col5\" >0.3460</td>\n",
              "      <td id=\"T_4ea45_row26_col6\" class=\"data row26 col6\" >0.3761</td>\n",
              "      <td id=\"T_4ea45_row26_col7\" class=\"data row26 col7\" >0.3395</td>\n",
              "      <td id=\"T_4ea45_row26_col8\" class=\"data row26 col8\" >0.3355</td>\n",
              "      <td id=\"T_4ea45_row26_col9\" class=\"data row26 col9\" >0.3529</td>\n",
              "      <td id=\"T_4ea45_row26_col10\" class=\"data row26 col10\" >0.3325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row27\" class=\"row_heading level1 row27\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_4ea45_row27_col0\" class=\"data row27 col0\" >0.4532</td>\n",
              "      <td id=\"T_4ea45_row27_col1\" class=\"data row27 col1\" >0.4074</td>\n",
              "      <td id=\"T_4ea45_row27_col2\" class=\"data row27 col2\" >0.8414</td>\n",
              "      <td id=\"T_4ea45_row27_col3\" class=\"data row27 col3\" >0.7198</td>\n",
              "      <td id=\"T_4ea45_row27_col4\" class=\"data row27 col4\" >0.9384</td>\n",
              "      <td id=\"T_4ea45_row27_col5\" class=\"data row27 col5\" >0.6397</td>\n",
              "      <td id=\"T_4ea45_row27_col6\" class=\"data row27 col6\" >0.4080</td>\n",
              "      <td id=\"T_4ea45_row27_col7\" class=\"data row27 col7\" >0.5535</td>\n",
              "      <td id=\"T_4ea45_row27_col8\" class=\"data row27 col8\" >0.6371</td>\n",
              "      <td id=\"T_4ea45_row27_col9\" class=\"data row27 col9\" >0.6560</td>\n",
              "      <td id=\"T_4ea45_row27_col10\" class=\"data row27 col10\" >0.4488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level0_row28\" class=\"row_heading level0 row28\" rowspan=\"7\">opt-66b</th>\n",
              "      <th id=\"T_4ea45_level1_row28\" class=\"row_heading level1 row28\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_4ea45_row28_col0\" class=\"data row28 col0\" >0.4121</td>\n",
              "      <td id=\"T_4ea45_row28_col1\" class=\"data row28 col1\" >0.5241</td>\n",
              "      <td id=\"T_4ea45_row28_col2\" class=\"data row28 col2\" >0.6850</td>\n",
              "      <td id=\"T_4ea45_row28_col3\" class=\"data row28 col3\" >0.6789</td>\n",
              "      <td id=\"T_4ea45_row28_col4\" class=\"data row28 col4\" >0.9079</td>\n",
              "      <td id=\"T_4ea45_row28_col5\" class=\"data row28 col5\" >0.5558</td>\n",
              "      <td id=\"T_4ea45_row28_col6\" class=\"data row28 col6\" >0.5274</td>\n",
              "      <td id=\"T_4ea45_row28_col7\" class=\"data row28 col7\" >0.6377</td>\n",
              "      <td id=\"T_4ea45_row28_col8\" class=\"data row28 col8\" >0.6104</td>\n",
              "      <td id=\"T_4ea45_row28_col9\" class=\"data row28 col9\" >0.6477</td>\n",
              "      <td id=\"T_4ea45_row28_col10\" class=\"data row28 col10\" >0.5323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row29\" class=\"row_heading level1 row29\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_4ea45_row29_col0\" class=\"data row29 col0\" >0.3969</td>\n",
              "      <td id=\"T_4ea45_row29_col1\" class=\"data row29 col1\" >0.6936</td>\n",
              "      <td id=\"T_4ea45_row29_col2\" class=\"data row29 col2\" >0.6463</td>\n",
              "      <td id=\"T_4ea45_row29_col3\" class=\"data row29 col3\" >0.6972</td>\n",
              "      <td id=\"T_4ea45_row29_col4\" class=\"data row29 col4\" >0.9314</td>\n",
              "      <td id=\"T_4ea45_row29_col5\" class=\"data row29 col5\" >0.6785</td>\n",
              "      <td id=\"T_4ea45_row29_col6\" class=\"data row29 col6\" >0.6637</td>\n",
              "      <td id=\"T_4ea45_row29_col7\" class=\"data row29 col7\" >0.7006</td>\n",
              "      <td id=\"T_4ea45_row29_col8\" class=\"data row29 col8\" >0.3390</td>\n",
              "      <td id=\"T_4ea45_row29_col9\" class=\"data row29 col9\" >0.3303</td>\n",
              "      <td id=\"T_4ea45_row29_col10\" class=\"data row29 col10\" >0.3311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row30\" class=\"row_heading level1 row30\" >gpt2-medium</th>\n",
              "      <td id=\"T_4ea45_row30_col0\" class=\"data row30 col0\" >0.4448</td>\n",
              "      <td id=\"T_4ea45_row30_col1\" class=\"data row30 col1\" >0.4992</td>\n",
              "      <td id=\"T_4ea45_row30_col2\" class=\"data row30 col2\" >0.4264</td>\n",
              "      <td id=\"T_4ea45_row30_col3\" class=\"data row30 col3\" >0.5941</td>\n",
              "      <td id=\"T_4ea45_row30_col4\" class=\"data row30 col4\" >0.8898</td>\n",
              "      <td id=\"T_4ea45_row30_col5\" class=\"data row30 col5\" >0.5432</td>\n",
              "      <td id=\"T_4ea45_row30_col6\" class=\"data row30 col6\" >0.4274</td>\n",
              "      <td id=\"T_4ea45_row30_col7\" class=\"data row30 col7\" >0.6423</td>\n",
              "      <td id=\"T_4ea45_row30_col8\" class=\"data row30 col8\" >0.4870</td>\n",
              "      <td id=\"T_4ea45_row30_col9\" class=\"data row30 col9\" >0.5507</td>\n",
              "      <td id=\"T_4ea45_row30_col10\" class=\"data row30 col10\" >0.4764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row31\" class=\"row_heading level1 row31\" >mGPT</th>\n",
              "      <td id=\"T_4ea45_row31_col0\" class=\"data row31 col0\" >0.1983</td>\n",
              "      <td id=\"T_4ea45_row31_col1\" class=\"data row31 col1\" >0.5890</td>\n",
              "      <td id=\"T_4ea45_row31_col2\" class=\"data row31 col2\" >0.5708</td>\n",
              "      <td id=\"T_4ea45_row31_col3\" class=\"data row31 col3\" >0.6564</td>\n",
              "      <td id=\"T_4ea45_row31_col4\" class=\"data row31 col4\" >0.9185</td>\n",
              "      <td id=\"T_4ea45_row31_col5\" class=\"data row31 col5\" >0.5605</td>\n",
              "      <td id=\"T_4ea45_row31_col6\" class=\"data row31 col6\" >0.5826</td>\n",
              "      <td id=\"T_4ea45_row31_col7\" class=\"data row31 col7\" >0.6449</td>\n",
              "      <td id=\"T_4ea45_row31_col8\" class=\"data row31 col8\" >0.4463</td>\n",
              "      <td id=\"T_4ea45_row31_col9\" class=\"data row31 col9\" >0.4981</td>\n",
              "      <td id=\"T_4ea45_row31_col10\" class=\"data row31 col10\" >0.4464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row32\" class=\"row_heading level1 row32\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_4ea45_row32_col0\" class=\"data row32 col0\" >0.2612</td>\n",
              "      <td id=\"T_4ea45_row32_col1\" class=\"data row32 col1\" >0.4316</td>\n",
              "      <td id=\"T_4ea45_row32_col2\" class=\"data row32 col2\" >0.5316</td>\n",
              "      <td id=\"T_4ea45_row32_col3\" class=\"data row32 col3\" >0.6257</td>\n",
              "      <td id=\"T_4ea45_row32_col4\" class=\"data row32 col4\" >0.7720</td>\n",
              "      <td id=\"T_4ea45_row32_col5\" class=\"data row32 col5\" >0.4713</td>\n",
              "      <td id=\"T_4ea45_row32_col6\" class=\"data row32 col6\" >0.4184</td>\n",
              "      <td id=\"T_4ea45_row32_col7\" class=\"data row32 col7\" >0.3897</td>\n",
              "      <td id=\"T_4ea45_row32_col8\" class=\"data row32 col8\" >0.5503</td>\n",
              "      <td id=\"T_4ea45_row32_col9\" class=\"data row32 col9\" >0.4716</td>\n",
              "      <td id=\"T_4ea45_row32_col10\" class=\"data row32 col10\" >0.5307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row33\" class=\"row_heading level1 row33\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_4ea45_row33_col0\" class=\"data row33 col0\" >0.3322</td>\n",
              "      <td id=\"T_4ea45_row33_col1\" class=\"data row33 col1\" >0.6773</td>\n",
              "      <td id=\"T_4ea45_row33_col2\" class=\"data row33 col2\" >0.4695</td>\n",
              "      <td id=\"T_4ea45_row33_col3\" class=\"data row33 col3\" >0.4529</td>\n",
              "      <td id=\"T_4ea45_row33_col4\" class=\"data row33 col4\" >0.9332</td>\n",
              "      <td id=\"T_4ea45_row33_col5\" class=\"data row33 col5\" >0.4155</td>\n",
              "      <td id=\"T_4ea45_row33_col6\" class=\"data row33 col6\" >0.6678</td>\n",
              "      <td id=\"T_4ea45_row33_col7\" class=\"data row33 col7\" >0.4105</td>\n",
              "      <td id=\"T_4ea45_row33_col8\" class=\"data row33 col8\" >0.3346</td>\n",
              "      <td id=\"T_4ea45_row33_col9\" class=\"data row33 col9\" >0.3886</td>\n",
              "      <td id=\"T_4ea45_row33_col10\" class=\"data row33 col10\" >0.3376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row34\" class=\"row_heading level1 row34\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_4ea45_row34_col0\" class=\"data row34 col0\" >0.4182</td>\n",
              "      <td id=\"T_4ea45_row34_col1\" class=\"data row34 col1\" >0.4089</td>\n",
              "      <td id=\"T_4ea45_row34_col2\" class=\"data row34 col2\" >0.6021</td>\n",
              "      <td id=\"T_4ea45_row34_col3\" class=\"data row34 col3\" >0.6750</td>\n",
              "      <td id=\"T_4ea45_row34_col4\" class=\"data row34 col4\" >0.9205</td>\n",
              "      <td id=\"T_4ea45_row34_col5\" class=\"data row34 col5\" >0.5871</td>\n",
              "      <td id=\"T_4ea45_row34_col6\" class=\"data row34 col6\" >0.4205</td>\n",
              "      <td id=\"T_4ea45_row34_col7\" class=\"data row34 col7\" >0.5935</td>\n",
              "      <td id=\"T_4ea45_row34_col8\" class=\"data row34 col8\" >0.5884</td>\n",
              "      <td id=\"T_4ea45_row34_col9\" class=\"data row34 col9\" >0.6220</td>\n",
              "      <td id=\"T_4ea45_row34_col10\" class=\"data row34 col10\" >0.5175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level0_row35\" class=\"row_heading level0 row35\" rowspan=\"7\">opt-iml-max-1.3b</th>\n",
              "      <th id=\"T_4ea45_level1_row35\" class=\"row_heading level1 row35\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_4ea45_row35_col0\" class=\"data row35 col0\" >0.5705</td>\n",
              "      <td id=\"T_4ea45_row35_col1\" class=\"data row35 col1\" >0.6372</td>\n",
              "      <td id=\"T_4ea45_row35_col2\" class=\"data row35 col2\" >0.7086</td>\n",
              "      <td id=\"T_4ea45_row35_col3\" class=\"data row35 col3\" >0.7003</td>\n",
              "      <td id=\"T_4ea45_row35_col4\" class=\"data row35 col4\" >0.9549</td>\n",
              "      <td id=\"T_4ea45_row35_col5\" class=\"data row35 col5\" >0.6327</td>\n",
              "      <td id=\"T_4ea45_row35_col6\" class=\"data row35 col6\" >0.6705</td>\n",
              "      <td id=\"T_4ea45_row35_col7\" class=\"data row35 col7\" >0.6447</td>\n",
              "      <td id=\"T_4ea45_row35_col8\" class=\"data row35 col8\" >0.5501</td>\n",
              "      <td id=\"T_4ea45_row35_col9\" class=\"data row35 col9\" >0.6380</td>\n",
              "      <td id=\"T_4ea45_row35_col10\" class=\"data row35 col10\" >0.5188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row36\" class=\"row_heading level1 row36\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_4ea45_row36_col0\" class=\"data row36 col0\" >0.3280</td>\n",
              "      <td id=\"T_4ea45_row36_col1\" class=\"data row36 col1\" >0.6391</td>\n",
              "      <td id=\"T_4ea45_row36_col2\" class=\"data row36 col2\" >0.6211</td>\n",
              "      <td id=\"T_4ea45_row36_col3\" class=\"data row36 col3\" >0.7352</td>\n",
              "      <td id=\"T_4ea45_row36_col4\" class=\"data row36 col4\" >0.9964</td>\n",
              "      <td id=\"T_4ea45_row36_col5\" class=\"data row36 col5\" >0.8875</td>\n",
              "      <td id=\"T_4ea45_row36_col6\" class=\"data row36 col6\" >0.8502</td>\n",
              "      <td id=\"T_4ea45_row36_col7\" class=\"data row36 col7\" >0.8454</td>\n",
              "      <td id=\"T_4ea45_row36_col8\" class=\"data row36 col8\" >0.3280</td>\n",
              "      <td id=\"T_4ea45_row36_col9\" class=\"data row36 col9\" >0.3275</td>\n",
              "      <td id=\"T_4ea45_row36_col10\" class=\"data row36 col10\" >0.3292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row37\" class=\"row_heading level1 row37\" >gpt2-medium</th>\n",
              "      <td id=\"T_4ea45_row37_col0\" class=\"data row37 col0\" >0.3740</td>\n",
              "      <td id=\"T_4ea45_row37_col1\" class=\"data row37 col1\" >0.8301</td>\n",
              "      <td id=\"T_4ea45_row37_col2\" class=\"data row37 col2\" >0.5994</td>\n",
              "      <td id=\"T_4ea45_row37_col3\" class=\"data row37 col3\" >0.6433</td>\n",
              "      <td id=\"T_4ea45_row37_col4\" class=\"data row37 col4\" >0.9801</td>\n",
              "      <td id=\"T_4ea45_row37_col5\" class=\"data row37 col5\" >0.7471</td>\n",
              "      <td id=\"T_4ea45_row37_col6\" class=\"data row37 col6\" >0.6005</td>\n",
              "      <td id=\"T_4ea45_row37_col7\" class=\"data row37 col7\" >0.6545</td>\n",
              "      <td id=\"T_4ea45_row37_col8\" class=\"data row37 col8\" >0.3770</td>\n",
              "      <td id=\"T_4ea45_row37_col9\" class=\"data row37 col9\" >0.3929</td>\n",
              "      <td id=\"T_4ea45_row37_col10\" class=\"data row37 col10\" >0.4977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row38\" class=\"row_heading level1 row38\" >mGPT</th>\n",
              "      <td id=\"T_4ea45_row38_col0\" class=\"data row38 col0\" >0.2607</td>\n",
              "      <td id=\"T_4ea45_row38_col1\" class=\"data row38 col1\" >0.5663</td>\n",
              "      <td id=\"T_4ea45_row38_col2\" class=\"data row38 col2\" >0.5731</td>\n",
              "      <td id=\"T_4ea45_row38_col3\" class=\"data row38 col3\" >0.6369</td>\n",
              "      <td id=\"T_4ea45_row38_col4\" class=\"data row38 col4\" >0.9729</td>\n",
              "      <td id=\"T_4ea45_row38_col5\" class=\"data row38 col5\" >0.5881</td>\n",
              "      <td id=\"T_4ea45_row38_col6\" class=\"data row38 col6\" >0.5901</td>\n",
              "      <td id=\"T_4ea45_row38_col7\" class=\"data row38 col7\" >0.4952</td>\n",
              "      <td id=\"T_4ea45_row38_col8\" class=\"data row38 col8\" >0.2563</td>\n",
              "      <td id=\"T_4ea45_row38_col9\" class=\"data row38 col9\" >0.2537</td>\n",
              "      <td id=\"T_4ea45_row38_col10\" class=\"data row38 col10\" >0.4156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row39\" class=\"row_heading level1 row39\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_4ea45_row39_col0\" class=\"data row39 col0\" >0.7309</td>\n",
              "      <td id=\"T_4ea45_row39_col1\" class=\"data row39 col1\" >0.8618</td>\n",
              "      <td id=\"T_4ea45_row39_col2\" class=\"data row39 col2\" >0.9212</td>\n",
              "      <td id=\"T_4ea45_row39_col3\" class=\"data row39 col3\" >0.8152</td>\n",
              "      <td id=\"T_4ea45_row39_col4\" class=\"data row39 col4\" >0.9729</td>\n",
              "      <td id=\"T_4ea45_row39_col5\" class=\"data row39 col5\" >0.7726</td>\n",
              "      <td id=\"T_4ea45_row39_col6\" class=\"data row39 col6\" >0.8558</td>\n",
              "      <td id=\"T_4ea45_row39_col7\" class=\"data row39 col7\" >0.7391</td>\n",
              "      <td id=\"T_4ea45_row39_col8\" class=\"data row39 col8\" >0.7542</td>\n",
              "      <td id=\"T_4ea45_row39_col9\" class=\"data row39 col9\" >0.8477</td>\n",
              "      <td id=\"T_4ea45_row39_col10\" class=\"data row39 col10\" >0.7328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row40\" class=\"row_heading level1 row40\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_4ea45_row40_col0\" class=\"data row40 col0\" >0.3280</td>\n",
              "      <td id=\"T_4ea45_row40_col1\" class=\"data row40 col1\" >0.8886</td>\n",
              "      <td id=\"T_4ea45_row40_col2\" class=\"data row40 col2\" >0.7955</td>\n",
              "      <td id=\"T_4ea45_row40_col3\" class=\"data row40 col3\" >0.6782</td>\n",
              "      <td id=\"T_4ea45_row40_col4\" class=\"data row40 col4\" >0.9856</td>\n",
              "      <td id=\"T_4ea45_row40_col5\" class=\"data row40 col5\" >0.7670</td>\n",
              "      <td id=\"T_4ea45_row40_col6\" class=\"data row40 col6\" >0.9075</td>\n",
              "      <td id=\"T_4ea45_row40_col7\" class=\"data row40 col7\" >0.6858</td>\n",
              "      <td id=\"T_4ea45_row40_col8\" class=\"data row40 col8\" >0.3429</td>\n",
              "      <td id=\"T_4ea45_row40_col9\" class=\"data row40 col9\" >0.4533</td>\n",
              "      <td id=\"T_4ea45_row40_col10\" class=\"data row40 col10\" >0.3737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row41\" class=\"row_heading level1 row41\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_4ea45_row41_col0\" class=\"data row41 col0\" >0.6694</td>\n",
              "      <td id=\"T_4ea45_row41_col1\" class=\"data row41 col1\" >0.7393</td>\n",
              "      <td id=\"T_4ea45_row41_col2\" class=\"data row41 col2\" >0.7423</td>\n",
              "      <td id=\"T_4ea45_row41_col3\" class=\"data row41 col3\" >0.8130</td>\n",
              "      <td id=\"T_4ea45_row41_col4\" class=\"data row41 col4\" >0.9711</td>\n",
              "      <td id=\"T_4ea45_row41_col5\" class=\"data row41 col5\" >0.7553</td>\n",
              "      <td id=\"T_4ea45_row41_col6\" class=\"data row41 col6\" >0.6195</td>\n",
              "      <td id=\"T_4ea45_row41_col7\" class=\"data row41 col7\" >0.6933</td>\n",
              "      <td id=\"T_4ea45_row41_col8\" class=\"data row41 col8\" >0.5689</td>\n",
              "      <td id=\"T_4ea45_row41_col9\" class=\"data row41 col9\" >0.5977</td>\n",
              "      <td id=\"T_4ea45_row41_col10\" class=\"data row41 col10\" >0.6165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level0_row42\" class=\"row_heading level0 row42\" rowspan=\"7\">text-davinci-003</th>\n",
              "      <th id=\"T_4ea45_level1_row42\" class=\"row_heading level1 row42\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_4ea45_row42_col0\" class=\"data row42 col0\" >0.7853</td>\n",
              "      <td id=\"T_4ea45_row42_col1\" class=\"data row42 col1\" >0.9064</td>\n",
              "      <td id=\"T_4ea45_row42_col2\" class=\"data row42 col2\" >0.8815</td>\n",
              "      <td id=\"T_4ea45_row42_col3\" class=\"data row42 col3\" >0.8951</td>\n",
              "      <td id=\"T_4ea45_row42_col4\" class=\"data row42 col4\" >0.9693</td>\n",
              "      <td id=\"T_4ea45_row42_col5\" class=\"data row42 col5\" >0.7987</td>\n",
              "      <td id=\"T_4ea45_row42_col6\" class=\"data row42 col6\" >0.9115</td>\n",
              "      <td id=\"T_4ea45_row42_col7\" class=\"data row42 col7\" >0.8632</td>\n",
              "      <td id=\"T_4ea45_row42_col8\" class=\"data row42 col8\" >0.7773</td>\n",
              "      <td id=\"T_4ea45_row42_col9\" class=\"data row42 col9\" >0.7333</td>\n",
              "      <td id=\"T_4ea45_row42_col10\" class=\"data row42 col10\" >0.8883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row43\" class=\"row_heading level1 row43\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_4ea45_row43_col0\" class=\"data row43 col0\" >0.3985</td>\n",
              "      <td id=\"T_4ea45_row43_col1\" class=\"data row43 col1\" >0.3333</td>\n",
              "      <td id=\"T_4ea45_row43_col2\" class=\"data row43 col2\" >0.3333</td>\n",
              "      <td id=\"T_4ea45_row43_col3\" class=\"data row43 col3\" >0.3303</td>\n",
              "      <td id=\"T_4ea45_row43_col4\" class=\"data row43 col4\" >0.9910</td>\n",
              "      <td id=\"T_4ea45_row43_col5\" class=\"data row43 col5\" >0.3309</td>\n",
              "      <td id=\"T_4ea45_row43_col6\" class=\"data row43 col6\" >0.3330</td>\n",
              "      <td id=\"T_4ea45_row43_col7\" class=\"data row43 col7\" >0.3284</td>\n",
              "      <td id=\"T_4ea45_row43_col8\" class=\"data row43 col8\" >0.3741</td>\n",
              "      <td id=\"T_4ea45_row43_col9\" class=\"data row43 col9\" >0.4089</td>\n",
              "      <td id=\"T_4ea45_row43_col10\" class=\"data row43 col10\" >0.3333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row44\" class=\"row_heading level1 row44\" >gpt2-medium</th>\n",
              "      <td id=\"T_4ea45_row44_col0\" class=\"data row44 col0\" >0.3330</td>\n",
              "      <td id=\"T_4ea45_row44_col1\" class=\"data row44 col1\" >0.3333</td>\n",
              "      <td id=\"T_4ea45_row44_col2\" class=\"data row44 col2\" >0.3333</td>\n",
              "      <td id=\"T_4ea45_row44_col3\" class=\"data row44 col3\" >0.3303</td>\n",
              "      <td id=\"T_4ea45_row44_col4\" class=\"data row44 col4\" >0.9892</td>\n",
              "      <td id=\"T_4ea45_row44_col5\" class=\"data row44 col5\" >0.3264</td>\n",
              "      <td id=\"T_4ea45_row44_col6\" class=\"data row44 col6\" >0.3330</td>\n",
              "      <td id=\"T_4ea45_row44_col7\" class=\"data row44 col7\" >0.3276</td>\n",
              "      <td id=\"T_4ea45_row44_col8\" class=\"data row44 col8\" >0.3283</td>\n",
              "      <td id=\"T_4ea45_row44_col9\" class=\"data row44 col9\" >0.3334</td>\n",
              "      <td id=\"T_4ea45_row44_col10\" class=\"data row44 col10\" >0.3326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row45\" class=\"row_heading level1 row45\" >mGPT</th>\n",
              "      <td id=\"T_4ea45_row45_col0\" class=\"data row45 col0\" >0.6136</td>\n",
              "      <td id=\"T_4ea45_row45_col1\" class=\"data row45 col1\" >0.8932</td>\n",
              "      <td id=\"T_4ea45_row45_col2\" class=\"data row45 col2\" >0.3333</td>\n",
              "      <td id=\"T_4ea45_row45_col3\" class=\"data row45 col3\" >0.8701</td>\n",
              "      <td id=\"T_4ea45_row45_col4\" class=\"data row45 col4\" >0.9892</td>\n",
              "      <td id=\"T_4ea45_row45_col5\" class=\"data row45 col5\" >0.8484</td>\n",
              "      <td id=\"T_4ea45_row45_col6\" class=\"data row45 col6\" >0.9382</td>\n",
              "      <td id=\"T_4ea45_row45_col7\" class=\"data row45 col7\" >0.9079</td>\n",
              "      <td id=\"T_4ea45_row45_col8\" class=\"data row45 col8\" >0.6940</td>\n",
              "      <td id=\"T_4ea45_row45_col9\" class=\"data row45 col9\" >0.5687</td>\n",
              "      <td id=\"T_4ea45_row45_col10\" class=\"data row45 col10\" >0.5904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row46\" class=\"row_heading level1 row46\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_4ea45_row46_col0\" class=\"data row46 col0\" >0.5791</td>\n",
              "      <td id=\"T_4ea45_row46_col1\" class=\"data row46 col1\" >0.7680</td>\n",
              "      <td id=\"T_4ea45_row46_col2\" class=\"data row46 col2\" >0.8940</td>\n",
              "      <td id=\"T_4ea45_row46_col3\" class=\"data row46 col3\" >0.8895</td>\n",
              "      <td id=\"T_4ea45_row46_col4\" class=\"data row46 col4\" >0.9074</td>\n",
              "      <td id=\"T_4ea45_row46_col5\" class=\"data row46 col5\" >0.6401</td>\n",
              "      <td id=\"T_4ea45_row46_col6\" class=\"data row46 col6\" >0.8748</td>\n",
              "      <td id=\"T_4ea45_row46_col7\" class=\"data row46 col7\" >0.7725</td>\n",
              "      <td id=\"T_4ea45_row46_col8\" class=\"data row46 col8\" >0.7341</td>\n",
              "      <td id=\"T_4ea45_row46_col9\" class=\"data row46 col9\" >0.6130</td>\n",
              "      <td id=\"T_4ea45_row46_col10\" class=\"data row46 col10\" >0.7199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row47\" class=\"row_heading level1 row47\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_4ea45_row47_col0\" class=\"data row47 col0\" >0.5757</td>\n",
              "      <td id=\"T_4ea45_row47_col1\" class=\"data row47 col1\" >0.7601</td>\n",
              "      <td id=\"T_4ea45_row47_col2\" class=\"data row47 col2\" >0.8132</td>\n",
              "      <td id=\"T_4ea45_row47_col3\" class=\"data row47 col3\" >0.8724</td>\n",
              "      <td id=\"T_4ea45_row47_col4\" class=\"data row47 col4\" >0.9946</td>\n",
              "      <td id=\"T_4ea45_row47_col5\" class=\"data row47 col5\" >0.7930</td>\n",
              "      <td id=\"T_4ea45_row47_col6\" class=\"data row47 col6\" >0.7339</td>\n",
              "      <td id=\"T_4ea45_row47_col7\" class=\"data row47 col7\" >0.7721</td>\n",
              "      <td id=\"T_4ea45_row47_col8\" class=\"data row47 col8\" >0.6125</td>\n",
              "      <td id=\"T_4ea45_row47_col9\" class=\"data row47 col9\" >0.3726</td>\n",
              "      <td id=\"T_4ea45_row47_col10\" class=\"data row47 col10\" >0.6949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row48\" class=\"row_heading level1 row48\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_4ea45_row48_col0\" class=\"data row48 col0\" >0.3899</td>\n",
              "      <td id=\"T_4ea45_row48_col1\" class=\"data row48 col1\" >0.9042</td>\n",
              "      <td id=\"T_4ea45_row48_col2\" class=\"data row48 col2\" >0.5268</td>\n",
              "      <td id=\"T_4ea45_row48_col3\" class=\"data row48 col3\" >0.9016</td>\n",
              "      <td id=\"T_4ea45_row48_col4\" class=\"data row48 col4\" >0.9856</td>\n",
              "      <td id=\"T_4ea45_row48_col5\" class=\"data row48 col5\" >0.8327</td>\n",
              "      <td id=\"T_4ea45_row48_col6\" class=\"data row48 col6\" >0.9482</td>\n",
              "      <td id=\"T_4ea45_row48_col7\" class=\"data row48 col7\" >0.8872</td>\n",
              "      <td id=\"T_4ea45_row48_col8\" class=\"data row48 col8\" >0.5981</td>\n",
              "      <td id=\"T_4ea45_row48_col9\" class=\"data row48 col9\" >0.3932</td>\n",
              "      <td id=\"T_4ea45_row48_col10\" class=\"data row48 col10\" >0.3987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level0_row49\" class=\"row_heading level0 row49\" rowspan=\"7\">vicuna-13b</th>\n",
              "      <th id=\"T_4ea45_level1_row49\" class=\"row_heading level1 row49\" >bert-base-multilingual-cased</th>\n",
              "      <td id=\"T_4ea45_row49_col0\" class=\"data row49 col0\" >0.7679</td>\n",
              "      <td id=\"T_4ea45_row49_col1\" class=\"data row49 col1\" >0.8821</td>\n",
              "      <td id=\"T_4ea45_row49_col2\" class=\"data row49 col2\" >0.8366</td>\n",
              "      <td id=\"T_4ea45_row49_col3\" class=\"data row49 col3\" >0.8410</td>\n",
              "      <td id=\"T_4ea45_row49_col4\" class=\"data row49 col4\" >0.9874</td>\n",
              "      <td id=\"T_4ea45_row49_col5\" class=\"data row49 col5\" >0.7861</td>\n",
              "      <td id=\"T_4ea45_row49_col6\" class=\"data row49 col6\" >0.9382</td>\n",
              "      <td id=\"T_4ea45_row49_col7\" class=\"data row49 col7\" >0.8218</td>\n",
              "      <td id=\"T_4ea45_row49_col8\" class=\"data row49 col8\" >0.8335</td>\n",
              "      <td id=\"T_4ea45_row49_col9\" class=\"data row49 col9\" >0.8378</td>\n",
              "      <td id=\"T_4ea45_row49_col10\" class=\"data row49 col10\" >0.6379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row50\" class=\"row_heading level1 row50\" >electra-large-discriminator</th>\n",
              "      <td id=\"T_4ea45_row50_col0\" class=\"data row50 col0\" >0.4056</td>\n",
              "      <td id=\"T_4ea45_row50_col1\" class=\"data row50 col1\" >0.5004</td>\n",
              "      <td id=\"T_4ea45_row50_col2\" class=\"data row50 col2\" >0.5494</td>\n",
              "      <td id=\"T_4ea45_row50_col3\" class=\"data row50 col3\" >0.5233</td>\n",
              "      <td id=\"T_4ea45_row50_col4\" class=\"data row50 col4\" >0.9838</td>\n",
              "      <td id=\"T_4ea45_row50_col5\" class=\"data row50 col5\" >0.7749</td>\n",
              "      <td id=\"T_4ea45_row50_col6\" class=\"data row50 col6\" >0.6136</td>\n",
              "      <td id=\"T_4ea45_row50_col7\" class=\"data row50 col7\" >0.6435</td>\n",
              "      <td id=\"T_4ea45_row50_col8\" class=\"data row50 col8\" >0.4744</td>\n",
              "      <td id=\"T_4ea45_row50_col9\" class=\"data row50 col9\" >0.4347</td>\n",
              "      <td id=\"T_4ea45_row50_col10\" class=\"data row50 col10\" >0.5352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row51\" class=\"row_heading level1 row51\" >gpt2-medium</th>\n",
              "      <td id=\"T_4ea45_row51_col0\" class=\"data row51 col0\" >0.5084</td>\n",
              "      <td id=\"T_4ea45_row51_col1\" class=\"data row51 col1\" >0.3728</td>\n",
              "      <td id=\"T_4ea45_row51_col2\" class=\"data row51 col2\" >0.4513</td>\n",
              "      <td id=\"T_4ea45_row51_col3\" class=\"data row51 col3\" >0.4422</td>\n",
              "      <td id=\"T_4ea45_row51_col4\" class=\"data row51 col4\" >0.9856</td>\n",
              "      <td id=\"T_4ea45_row51_col5\" class=\"data row51 col5\" >0.5856</td>\n",
              "      <td id=\"T_4ea45_row51_col6\" class=\"data row51 col6\" >0.3776</td>\n",
              "      <td id=\"T_4ea45_row51_col7\" class=\"data row51 col7\" >0.4527</td>\n",
              "      <td id=\"T_4ea45_row51_col8\" class=\"data row51 col8\" >0.4134</td>\n",
              "      <td id=\"T_4ea45_row51_col9\" class=\"data row51 col9\" >0.4440</td>\n",
              "      <td id=\"T_4ea45_row51_col10\" class=\"data row51 col10\" >0.3841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row52\" class=\"row_heading level1 row52\" >mGPT</th>\n",
              "      <td id=\"T_4ea45_row52_col0\" class=\"data row52 col0\" >0.3881</td>\n",
              "      <td id=\"T_4ea45_row52_col1\" class=\"data row52 col1\" >0.9079</td>\n",
              "      <td id=\"T_4ea45_row52_col2\" class=\"data row52 col2\" >0.4889</td>\n",
              "      <td id=\"T_4ea45_row52_col3\" class=\"data row52 col3\" >0.8254</td>\n",
              "      <td id=\"T_4ea45_row52_col4\" class=\"data row52 col4\" >0.9910</td>\n",
              "      <td id=\"T_4ea45_row52_col5\" class=\"data row52 col5\" >0.6963</td>\n",
              "      <td id=\"T_4ea45_row52_col6\" class=\"data row52 col6\" >0.9161</td>\n",
              "      <td id=\"T_4ea45_row52_col7\" class=\"data row52 col7\" >0.8043</td>\n",
              "      <td id=\"T_4ea45_row52_col8\" class=\"data row52 col8\" >0.6361</td>\n",
              "      <td id=\"T_4ea45_row52_col9\" class=\"data row52 col9\" >0.7215</td>\n",
              "      <td id=\"T_4ea45_row52_col10\" class=\"data row52 col10\" >0.6778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row53\" class=\"row_heading level1 row53\" >mdeberta-v3-base</th>\n",
              "      <td id=\"T_4ea45_row53_col0\" class=\"data row53 col0\" >0.2578</td>\n",
              "      <td id=\"T_4ea45_row53_col1\" class=\"data row53 col1\" >0.7861</td>\n",
              "      <td id=\"T_4ea45_row53_col2\" class=\"data row53 col2\" >0.6983</td>\n",
              "      <td id=\"T_4ea45_row53_col3\" class=\"data row53 col3\" >0.8225</td>\n",
              "      <td id=\"T_4ea45_row53_col4\" class=\"data row53 col4\" >0.7928</td>\n",
              "      <td id=\"T_4ea45_row53_col5\" class=\"data row53 col5\" >0.6142</td>\n",
              "      <td id=\"T_4ea45_row53_col6\" class=\"data row53 col6\" >0.8809</td>\n",
              "      <td id=\"T_4ea45_row53_col7\" class=\"data row53 col7\" >0.7064</td>\n",
              "      <td id=\"T_4ea45_row53_col8\" class=\"data row53 col8\" >0.6031</td>\n",
              "      <td id=\"T_4ea45_row53_col9\" class=\"data row53 col9\" >0.6765</td>\n",
              "      <td id=\"T_4ea45_row53_col10\" class=\"data row53 col10\" >0.5488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row54\" class=\"row_heading level1 row54\" >roberta-large-openai-detector</th>\n",
              "      <td id=\"T_4ea45_row54_col0\" class=\"data row54 col0\" >0.3330</td>\n",
              "      <td id=\"T_4ea45_row54_col1\" class=\"data row54 col1\" >0.4906</td>\n",
              "      <td id=\"T_4ea45_row54_col2\" class=\"data row54 col2\" >0.4113</td>\n",
              "      <td id=\"T_4ea45_row54_col3\" class=\"data row54 col3\" >0.4794</td>\n",
              "      <td id=\"T_4ea45_row54_col4\" class=\"data row54 col4\" >0.9511</td>\n",
              "      <td id=\"T_4ea45_row54_col5\" class=\"data row54 col5\" >0.4425</td>\n",
              "      <td id=\"T_4ea45_row54_col6\" class=\"data row54 col6\" >0.6971</td>\n",
              "      <td id=\"T_4ea45_row54_col7\" class=\"data row54 col7\" >0.3821</td>\n",
              "      <td id=\"T_4ea45_row54_col8\" class=\"data row54 col8\" >0.3326</td>\n",
              "      <td id=\"T_4ea45_row54_col9\" class=\"data row54 col9\" >0.3341</td>\n",
              "      <td id=\"T_4ea45_row54_col10\" class=\"data row54 col10\" >0.3488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4ea45_level1_row55\" class=\"row_heading level1 row55\" >xlm-roberta-large</th>\n",
              "      <td id=\"T_4ea45_row55_col0\" class=\"data row55 col0\" >0.4062</td>\n",
              "      <td id=\"T_4ea45_row55_col1\" class=\"data row55 col1\" >0.9009</td>\n",
              "      <td id=\"T_4ea45_row55_col2\" class=\"data row55 col2\" >0.7648</td>\n",
              "      <td id=\"T_4ea45_row55_col3\" class=\"data row55 col3\" >0.9324</td>\n",
              "      <td id=\"T_4ea45_row55_col4\" class=\"data row55 col4\" >0.9892</td>\n",
              "      <td id=\"T_4ea45_row55_col5\" class=\"data row55 col5\" >0.6761</td>\n",
              "      <td id=\"T_4ea45_row55_col6\" class=\"data row55 col6\" >0.8613</td>\n",
              "      <td id=\"T_4ea45_row55_col7\" class=\"data row55 col7\" >0.7444</td>\n",
              "      <td id=\"T_4ea45_row55_col8\" class=\"data row55 col8\" >0.7618</td>\n",
              "      <td id=\"T_4ea45_row55_col9\" class=\"data row55 col9\" >0.7866</td>\n",
              "      <td id=\"T_4ea45_row55_col10\" class=\"data row55 col10\" >0.7595</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{llrrrrrrrrrrr}\n",
            " &  & \\bfseries ar & \\bfseries ca & \\bfseries cs & \\bfseries de & \\bfseries en & \\bfseries es & \\bfseries nl & \\bfseries pt & \\bfseries ru & \\bfseries uk & \\bfseries zh \\\\\n",
            "Train LLM & Model &  &  &  &  &  &  &  &  &  &  &  \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries alpaca-lora-30b} & \\bfseries bert-base-multilingual-cased & 0.4951 & 0.8743 & 0.8600 & 0.9053 & \\bfseries 0.9838 & 0.7617 & 0.9079 & 0.8579 & 0.8461 & 0.8144 & 0.5719 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.3770 & 0.4539 & 0.4389 & 0.3827 & \\bfseries 0.9892 & 0.5344 & 0.4379 & 0.4684 & 0.3363 & 0.3333 & 0.4711 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.3656 & 0.4420 & 0.4544 & 0.4038 & \\bfseries 0.9928 & 0.4063 & 0.4297 & 0.3814 & 0.3854 & 0.4195 & 0.4141 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.2930 & 0.7623 & 0.4958 & 0.8850 & \\bfseries 0.9801 & 0.7360 & 0.8594 & 0.8655 & 0.8209 & 0.8043 & 0.4268 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.1764 & 0.6872 & 0.8096 & 0.8529 & \\bfseries 0.9385 & 0.6356 & 0.7888 & 0.7340 & 0.7332 & 0.6824 & 0.4177 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.3448 & 0.4963 & 0.4613 & 0.4216 & \\bfseries 0.9711 & 0.3945 & 0.6572 & 0.3721 & 0.3571 & 0.4340 & 0.4346 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.3306 & 0.8378 & 0.8782 & 0.9425 & \\bfseries 0.9874 & 0.6985 & 0.9007 & 0.7872 & 0.8294 & 0.8524 & 0.5984 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries gpt-3.5-turbo} & \\bfseries bert-base-multilingual-cased & 0.9449 & 0.9178 & 0.9383 & 0.9257 & \\bfseries 0.9819 & 0.8718 & 0.9499 & 0.9298 & 0.9100 & 0.8946 & 0.9083 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.4179 & 0.4001 & 0.3407 & 0.3524 & \\bfseries 0.9856 & 0.6747 & 0.4557 & 0.5561 & 0.4058 & 0.3972 & 0.3132 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.3690 & 0.3407 & 0.3370 & 0.3450 & \\bfseries 0.9910 & 0.3301 & 0.3432 & 0.3313 & 0.3658 & 0.3539 & 0.3333 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.6786 & 0.9083 & 0.3333 & 0.9049 & \\bfseries 0.9874 & 0.8874 & 0.9633 & 0.9284 & 0.9232 & 0.8860 & 0.5494 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.8148 & 0.7353 & 0.8158 & 0.8783 & \\bfseries 0.9258 & 0.7020 & 0.8267 & 0.7834 & 0.6769 & 0.7348 & 0.7295 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.6747 & 0.9182 & 0.8488 & 0.8416 & \\bfseries 0.9693 & 0.7811 & 0.8637 & 0.7284 & 0.6661 & 0.3600 & 0.5888 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.9199 & 0.9800 & 0.9750 & 0.9408 & \\bfseries 0.9838 & 0.9192 & 0.9549 & 0.9385 & 0.9517 & 0.9464 & 0.7963 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries gpt-4} & \\bfseries bert-base-multilingual-cased & 0.8350 & 0.9009 & 0.9200 & 0.9307 & \\bfseries 0.9964 & 0.7323 & 0.9382 & 0.8389 & 0.8766 & 0.8662 & 0.8079 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.4747 & 0.6222 & 0.3363 & 0.4691 & \\bfseries 0.9910 & 0.8320 & 0.5694 & 0.7838 & 0.3804 & 0.3378 & 0.3370 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.3759 & 0.3516 & 0.3370 & 0.3413 & \\bfseries 0.9982 & 0.3553 & 0.3540 & 0.3565 & 0.3725 & 0.3468 & 0.3407 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.8481 & 0.9299 & 0.3333 & 0.9104 & \\bfseries 0.9928 & 0.7951 & 0.9314 & 0.9072 & 0.8798 & 0.8067 & 0.6570 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.8199 & 0.7249 & \\bfseries 0.9076 & 0.8679 & 0.6291 & 0.6506 & 0.8023 & 0.7064 & 0.7794 & 0.8271 & 0.6940 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.5115 & 0.6923 & 0.5428 & 0.6090 & \\bfseries 0.9892 & 0.4870 & 0.7771 & 0.4381 & 0.4911 & 0.4030 & 0.6774 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.8780 & 0.9093 & 0.9566 & 0.9645 & \\bfseries 0.9946 & 0.8286 & 0.9248 & 0.8808 & 0.9066 & 0.9298 & 0.8614 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries llama-65b} & \\bfseries bert-base-multilingual-cased & 0.6552 & 0.6417 & 0.7971 & 0.7019 & \\bfseries 0.9219 & 0.8057 & 0.6370 & 0.6880 & 0.7016 & 0.8031 & 0.5659 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.3311 & 0.3363 & 0.3336 & 0.3536 & \\bfseries 0.9365 & 0.3537 & 0.3330 & 0.3395 & 0.3311 & 0.3326 & 0.3318 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.3472 & 0.3979 & 0.5536 & 0.4564 & \\bfseries 0.9384 & 0.5798 & 0.3912 & 0.4192 & 0.3844 & 0.3912 & 0.4145 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.3369 & 0.3616 & 0.7761 & 0.7157 & \\bfseries 0.9147 & 0.5517 & 0.4591 & 0.5616 & 0.4846 & 0.5137 & 0.5556 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.7477 & 0.6646 & \\bfseries 0.9272 & 0.7560 & 0.8959 & 0.8662 & 0.6821 & 0.7176 & 0.7734 & 0.8533 & 0.5902 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.3311 & 0.3607 & 0.3545 & 0.3408 & \\bfseries 0.9457 & 0.3460 & 0.3761 & 0.3395 & 0.3355 & 0.3529 & 0.3325 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.4532 & 0.4074 & 0.8414 & 0.7198 & \\bfseries 0.9384 & 0.6397 & 0.4080 & 0.5535 & 0.6371 & 0.6560 & 0.4488 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries opt-66b} & \\bfseries bert-base-multilingual-cased & 0.4121 & 0.5241 & 0.6850 & 0.6789 & \\bfseries 0.9079 & 0.5558 & 0.5274 & 0.6377 & 0.6104 & 0.6477 & 0.5323 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.3969 & 0.6936 & 0.6463 & 0.6972 & \\bfseries 0.9314 & 0.6785 & 0.6637 & 0.7006 & 0.3390 & 0.3303 & 0.3311 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.4448 & 0.4992 & 0.4264 & 0.5941 & \\bfseries 0.8898 & 0.5432 & 0.4274 & 0.6423 & 0.4870 & 0.5507 & 0.4764 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.1983 & 0.5890 & 0.5708 & 0.6564 & \\bfseries 0.9185 & 0.5605 & 0.5826 & 0.6449 & 0.4463 & 0.4981 & 0.4464 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.2612 & 0.4316 & 0.5316 & 0.6257 & \\bfseries 0.7720 & 0.4713 & 0.4184 & 0.3897 & 0.5503 & 0.4716 & 0.5307 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.3322 & 0.6773 & 0.4695 & 0.4529 & \\bfseries 0.9332 & 0.4155 & 0.6678 & 0.4105 & 0.3346 & 0.3886 & 0.3376 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.4182 & 0.4089 & 0.6021 & 0.6750 & \\bfseries 0.9205 & 0.5871 & 0.4205 & 0.5935 & 0.5884 & 0.6220 & 0.5175 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries opt-iml-max-1.3b} & \\bfseries bert-base-multilingual-cased & 0.5705 & 0.6372 & 0.7086 & 0.7003 & \\bfseries 0.9549 & 0.6327 & 0.6705 & 0.6447 & 0.5501 & 0.6380 & 0.5188 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.3280 & 0.6391 & 0.6211 & 0.7352 & \\bfseries 0.9964 & 0.8875 & 0.8502 & 0.8454 & 0.3280 & 0.3275 & 0.3292 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.3740 & 0.8301 & 0.5994 & 0.6433 & \\bfseries 0.9801 & 0.7471 & 0.6005 & 0.6545 & 0.3770 & 0.3929 & 0.4977 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.2607 & 0.5663 & 0.5731 & 0.6369 & \\bfseries 0.9729 & 0.5881 & 0.5901 & 0.4952 & 0.2563 & 0.2537 & 0.4156 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.7309 & 0.8618 & 0.9212 & 0.8152 & \\bfseries 0.9729 & 0.7726 & 0.8558 & 0.7391 & 0.7542 & 0.8477 & 0.7328 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.3280 & 0.8886 & 0.7955 & 0.6782 & \\bfseries 0.9856 & 0.7670 & 0.9075 & 0.6858 & 0.3429 & 0.4533 & 0.3737 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.6694 & 0.7393 & 0.7423 & 0.8130 & \\bfseries 0.9711 & 0.7553 & 0.6195 & 0.6933 & 0.5689 & 0.5977 & 0.6165 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries text-davinci-003} & \\bfseries bert-base-multilingual-cased & 0.7853 & 0.9064 & 0.8815 & 0.8951 & \\bfseries 0.9693 & 0.7987 & 0.9115 & 0.8632 & 0.7773 & 0.7333 & 0.8883 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.3985 & 0.3333 & 0.3333 & 0.3303 & \\bfseries 0.9910 & 0.3309 & 0.3330 & 0.3284 & 0.3741 & 0.4089 & 0.3333 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.3330 & 0.3333 & 0.3333 & 0.3303 & \\bfseries 0.9892 & 0.3264 & 0.3330 & 0.3276 & 0.3283 & 0.3334 & 0.3326 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.6136 & 0.8932 & 0.3333 & 0.8701 & \\bfseries 0.9892 & 0.8484 & 0.9382 & 0.9079 & 0.6940 & 0.5687 & 0.5904 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.5791 & 0.7680 & 0.8940 & 0.8895 & \\bfseries 0.9074 & 0.6401 & 0.8748 & 0.7725 & 0.7341 & 0.6130 & 0.7199 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.5757 & 0.7601 & 0.8132 & 0.8724 & \\bfseries 0.9946 & 0.7930 & 0.7339 & 0.7721 & 0.6125 & 0.3726 & 0.6949 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.3899 & 0.9042 & 0.5268 & 0.9016 & \\bfseries 0.9856 & 0.8327 & 0.9482 & 0.8872 & 0.5981 & 0.3932 & 0.3987 \\\\\n",
            "\\multirow[c]{7}{*}{\\bfseries vicuna-13b} & \\bfseries bert-base-multilingual-cased & 0.7679 & 0.8821 & 0.8366 & 0.8410 & \\bfseries 0.9874 & 0.7861 & 0.9382 & 0.8218 & 0.8335 & 0.8378 & 0.6379 \\\\\n",
            "\\bfseries  & \\bfseries electra-large-discriminator & 0.4056 & 0.5004 & 0.5494 & 0.5233 & \\bfseries 0.9838 & 0.7749 & 0.6136 & 0.6435 & 0.4744 & 0.4347 & 0.5352 \\\\\n",
            "\\bfseries  & \\bfseries gpt2-medium & 0.5084 & 0.3728 & 0.4513 & 0.4422 & \\bfseries 0.9856 & 0.5856 & 0.3776 & 0.4527 & 0.4134 & 0.4440 & 0.3841 \\\\\n",
            "\\bfseries  & \\bfseries mGPT & 0.3881 & 0.9079 & 0.4889 & 0.8254 & \\bfseries 0.9910 & 0.6963 & 0.9161 & 0.8043 & 0.6361 & 0.7215 & 0.6778 \\\\\n",
            "\\bfseries  & \\bfseries mdeberta-v3-base & 0.2578 & 0.7861 & 0.6983 & 0.8225 & 0.7928 & 0.6142 & \\bfseries 0.8809 & 0.7064 & 0.6031 & 0.6765 & 0.5488 \\\\\n",
            "\\bfseries  & \\bfseries roberta-large-openai-detector & 0.3330 & 0.4906 & 0.4113 & 0.4794 & \\bfseries 0.9511 & 0.4425 & 0.6971 & 0.3821 & 0.3326 & 0.3341 & 0.3488 \\\\\n",
            "\\bfseries  & \\bfseries xlm-roberta-large & 0.4062 & 0.9009 & 0.7648 & 0.9324 & \\bfseries 0.9892 & 0.6761 & 0.8613 & 0.7444 & 0.7618 & 0.7866 & 0.7595 \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd004d608b0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_2fbb6_row0_col0 {\n",
              "  background-color: #d1d2e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row0_col1 {\n",
              "  background-color: #b5c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row0_col2 {\n",
              "  background-color: #bbc7e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row0_col3, #T_2fbb6_row0_col6 {\n",
              "  background-color: #b1c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row0_col4 {\n",
              "  background-color: #7eadd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row0_col5 {\n",
              "  background-color: #b8c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row0_col7 {\n",
              "  background-color: #b7c5df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row0_col8, #T_2fbb6_row0_col9 {\n",
              "  background-color: #c4cbe3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row0_col10 {\n",
              "  background-color: #cccfe5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row1_col0, #T_2fbb6_row2_col5 {\n",
              "  background-color: #c6cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row1_col1 {\n",
              "  background-color: #a7bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row1_col2 {\n",
              "  background-color: #a9bfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row1_col3 {\n",
              "  background-color: #97b7d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row1_col4 {\n",
              "  background-color: #80aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row1_col5, #T_2fbb6_row1_col9 {\n",
              "  background-color: #acc0dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row1_col6 {\n",
              "  background-color: #a1bbda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row1_col7 {\n",
              "  background-color: #a5bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row1_col8 {\n",
              "  background-color: #adc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row1_col10 {\n",
              "  background-color: #bdc8e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row2_col0 {\n",
              "  background-color: #dbdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row2_col1 {\n",
              "  background-color: #cacee5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row2_col2 {\n",
              "  background-color: #d2d2e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row2_col3 {\n",
              "  background-color: #d0d1e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row2_col4 {\n",
              "  background-color: #79abd0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row2_col6 {\n",
              "  background-color: #c8cde4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row2_col7 {\n",
              "  background-color: #ced0e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row2_col8 {\n",
              "  background-color: #dcdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row2_col9 {\n",
              "  background-color: #dddbec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2fbb6_row2_col10 {\n",
              "  background-color: #d9d8ea;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_2fbb6\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_2fbb6_level0_col0\" class=\"col_heading level0 col0\" >ar</th>\n",
              "      <th id=\"T_2fbb6_level0_col1\" class=\"col_heading level0 col1\" >ca</th>\n",
              "      <th id=\"T_2fbb6_level0_col2\" class=\"col_heading level0 col2\" >cs</th>\n",
              "      <th id=\"T_2fbb6_level0_col3\" class=\"col_heading level0 col3\" >de</th>\n",
              "      <th id=\"T_2fbb6_level0_col4\" class=\"col_heading level0 col4\" >en</th>\n",
              "      <th id=\"T_2fbb6_level0_col5\" class=\"col_heading level0 col5\" >es</th>\n",
              "      <th id=\"T_2fbb6_level0_col6\" class=\"col_heading level0 col6\" >nl</th>\n",
              "      <th id=\"T_2fbb6_level0_col7\" class=\"col_heading level0 col7\" >pt</th>\n",
              "      <th id=\"T_2fbb6_level0_col8\" class=\"col_heading level0 col8\" >ru</th>\n",
              "      <th id=\"T_2fbb6_level0_col9\" class=\"col_heading level0 col9\" >uk</th>\n",
              "      <th id=\"T_2fbb6_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_2fbb6_level0_row0\" class=\"row_heading level0 row0\" >All Detectors Mean</th>\n",
              "      <td id=\"T_2fbb6_row0_col0\" class=\"data row0 col0\" >0.4931</td>\n",
              "      <td id=\"T_2fbb6_row0_col1\" class=\"data row0 col1\" >0.6568</td>\n",
              "      <td id=\"T_2fbb6_row0_col2\" class=\"data row0 col2\" >0.6270</td>\n",
              "      <td id=\"T_2fbb6_row0_col3\" class=\"data row0 col3\" >0.6871</td>\n",
              "      <td id=\"T_2fbb6_row0_col4\" class=\"data row0 col4\" >0.9529</td>\n",
              "      <td id=\"T_2fbb6_row0_col5\" class=\"data row0 col5\" >0.6476</td>\n",
              "      <td id=\"T_2fbb6_row0_col6\" class=\"data row0 col6\" >0.6800</td>\n",
              "      <td id=\"T_2fbb6_row0_col7\" class=\"data row0 col7\" >0.6497</td>\n",
              "      <td id=\"T_2fbb6_row0_col8\" class=\"data row0 col8\" >0.5759</td>\n",
              "      <td id=\"T_2fbb6_row0_col9\" class=\"data row0 col9\" >0.5716</td>\n",
              "      <td id=\"T_2fbb6_row0_col10\" class=\"data row0 col10\" >0.5299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2fbb6_level0_row1\" class=\"row_heading level0 row1\" >Multilingual Base Models Mean</th>\n",
              "      <td id=\"T_2fbb6_row1_col0\" class=\"data row1 col0\" >0.5605</td>\n",
              "      <td id=\"T_2fbb6_row1_col1\" class=\"data row1 col1\" >0.7484</td>\n",
              "      <td id=\"T_2fbb6_row1_col2\" class=\"data row1 col2\" >0.7289</td>\n",
              "      <td id=\"T_2fbb6_row1_col3\" class=\"data row1 col3\" >0.8244</td>\n",
              "      <td id=\"T_2fbb6_row1_col4\" class=\"data row1 col4\" >0.9392</td>\n",
              "      <td id=\"T_2fbb6_row1_col5\" class=\"data row1 col5\" >0.7156</td>\n",
              "      <td id=\"T_2fbb6_row1_col6\" class=\"data row1 col6\" >0.7778</td>\n",
              "      <td id=\"T_2fbb6_row1_col7\" class=\"data row1 col7\" >0.7508</td>\n",
              "      <td id=\"T_2fbb6_row1_col8\" class=\"data row1 col8\" >0.7092</td>\n",
              "      <td id=\"T_2fbb6_row1_col9\" class=\"data row1 col9\" >0.7118</td>\n",
              "      <td id=\"T_2fbb6_row1_col10\" class=\"data row1 col10\" >0.6160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2fbb6_level0_row2\" class=\"row_heading level0 row2\" >Monolingual Base Models Mean</th>\n",
              "      <td id=\"T_2fbb6_row2_col0\" class=\"data row2 col0\" >0.4033</td>\n",
              "      <td id=\"T_2fbb6_row2_col1\" class=\"data row2 col1\" >0.5346</td>\n",
              "      <td id=\"T_2fbb6_row2_col2\" class=\"data row2 col2\" >0.4912</td>\n",
              "      <td id=\"T_2fbb6_row2_col3\" class=\"data row2 col3\" >0.5040</td>\n",
              "      <td id=\"T_2fbb6_row2_col4\" class=\"data row2 col4\" >0.9712</td>\n",
              "      <td id=\"T_2fbb6_row2_col5\" class=\"data row2 col5\" >0.5570</td>\n",
              "      <td id=\"T_2fbb6_row2_col6\" class=\"data row2 col6\" >0.5497</td>\n",
              "      <td id=\"T_2fbb6_row2_col7\" class=\"data row2 col7\" >0.5150</td>\n",
              "      <td id=\"T_2fbb6_row2_col8\" class=\"data row2 col8\" >0.3981</td>\n",
              "      <td id=\"T_2fbb6_row2_col9\" class=\"data row2 col9\" >0.3847</td>\n",
              "      <td id=\"T_2fbb6_row2_col10\" class=\"data row2 col10\" >0.4152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{lrrrrrrrrrrr}\n",
            " & \\bfseries ar & \\bfseries ca & \\bfseries cs & \\bfseries de & \\bfseries en & \\bfseries es & \\bfseries nl & \\bfseries pt & \\bfseries ru & \\bfseries uk & \\bfseries zh \\\\\n",
            "\\multicolumn{2}{r|}{\\bfseries All Detectors Mean} & {\\cellcolor[HTML]{D1D2E6}} \\textcolor{black}{0.4931} & {\\cellcolor[HTML]{B5C4DF}} \\textcolor{black}{0.6568} & {\\cellcolor[HTML]{BBC7E0}} \\textcolor{black}{0.6270} & {\\cellcolor[HTML]{B1C2DE}} \\textcolor{black}{0.6871} & {\\cellcolor[HTML]{7EADD1}} \\textcolor{black}{0.9529} & {\\cellcolor[HTML]{B8C6E0}} \\textcolor{black}{0.6476} & {\\cellcolor[HTML]{B1C2DE}} \\textcolor{black}{0.6800} & {\\cellcolor[HTML]{B7C5DF}} \\textcolor{black}{0.6497} & {\\cellcolor[HTML]{C4CBE3}} \\textcolor{black}{0.5759} & {\\cellcolor[HTML]{C4CBE3}} \\textcolor{black}{0.5716} & {\\cellcolor[HTML]{CCCFE5}} \\textcolor{black}{0.5299} \\\\\n",
            "\\multicolumn{2}{r|}{\\bfseries Multilingual Base Models Mean} & {\\cellcolor[HTML]{C6CCE3}} \\textcolor{black}{0.5605} & {\\cellcolor[HTML]{A7BDDB}} \\textcolor{black}{0.7484} & {\\cellcolor[HTML]{A9BFDC}} \\textcolor{black}{0.7289} & {\\cellcolor[HTML]{97B7D7}} \\textcolor{black}{0.8244} & {\\cellcolor[HTML]{80AED2}} \\textcolor{black}{0.9392} & {\\cellcolor[HTML]{ACC0DD}} \\textcolor{black}{0.7156} & {\\cellcolor[HTML]{A1BBDA}} \\textcolor{black}{0.7778} & {\\cellcolor[HTML]{A5BDDB}} \\textcolor{black}{0.7508} & {\\cellcolor[HTML]{ADC1DD}} \\textcolor{black}{0.7092} & {\\cellcolor[HTML]{ACC0DD}} \\textcolor{black}{0.7118} & {\\cellcolor[HTML]{BDC8E1}} \\textcolor{black}{0.6160} \\\\\n",
            "\\multicolumn{2}{r|}{\\bfseries Monolingual Base Models Mean} & {\\cellcolor[HTML]{DBDAEB}} \\textcolor{black}{0.4033} & {\\cellcolor[HTML]{CACEE5}} \\textcolor{black}{0.5346} & {\\cellcolor[HTML]{D2D2E7}} \\textcolor{black}{0.4912} & {\\cellcolor[HTML]{D0D1E6}} \\textcolor{black}{0.5040} & {\\cellcolor[HTML]{79ABD0}} \\textcolor{black}{0.9712} & {\\cellcolor[HTML]{C6CCE3}} \\textcolor{black}{0.5570} & {\\cellcolor[HTML]{C8CDE4}} \\textcolor{black}{0.5497} & {\\cellcolor[HTML]{CED0E6}} \\textcolor{black}{0.5150} & {\\cellcolor[HTML]{DCDAEB}} \\textcolor{black}{0.3981} & {\\cellcolor[HTML]{DDDBEC}} \\textcolor{black}{0.3847} & {\\cellcolor[HTML]{D9D8EA}} \\textcolor{black}{0.4152} \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistics"
      ],
      "metadata": {
        "id": "0XhIGPSPPbp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "import itertools\n",
        "from statsmodels.stats.anova import AnovaRM"
      ],
      "metadata": {
        "id": "lCJnFbpYPd2h"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#in the submitted paper (correlation also from all and en3)\n",
        "results_all_multilingual[sorted_languages].corr().style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "KR7Q35iKPd40",
        "outputId": "6d5aff73-c571-4bb7-ea05-824ca7dffdf5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001ab6ce0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_a9934_row0_col0, #T_a9934_row1_col1, #T_a9934_row2_col2, #T_a9934_row3_col3, #T_a9934_row4_col4, #T_a9934_row5_col5, #T_a9934_row6_col6, #T_a9934_row7_col7, #T_a9934_row8_col8, #T_a9934_row9_col9, #T_a9934_row10_col10 {\n",
              "  background-color: #73a9cf;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row0_col1, #T_a9934_row0_col2, #T_a9934_row1_col0, #T_a9934_row2_col0, #T_a9934_row8_col10, #T_a9934_row10_col8 {\n",
              "  background-color: #cccfe5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row0_col3, #T_a9934_row3_col0, #T_a9934_row6_col7, #T_a9934_row7_col6 {\n",
              "  background-color: #dedcec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row0_col4, #T_a9934_row4_col0 {\n",
              "  background-color: #d1d2e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row0_col5, #T_a9934_row3_col7, #T_a9934_row5_col0, #T_a9934_row7_col3 {\n",
              "  background-color: #d9d8ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row0_col6, #T_a9934_row0_col7, #T_a9934_row0_col8, #T_a9934_row0_col9, #T_a9934_row6_col0, #T_a9934_row7_col0, #T_a9934_row8_col0, #T_a9934_row9_col0 {\n",
              "  background-color: #fff7fb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row0_col10, #T_a9934_row10_col0 {\n",
              "  background-color: #fcf4fa;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row1_col2, #T_a9934_row2_col1 {\n",
              "  background-color: #b3c3de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row1_col3, #T_a9934_row3_col1, #T_a9934_row8_col9, #T_a9934_row9_col8 {\n",
              "  background-color: #a5bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row1_col4, #T_a9934_row4_col1 {\n",
              "  background-color: #99b8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row1_col5, #T_a9934_row5_col1 {\n",
              "  background-color: #afc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row1_col6, #T_a9934_row6_col1 {\n",
              "  background-color: #ede7f2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row1_col7, #T_a9934_row2_col7, #T_a9934_row7_col1, #T_a9934_row7_col2 {\n",
              "  background-color: #e0dded;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row1_col8, #T_a9934_row8_col1 {\n",
              "  background-color: #ece7f2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row1_col9, #T_a9934_row1_col10, #T_a9934_row4_col8, #T_a9934_row5_col8, #T_a9934_row8_col4, #T_a9934_row8_col5, #T_a9934_row9_col1, #T_a9934_row10_col1 {\n",
              "  background-color: #e7e3f0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row2_col3, #T_a9934_row3_col2 {\n",
              "  background-color: #c2cbe2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row2_col4, #T_a9934_row4_col2 {\n",
              "  background-color: #abbfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row2_col5, #T_a9934_row4_col5, #T_a9934_row5_col2, #T_a9934_row5_col4 {\n",
              "  background-color: #93b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row2_col6, #T_a9934_row6_col2 {\n",
              "  background-color: #f8f1f8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row2_col8, #T_a9934_row3_col6, #T_a9934_row6_col3, #T_a9934_row8_col2 {\n",
              "  background-color: #efe9f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row2_col9, #T_a9934_row4_col7, #T_a9934_row4_col9, #T_a9934_row7_col4, #T_a9934_row9_col2, #T_a9934_row9_col4 {\n",
              "  background-color: #dddbec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row2_col10, #T_a9934_row7_col10, #T_a9934_row10_col2, #T_a9934_row10_col7 {\n",
              "  background-color: #ced0e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row3_col4, #T_a9934_row4_col3 {\n",
              "  background-color: #83afd3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row3_col5, #T_a9934_row5_col3 {\n",
              "  background-color: #a7bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row3_col8, #T_a9934_row8_col3 {\n",
              "  background-color: #e0deed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row3_col9, #T_a9934_row9_col3 {\n",
              "  background-color: #d5d5e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row3_col10, #T_a9934_row10_col3 {\n",
              "  background-color: #e5e1ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row4_col6, #T_a9934_row6_col4 {\n",
              "  background-color: #f4edf6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row4_col10, #T_a9934_row10_col4 {\n",
              "  background-color: #e6e2ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row5_col6, #T_a9934_row6_col5 {\n",
              "  background-color: #f0eaf4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row5_col7, #T_a9934_row7_col5 {\n",
              "  background-color: #dbdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row5_col9, #T_a9934_row9_col5 {\n",
              "  background-color: #d8d7e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row5_col10, #T_a9934_row6_col9, #T_a9934_row9_col6, #T_a9934_row10_col5 {\n",
              "  background-color: #dad9ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row6_col8, #T_a9934_row8_col6 {\n",
              "  background-color: #d3d4e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row6_col10, #T_a9934_row10_col6 {\n",
              "  background-color: #d2d3e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row7_col8, #T_a9934_row8_col7 {\n",
              "  background-color: #89b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row7_col9, #T_a9934_row9_col7 {\n",
              "  background-color: #a4bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9934_row9_col10, #T_a9934_row10_col9 {\n",
              "  background-color: #acc0dd;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_a9934\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_a9934_level0_col0\" class=\"col_heading level0 col0\" >en</th>\n",
              "      <th id=\"T_a9934_level0_col1\" class=\"col_heading level0 col1\" >de</th>\n",
              "      <th id=\"T_a9934_level0_col2\" class=\"col_heading level0 col2\" >nl</th>\n",
              "      <th id=\"T_a9934_level0_col3\" class=\"col_heading level0 col3\" >es</th>\n",
              "      <th id=\"T_a9934_level0_col4\" class=\"col_heading level0 col4\" >pt</th>\n",
              "      <th id=\"T_a9934_level0_col5\" class=\"col_heading level0 col5\" >ca</th>\n",
              "      <th id=\"T_a9934_level0_col6\" class=\"col_heading level0 col6\" >cs</th>\n",
              "      <th id=\"T_a9934_level0_col7\" class=\"col_heading level0 col7\" >ru</th>\n",
              "      <th id=\"T_a9934_level0_col8\" class=\"col_heading level0 col8\" >uk</th>\n",
              "      <th id=\"T_a9934_level0_col9\" class=\"col_heading level0 col9\" >ar</th>\n",
              "      <th id=\"T_a9934_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_a9934_level0_row0\" class=\"row_heading level0 row0\" >en</th>\n",
              "      <td id=\"T_a9934_row0_col0\" class=\"data row0 col0\" >1.0000</td>\n",
              "      <td id=\"T_a9934_row0_col1\" class=\"data row0 col1\" >0.5248</td>\n",
              "      <td id=\"T_a9934_row0_col2\" class=\"data row0 col2\" >0.5249</td>\n",
              "      <td id=\"T_a9934_row0_col3\" class=\"data row0 col3\" >0.3774</td>\n",
              "      <td id=\"T_a9934_row0_col4\" class=\"data row0 col4\" >0.4949</td>\n",
              "      <td id=\"T_a9934_row0_col5\" class=\"data row0 col5\" >0.4240</td>\n",
              "      <td id=\"T_a9934_row0_col6\" class=\"data row0 col6\" >-0.0870</td>\n",
              "      <td id=\"T_a9934_row0_col7\" class=\"data row0 col7\" >-0.1903</td>\n",
              "      <td id=\"T_a9934_row0_col8\" class=\"data row0 col8\" >-0.3074</td>\n",
              "      <td id=\"T_a9934_row0_col9\" class=\"data row0 col9\" >-0.1778</td>\n",
              "      <td id=\"T_a9934_row0_col10\" class=\"data row0 col10\" >0.0412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9934_level0_row1\" class=\"row_heading level0 row1\" >de</th>\n",
              "      <td id=\"T_a9934_row1_col0\" class=\"data row1 col0\" >0.5248</td>\n",
              "      <td id=\"T_a9934_row1_col1\" class=\"data row1 col1\" >1.0000</td>\n",
              "      <td id=\"T_a9934_row1_col2\" class=\"data row1 col2\" >0.6765</td>\n",
              "      <td id=\"T_a9934_row1_col3\" class=\"data row1 col3\" >0.7511</td>\n",
              "      <td id=\"T_a9934_row1_col4\" class=\"data row1 col4\" >0.8161</td>\n",
              "      <td id=\"T_a9934_row1_col5\" class=\"data row1 col5\" >0.7022</td>\n",
              "      <td id=\"T_a9934_row1_col6\" class=\"data row1 col6\" >0.2435</td>\n",
              "      <td id=\"T_a9934_row1_col7\" class=\"data row1 col7\" >0.3652</td>\n",
              "      <td id=\"T_a9934_row1_col8\" class=\"data row1 col8\" >0.2556</td>\n",
              "      <td id=\"T_a9934_row1_col9\" class=\"data row1 col9\" >0.3026</td>\n",
              "      <td id=\"T_a9934_row1_col10\" class=\"data row1 col10\" >0.2963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9934_level0_row2\" class=\"row_heading level0 row2\" >nl</th>\n",
              "      <td id=\"T_a9934_row2_col0\" class=\"data row2 col0\" >0.5249</td>\n",
              "      <td id=\"T_a9934_row2_col1\" class=\"data row2 col1\" >0.6765</td>\n",
              "      <td id=\"T_a9934_row2_col2\" class=\"data row2 col2\" >1.0000</td>\n",
              "      <td id=\"T_a9934_row2_col3\" class=\"data row2 col3\" >0.5792</td>\n",
              "      <td id=\"T_a9934_row2_col4\" class=\"data row2 col4\" >0.7243</td>\n",
              "      <td id=\"T_a9934_row2_col5\" class=\"data row2 col5\" >0.8492</td>\n",
              "      <td id=\"T_a9934_row2_col6\" class=\"data row2 col6\" >0.0985</td>\n",
              "      <td id=\"T_a9934_row2_col7\" class=\"data row2 col7\" >0.3641</td>\n",
              "      <td id=\"T_a9934_row2_col8\" class=\"data row2 col8\" >0.2168</td>\n",
              "      <td id=\"T_a9934_row2_col9\" class=\"data row2 col9\" >0.3890</td>\n",
              "      <td id=\"T_a9934_row2_col10\" class=\"data row2 col10\" >0.5100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9934_level0_row3\" class=\"row_heading level0 row3\" >es</th>\n",
              "      <td id=\"T_a9934_row3_col0\" class=\"data row3 col0\" >0.3774</td>\n",
              "      <td id=\"T_a9934_row3_col1\" class=\"data row3 col1\" >0.7511</td>\n",
              "      <td id=\"T_a9934_row3_col2\" class=\"data row3 col2\" >0.5792</td>\n",
              "      <td id=\"T_a9934_row3_col3\" class=\"data row3 col3\" >1.0000</td>\n",
              "      <td id=\"T_a9934_row3_col4\" class=\"data row3 col4\" >0.9243</td>\n",
              "      <td id=\"T_a9934_row3_col5\" class=\"data row3 col5\" >0.7478</td>\n",
              "      <td id=\"T_a9934_row3_col6\" class=\"data row3 col6\" >0.2131</td>\n",
              "      <td id=\"T_a9934_row3_col7\" class=\"data row3 col7\" >0.4184</td>\n",
              "      <td id=\"T_a9934_row3_col8\" class=\"data row3 col8\" >0.3518</td>\n",
              "      <td id=\"T_a9934_row3_col9\" class=\"data row3 col9\" >0.4588</td>\n",
              "      <td id=\"T_a9934_row3_col10\" class=\"data row3 col10\" >0.3148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9934_level0_row4\" class=\"row_heading level0 row4\" >pt</th>\n",
              "      <td id=\"T_a9934_row4_col0\" class=\"data row4 col0\" >0.4949</td>\n",
              "      <td id=\"T_a9934_row4_col1\" class=\"data row4 col1\" >0.8161</td>\n",
              "      <td id=\"T_a9934_row4_col2\" class=\"data row4 col2\" >0.7243</td>\n",
              "      <td id=\"T_a9934_row4_col3\" class=\"data row4 col3\" >0.9243</td>\n",
              "      <td id=\"T_a9934_row4_col4\" class=\"data row4 col4\" >1.0000</td>\n",
              "      <td id=\"T_a9934_row4_col5\" class=\"data row4 col5\" >0.8457</td>\n",
              "      <td id=\"T_a9934_row4_col6\" class=\"data row4 col6\" >0.1508</td>\n",
              "      <td id=\"T_a9934_row4_col7\" class=\"data row4 col7\" >0.3831</td>\n",
              "      <td id=\"T_a9934_row4_col8\" class=\"data row4 col8\" >0.2955</td>\n",
              "      <td id=\"T_a9934_row4_col9\" class=\"data row4 col9\" >0.3898</td>\n",
              "      <td id=\"T_a9934_row4_col10\" class=\"data row4 col10\" >0.3068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9934_level0_row5\" class=\"row_heading level0 row5\" >ca</th>\n",
              "      <td id=\"T_a9934_row5_col0\" class=\"data row5 col0\" >0.4240</td>\n",
              "      <td id=\"T_a9934_row5_col1\" class=\"data row5 col1\" >0.7022</td>\n",
              "      <td id=\"T_a9934_row5_col2\" class=\"data row5 col2\" >0.8492</td>\n",
              "      <td id=\"T_a9934_row5_col3\" class=\"data row5 col3\" >0.7478</td>\n",
              "      <td id=\"T_a9934_row5_col4\" class=\"data row5 col4\" >0.8457</td>\n",
              "      <td id=\"T_a9934_row5_col5\" class=\"data row5 col5\" >1.0000</td>\n",
              "      <td id=\"T_a9934_row5_col6\" class=\"data row5 col6\" >0.2092</td>\n",
              "      <td id=\"T_a9934_row5_col7\" class=\"data row5 col7\" >0.4019</td>\n",
              "      <td id=\"T_a9934_row5_col8\" class=\"data row5 col8\" >0.2981</td>\n",
              "      <td id=\"T_a9934_row5_col9\" class=\"data row5 col9\" >0.4311</td>\n",
              "      <td id=\"T_a9934_row5_col10\" class=\"data row5 col10\" >0.4134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9934_level0_row6\" class=\"row_heading level0 row6\" >cs</th>\n",
              "      <td id=\"T_a9934_row6_col0\" class=\"data row6 col0\" >-0.0870</td>\n",
              "      <td id=\"T_a9934_row6_col1\" class=\"data row6 col1\" >0.2435</td>\n",
              "      <td id=\"T_a9934_row6_col2\" class=\"data row6 col2\" >0.0985</td>\n",
              "      <td id=\"T_a9934_row6_col3\" class=\"data row6 col3\" >0.2131</td>\n",
              "      <td id=\"T_a9934_row6_col4\" class=\"data row6 col4\" >0.1508</td>\n",
              "      <td id=\"T_a9934_row6_col5\" class=\"data row6 col5\" >0.2092</td>\n",
              "      <td id=\"T_a9934_row6_col6\" class=\"data row6 col6\" >1.0000</td>\n",
              "      <td id=\"T_a9934_row6_col7\" class=\"data row6 col7\" >0.3783</td>\n",
              "      <td id=\"T_a9934_row6_col8\" class=\"data row6 col8\" >0.4723</td>\n",
              "      <td id=\"T_a9934_row6_col9\" class=\"data row6 col9\" >0.4136</td>\n",
              "      <td id=\"T_a9934_row6_col10\" class=\"data row6 col10\" >0.4772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9934_level0_row7\" class=\"row_heading level0 row7\" >ru</th>\n",
              "      <td id=\"T_a9934_row7_col0\" class=\"data row7 col0\" >-0.1903</td>\n",
              "      <td id=\"T_a9934_row7_col1\" class=\"data row7 col1\" >0.3652</td>\n",
              "      <td id=\"T_a9934_row7_col2\" class=\"data row7 col2\" >0.3641</td>\n",
              "      <td id=\"T_a9934_row7_col3\" class=\"data row7 col3\" >0.4184</td>\n",
              "      <td id=\"T_a9934_row7_col4\" class=\"data row7 col4\" >0.3831</td>\n",
              "      <td id=\"T_a9934_row7_col5\" class=\"data row7 col5\" >0.4019</td>\n",
              "      <td id=\"T_a9934_row7_col6\" class=\"data row7 col6\" >0.3783</td>\n",
              "      <td id=\"T_a9934_row7_col7\" class=\"data row7 col7\" >1.0000</td>\n",
              "      <td id=\"T_a9934_row7_col8\" class=\"data row7 col8\" >0.8976</td>\n",
              "      <td id=\"T_a9934_row7_col9\" class=\"data row7 col9\" >0.7622</td>\n",
              "      <td id=\"T_a9934_row7_col10\" class=\"data row7 col10\" >0.5121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9934_level0_row8\" class=\"row_heading level0 row8\" >uk</th>\n",
              "      <td id=\"T_a9934_row8_col0\" class=\"data row8 col0\" >-0.3074</td>\n",
              "      <td id=\"T_a9934_row8_col1\" class=\"data row8 col1\" >0.2556</td>\n",
              "      <td id=\"T_a9934_row8_col2\" class=\"data row8 col2\" >0.2168</td>\n",
              "      <td id=\"T_a9934_row8_col3\" class=\"data row8 col3\" >0.3518</td>\n",
              "      <td id=\"T_a9934_row8_col4\" class=\"data row8 col4\" >0.2955</td>\n",
              "      <td id=\"T_a9934_row8_col5\" class=\"data row8 col5\" >0.2981</td>\n",
              "      <td id=\"T_a9934_row8_col6\" class=\"data row8 col6\" >0.4723</td>\n",
              "      <td id=\"T_a9934_row8_col7\" class=\"data row8 col7\" >0.8976</td>\n",
              "      <td id=\"T_a9934_row8_col8\" class=\"data row8 col8\" >1.0000</td>\n",
              "      <td id=\"T_a9934_row8_col9\" class=\"data row8 col9\" >0.7574</td>\n",
              "      <td id=\"T_a9934_row8_col10\" class=\"data row8 col10\" >0.5292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9934_level0_row9\" class=\"row_heading level0 row9\" >ar</th>\n",
              "      <td id=\"T_a9934_row9_col0\" class=\"data row9 col0\" >-0.1778</td>\n",
              "      <td id=\"T_a9934_row9_col1\" class=\"data row9 col1\" >0.3026</td>\n",
              "      <td id=\"T_a9934_row9_col2\" class=\"data row9 col2\" >0.3890</td>\n",
              "      <td id=\"T_a9934_row9_col3\" class=\"data row9 col3\" >0.4588</td>\n",
              "      <td id=\"T_a9934_row9_col4\" class=\"data row9 col4\" >0.3898</td>\n",
              "      <td id=\"T_a9934_row9_col5\" class=\"data row9 col5\" >0.4311</td>\n",
              "      <td id=\"T_a9934_row9_col6\" class=\"data row9 col6\" >0.4136</td>\n",
              "      <td id=\"T_a9934_row9_col7\" class=\"data row9 col7\" >0.7622</td>\n",
              "      <td id=\"T_a9934_row9_col8\" class=\"data row9 col8\" >0.7574</td>\n",
              "      <td id=\"T_a9934_row9_col9\" class=\"data row9 col9\" >1.0000</td>\n",
              "      <td id=\"T_a9934_row9_col10\" class=\"data row9 col10\" >0.7178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9934_level0_row10\" class=\"row_heading level0 row10\" >zh</th>\n",
              "      <td id=\"T_a9934_row10_col0\" class=\"data row10 col0\" >0.0412</td>\n",
              "      <td id=\"T_a9934_row10_col1\" class=\"data row10 col1\" >0.2963</td>\n",
              "      <td id=\"T_a9934_row10_col2\" class=\"data row10 col2\" >0.5100</td>\n",
              "      <td id=\"T_a9934_row10_col3\" class=\"data row10 col3\" >0.3148</td>\n",
              "      <td id=\"T_a9934_row10_col4\" class=\"data row10 col4\" >0.3068</td>\n",
              "      <td id=\"T_a9934_row10_col5\" class=\"data row10 col5\" >0.4134</td>\n",
              "      <td id=\"T_a9934_row10_col6\" class=\"data row10 col6\" >0.4772</td>\n",
              "      <td id=\"T_a9934_row10_col7\" class=\"data row10 col7\" >0.5121</td>\n",
              "      <td id=\"T_a9934_row10_col8\" class=\"data row10 col8\" >0.5292</td>\n",
              "      <td id=\"T_a9934_row10_col9\" class=\"data row10 col9\" >0.7178</td>\n",
              "      <td id=\"T_a9934_row10_col10\" class=\"data row10 col10\" >1.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#there are just small differences when only monolingual generalization correlated\n",
        "temp = results_all_multilingual.reset_index()\n",
        "temp[~temp['Train Language'].isin([\"en3\", \"all\"])][sorted_languages].corr().style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "zLDpMTyATpR6",
        "outputId": "ad8c1174-4689-4155-b962-1cc4e817bef7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001a45ff0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_425a5_row0_col0, #T_425a5_row1_col1, #T_425a5_row2_col2, #T_425a5_row3_col3, #T_425a5_row4_col4, #T_425a5_row5_col5, #T_425a5_row6_col6, #T_425a5_row7_col7, #T_425a5_row8_col8, #T_425a5_row9_col9, #T_425a5_row10_col10 {\n",
              "  background-color: #73a9cf;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row0_col1, #T_425a5_row1_col0 {\n",
              "  background-color: #c9cee4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row0_col2, #T_425a5_row2_col0, #T_425a5_row2_col3, #T_425a5_row3_col2 {\n",
              "  background-color: #c6cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row0_col3, #T_425a5_row3_col0 {\n",
              "  background-color: #dedcec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row0_col4, #T_425a5_row4_col0 {\n",
              "  background-color: #d1d2e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row0_col5, #T_425a5_row5_col0, #T_425a5_row6_col9, #T_425a5_row9_col6 {\n",
              "  background-color: #d9d8ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row0_col6, #T_425a5_row0_col7, #T_425a5_row0_col8, #T_425a5_row0_col9, #T_425a5_row0_col10, #T_425a5_row6_col0, #T_425a5_row7_col0, #T_425a5_row8_col0, #T_425a5_row9_col0, #T_425a5_row10_col0 {\n",
              "  background-color: #fff7fb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row1_col2, #T_425a5_row2_col1 {\n",
              "  background-color: #c0c9e2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row1_col3, #T_425a5_row3_col1 {\n",
              "  background-color: #a2bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row1_col4, #T_425a5_row4_col1 {\n",
              "  background-color: #9cb9d9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row1_col5, #T_425a5_row5_col1 {\n",
              "  background-color: #b7c5df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row1_col6, #T_425a5_row6_col1 {\n",
              "  background-color: #efe9f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row1_col7, #T_425a5_row1_col9, #T_425a5_row3_col10, #T_425a5_row5_col7, #T_425a5_row7_col1, #T_425a5_row7_col5, #T_425a5_row9_col1, #T_425a5_row10_col3 {\n",
              "  background-color: #f0eaf4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row1_col8, #T_425a5_row5_col8, #T_425a5_row8_col1, #T_425a5_row8_col5 {\n",
              "  background-color: #f9f2f8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row1_col10, #T_425a5_row3_col7, #T_425a5_row7_col3, #T_425a5_row10_col1 {\n",
              "  background-color: #f2ecf5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row2_col4, #T_425a5_row4_col2 {\n",
              "  background-color: #b0c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row2_col5, #T_425a5_row5_col2 {\n",
              "  background-color: #96b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row2_col6, #T_425a5_row6_col2 {\n",
              "  background-color: #faf3f9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row2_col7, #T_425a5_row7_col2 {\n",
              "  background-color: #ede8f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row2_col8, #T_425a5_row4_col8, #T_425a5_row8_col2, #T_425a5_row8_col4 {\n",
              "  background-color: #fbf4f9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row2_col9, #T_425a5_row9_col2 {\n",
              "  background-color: #e0deed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row2_col10, #T_425a5_row10_col2 {\n",
              "  background-color: #d3d4e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row3_col4, #T_425a5_row4_col3 {\n",
              "  background-color: #81aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row3_col5, #T_425a5_row5_col3 {\n",
              "  background-color: #a9bfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row3_col6, #T_425a5_row6_col3 {\n",
              "  background-color: #f3edf5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row3_col8, #T_425a5_row8_col3 {\n",
              "  background-color: #f6eff7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row3_col9, #T_425a5_row9_col3 {\n",
              "  background-color: #e7e3f0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row4_col5, #T_425a5_row5_col4 {\n",
              "  background-color: #97b7d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row4_col6, #T_425a5_row6_col4 {\n",
              "  background-color: #f8f1f8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row4_col7, #T_425a5_row7_col4 {\n",
              "  background-color: #f5eff6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row4_col9, #T_425a5_row9_col4 {\n",
              "  background-color: #ede7f2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row4_col10, #T_425a5_row10_col4 {\n",
              "  background-color: #f1ebf5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row5_col6, #T_425a5_row6_col5 {\n",
              "  background-color: #f4edf6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row5_col9, #T_425a5_row9_col5 {\n",
              "  background-color: #e3e0ee;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row5_col10, #T_425a5_row10_col5 {\n",
              "  background-color: #e5e1ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row6_col7, #T_425a5_row7_col6 {\n",
              "  background-color: #dfddec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row6_col8, #T_425a5_row6_col10, #T_425a5_row7_col10, #T_425a5_row8_col6, #T_425a5_row10_col6, #T_425a5_row10_col7 {\n",
              "  background-color: #d6d6e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row7_col8, #T_425a5_row8_col7 {\n",
              "  background-color: #8fb4d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row7_col9, #T_425a5_row8_col9, #T_425a5_row9_col7, #T_425a5_row9_col8 {\n",
              "  background-color: #a8bedc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row8_col10, #T_425a5_row10_col8 {\n",
              "  background-color: #d4d4e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_425a5_row9_col10, #T_425a5_row10_col9 {\n",
              "  background-color: #abbfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_425a5\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_425a5_level0_col0\" class=\"col_heading level0 col0\" >en</th>\n",
              "      <th id=\"T_425a5_level0_col1\" class=\"col_heading level0 col1\" >de</th>\n",
              "      <th id=\"T_425a5_level0_col2\" class=\"col_heading level0 col2\" >nl</th>\n",
              "      <th id=\"T_425a5_level0_col3\" class=\"col_heading level0 col3\" >es</th>\n",
              "      <th id=\"T_425a5_level0_col4\" class=\"col_heading level0 col4\" >pt</th>\n",
              "      <th id=\"T_425a5_level0_col5\" class=\"col_heading level0 col5\" >ca</th>\n",
              "      <th id=\"T_425a5_level0_col6\" class=\"col_heading level0 col6\" >cs</th>\n",
              "      <th id=\"T_425a5_level0_col7\" class=\"col_heading level0 col7\" >ru</th>\n",
              "      <th id=\"T_425a5_level0_col8\" class=\"col_heading level0 col8\" >uk</th>\n",
              "      <th id=\"T_425a5_level0_col9\" class=\"col_heading level0 col9\" >ar</th>\n",
              "      <th id=\"T_425a5_level0_col10\" class=\"col_heading level0 col10\" >zh</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_425a5_level0_row0\" class=\"row_heading level0 row0\" >en</th>\n",
              "      <td id=\"T_425a5_row0_col0\" class=\"data row0 col0\" >1.0000</td>\n",
              "      <td id=\"T_425a5_row0_col1\" class=\"data row0 col1\" >0.5420</td>\n",
              "      <td id=\"T_425a5_row0_col2\" class=\"data row0 col2\" >0.5551</td>\n",
              "      <td id=\"T_425a5_row0_col3\" class=\"data row0 col3\" >0.3794</td>\n",
              "      <td id=\"T_425a5_row0_col4\" class=\"data row0 col4\" >0.4960</td>\n",
              "      <td id=\"T_425a5_row0_col5\" class=\"data row0 col5\" >0.4237</td>\n",
              "      <td id=\"T_425a5_row0_col6\" class=\"data row0 col6\" >-0.1613</td>\n",
              "      <td id=\"T_425a5_row0_col7\" class=\"data row0 col7\" >-0.3235</td>\n",
              "      <td id=\"T_425a5_row0_col8\" class=\"data row0 col8\" >-0.4988</td>\n",
              "      <td id=\"T_425a5_row0_col9\" class=\"data row0 col9\" >-0.2672</td>\n",
              "      <td id=\"T_425a5_row0_col10\" class=\"data row0 col10\" >-0.0033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_425a5_level0_row1\" class=\"row_heading level0 row1\" >de</th>\n",
              "      <td id=\"T_425a5_row1_col0\" class=\"data row1 col0\" >0.5420</td>\n",
              "      <td id=\"T_425a5_row1_col1\" class=\"data row1 col1\" >1.0000</td>\n",
              "      <td id=\"T_425a5_row1_col2\" class=\"data row1 col2\" >0.6006</td>\n",
              "      <td id=\"T_425a5_row1_col3\" class=\"data row1 col3\" >0.7657</td>\n",
              "      <td id=\"T_425a5_row1_col4\" class=\"data row1 col4\" >0.8022</td>\n",
              "      <td id=\"T_425a5_row1_col5\" class=\"data row1 col5\" >0.6491</td>\n",
              "      <td id=\"T_425a5_row1_col6\" class=\"data row1 col6\" >0.2176</td>\n",
              "      <td id=\"T_425a5_row1_col7\" class=\"data row1 col7\" >0.1990</td>\n",
              "      <td id=\"T_425a5_row1_col8\" class=\"data row1 col8\" >0.0784</td>\n",
              "      <td id=\"T_425a5_row1_col9\" class=\"data row1 col9\" >0.1954</td>\n",
              "      <td id=\"T_425a5_row1_col10\" class=\"data row1 col10\" >0.1656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_425a5_level0_row2\" class=\"row_heading level0 row2\" >nl</th>\n",
              "      <td id=\"T_425a5_row2_col0\" class=\"data row2 col0\" >0.5551</td>\n",
              "      <td id=\"T_425a5_row2_col1\" class=\"data row2 col1\" >0.6006</td>\n",
              "      <td id=\"T_425a5_row2_col2\" class=\"data row2 col2\" >1.0000</td>\n",
              "      <td id=\"T_425a5_row2_col3\" class=\"data row2 col3\" >0.5585</td>\n",
              "      <td id=\"T_425a5_row2_col4\" class=\"data row2 col4\" >0.6905</td>\n",
              "      <td id=\"T_425a5_row2_col5\" class=\"data row2 col5\" >0.8342</td>\n",
              "      <td id=\"T_425a5_row2_col6\" class=\"data row2 col6\" >0.0626</td>\n",
              "      <td id=\"T_425a5_row2_col7\" class=\"data row2 col7\" >0.2403</td>\n",
              "      <td id=\"T_425a5_row2_col8\" class=\"data row2 col8\" >0.0503</td>\n",
              "      <td id=\"T_425a5_row2_col9\" class=\"data row2 col9\" >0.3516</td>\n",
              "      <td id=\"T_425a5_row2_col10\" class=\"data row2 col10\" >0.4694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_425a5_level0_row3\" class=\"row_heading level0 row3\" >es</th>\n",
              "      <td id=\"T_425a5_row3_col0\" class=\"data row3 col0\" >0.3794</td>\n",
              "      <td id=\"T_425a5_row3_col1\" class=\"data row3 col1\" >0.7657</td>\n",
              "      <td id=\"T_425a5_row3_col2\" class=\"data row3 col2\" >0.5585</td>\n",
              "      <td id=\"T_425a5_row3_col3\" class=\"data row3 col3\" >1.0000</td>\n",
              "      <td id=\"T_425a5_row3_col4\" class=\"data row3 col4\" >0.9317</td>\n",
              "      <td id=\"T_425a5_row3_col5\" class=\"data row3 col5\" >0.7331</td>\n",
              "      <td id=\"T_425a5_row3_col6\" class=\"data row3 col6\" >0.1570</td>\n",
              "      <td id=\"T_425a5_row3_col7\" class=\"data row3 col7\" >0.1774</td>\n",
              "      <td id=\"T_425a5_row3_col8\" class=\"data row3 col8\" >0.1224</td>\n",
              "      <td id=\"T_425a5_row3_col9\" class=\"data row3 col9\" >0.2989</td>\n",
              "      <td id=\"T_425a5_row3_col10\" class=\"data row3 col10\" >0.2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_425a5_level0_row4\" class=\"row_heading level0 row4\" >pt</th>\n",
              "      <td id=\"T_425a5_row4_col0\" class=\"data row4 col0\" >0.4960</td>\n",
              "      <td id=\"T_425a5_row4_col1\" class=\"data row4 col1\" >0.8022</td>\n",
              "      <td id=\"T_425a5_row4_col2\" class=\"data row4 col2\" >0.6905</td>\n",
              "      <td id=\"T_425a5_row4_col3\" class=\"data row4 col3\" >0.9317</td>\n",
              "      <td id=\"T_425a5_row4_col4\" class=\"data row4 col4\" >1.0000</td>\n",
              "      <td id=\"T_425a5_row4_col5\" class=\"data row4 col5\" >0.8251</td>\n",
              "      <td id=\"T_425a5_row4_col6\" class=\"data row4 col6\" >0.0893</td>\n",
              "      <td id=\"T_425a5_row4_col7\" class=\"data row4 col7\" >0.1321</td>\n",
              "      <td id=\"T_425a5_row4_col8\" class=\"data row4 col8\" >0.0528</td>\n",
              "      <td id=\"T_425a5_row4_col9\" class=\"data row4 col9\" >0.2483</td>\n",
              "      <td id=\"T_425a5_row4_col10\" class=\"data row4 col10\" >0.1850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_425a5_level0_row5\" class=\"row_heading level0 row5\" >ca</th>\n",
              "      <td id=\"T_425a5_row5_col0\" class=\"data row5 col0\" >0.4237</td>\n",
              "      <td id=\"T_425a5_row5_col1\" class=\"data row5 col1\" >0.6491</td>\n",
              "      <td id=\"T_425a5_row5_col2\" class=\"data row5 col2\" >0.8342</td>\n",
              "      <td id=\"T_425a5_row5_col3\" class=\"data row5 col3\" >0.7331</td>\n",
              "      <td id=\"T_425a5_row5_col4\" class=\"data row5 col4\" >0.8251</td>\n",
              "      <td id=\"T_425a5_row5_col5\" class=\"data row5 col5\" >1.0000</td>\n",
              "      <td id=\"T_425a5_row5_col6\" class=\"data row5 col6\" >0.1521</td>\n",
              "      <td id=\"T_425a5_row5_col7\" class=\"data row5 col7\" >0.2103</td>\n",
              "      <td id=\"T_425a5_row5_col8\" class=\"data row5 col8\" >0.0826</td>\n",
              "      <td id=\"T_425a5_row5_col9\" class=\"data row5 col9\" >0.3345</td>\n",
              "      <td id=\"T_425a5_row5_col10\" class=\"data row5 col10\" >0.3160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_425a5_level0_row6\" class=\"row_heading level0 row6\" >cs</th>\n",
              "      <td id=\"T_425a5_row6_col0\" class=\"data row6 col0\" >-0.1613</td>\n",
              "      <td id=\"T_425a5_row6_col1\" class=\"data row6 col1\" >0.2176</td>\n",
              "      <td id=\"T_425a5_row6_col2\" class=\"data row6 col2\" >0.0626</td>\n",
              "      <td id=\"T_425a5_row6_col3\" class=\"data row6 col3\" >0.1570</td>\n",
              "      <td id=\"T_425a5_row6_col4\" class=\"data row6 col4\" >0.0893</td>\n",
              "      <td id=\"T_425a5_row6_col5\" class=\"data row6 col5\" >0.1521</td>\n",
              "      <td id=\"T_425a5_row6_col6\" class=\"data row6 col6\" >1.0000</td>\n",
              "      <td id=\"T_425a5_row6_col7\" class=\"data row6 col7\" >0.3690</td>\n",
              "      <td id=\"T_425a5_row6_col8\" class=\"data row6 col8\" >0.4489</td>\n",
              "      <td id=\"T_425a5_row6_col9\" class=\"data row6 col9\" >0.4264</td>\n",
              "      <td id=\"T_425a5_row6_col10\" class=\"data row6 col10\" >0.4500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_425a5_level0_row7\" class=\"row_heading level0 row7\" >ru</th>\n",
              "      <td id=\"T_425a5_row7_col0\" class=\"data row7 col0\" >-0.3235</td>\n",
              "      <td id=\"T_425a5_row7_col1\" class=\"data row7 col1\" >0.1990</td>\n",
              "      <td id=\"T_425a5_row7_col2\" class=\"data row7 col2\" >0.2403</td>\n",
              "      <td id=\"T_425a5_row7_col3\" class=\"data row7 col3\" >0.1774</td>\n",
              "      <td id=\"T_425a5_row7_col4\" class=\"data row7 col4\" >0.1321</td>\n",
              "      <td id=\"T_425a5_row7_col5\" class=\"data row7 col5\" >0.2103</td>\n",
              "      <td id=\"T_425a5_row7_col6\" class=\"data row7 col6\" >0.3690</td>\n",
              "      <td id=\"T_425a5_row7_col7\" class=\"data row7 col7\" >1.0000</td>\n",
              "      <td id=\"T_425a5_row7_col8\" class=\"data row7 col8\" >0.8606</td>\n",
              "      <td id=\"T_425a5_row7_col9\" class=\"data row7 col9\" >0.7378</td>\n",
              "      <td id=\"T_425a5_row7_col10\" class=\"data row7 col10\" >0.4463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_425a5_level0_row8\" class=\"row_heading level0 row8\" >uk</th>\n",
              "      <td id=\"T_425a5_row8_col0\" class=\"data row8 col0\" >-0.4988</td>\n",
              "      <td id=\"T_425a5_row8_col1\" class=\"data row8 col1\" >0.0784</td>\n",
              "      <td id=\"T_425a5_row8_col2\" class=\"data row8 col2\" >0.0503</td>\n",
              "      <td id=\"T_425a5_row8_col3\" class=\"data row8 col3\" >0.1224</td>\n",
              "      <td id=\"T_425a5_row8_col4\" class=\"data row8 col4\" >0.0528</td>\n",
              "      <td id=\"T_425a5_row8_col5\" class=\"data row8 col5\" >0.0826</td>\n",
              "      <td id=\"T_425a5_row8_col6\" class=\"data row8 col6\" >0.4489</td>\n",
              "      <td id=\"T_425a5_row8_col7\" class=\"data row8 col7\" >0.8606</td>\n",
              "      <td id=\"T_425a5_row8_col8\" class=\"data row8 col8\" >1.0000</td>\n",
              "      <td id=\"T_425a5_row8_col9\" class=\"data row8 col9\" >0.7398</td>\n",
              "      <td id=\"T_425a5_row8_col10\" class=\"data row8 col10\" >0.4664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_425a5_level0_row9\" class=\"row_heading level0 row9\" >ar</th>\n",
              "      <td id=\"T_425a5_row9_col0\" class=\"data row9 col0\" >-0.2672</td>\n",
              "      <td id=\"T_425a5_row9_col1\" class=\"data row9 col1\" >0.1954</td>\n",
              "      <td id=\"T_425a5_row9_col2\" class=\"data row9 col2\" >0.3516</td>\n",
              "      <td id=\"T_425a5_row9_col3\" class=\"data row9 col3\" >0.2989</td>\n",
              "      <td id=\"T_425a5_row9_col4\" class=\"data row9 col4\" >0.2483</td>\n",
              "      <td id=\"T_425a5_row9_col5\" class=\"data row9 col5\" >0.3345</td>\n",
              "      <td id=\"T_425a5_row9_col6\" class=\"data row9 col6\" >0.4264</td>\n",
              "      <td id=\"T_425a5_row9_col7\" class=\"data row9 col7\" >0.7378</td>\n",
              "      <td id=\"T_425a5_row9_col8\" class=\"data row9 col8\" >0.7398</td>\n",
              "      <td id=\"T_425a5_row9_col9\" class=\"data row9 col9\" >1.0000</td>\n",
              "      <td id=\"T_425a5_row9_col10\" class=\"data row9 col10\" >0.7249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_425a5_level0_row10\" class=\"row_heading level0 row10\" >zh</th>\n",
              "      <td id=\"T_425a5_row10_col0\" class=\"data row10 col0\" >-0.0033</td>\n",
              "      <td id=\"T_425a5_row10_col1\" class=\"data row10 col1\" >0.1656</td>\n",
              "      <td id=\"T_425a5_row10_col2\" class=\"data row10 col2\" >0.4694</td>\n",
              "      <td id=\"T_425a5_row10_col3\" class=\"data row10 col3\" >0.2015</td>\n",
              "      <td id=\"T_425a5_row10_col4\" class=\"data row10 col4\" >0.1850</td>\n",
              "      <td id=\"T_425a5_row10_col5\" class=\"data row10 col5\" >0.3160</td>\n",
              "      <td id=\"T_425a5_row10_col6\" class=\"data row10 col6\" >0.4500</td>\n",
              "      <td id=\"T_425a5_row10_col7\" class=\"data row10 col7\" >0.4463</td>\n",
              "      <td id=\"T_425a5_row10_col8\" class=\"data row10 col8\" >0.4664</td>\n",
              "      <td id=\"T_425a5_row10_col9\" class=\"data row10 col9\" >0.7249</td>\n",
              "      <td id=\"T_425a5_row10_col10\" class=\"data row10 col10\" >1.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tex_temp = temp[~temp['Train Language'].isin([\"en3\", \"all\"])][sorted_languages].corr().style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4)\n",
        "print(tex_temp.applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PeFbbzLtAfV",
        "outputId": "d923a933-d359-4f3a-9906-eefcea76c0a7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{lrrrrrrrrrrr}\n",
            " & \\bfseries en & \\bfseries de & \\bfseries nl & \\bfseries es & \\bfseries pt & \\bfseries ca & \\bfseries cs & \\bfseries ru & \\bfseries uk & \\bfseries ar & \\bfseries zh \\\\\n",
            "\\bfseries en & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{C9CEE4}} \\color[HTML]{000000} 0.5420 & {\\cellcolor[HTML]{C6CCE3}} \\color[HTML]{000000} 0.5551 & {\\cellcolor[HTML]{DEDCEC}} \\color[HTML]{000000} 0.3794 & {\\cellcolor[HTML]{D1D2E6}} \\color[HTML]{000000} 0.4960 & {\\cellcolor[HTML]{D9D8EA}} \\color[HTML]{000000} 0.4237 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.1613 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.3235 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.4988 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.2672 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.0033 \\\\\n",
            "\\bfseries de & {\\cellcolor[HTML]{C9CEE4}} \\color[HTML]{000000} 0.5420 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{C0C9E2}} \\color[HTML]{000000} 0.6006 & {\\cellcolor[HTML]{A2BCDA}} \\color[HTML]{000000} 0.7657 & {\\cellcolor[HTML]{9CB9D9}} \\color[HTML]{000000} 0.8022 & {\\cellcolor[HTML]{B7C5DF}} \\color[HTML]{000000} 0.6491 & {\\cellcolor[HTML]{EFE9F3}} \\color[HTML]{000000} 0.2176 & {\\cellcolor[HTML]{F0EAF4}} \\color[HTML]{000000} 0.1990 & {\\cellcolor[HTML]{F9F2F8}} \\color[HTML]{000000} 0.0784 & {\\cellcolor[HTML]{F0EAF4}} \\color[HTML]{000000} 0.1954 & {\\cellcolor[HTML]{F2ECF5}} \\color[HTML]{000000} 0.1656 \\\\\n",
            "\\bfseries nl & {\\cellcolor[HTML]{C6CCE3}} \\color[HTML]{000000} 0.5551 & {\\cellcolor[HTML]{C0C9E2}} \\color[HTML]{000000} 0.6006 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{C6CCE3}} \\color[HTML]{000000} 0.5585 & {\\cellcolor[HTML]{B0C2DE}} \\color[HTML]{000000} 0.6905 & {\\cellcolor[HTML]{96B6D7}} \\color[HTML]{000000} 0.8342 & {\\cellcolor[HTML]{FAF3F9}} \\color[HTML]{000000} 0.0626 & {\\cellcolor[HTML]{EDE8F3}} \\color[HTML]{000000} 0.2403 & {\\cellcolor[HTML]{FBF4F9}} \\color[HTML]{000000} 0.0503 & {\\cellcolor[HTML]{E0DEED}} \\color[HTML]{000000} 0.3516 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4694 \\\\\n",
            "\\bfseries es & {\\cellcolor[HTML]{DEDCEC}} \\color[HTML]{000000} 0.3794 & {\\cellcolor[HTML]{A2BCDA}} \\color[HTML]{000000} 0.7657 & {\\cellcolor[HTML]{C6CCE3}} \\color[HTML]{000000} 0.5585 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{81AED2}} \\color[HTML]{000000} 0.9317 & {\\cellcolor[HTML]{A9BFDC}} \\color[HTML]{000000} 0.7331 & {\\cellcolor[HTML]{F3EDF5}} \\color[HTML]{000000} 0.1570 & {\\cellcolor[HTML]{F2ECF5}} \\color[HTML]{000000} 0.1774 & {\\cellcolor[HTML]{F6EFF7}} \\color[HTML]{000000} 0.1224 & {\\cellcolor[HTML]{E7E3F0}} \\color[HTML]{000000} 0.2989 & {\\cellcolor[HTML]{F0EAF4}} \\color[HTML]{000000} 0.2015 \\\\\n",
            "\\bfseries pt & {\\cellcolor[HTML]{D1D2E6}} \\color[HTML]{000000} 0.4960 & {\\cellcolor[HTML]{9CB9D9}} \\color[HTML]{000000} 0.8022 & {\\cellcolor[HTML]{B0C2DE}} \\color[HTML]{000000} 0.6905 & {\\cellcolor[HTML]{81AED2}} \\color[HTML]{000000} 0.9317 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{97B7D7}} \\color[HTML]{000000} 0.8251 & {\\cellcolor[HTML]{F8F1F8}} \\color[HTML]{000000} 0.0893 & {\\cellcolor[HTML]{F5EFF6}} \\color[HTML]{000000} 0.1321 & {\\cellcolor[HTML]{FBF4F9}} \\color[HTML]{000000} 0.0528 & {\\cellcolor[HTML]{EDE7F2}} \\color[HTML]{000000} 0.2483 & {\\cellcolor[HTML]{F1EBF5}} \\color[HTML]{000000} 0.1850 \\\\\n",
            "\\bfseries ca & {\\cellcolor[HTML]{D9D8EA}} \\color[HTML]{000000} 0.4237 & {\\cellcolor[HTML]{B7C5DF}} \\color[HTML]{000000} 0.6491 & {\\cellcolor[HTML]{96B6D7}} \\color[HTML]{000000} 0.8342 & {\\cellcolor[HTML]{A9BFDC}} \\color[HTML]{000000} 0.7331 & {\\cellcolor[HTML]{97B7D7}} \\color[HTML]{000000} 0.8251 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{F4EDF6}} \\color[HTML]{000000} 0.1521 & {\\cellcolor[HTML]{F0EAF4}} \\color[HTML]{000000} 0.2103 & {\\cellcolor[HTML]{F9F2F8}} \\color[HTML]{000000} 0.0826 & {\\cellcolor[HTML]{E3E0EE}} \\color[HTML]{000000} 0.3345 & {\\cellcolor[HTML]{E5E1EF}} \\color[HTML]{000000} 0.3160 \\\\\n",
            "\\bfseries cs & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.1613 & {\\cellcolor[HTML]{EFE9F3}} \\color[HTML]{000000} 0.2176 & {\\cellcolor[HTML]{FAF3F9}} \\color[HTML]{000000} 0.0626 & {\\cellcolor[HTML]{F3EDF5}} \\color[HTML]{000000} 0.1570 & {\\cellcolor[HTML]{F8F1F8}} \\color[HTML]{000000} 0.0893 & {\\cellcolor[HTML]{F4EDF6}} \\color[HTML]{000000} 0.1521 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{DFDDEC}} \\color[HTML]{000000} 0.3690 & {\\cellcolor[HTML]{D6D6E9}} \\color[HTML]{000000} 0.4489 & {\\cellcolor[HTML]{D9D8EA}} \\color[HTML]{000000} 0.4264 & {\\cellcolor[HTML]{D6D6E9}} \\color[HTML]{000000} 0.4500 \\\\\n",
            "\\bfseries ru & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.3235 & {\\cellcolor[HTML]{F0EAF4}} \\color[HTML]{000000} 0.1990 & {\\cellcolor[HTML]{EDE8F3}} \\color[HTML]{000000} 0.2403 & {\\cellcolor[HTML]{F2ECF5}} \\color[HTML]{000000} 0.1774 & {\\cellcolor[HTML]{F5EFF6}} \\color[HTML]{000000} 0.1321 & {\\cellcolor[HTML]{F0EAF4}} \\color[HTML]{000000} 0.2103 & {\\cellcolor[HTML]{DFDDEC}} \\color[HTML]{000000} 0.3690 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{8FB4D6}} \\color[HTML]{000000} 0.8606 & {\\cellcolor[HTML]{A8BEDC}} \\color[HTML]{000000} 0.7378 & {\\cellcolor[HTML]{D6D6E9}} \\color[HTML]{000000} 0.4463 \\\\\n",
            "\\bfseries uk & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.4988 & {\\cellcolor[HTML]{F9F2F8}} \\color[HTML]{000000} 0.0784 & {\\cellcolor[HTML]{FBF4F9}} \\color[HTML]{000000} 0.0503 & {\\cellcolor[HTML]{F6EFF7}} \\color[HTML]{000000} 0.1224 & {\\cellcolor[HTML]{FBF4F9}} \\color[HTML]{000000} 0.0528 & {\\cellcolor[HTML]{F9F2F8}} \\color[HTML]{000000} 0.0826 & {\\cellcolor[HTML]{D6D6E9}} \\color[HTML]{000000} 0.4489 & {\\cellcolor[HTML]{8FB4D6}} \\color[HTML]{000000} 0.8606 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{A8BEDC}} \\color[HTML]{000000} 0.7398 & {\\cellcolor[HTML]{D4D4E8}} \\color[HTML]{000000} 0.4664 \\\\\n",
            "\\bfseries ar & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.2672 & {\\cellcolor[HTML]{F0EAF4}} \\color[HTML]{000000} 0.1954 & {\\cellcolor[HTML]{E0DEED}} \\color[HTML]{000000} 0.3516 & {\\cellcolor[HTML]{E7E3F0}} \\color[HTML]{000000} 0.2989 & {\\cellcolor[HTML]{EDE7F2}} \\color[HTML]{000000} 0.2483 & {\\cellcolor[HTML]{E3E0EE}} \\color[HTML]{000000} 0.3345 & {\\cellcolor[HTML]{D9D8EA}} \\color[HTML]{000000} 0.4264 & {\\cellcolor[HTML]{A8BEDC}} \\color[HTML]{000000} 0.7378 & {\\cellcolor[HTML]{A8BEDC}} \\color[HTML]{000000} 0.7398 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{ABBFDC}} \\color[HTML]{000000} 0.7249 \\\\\n",
            "\\bfseries zh & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.0033 & {\\cellcolor[HTML]{F2ECF5}} \\color[HTML]{000000} 0.1656 & {\\cellcolor[HTML]{D3D4E7}} \\color[HTML]{000000} 0.4694 & {\\cellcolor[HTML]{F0EAF4}} \\color[HTML]{000000} 0.2015 & {\\cellcolor[HTML]{F1EBF5}} \\color[HTML]{000000} 0.1850 & {\\cellcolor[HTML]{E5E1EF}} \\color[HTML]{000000} 0.3160 & {\\cellcolor[HTML]{D6D6E9}} \\color[HTML]{000000} 0.4500 & {\\cellcolor[HTML]{D6D6E9}} \\color[HTML]{000000} 0.4463 & {\\cellcolor[HTML]{D4D4E8}} \\color[HTML]{000000} 0.4664 & {\\cellcolor[HTML]{ABBFDC}} \\color[HTML]{000000} 0.7249 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = temp[sorted_languages]"
      ],
      "metadata": {
        "id": "Funpd2v1qjgE"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "langs = sorted_languages\n",
        "df = temp[~temp['Train Language'].isin([\"en3\", \"all\"])][sorted_languages]"
      ],
      "metadata": {
        "id": "zF_p1ln9Pd7S"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_df = pd.DataFrame()\n",
        "for (src, trg) in itertools.combinations_with_replacement(langs, 2):\n",
        "  print(src, trg)\n",
        "\n",
        "  res = stats.pearsonr(df[src], df[trg])\n",
        "\n",
        "  print(res)\n",
        "  print(res.confidence_interval(0.95))\n",
        "\n",
        "  res_df = pd.concat([res_df, pd.DataFrame({'src':src, 'trg':trg, 'PearsonR p-value':res[1], '0.95 Confidence Intrval low':res.confidence_interval(0.95)[0], '0.95 Confidence Intrval high':res.confidence_interval(0.95)[1]}, index=[0])], ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUYXVzfDPd-q",
        "outputId": "e9bc4013-1e5c-4c4e-d7d2-9c02f66aeb2e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en en\n",
            "PearsonRResult(statistic=1.0, pvalue=0.0)\n",
            "ConfidenceInterval(low=1.0, high=1.0)\n",
            "en de\n",
            "PearsonRResult(statistic=0.5420339360665481, pvalue=1.17353745159646e-08)\n",
            "ConfidenceInterval(low=0.383189237088439, high=0.6697391976407968)\n",
            "en nl\n",
            "PearsonRResult(statistic=0.555080654338777, pvalue=4.383673414223362e-09)\n",
            "ConfidenceInterval(low=0.3989974926376806, high=0.6799030022255345)\n",
            "en es\n",
            "PearsonRResult(statistic=0.3793888761599111, pvalue=0.0001377950137024026)\n",
            "ConfidenceInterval(low=0.19363091270041108, high=0.5388859998238579)\n",
            "en pt\n",
            "PearsonRResult(statistic=0.4959527564251869, pvalue=2.77425475348252e-07)\n",
            "ConfidenceInterval(low=0.3280892353651722, high=0.6334534392972023)\n",
            "en ca\n",
            "PearsonRResult(statistic=0.4237135259461428, pvalue=1.695622254234994e-05)\n",
            "ConfidenceInterval(low=0.24395086769690397, high=0.5753263027746047)\n",
            "en cs\n",
            "PearsonRResult(statistic=-0.16132211381149508, pvalue=0.11636259493873008)\n",
            "ConfidenceInterval(low=-0.3504727815031259, high=0.04047283619885538)\n",
            "en ru\n",
            "PearsonRResult(statistic=-0.3235462435817845, pvalue=0.001302089973236555)\n",
            "ConfidenceInterval(low=-0.49211070191710754, high=-0.13159650006056828)\n",
            "en uk\n",
            "PearsonRResult(statistic=-0.4988322077853846, pvalue=2.3074279906023574e-07)\n",
            "ConfidenceInterval(low=-0.6357386621234871, high=-0.3314991064808005)\n",
            "en ar\n",
            "PearsonRResult(statistic=-0.2671765569005686, pvalue=0.008500537153663496)\n",
            "ConfidenceInterval(low=-0.4438856998129641, high=-0.07046508484057498)\n",
            "en zh\n",
            "PearsonRResult(statistic=-0.0033334191839927835, pvalue=0.9742862799054467)\n",
            "ConfidenceInterval(low=-0.20368324142318447, high=0.19728437119169254)\n",
            "de de\n",
            "PearsonRResult(statistic=1.0, pvalue=0.0)\n",
            "ConfidenceInterval(low=1.0, high=1.0)\n",
            "de nl\n",
            "PearsonRResult(statistic=0.6005508986324432, pvalue=9.938915799707921e-11)\n",
            "ConfidenceInterval(low=0.4548270466270807, high=0.7149548403423345)\n",
            "de es\n",
            "PearsonRResult(statistic=0.765733590943003, pvalue=1.0206142304865912e-19)\n",
            "ConfidenceInterval(low=0.667761624566341, high=0.837627888540035)\n",
            "de pt\n",
            "PearsonRResult(statistic=0.8021985555843967, pvalue=8.973793878659679e-23)\n",
            "ConfidenceInterval(low=0.7170326366887202, high=0.8637654968045857)\n",
            "de ca\n",
            "PearsonRResult(statistic=0.6490946727458742, pvalue=8.518711537601647e-13)\n",
            "ConfidenceInterval(low=0.5157218619295908, high=0.7517518698819152)\n",
            "de cs\n",
            "PearsonRResult(statistic=0.21760858958751209, pvalue=0.033189514485372495)\n",
            "ConfidenceInterval(low=0.017903739844405304, high=0.40061664594468266)\n",
            "de ru\n",
            "PearsonRResult(statistic=0.19897529354311413, pvalue=0.051956517078766494)\n",
            "ConfidenceInterval(low=-0.0015734173288190019, high=0.3841373279247955)\n",
            "de uk\n",
            "PearsonRResult(statistic=0.0784088274615922, pvalue=0.4476414481117586)\n",
            "ConfidenceInterval(low=-0.12402680197468402, high=0.2745784352519565)\n",
            "de ar\n",
            "PearsonRResult(statistic=0.19538045105555943, pvalue=0.056432378613650254)\n",
            "ConfidenceInterval(low=-0.005313633985096929, high=0.38094440905631666)\n",
            "de zh\n",
            "PearsonRResult(statistic=0.16558345384819556, pvalue=0.10690142122444854)\n",
            "ConfidenceInterval(low=-0.03610093913051338, high=0.3543074006052856)\n",
            "nl nl\n",
            "PearsonRResult(statistic=1.0, pvalue=0.0)\n",
            "ConfidenceInterval(low=1.0, high=1.0)\n",
            "nl es\n",
            "PearsonRResult(statistic=0.5584726998487256, pvalue=3.369866853314986e-09)\n",
            "ConfidenceInterval(low=0.40312276985019824, high=0.6825376804464225)\n",
            "nl pt\n",
            "PearsonRResult(statistic=0.6905089716084909, pvalue=7.038817566570753e-15)\n",
            "ConfidenceInterval(low=0.5687607550421249, high=0.7826473027322279)\n",
            "nl ca\n",
            "PearsonRResult(statistic=0.8341561924602696, pvalue=4.993323067144427e-26)\n",
            "ConfidenceInterval(low=0.7609247019655679, high=0.8864031028139969)\n",
            "nl cs\n",
            "PearsonRResult(statistic=0.06262311443806762, pvalue=0.544424113712105)\n",
            "ConfidenceInterval(low=-0.13961571291285688, high=0.25984666990359234)\n",
            "nl ru\n",
            "PearsonRResult(statistic=0.24028308474766752, pvalue=0.01836861172045522)\n",
            "ConfidenceInterval(low=0.04181133355549857, high=0.42051156531714295)\n",
            "nl uk\n",
            "PearsonRResult(statistic=0.05033451622061072, pvalue=0.6262412754065478)\n",
            "ConfidenceInterval(low=-0.15168210749231484, high=0.24831462766998777)\n",
            "nl ar\n",
            "PearsonRResult(statistic=0.3515711295058527, pvalue=0.0004441044267999328)\n",
            "ConfidenceInterval(low=0.16254196691318104, high=0.515707401719283)\n",
            "nl zh\n",
            "PearsonRResult(statistic=0.46937195744928073, pvalue=1.406895569289087e-06)\n",
            "ConfidenceInterval(low=0.2968172525855003, high=0.6122442038713957)\n",
            "es es\n",
            "PearsonRResult(statistic=1.0, pvalue=0.0)\n",
            "ConfidenceInterval(low=1.0, high=1.0)\n",
            "es pt\n",
            "PearsonRResult(statistic=0.9317313269266111, pvalue=3.909763570982933e-43)\n",
            "ConfidenceInterval(low=0.8992185375916995, high=0.9540092618395528)\n",
            "es ca\n",
            "PearsonRResult(statistic=0.7331266465269141, pvalue=2.025815329683141e-17)\n",
            "ConfidenceInterval(low=0.6244187635779751, high=0.8139734766968463)\n",
            "es cs\n",
            "PearsonRResult(statistic=0.15703796364973704, pvalue=0.1265142157387347)\n",
            "ConfidenceInterval(low=-0.04486036014507727, high=0.3466112331989666)\n",
            "es ru\n",
            "PearsonRResult(statistic=0.1774231968527462, pvalue=0.08374434398336683)\n",
            "ConfidenceInterval(low=-0.02391336659734391, high=0.36492832791489155)\n",
            "es uk\n",
            "PearsonRResult(statistic=0.12243359469404205, pvalue=0.23469083076426356)\n",
            "ConfidenceInterval(low=-0.08001645080230806, high=0.31518298980437004)\n",
            "es ar\n",
            "PearsonRResult(statistic=0.2988691838522082, pvalue=0.0030977482374766243)\n",
            "ConfidenceInterval(low=0.10465401038823062, high=0.4711257134640883)\n",
            "es zh\n",
            "PearsonRResult(statistic=0.2014914922122984, pvalue=0.049000840368839366)\n",
            "ConfidenceInterval(low=0.0010478780130514411, high=0.38636956962089536)\n",
            "pt pt\n",
            "PearsonRResult(statistic=0.9999999999999999, pvalue=0.0)\n",
            "ConfidenceInterval(low=0.9999999999999999, high=0.9999999999999999)\n",
            "pt ca\n",
            "PearsonRResult(statistic=0.8251162975146612, pvalue=4.84712488736722e-25)\n",
            "ConfidenceInterval(low=0.7484405468677916, high=0.8800248220455823)\n",
            "pt cs\n",
            "PearsonRResult(statistic=0.08925923538701958, pvalue=0.38713208696259477)\n",
            "ConfidenceInterval(low=-0.11325340393305397, high=0.2846512819733643)\n",
            "pt ru\n",
            "PearsonRResult(statistic=0.13211950282583493, pvalue=0.19942994560199478)\n",
            "ConfidenceInterval(low=-0.07022661141424018, high=0.32402270055999144)\n",
            "pt uk\n",
            "PearsonRResult(statistic=0.05280157623907532, pvalue=0.6094019517210059)\n",
            "ConfidenceInterval(low=-0.14926447812988122, high=0.2506343124548697)\n",
            "pt ar\n",
            "PearsonRResult(statistic=0.24828801406100137, pvalue=0.014720914813793522)\n",
            "ConfidenceInterval(low=0.05030622579600792, high=0.427494049878713)\n",
            "pt zh\n",
            "PearsonRResult(statistic=0.18502130680405704, pvalue=0.07112168862076806)\n",
            "ConfidenceInterval(low=-0.0160603843482128, high=0.3717186557588268)\n",
            "ca ca\n",
            "PearsonRResult(statistic=1.0, pvalue=0.0)\n",
            "ConfidenceInterval(low=1.0, high=1.0)\n",
            "ca cs\n",
            "PearsonRResult(statistic=0.15205988192723893, pvalue=0.1391470157702135)\n",
            "ConfidenceInterval(low=-0.049948794997968345, high=0.3421161192189203)\n",
            "ca ru\n",
            "PearsonRResult(statistic=0.21026848184834535, pvalue=0.0397571709176407)\n",
            "ConfidenceInterval(low=0.010213078613875465, high=0.39413914571870745)\n",
            "ca uk\n",
            "PearsonRResult(statistic=0.08256092754928215, pvalue=0.4238854642530966)\n",
            "ConfidenceInterval(low=-0.11990980060922482, high=0.2784380798792669)\n",
            "ca ar\n",
            "PearsonRResult(statistic=0.3344601101068426, pvalue=0.0008667096702428073)\n",
            "ConfidenceInterval(low=0.14360342907420273, high=0.501329621874248)\n",
            "ca zh\n",
            "PearsonRResult(statistic=0.31604516014863815, pvalue=0.0017077455119486457)\n",
            "ConfidenceInterval(low=0.12337668470360567, high=0.485752553840791)\n",
            "cs cs\n",
            "PearsonRResult(statistic=1.0, pvalue=0.0)\n",
            "ConfidenceInterval(low=1.0, high=1.0)\n",
            "cs ru\n",
            "PearsonRResult(statistic=0.3690435108759075, pvalue=0.00021559329070564416)\n",
            "ConfidenceInterval(low=0.18202525785844034, high=0.5302940403817499)\n",
            "cs uk\n",
            "PearsonRResult(statistic=0.44886220698605406, pvalue=4.499943909075156e-06)\n",
            "ConfidenceInterval(low=0.27293811748228425, high=0.5957374065620851)\n",
            "cs ar\n",
            "PearsonRResult(statistic=0.42640828106602313, pvalue=1.4783077967690474e-05)\n",
            "ConfidenceInterval(low=0.24704164706716744, high=0.5775224828569571)\n",
            "cs zh\n",
            "PearsonRResult(statistic=0.4500233043238058, pvalue=4.221689446252267e-06)\n",
            "ConfidenceInterval(low=0.27428419827902034, high=0.5966752138332003)\n",
            "ru ru\n",
            "PearsonRResult(statistic=1.0, pvalue=0.0)\n",
            "ConfidenceInterval(low=1.0, high=1.0)\n",
            "ru uk\n",
            "PearsonRResult(statistic=0.8605550871934471, pvalue=2.741550172173496e-29)\n",
            "ConfidenceInterval(low=0.7976947854285301, high=0.904916510557949)\n",
            "ru ar\n",
            "PearsonRResult(statistic=0.7377767172789724, pvalue=9.996588948095014e-18)\n",
            "ConfidenceInterval(low=0.6305592226100979, high=0.817363299498813)\n",
            "ru zh\n",
            "PearsonRResult(statistic=0.4462714033083677, pvalue=5.184375520264454e-06)\n",
            "ConfidenceInterval(low=0.2699370339907866, high=0.5936433943136901)\n",
            "uk uk\n",
            "PearsonRResult(statistic=0.9999999999999999, pvalue=0.0)\n",
            "ConfidenceInterval(low=0.9999999999999999, high=0.9999999999999999)\n",
            "uk ar\n",
            "PearsonRResult(statistic=0.7397557408697537, pvalue=7.367650902842488e-18)\n",
            "ConfidenceInterval(low=0.6331766185605173, high=0.8188043044366226)\n",
            "uk zh\n",
            "PearsonRResult(statistic=0.46642054212811546, pvalue=1.670921315140885e-06)\n",
            "ConfidenceInterval(low=0.2933676283725694, high=0.6098764724060348)\n",
            "ar ar\n",
            "PearsonRResult(statistic=1.0, pvalue=0.0)\n",
            "ConfidenceInterval(low=1.0, high=1.0)\n",
            "ar zh\n",
            "PearsonRResult(statistic=0.7249045502276373, pvalue=6.817471512484208e-17)\n",
            "ConfidenceInterval(low=0.6135941976688941, high=0.807966202499216)\n",
            "zh zh\n",
            "PearsonRResult(statistic=0.9999999999999999, pvalue=0.0)\n",
            "ConfidenceInterval(low=0.9999999999999999, high=0.9999999999999999)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#res_df[res_df['PearsonR p-value'] >0.05]\n",
        "res_df#.style.format(na_rep=0, precision=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lhLvf0vu0ic_",
        "outputId": "706d6dcb-9ec3-4375-8871-d0e3ab038d54"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   src trg  PearsonR p-value  0.95 Confidence Intrval low  \\\n",
              "0   en  en      0.000000e+00                     1.000000   \n",
              "1   en  de      1.173537e-08                     0.383189   \n",
              "2   en  nl      4.383673e-09                     0.398997   \n",
              "3   en  es      1.377950e-04                     0.193631   \n",
              "4   en  pt      2.774255e-07                     0.328089   \n",
              "5   en  ca      1.695622e-05                     0.243951   \n",
              "6   en  cs      1.163626e-01                    -0.350473   \n",
              "7   en  ru      1.302090e-03                    -0.492111   \n",
              "8   en  uk      2.307428e-07                    -0.635739   \n",
              "9   en  ar      8.500537e-03                    -0.443886   \n",
              "10  en  zh      9.742863e-01                    -0.203683   \n",
              "11  de  de      0.000000e+00                     1.000000   \n",
              "12  de  nl      9.938916e-11                     0.454827   \n",
              "13  de  es      1.020614e-19                     0.667762   \n",
              "14  de  pt      8.973794e-23                     0.717033   \n",
              "15  de  ca      8.518712e-13                     0.515722   \n",
              "16  de  cs      3.318951e-02                     0.017904   \n",
              "17  de  ru      5.195652e-02                    -0.001573   \n",
              "18  de  uk      4.476414e-01                    -0.124027   \n",
              "19  de  ar      5.643238e-02                    -0.005314   \n",
              "20  de  zh      1.069014e-01                    -0.036101   \n",
              "21  nl  nl      0.000000e+00                     1.000000   \n",
              "22  nl  es      3.369867e-09                     0.403123   \n",
              "23  nl  pt      7.038818e-15                     0.568761   \n",
              "24  nl  ca      4.993323e-26                     0.760925   \n",
              "25  nl  cs      5.444241e-01                    -0.139616   \n",
              "26  nl  ru      1.836861e-02                     0.041811   \n",
              "27  nl  uk      6.262413e-01                    -0.151682   \n",
              "28  nl  ar      4.441044e-04                     0.162542   \n",
              "29  nl  zh      1.406896e-06                     0.296817   \n",
              "30  es  es      0.000000e+00                     1.000000   \n",
              "31  es  pt      3.909764e-43                     0.899219   \n",
              "32  es  ca      2.025815e-17                     0.624419   \n",
              "33  es  cs      1.265142e-01                    -0.044860   \n",
              "34  es  ru      8.374434e-02                    -0.023913   \n",
              "35  es  uk      2.346908e-01                    -0.080016   \n",
              "36  es  ar      3.097748e-03                     0.104654   \n",
              "37  es  zh      4.900084e-02                     0.001048   \n",
              "38  pt  pt      0.000000e+00                     1.000000   \n",
              "39  pt  ca      4.847125e-25                     0.748441   \n",
              "40  pt  cs      3.871321e-01                    -0.113253   \n",
              "41  pt  ru      1.994299e-01                    -0.070227   \n",
              "42  pt  uk      6.094020e-01                    -0.149264   \n",
              "43  pt  ar      1.472091e-02                     0.050306   \n",
              "44  pt  zh      7.112169e-02                    -0.016060   \n",
              "45  ca  ca      0.000000e+00                     1.000000   \n",
              "46  ca  cs      1.391470e-01                    -0.049949   \n",
              "47  ca  ru      3.975717e-02                     0.010213   \n",
              "48  ca  uk      4.238855e-01                    -0.119910   \n",
              "49  ca  ar      8.667097e-04                     0.143603   \n",
              "50  ca  zh      1.707746e-03                     0.123377   \n",
              "51  cs  cs      0.000000e+00                     1.000000   \n",
              "52  cs  ru      2.155933e-04                     0.182025   \n",
              "53  cs  uk      4.499944e-06                     0.272938   \n",
              "54  cs  ar      1.478308e-05                     0.247042   \n",
              "55  cs  zh      4.221689e-06                     0.274284   \n",
              "56  ru  ru      0.000000e+00                     1.000000   \n",
              "57  ru  uk      2.741550e-29                     0.797695   \n",
              "58  ru  ar      9.996589e-18                     0.630559   \n",
              "59  ru  zh      5.184376e-06                     0.269937   \n",
              "60  uk  uk      0.000000e+00                     1.000000   \n",
              "61  uk  ar      7.367651e-18                     0.633177   \n",
              "62  uk  zh      1.670921e-06                     0.293368   \n",
              "63  ar  ar      0.000000e+00                     1.000000   \n",
              "64  ar  zh      6.817472e-17                     0.613594   \n",
              "65  zh  zh      0.000000e+00                     1.000000   \n",
              "\n",
              "    0.95 Confidence Intrval high  \n",
              "0                       1.000000  \n",
              "1                       0.669739  \n",
              "2                       0.679903  \n",
              "3                       0.538886  \n",
              "4                       0.633453  \n",
              "5                       0.575326  \n",
              "6                       0.040473  \n",
              "7                      -0.131597  \n",
              "8                      -0.331499  \n",
              "9                      -0.070465  \n",
              "10                      0.197284  \n",
              "11                      1.000000  \n",
              "12                      0.714955  \n",
              "13                      0.837628  \n",
              "14                      0.863765  \n",
              "15                      0.751752  \n",
              "16                      0.400617  \n",
              "17                      0.384137  \n",
              "18                      0.274578  \n",
              "19                      0.380944  \n",
              "20                      0.354307  \n",
              "21                      1.000000  \n",
              "22                      0.682538  \n",
              "23                      0.782647  \n",
              "24                      0.886403  \n",
              "25                      0.259847  \n",
              "26                      0.420512  \n",
              "27                      0.248315  \n",
              "28                      0.515707  \n",
              "29                      0.612244  \n",
              "30                      1.000000  \n",
              "31                      0.954009  \n",
              "32                      0.813973  \n",
              "33                      0.346611  \n",
              "34                      0.364928  \n",
              "35                      0.315183  \n",
              "36                      0.471126  \n",
              "37                      0.386370  \n",
              "38                      1.000000  \n",
              "39                      0.880025  \n",
              "40                      0.284651  \n",
              "41                      0.324023  \n",
              "42                      0.250634  \n",
              "43                      0.427494  \n",
              "44                      0.371719  \n",
              "45                      1.000000  \n",
              "46                      0.342116  \n",
              "47                      0.394139  \n",
              "48                      0.278438  \n",
              "49                      0.501330  \n",
              "50                      0.485753  \n",
              "51                      1.000000  \n",
              "52                      0.530294  \n",
              "53                      0.595737  \n",
              "54                      0.577522  \n",
              "55                      0.596675  \n",
              "56                      1.000000  \n",
              "57                      0.904917  \n",
              "58                      0.817363  \n",
              "59                      0.593643  \n",
              "60                      1.000000  \n",
              "61                      0.818804  \n",
              "62                      0.609876  \n",
              "63                      1.000000  \n",
              "64                      0.807966  \n",
              "65                      1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-78f4cffa-4e1c-41ef-a3ad-42c7b9413bc1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>trg</th>\n",
              "      <th>PearsonR p-value</th>\n",
              "      <th>0.95 Confidence Intrval low</th>\n",
              "      <th>0.95 Confidence Intrval high</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>en</td>\n",
              "      <td>en</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>en</td>\n",
              "      <td>de</td>\n",
              "      <td>1.173537e-08</td>\n",
              "      <td>0.383189</td>\n",
              "      <td>0.669739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>en</td>\n",
              "      <td>nl</td>\n",
              "      <td>4.383673e-09</td>\n",
              "      <td>0.398997</td>\n",
              "      <td>0.679903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>en</td>\n",
              "      <td>es</td>\n",
              "      <td>1.377950e-04</td>\n",
              "      <td>0.193631</td>\n",
              "      <td>0.538886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>en</td>\n",
              "      <td>pt</td>\n",
              "      <td>2.774255e-07</td>\n",
              "      <td>0.328089</td>\n",
              "      <td>0.633453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>en</td>\n",
              "      <td>ca</td>\n",
              "      <td>1.695622e-05</td>\n",
              "      <td>0.243951</td>\n",
              "      <td>0.575326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>en</td>\n",
              "      <td>cs</td>\n",
              "      <td>1.163626e-01</td>\n",
              "      <td>-0.350473</td>\n",
              "      <td>0.040473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>en</td>\n",
              "      <td>ru</td>\n",
              "      <td>1.302090e-03</td>\n",
              "      <td>-0.492111</td>\n",
              "      <td>-0.131597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>en</td>\n",
              "      <td>uk</td>\n",
              "      <td>2.307428e-07</td>\n",
              "      <td>-0.635739</td>\n",
              "      <td>-0.331499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>en</td>\n",
              "      <td>ar</td>\n",
              "      <td>8.500537e-03</td>\n",
              "      <td>-0.443886</td>\n",
              "      <td>-0.070465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>en</td>\n",
              "      <td>zh</td>\n",
              "      <td>9.742863e-01</td>\n",
              "      <td>-0.203683</td>\n",
              "      <td>0.197284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>de</td>\n",
              "      <td>de</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>de</td>\n",
              "      <td>nl</td>\n",
              "      <td>9.938916e-11</td>\n",
              "      <td>0.454827</td>\n",
              "      <td>0.714955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>de</td>\n",
              "      <td>es</td>\n",
              "      <td>1.020614e-19</td>\n",
              "      <td>0.667762</td>\n",
              "      <td>0.837628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>de</td>\n",
              "      <td>pt</td>\n",
              "      <td>8.973794e-23</td>\n",
              "      <td>0.717033</td>\n",
              "      <td>0.863765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>de</td>\n",
              "      <td>ca</td>\n",
              "      <td>8.518712e-13</td>\n",
              "      <td>0.515722</td>\n",
              "      <td>0.751752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>de</td>\n",
              "      <td>cs</td>\n",
              "      <td>3.318951e-02</td>\n",
              "      <td>0.017904</td>\n",
              "      <td>0.400617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>de</td>\n",
              "      <td>ru</td>\n",
              "      <td>5.195652e-02</td>\n",
              "      <td>-0.001573</td>\n",
              "      <td>0.384137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>de</td>\n",
              "      <td>uk</td>\n",
              "      <td>4.476414e-01</td>\n",
              "      <td>-0.124027</td>\n",
              "      <td>0.274578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>de</td>\n",
              "      <td>ar</td>\n",
              "      <td>5.643238e-02</td>\n",
              "      <td>-0.005314</td>\n",
              "      <td>0.380944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>de</td>\n",
              "      <td>zh</td>\n",
              "      <td>1.069014e-01</td>\n",
              "      <td>-0.036101</td>\n",
              "      <td>0.354307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>nl</td>\n",
              "      <td>nl</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>nl</td>\n",
              "      <td>es</td>\n",
              "      <td>3.369867e-09</td>\n",
              "      <td>0.403123</td>\n",
              "      <td>0.682538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>nl</td>\n",
              "      <td>pt</td>\n",
              "      <td>7.038818e-15</td>\n",
              "      <td>0.568761</td>\n",
              "      <td>0.782647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>nl</td>\n",
              "      <td>ca</td>\n",
              "      <td>4.993323e-26</td>\n",
              "      <td>0.760925</td>\n",
              "      <td>0.886403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>nl</td>\n",
              "      <td>cs</td>\n",
              "      <td>5.444241e-01</td>\n",
              "      <td>-0.139616</td>\n",
              "      <td>0.259847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>nl</td>\n",
              "      <td>ru</td>\n",
              "      <td>1.836861e-02</td>\n",
              "      <td>0.041811</td>\n",
              "      <td>0.420512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>nl</td>\n",
              "      <td>uk</td>\n",
              "      <td>6.262413e-01</td>\n",
              "      <td>-0.151682</td>\n",
              "      <td>0.248315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>nl</td>\n",
              "      <td>ar</td>\n",
              "      <td>4.441044e-04</td>\n",
              "      <td>0.162542</td>\n",
              "      <td>0.515707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>nl</td>\n",
              "      <td>zh</td>\n",
              "      <td>1.406896e-06</td>\n",
              "      <td>0.296817</td>\n",
              "      <td>0.612244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>es</td>\n",
              "      <td>es</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>es</td>\n",
              "      <td>pt</td>\n",
              "      <td>3.909764e-43</td>\n",
              "      <td>0.899219</td>\n",
              "      <td>0.954009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>es</td>\n",
              "      <td>ca</td>\n",
              "      <td>2.025815e-17</td>\n",
              "      <td>0.624419</td>\n",
              "      <td>0.813973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>es</td>\n",
              "      <td>cs</td>\n",
              "      <td>1.265142e-01</td>\n",
              "      <td>-0.044860</td>\n",
              "      <td>0.346611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>es</td>\n",
              "      <td>ru</td>\n",
              "      <td>8.374434e-02</td>\n",
              "      <td>-0.023913</td>\n",
              "      <td>0.364928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>es</td>\n",
              "      <td>uk</td>\n",
              "      <td>2.346908e-01</td>\n",
              "      <td>-0.080016</td>\n",
              "      <td>0.315183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>es</td>\n",
              "      <td>ar</td>\n",
              "      <td>3.097748e-03</td>\n",
              "      <td>0.104654</td>\n",
              "      <td>0.471126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>es</td>\n",
              "      <td>zh</td>\n",
              "      <td>4.900084e-02</td>\n",
              "      <td>0.001048</td>\n",
              "      <td>0.386370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>pt</td>\n",
              "      <td>pt</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>pt</td>\n",
              "      <td>ca</td>\n",
              "      <td>4.847125e-25</td>\n",
              "      <td>0.748441</td>\n",
              "      <td>0.880025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>pt</td>\n",
              "      <td>cs</td>\n",
              "      <td>3.871321e-01</td>\n",
              "      <td>-0.113253</td>\n",
              "      <td>0.284651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>pt</td>\n",
              "      <td>ru</td>\n",
              "      <td>1.994299e-01</td>\n",
              "      <td>-0.070227</td>\n",
              "      <td>0.324023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>pt</td>\n",
              "      <td>uk</td>\n",
              "      <td>6.094020e-01</td>\n",
              "      <td>-0.149264</td>\n",
              "      <td>0.250634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>pt</td>\n",
              "      <td>ar</td>\n",
              "      <td>1.472091e-02</td>\n",
              "      <td>0.050306</td>\n",
              "      <td>0.427494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>pt</td>\n",
              "      <td>zh</td>\n",
              "      <td>7.112169e-02</td>\n",
              "      <td>-0.016060</td>\n",
              "      <td>0.371719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>ca</td>\n",
              "      <td>ca</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>ca</td>\n",
              "      <td>cs</td>\n",
              "      <td>1.391470e-01</td>\n",
              "      <td>-0.049949</td>\n",
              "      <td>0.342116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>ca</td>\n",
              "      <td>ru</td>\n",
              "      <td>3.975717e-02</td>\n",
              "      <td>0.010213</td>\n",
              "      <td>0.394139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>ca</td>\n",
              "      <td>uk</td>\n",
              "      <td>4.238855e-01</td>\n",
              "      <td>-0.119910</td>\n",
              "      <td>0.278438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>ca</td>\n",
              "      <td>ar</td>\n",
              "      <td>8.667097e-04</td>\n",
              "      <td>0.143603</td>\n",
              "      <td>0.501330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>ca</td>\n",
              "      <td>zh</td>\n",
              "      <td>1.707746e-03</td>\n",
              "      <td>0.123377</td>\n",
              "      <td>0.485753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>cs</td>\n",
              "      <td>cs</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>cs</td>\n",
              "      <td>ru</td>\n",
              "      <td>2.155933e-04</td>\n",
              "      <td>0.182025</td>\n",
              "      <td>0.530294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>cs</td>\n",
              "      <td>uk</td>\n",
              "      <td>4.499944e-06</td>\n",
              "      <td>0.272938</td>\n",
              "      <td>0.595737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>cs</td>\n",
              "      <td>ar</td>\n",
              "      <td>1.478308e-05</td>\n",
              "      <td>0.247042</td>\n",
              "      <td>0.577522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>cs</td>\n",
              "      <td>zh</td>\n",
              "      <td>4.221689e-06</td>\n",
              "      <td>0.274284</td>\n",
              "      <td>0.596675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>ru</td>\n",
              "      <td>ru</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>ru</td>\n",
              "      <td>uk</td>\n",
              "      <td>2.741550e-29</td>\n",
              "      <td>0.797695</td>\n",
              "      <td>0.904917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>ru</td>\n",
              "      <td>ar</td>\n",
              "      <td>9.996589e-18</td>\n",
              "      <td>0.630559</td>\n",
              "      <td>0.817363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>ru</td>\n",
              "      <td>zh</td>\n",
              "      <td>5.184376e-06</td>\n",
              "      <td>0.269937</td>\n",
              "      <td>0.593643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>uk</td>\n",
              "      <td>uk</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>uk</td>\n",
              "      <td>ar</td>\n",
              "      <td>7.367651e-18</td>\n",
              "      <td>0.633177</td>\n",
              "      <td>0.818804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>uk</td>\n",
              "      <td>zh</td>\n",
              "      <td>1.670921e-06</td>\n",
              "      <td>0.293368</td>\n",
              "      <td>0.609876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>ar</td>\n",
              "      <td>ar</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>ar</td>\n",
              "      <td>zh</td>\n",
              "      <td>6.817472e-17</td>\n",
              "      <td>0.613594</td>\n",
              "      <td>0.807966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>zh</td>\n",
              "      <td>zh</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-78f4cffa-4e1c-41ef-a3ad-42c7b9413bc1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-78f4cffa-4e1c-41ef-a3ad-42c7b9413bc1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-78f4cffa-4e1c-41ef-a3ad-42c7b9413bc1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e11d0155-092b-48d0-a466-9686ec27a5e7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e11d0155-092b-48d0-a466-9686ec27a5e7')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e11d0155-092b-48d0-a466-9686ec27a5e7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#result only for not statistically significant correlations (whether these are high or low)\n",
        "for (src, trg) in itertools.combinations_with_replacement(langs, 2):\n",
        "  res = stats.pearsonr(df[src], df[trg])\n",
        "  if (res.pvalue < 0.05): continue #or (res.statistic < 0.1)\n",
        "  print(src, trg)\n",
        "  print(res)\n",
        "  print(res.confidence_interval(0.95))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH5EKQpcoa8O",
        "outputId": "03387ec4-ada6-4742-d804-e6ab6af17805"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en cs\n",
            "PearsonRResult(statistic=-0.16132211381149508, pvalue=0.11636259493873008)\n",
            "ConfidenceInterval(low=-0.3504727815031259, high=0.04047283619885538)\n",
            "en zh\n",
            "PearsonRResult(statistic=-0.0033334191839927835, pvalue=0.9742862799054467)\n",
            "ConfidenceInterval(low=-0.20368324142318447, high=0.19728437119169254)\n",
            "de ru\n",
            "PearsonRResult(statistic=0.19897529354311413, pvalue=0.051956517078766494)\n",
            "ConfidenceInterval(low=-0.0015734173288190019, high=0.3841373279247955)\n",
            "de uk\n",
            "PearsonRResult(statistic=0.0784088274615922, pvalue=0.4476414481117586)\n",
            "ConfidenceInterval(low=-0.12402680197468402, high=0.2745784352519565)\n",
            "de ar\n",
            "PearsonRResult(statistic=0.19538045105555943, pvalue=0.056432378613650254)\n",
            "ConfidenceInterval(low=-0.005313633985096929, high=0.38094440905631666)\n",
            "de zh\n",
            "PearsonRResult(statistic=0.16558345384819556, pvalue=0.10690142122444854)\n",
            "ConfidenceInterval(low=-0.03610093913051338, high=0.3543074006052856)\n",
            "nl cs\n",
            "PearsonRResult(statistic=0.06262311443806762, pvalue=0.544424113712105)\n",
            "ConfidenceInterval(low=-0.13961571291285688, high=0.25984666990359234)\n",
            "nl uk\n",
            "PearsonRResult(statistic=0.05033451622061072, pvalue=0.6262412754065478)\n",
            "ConfidenceInterval(low=-0.15168210749231484, high=0.24831462766998777)\n",
            "es cs\n",
            "PearsonRResult(statistic=0.15703796364973704, pvalue=0.1265142157387347)\n",
            "ConfidenceInterval(low=-0.04486036014507727, high=0.3466112331989666)\n",
            "es ru\n",
            "PearsonRResult(statistic=0.1774231968527462, pvalue=0.08374434398336683)\n",
            "ConfidenceInterval(low=-0.02391336659734391, high=0.36492832791489155)\n",
            "es uk\n",
            "PearsonRResult(statistic=0.12243359469404205, pvalue=0.23469083076426356)\n",
            "ConfidenceInterval(low=-0.08001645080230806, high=0.31518298980437004)\n",
            "pt cs\n",
            "PearsonRResult(statistic=0.08925923538701958, pvalue=0.38713208696259477)\n",
            "ConfidenceInterval(low=-0.11325340393305397, high=0.2846512819733643)\n",
            "pt ru\n",
            "PearsonRResult(statistic=0.13211950282583493, pvalue=0.19942994560199478)\n",
            "ConfidenceInterval(low=-0.07022661141424018, high=0.32402270055999144)\n",
            "pt uk\n",
            "PearsonRResult(statistic=0.05280157623907532, pvalue=0.6094019517210059)\n",
            "ConfidenceInterval(low=-0.14926447812988122, high=0.2506343124548697)\n",
            "pt zh\n",
            "PearsonRResult(statistic=0.18502130680405704, pvalue=0.07112168862076806)\n",
            "ConfidenceInterval(low=-0.0160603843482128, high=0.3717186557588268)\n",
            "ca cs\n",
            "PearsonRResult(statistic=0.15205988192723893, pvalue=0.1391470157702135)\n",
            "ConfidenceInterval(low=-0.049948794997968345, high=0.3421161192189203)\n",
            "ca uk\n",
            "PearsonRResult(statistic=0.08256092754928215, pvalue=0.4238854642530966)\n",
            "ConfidenceInterval(low=-0.11990980060922482, high=0.2784380798792669)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp['subject_id'] = temp['Model'] + '_' + temp['Train LLM']\n",
        "temp['within'] = temp['Train Language']"
      ],
      "metadata": {
        "id": "bWLHZKnca9_7"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(AnovaRM(data=temp, depvar='en', subject='subject_id', within=['within']).fit())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQjHhFPjnxi4",
        "outputId": "3f2c4c89-b95a-4567-cc7d-2b41e16cd3f8"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                Anova\n",
            "=====================================\n",
            "       F Value Num DF  Den DF  Pr > F\n",
            "-------------------------------------\n",
            "within 49.4569 4.0000 124.0000 0.0000\n",
            "=====================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for lang in sorted_languages:\n",
        "  print(lang)\n",
        "  display(AnovaRM(data=temp, depvar=lang, subject='subject_id', within=['within']).fit().anova_table.style.format(na_rep=0, precision=25))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925
        },
        "id": "B2Hh3SMtV7PA",
        "outputId": "bae5b5d8-3314-4596-8436-dd48266aa9e4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001af5600>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_89fa7\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_89fa7_level0_col0\" class=\"col_heading level0 col0\" >F Value</th>\n",
              "      <th id=\"T_89fa7_level0_col1\" class=\"col_heading level0 col1\" >Num DF</th>\n",
              "      <th id=\"T_89fa7_level0_col2\" class=\"col_heading level0 col2\" >Den DF</th>\n",
              "      <th id=\"T_89fa7_level0_col3\" class=\"col_heading level0 col3\" >Pr > F</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_89fa7_level0_row0\" class=\"row_heading level0 row0\" >within</th>\n",
              "      <td id=\"T_89fa7_row0_col0\" class=\"data row0 col0\" >49.4568675396506804986529460</td>\n",
              "      <td id=\"T_89fa7_row0_col1\" class=\"data row0 col1\" >4.0000000000000000000000000</td>\n",
              "      <td id=\"T_89fa7_row0_col2\" class=\"data row0 col2\" >124.0000000000000000000000000</td>\n",
              "      <td id=\"T_89fa7_row0_col3\" class=\"data row0 col3\" >0.0000000000000000000000008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "de\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001cee950>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_041eb\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_041eb_level0_col0\" class=\"col_heading level0 col0\" >F Value</th>\n",
              "      <th id=\"T_041eb_level0_col1\" class=\"col_heading level0 col1\" >Num DF</th>\n",
              "      <th id=\"T_041eb_level0_col2\" class=\"col_heading level0 col2\" >Den DF</th>\n",
              "      <th id=\"T_041eb_level0_col3\" class=\"col_heading level0 col3\" >Pr > F</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_041eb_level0_row0\" class=\"row_heading level0 row0\" >within</th>\n",
              "      <td id=\"T_041eb_row0_col0\" class=\"data row0 col0\" >16.6094980607194990795960621</td>\n",
              "      <td id=\"T_041eb_row0_col1\" class=\"data row0 col1\" >4.0000000000000000000000000</td>\n",
              "      <td id=\"T_041eb_row0_col2\" class=\"data row0 col2\" >124.0000000000000000000000000</td>\n",
              "      <td id=\"T_041eb_row0_col3\" class=\"data row0 col3\" >0.0000000000634022044900744</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001cee950>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_e180d\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_e180d_level0_col0\" class=\"col_heading level0 col0\" >F Value</th>\n",
              "      <th id=\"T_e180d_level0_col1\" class=\"col_heading level0 col1\" >Num DF</th>\n",
              "      <th id=\"T_e180d_level0_col2\" class=\"col_heading level0 col2\" >Den DF</th>\n",
              "      <th id=\"T_e180d_level0_col3\" class=\"col_heading level0 col3\" >Pr > F</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_e180d_level0_row0\" class=\"row_heading level0 row0\" >within</th>\n",
              "      <td id=\"T_e180d_row0_col0\" class=\"data row0 col0\" >12.3872692784732389981172673</td>\n",
              "      <td id=\"T_e180d_row0_col1\" class=\"data row0 col1\" >4.0000000000000000000000000</td>\n",
              "      <td id=\"T_e180d_row0_col2\" class=\"data row0 col2\" >124.0000000000000000000000000</td>\n",
              "      <td id=\"T_e180d_row0_col3\" class=\"data row0 col3\" >0.0000000165894866122734910</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "es\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001af6770>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_defe5\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_defe5_level0_col0\" class=\"col_heading level0 col0\" >F Value</th>\n",
              "      <th id=\"T_defe5_level0_col1\" class=\"col_heading level0 col1\" >Num DF</th>\n",
              "      <th id=\"T_defe5_level0_col2\" class=\"col_heading level0 col2\" >Den DF</th>\n",
              "      <th id=\"T_defe5_level0_col3\" class=\"col_heading level0 col3\" >Pr > F</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_defe5_level0_row0\" class=\"row_heading level0 row0\" >within</th>\n",
              "      <td id=\"T_defe5_row0_col0\" class=\"data row0 col0\" >49.2163144366804488072375534</td>\n",
              "      <td id=\"T_defe5_row0_col1\" class=\"data row0 col1\" >4.0000000000000000000000000</td>\n",
              "      <td id=\"T_defe5_row0_col2\" class=\"data row0 col2\" >124.0000000000000000000000000</td>\n",
              "      <td id=\"T_defe5_row0_col3\" class=\"data row0 col3\" >0.0000000000000000000000010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001cee950>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_de282\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_de282_level0_col0\" class=\"col_heading level0 col0\" >F Value</th>\n",
              "      <th id=\"T_de282_level0_col1\" class=\"col_heading level0 col1\" >Num DF</th>\n",
              "      <th id=\"T_de282_level0_col2\" class=\"col_heading level0 col2\" >Den DF</th>\n",
              "      <th id=\"T_de282_level0_col3\" class=\"col_heading level0 col3\" >Pr > F</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_de282_level0_row0\" class=\"row_heading level0 row0\" >within</th>\n",
              "      <td id=\"T_de282_row0_col0\" class=\"data row0 col0\" >35.0908732412268378197950369</td>\n",
              "      <td id=\"T_de282_row0_col1\" class=\"data row0 col1\" >4.0000000000000000000000000</td>\n",
              "      <td id=\"T_de282_row0_col2\" class=\"data row0 col2\" >124.0000000000000000000000000</td>\n",
              "      <td id=\"T_de282_row0_col3\" class=\"data row0 col3\" >0.0000000000000000001399889</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ca\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001cee950>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_c503b\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_c503b_level0_col0\" class=\"col_heading level0 col0\" >F Value</th>\n",
              "      <th id=\"T_c503b_level0_col1\" class=\"col_heading level0 col1\" >Num DF</th>\n",
              "      <th id=\"T_c503b_level0_col2\" class=\"col_heading level0 col2\" >Den DF</th>\n",
              "      <th id=\"T_c503b_level0_col3\" class=\"col_heading level0 col3\" >Pr > F</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_c503b_level0_row0\" class=\"row_heading level0 row0\" >within</th>\n",
              "      <td id=\"T_c503b_row0_col0\" class=\"data row0 col0\" >20.6543381170094910714851721</td>\n",
              "      <td id=\"T_c503b_row0_col1\" class=\"data row0 col1\" >4.0000000000000000000000000</td>\n",
              "      <td id=\"T_c503b_row0_col2\" class=\"data row0 col2\" >124.0000000000000000000000000</td>\n",
              "      <td id=\"T_c503b_row0_col3\" class=\"data row0 col3\" >0.0000000000004605465997584</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001cee950>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_a037d\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_a037d_level0_col0\" class=\"col_heading level0 col0\" >F Value</th>\n",
              "      <th id=\"T_a037d_level0_col1\" class=\"col_heading level0 col1\" >Num DF</th>\n",
              "      <th id=\"T_a037d_level0_col2\" class=\"col_heading level0 col2\" >Den DF</th>\n",
              "      <th id=\"T_a037d_level0_col3\" class=\"col_heading level0 col3\" >Pr > F</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_a037d_level0_row0\" class=\"row_heading level0 row0\" >within</th>\n",
              "      <td id=\"T_a037d_row0_col0\" class=\"data row0 col0\" >12.7578696127956945360892860</td>\n",
              "      <td id=\"T_a037d_row0_col1\" class=\"data row0 col1\" >4.0000000000000000000000000</td>\n",
              "      <td id=\"T_a037d_row0_col2\" class=\"data row0 col2\" >124.0000000000000000000000000</td>\n",
              "      <td id=\"T_a037d_row0_col3\" class=\"data row0 col3\" >0.0000000099871265152530278</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ru\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001cee950>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_2cc4a\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_2cc4a_level0_col0\" class=\"col_heading level0 col0\" >F Value</th>\n",
              "      <th id=\"T_2cc4a_level0_col1\" class=\"col_heading level0 col1\" >Num DF</th>\n",
              "      <th id=\"T_2cc4a_level0_col2\" class=\"col_heading level0 col2\" >Den DF</th>\n",
              "      <th id=\"T_2cc4a_level0_col3\" class=\"col_heading level0 col3\" >Pr > F</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_2cc4a_level0_row0\" class=\"row_heading level0 row0\" >within</th>\n",
              "      <td id=\"T_2cc4a_row0_col0\" class=\"data row0 col0\" >48.8756627598690585045915213</td>\n",
              "      <td id=\"T_2cc4a_row0_col1\" class=\"data row0 col1\" >4.0000000000000000000000000</td>\n",
              "      <td id=\"T_2cc4a_row0_col2\" class=\"data row0 col2\" >124.0000000000000000000000000</td>\n",
              "      <td id=\"T_2cc4a_row0_col3\" class=\"data row0 col3\" >0.0000000000000000000000013</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uk\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001af6dd0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_20b01\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_20b01_level0_col0\" class=\"col_heading level0 col0\" >F Value</th>\n",
              "      <th id=\"T_20b01_level0_col1\" class=\"col_heading level0 col1\" >Num DF</th>\n",
              "      <th id=\"T_20b01_level0_col2\" class=\"col_heading level0 col2\" >Den DF</th>\n",
              "      <th id=\"T_20b01_level0_col3\" class=\"col_heading level0 col3\" >Pr > F</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_20b01_level0_row0\" class=\"row_heading level0 row0\" >within</th>\n",
              "      <td id=\"T_20b01_row0_col0\" class=\"data row0 col0\" >40.4784664292084386261194595</td>\n",
              "      <td id=\"T_20b01_row0_col1\" class=\"data row0 col1\" >4.0000000000000000000000000</td>\n",
              "      <td id=\"T_20b01_row0_col2\" class=\"data row0 col2\" >124.0000000000000000000000000</td>\n",
              "      <td id=\"T_20b01_row0_col3\" class=\"data row0 col3\" >0.0000000000000000000011566</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ar\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001cee950>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_a1b3a\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_a1b3a_level0_col0\" class=\"col_heading level0 col0\" >F Value</th>\n",
              "      <th id=\"T_a1b3a_level0_col1\" class=\"col_heading level0 col1\" >Num DF</th>\n",
              "      <th id=\"T_a1b3a_level0_col2\" class=\"col_heading level0 col2\" >Den DF</th>\n",
              "      <th id=\"T_a1b3a_level0_col3\" class=\"col_heading level0 col3\" >Pr > F</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_a1b3a_level0_row0\" class=\"row_heading level0 row0\" >within</th>\n",
              "      <td id=\"T_a1b3a_row0_col0\" class=\"data row0 col0\" >38.5197265218412709941731009</td>\n",
              "      <td id=\"T_a1b3a_row0_col1\" class=\"data row0 col1\" >4.0000000000000000000000000</td>\n",
              "      <td id=\"T_a1b3a_row0_col2\" class=\"data row0 col2\" >124.0000000000000000000000000</td>\n",
              "      <td id=\"T_a1b3a_row0_col3\" class=\"data row0 col3\" >0.0000000000000000000063409</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zh\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001af5600>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_8879d\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_8879d_level0_col0\" class=\"col_heading level0 col0\" >F Value</th>\n",
              "      <th id=\"T_8879d_level0_col1\" class=\"col_heading level0 col1\" >Num DF</th>\n",
              "      <th id=\"T_8879d_level0_col2\" class=\"col_heading level0 col2\" >Den DF</th>\n",
              "      <th id=\"T_8879d_level0_col3\" class=\"col_heading level0 col3\" >Pr > F</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_8879d_level0_row0\" class=\"row_heading level0 row0\" >within</th>\n",
              "      <td id=\"T_8879d_row0_col0\" class=\"data row0 col0\" >19.2015169467462385455291951</td>\n",
              "      <td id=\"T_8879d_row0_col1\" class=\"data row0 col1\" >4.0000000000000000000000000</td>\n",
              "      <td id=\"T_8879d_row0_col2\" class=\"data row0 col2\" >124.0000000000000000000000000</td>\n",
              "      <td id=\"T_8879d_row0_col3\" class=\"data row0 col3\" >0.0000000000025877915947040</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xUWfP3IpZFZ_",
        "outputId": "6506e076-46d8-4c7e-e763-c4f8441cdada"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Train Language         Train LLM                         Model        ar  \\\n",
              "0               en   alpaca-lora-30b  bert-base-multilingual-cased  0.537542   \n",
              "1               en   alpaca-lora-30b                          mGPT  0.402388   \n",
              "2               en   alpaca-lora-30b              mdeberta-v3-base  0.207984   \n",
              "3               en   alpaca-lora-30b             xlm-roberta-large  0.447388   \n",
              "4               en     gpt-3.5-turbo  bert-base-multilingual-cased  0.921465   \n",
              "5               en     gpt-3.5-turbo                          mGPT  0.700916   \n",
              "6               en     gpt-3.5-turbo              mdeberta-v3-base  0.787647   \n",
              "7               en     gpt-3.5-turbo             xlm-roberta-large  0.721820   \n",
              "8               en             gpt-4  bert-base-multilingual-cased  0.841926   \n",
              "9               en             gpt-4                          mGPT  0.694497   \n",
              "10              en             gpt-4              mdeberta-v3-base  0.719857   \n",
              "11              en             gpt-4             xlm-roberta-large  0.565875   \n",
              "12              en         llama-65b  bert-base-multilingual-cased  0.589577   \n",
              "13              en         llama-65b                          mGPT  0.384803   \n",
              "14              en         llama-65b              mdeberta-v3-base  0.511518   \n",
              "15              en         llama-65b             xlm-roberta-large  0.477913   \n",
              "16              en           opt-66b  bert-base-multilingual-cased  0.455733   \n",
              "17              en           opt-66b                          mGPT  0.295934   \n",
              "18              en           opt-66b              mdeberta-v3-base  0.390341   \n",
              "19              en           opt-66b             xlm-roberta-large  0.490988   \n",
              "20              en  opt-iml-max-1.3b  bert-base-multilingual-cased  0.459169   \n",
              "21              en  opt-iml-max-1.3b                          mGPT  0.338008   \n",
              "22              en  opt-iml-max-1.3b              mdeberta-v3-base  0.793226   \n",
              "23              en  opt-iml-max-1.3b             xlm-roberta-large  0.580995   \n",
              "24              en  text-davinci-003  bert-base-multilingual-cased  0.764651   \n",
              "25              en  text-davinci-003                          mGPT  0.539324   \n",
              "26              en  text-davinci-003              mdeberta-v3-base  0.543187   \n",
              "27              en  text-davinci-003             xlm-roberta-large  0.503535   \n",
              "28              en        vicuna-13b  bert-base-multilingual-cased  0.791310   \n",
              "29              en        vicuna-13b                          mGPT  0.408638   \n",
              "30              en        vicuna-13b              mdeberta-v3-base  0.178446   \n",
              "31              en        vicuna-13b             xlm-roberta-large  0.386417   \n",
              "32              es   alpaca-lora-30b  bert-base-multilingual-cased  0.601879   \n",
              "33              es   alpaca-lora-30b                          mGPT  0.697994   \n",
              "34              es   alpaca-lora-30b              mdeberta-v3-base  0.825526   \n",
              "35              es   alpaca-lora-30b             xlm-roberta-large  0.699932   \n",
              "36              es     gpt-3.5-turbo  bert-base-multilingual-cased  0.889624   \n",
              "37              es     gpt-3.5-turbo                          mGPT  0.894145   \n",
              "38              es     gpt-3.5-turbo              mdeberta-v3-base  0.928117   \n",
              "39              es     gpt-3.5-turbo             xlm-roberta-large  0.767102   \n",
              "40              es             gpt-4  bert-base-multilingual-cased  0.876361   \n",
              "41              es             gpt-4                          mGPT  0.849110   \n",
              "42              es             gpt-4              mdeberta-v3-base  0.933106   \n",
              "43              es             gpt-4             xlm-roberta-large  0.512410   \n",
              "44              es         llama-65b  bert-base-multilingual-cased  0.795561   \n",
              "45              es         llama-65b                          mGPT  0.591239   \n",
              "46              es         llama-65b              mdeberta-v3-base  0.913549   \n",
              "47              es         llama-65b             xlm-roberta-large  0.846051   \n",
              "48              es           opt-66b  bert-base-multilingual-cased  0.737808   \n",
              "49              es           opt-66b                          mGPT  0.390615   \n",
              "50              es           opt-66b              mdeberta-v3-base  0.913856   \n",
              "51              es           opt-66b             xlm-roberta-large  0.605757   \n",
              "52              es  opt-iml-max-1.3b  bert-base-multilingual-cased  0.953707   \n",
              "53              es  opt-iml-max-1.3b                          mGPT  0.943433   \n",
              "54              es  opt-iml-max-1.3b              mdeberta-v3-base  0.993148   \n",
              "55              es  opt-iml-max-1.3b             xlm-roberta-large  0.979411   \n",
              "56              es  text-davinci-003  bert-base-multilingual-cased  0.731949   \n",
              "57              es  text-davinci-003                          mGPT  0.718775   \n",
              "58              es  text-davinci-003              mdeberta-v3-base  0.643584   \n",
              "59              es  text-davinci-003             xlm-roberta-large  0.799302   \n",
              "60              es        vicuna-13b  bert-base-multilingual-cased  0.862826   \n",
              "61              es        vicuna-13b                          mGPT  0.723426   \n",
              "62              es        vicuna-13b              mdeberta-v3-base  0.678396   \n",
              "63              es        vicuna-13b             xlm-roberta-large  0.844734   \n",
              "64              ru   alpaca-lora-30b  bert-base-multilingual-cased  0.602643   \n",
              "65              ru   alpaca-lora-30b                          mGPT  0.563543   \n",
              "66              ru   alpaca-lora-30b              mdeberta-v3-base  0.971599   \n",
              "67              ru   alpaca-lora-30b             xlm-roberta-large  0.918018   \n",
              "68              ru     gpt-3.5-turbo  bert-base-multilingual-cased  0.941563   \n",
              "69              ru     gpt-3.5-turbo                          mGPT  0.784573   \n",
              "70              ru     gpt-3.5-turbo              mdeberta-v3-base  0.936497   \n",
              "71              ru     gpt-3.5-turbo             xlm-roberta-large  0.933217   \n",
              "72              ru             gpt-4  bert-base-multilingual-cased  0.866685   \n",
              "73              ru             gpt-4                          mGPT  0.897834   \n",
              "74              ru             gpt-4              mdeberta-v3-base  0.934891   \n",
              "75              ru             gpt-4             xlm-roberta-large  0.921514   \n",
              "76              ru         llama-65b  bert-base-multilingual-cased  0.707838   \n",
              "77              ru         llama-65b                          mGPT  0.534740   \n",
              "78              ru         llama-65b              mdeberta-v3-base  0.762908   \n",
              "79              ru         llama-65b             xlm-roberta-large  0.885839   \n",
              "80              ru           opt-66b  bert-base-multilingual-cased  0.759294   \n",
              "81              ru           opt-66b                          mGPT  0.971423   \n",
              "82              ru           opt-66b              mdeberta-v3-base  0.939331   \n",
              "83              ru           opt-66b             xlm-roberta-large  0.954589   \n",
              "84              ru  opt-iml-max-1.3b  bert-base-multilingual-cased  0.958780   \n",
              "85              ru  opt-iml-max-1.3b                          mGPT  0.963964   \n",
              "86              ru  opt-iml-max-1.3b              mdeberta-v3-base  0.890215   \n",
              "87              ru  opt-iml-max-1.3b             xlm-roberta-large  0.993146   \n",
              "88              ru  text-davinci-003  bert-base-multilingual-cased  0.748031   \n",
              "89              ru  text-davinci-003                          mGPT  0.879773   \n",
              "90              ru  text-davinci-003              mdeberta-v3-base  0.880012   \n",
              "91              ru  text-davinci-003             xlm-roberta-large  0.866381   \n",
              "92              ru        vicuna-13b  bert-base-multilingual-cased  0.545024   \n",
              "93              ru        vicuna-13b                          mGPT  0.684569   \n",
              "94              ru        vicuna-13b              mdeberta-v3-base  0.988313   \n",
              "95              ru        vicuna-13b             xlm-roberta-large  0.971604   \n",
              "96             all   alpaca-lora-30b  bert-base-multilingual-cased  0.509603   \n",
              "97             all   alpaca-lora-30b                          mGPT  0.695488   \n",
              "98             all   alpaca-lora-30b              mdeberta-v3-base  0.843820   \n",
              "99             all   alpaca-lora-30b             xlm-roberta-large  0.774189   \n",
              "100            all     gpt-3.5-turbo  bert-base-multilingual-cased  0.848292   \n",
              "101            all     gpt-3.5-turbo                          mGPT  0.885649   \n",
              "102            all     gpt-3.5-turbo              mdeberta-v3-base  0.946576   \n",
              "103            all     gpt-3.5-turbo             xlm-roberta-large  0.928008   \n",
              "104            all             gpt-4  bert-base-multilingual-cased  0.941559   \n",
              "105            all             gpt-4                          mGPT  0.891135   \n",
              "106            all             gpt-4              mdeberta-v3-base  0.938224   \n",
              "107            all             gpt-4             xlm-roberta-large  0.959932   \n",
              "108            all         llama-65b  bert-base-multilingual-cased  0.782089   \n",
              "109            all         llama-65b                          mGPT  0.891892   \n",
              "110            all         llama-65b              mdeberta-v3-base  0.800609   \n",
              "111            all         llama-65b             xlm-roberta-large  0.744559   \n",
              "112            all           opt-66b  bert-base-multilingual-cased  0.796683   \n",
              "113            all           opt-66b                          mGPT  0.906852   \n",
              "114            all           opt-66b              mdeberta-v3-base  0.891294   \n",
              "115            all           opt-66b             xlm-roberta-large  0.941006   \n",
              "116            all  opt-iml-max-1.3b  bert-base-multilingual-cased  0.950318   \n",
              "117            all  opt-iml-max-1.3b                          mGPT  0.976027   \n",
              "118            all  opt-iml-max-1.3b              mdeberta-v3-base  0.976027   \n",
              "119            all  opt-iml-max-1.3b             xlm-roberta-large  0.986297   \n",
              "120            all  text-davinci-003  bert-base-multilingual-cased  0.824558   \n",
              "121            all  text-davinci-003                          mGPT  0.846222   \n",
              "122            all  text-davinci-003              mdeberta-v3-base  0.759815   \n",
              "123            all  text-davinci-003             xlm-roberta-large  0.670553   \n",
              "124            all        vicuna-13b  bert-base-multilingual-cased  0.829439   \n",
              "125            all        vicuna-13b                          mGPT  0.846662   \n",
              "126            all        vicuna-13b              mdeberta-v3-base  0.854263   \n",
              "127            all        vicuna-13b             xlm-roberta-large  0.880938   \n",
              "128            en3   alpaca-lora-30b  bert-base-multilingual-cased  0.495071   \n",
              "129            en3   alpaca-lora-30b                          mGPT  0.293049   \n",
              "130            en3   alpaca-lora-30b              mdeberta-v3-base  0.176374   \n",
              "131            en3   alpaca-lora-30b             xlm-roberta-large  0.330592   \n",
              "132            en3     gpt-3.5-turbo  bert-base-multilingual-cased  0.944893   \n",
              "133            en3     gpt-3.5-turbo                          mGPT  0.678622   \n",
              "134            en3     gpt-3.5-turbo              mdeberta-v3-base  0.814844   \n",
              "135            en3     gpt-3.5-turbo             xlm-roberta-large  0.919866   \n",
              "136            en3             gpt-4  bert-base-multilingual-cased  0.834971   \n",
              "137            en3             gpt-4                          mGPT  0.848069   \n",
              "138            en3             gpt-4              mdeberta-v3-base  0.819887   \n",
              "139            en3             gpt-4             xlm-roberta-large  0.877966   \n",
              "140            en3         llama-65b  bert-base-multilingual-cased  0.655237   \n",
              "141            en3         llama-65b                          mGPT  0.336877   \n",
              "142            en3         llama-65b              mdeberta-v3-base  0.747725   \n",
              "143            en3         llama-65b             xlm-roberta-large  0.453154   \n",
              "144            en3           opt-66b  bert-base-multilingual-cased  0.412144   \n",
              "145            en3           opt-66b                          mGPT  0.198296   \n",
              "146            en3           opt-66b              mdeberta-v3-base  0.261175   \n",
              "147            en3           opt-66b             xlm-roberta-large  0.418210   \n",
              "148            en3  opt-iml-max-1.3b  bert-base-multilingual-cased  0.570502   \n",
              "149            en3  opt-iml-max-1.3b                          mGPT  0.260741   \n",
              "150            en3  opt-iml-max-1.3b              mdeberta-v3-base  0.730906   \n",
              "151            en3  opt-iml-max-1.3b             xlm-roberta-large  0.669366   \n",
              "152            en3  text-davinci-003  bert-base-multilingual-cased  0.785305   \n",
              "153            en3  text-davinci-003                          mGPT  0.613629   \n",
              "154            en3  text-davinci-003              mdeberta-v3-base  0.579101   \n",
              "155            en3  text-davinci-003             xlm-roberta-large  0.389863   \n",
              "156            en3        vicuna-13b  bert-base-multilingual-cased  0.767944   \n",
              "157            en3        vicuna-13b                          mGPT  0.388148   \n",
              "158            en3        vicuna-13b              mdeberta-v3-base  0.257780   \n",
              "159            en3        vicuna-13b             xlm-roberta-large  0.406231   \n",
              "\n",
              "           ca        cs        de        en        es        nl        pt  \\\n",
              "0    0.827078  0.854560  0.891714  0.956665  0.751676  0.856264  0.805481   \n",
              "1    0.808877  0.613177  0.876305  0.963891  0.738976  0.879058  0.833282   \n",
              "2    0.859243  0.769121  0.900331  0.943947  0.774384  0.897682  0.857008   \n",
              "3    0.848775  0.871277  0.932405  0.980144  0.631892  0.770589  0.712993   \n",
              "4    0.890380  0.914981  0.902026  0.978339  0.854525  0.934819  0.912430   \n",
              "5    0.926614  0.337026  0.907073  0.989169  0.850593  0.943226  0.914732   \n",
              "6    0.559878  0.780318  0.805155  0.717787  0.602325  0.779041  0.678157   \n",
              "7    0.973319  0.721150  0.939164  0.983754  0.786081  0.922878  0.928099   \n",
              "8    0.926529  0.759176  0.816308  0.976534  0.826112  0.926544  0.870419   \n",
              "9    0.788771  0.386602  0.836477  0.990975  0.795370  0.833370  0.865765   \n",
              "10   0.973331  0.862618  0.894392  0.825513  0.943480  0.933106  0.931852   \n",
              "11   0.934778  0.498656  0.892721  0.989170  0.859221  0.949913  0.926661   \n",
              "12   0.606979  0.695458  0.689542  0.901658  0.799832  0.577084  0.675244   \n",
              "13   0.350849  0.689755  0.711503  0.903682  0.524759  0.410240  0.483544   \n",
              "14   0.405992  0.830677  0.716159  0.900289  0.607432  0.459106  0.541285   \n",
              "15   0.489240  0.850697  0.798271  0.916509  0.724079  0.436761  0.608223   \n",
              "16   0.458569  0.610699  0.588965  0.859176  0.539436  0.426829  0.563455   \n",
              "17   0.504866  0.395400  0.635131  0.893075  0.506420  0.494248  0.601702   \n",
              "18   0.518131  0.627795  0.656829  0.853764  0.605320  0.492703  0.643090   \n",
              "19   0.461322  0.549451  0.673485  0.882653  0.561258  0.418667  0.589688   \n",
              "20   0.517135  0.579223  0.591575  0.906132  0.514289  0.513633  0.472087   \n",
              "21   0.513427  0.641027  0.745042  0.958480  0.528383  0.695527  0.521178   \n",
              "22   0.973103  1.000000  0.960929  0.925661  0.902335  0.961500  0.902473   \n",
              "23   0.856666  0.865734  0.841577  0.943910  0.726786  0.768422  0.699108   \n",
              "24   0.901368  0.853679  0.855428  0.969314  0.774992  0.903165  0.840752   \n",
              "25   0.919928  0.362297  0.893556  0.978335  0.767779  0.902670  0.838866   \n",
              "26   0.639753  0.823919  0.856322  0.951251  0.577358  0.759612  0.633527   \n",
              "27   0.921334  0.696479  0.891345  0.985559  0.771487  0.897266  0.823210   \n",
              "28   0.885598  0.801202  0.844550  0.974726  0.785887  0.936540  0.851997   \n",
              "29   0.788775  0.504333  0.851064  0.976534  0.663255  0.887594  0.807120   \n",
              "30   0.500000  0.501832  0.629463  0.777510  0.475434  0.619061  0.507817   \n",
              "31   0.941659  0.489440  0.908579  0.980144  0.686643  0.840000  0.716308   \n",
              "32   0.894965  0.810758  0.876534  0.751693  0.909150  0.894101  0.894326   \n",
              "33   0.863938  0.698896  0.900098  0.658906  0.945195  0.708766  0.940350   \n",
              "34   0.733757  0.911024  0.917144  0.877062  0.825619  0.803750  0.777745   \n",
              "35   0.834861  0.900833  0.949315  0.929459  0.928069  0.953177  0.930124   \n",
              "36   0.936548  0.899715  0.890037  0.916880  0.926344  0.923169  0.885839   \n",
              "37   0.964984  0.355178  0.930710  0.839701  0.969155  0.897390  0.955613   \n",
              "38   0.881723  0.966649  0.957743  0.936777  0.957037  0.910947  0.908940   \n",
              "39   0.954955  0.860860  0.853948  0.980144  0.974282  0.961602  0.967608   \n",
              "40   0.885192  0.929950  0.880837  0.730575  0.927672  0.907483  0.867887   \n",
              "41   0.870498  0.337026  0.918737  0.947633  0.982867  0.924467  0.982956   \n",
              "42   0.889724  0.954909  0.935623  0.882561  0.919452  0.933140  0.902705   \n",
              "43   0.959946  0.568764  0.801054  0.972914  0.981155  0.961602  0.967626   \n",
              "44   0.917710  0.964457  0.771126  0.361111  0.991388  0.666485  0.979375   \n",
              "45   0.689859  0.860581  0.754042  0.350532  0.967288  0.360353  0.950147   \n",
              "46   0.926183  0.988152  0.960824  0.625957  0.986219  0.659130  0.977658   \n",
              "47   0.830698  0.983077  0.955682  0.668031  0.987945  0.458293  0.972497   \n",
              "48   0.760815  0.644272  0.587242  0.470484  0.765752  0.703752  0.766234   \n",
              "49   0.729738  0.455011  0.695724  0.372230  0.799220  0.386565  0.594937   \n",
              "50   0.850023  0.988333  0.949266  0.624038  0.876242  0.853753  0.787190   \n",
              "51   0.745234  0.876505  0.810358  0.444280  0.812310  0.470239  0.633545   \n",
              "52   0.969710  0.963200  0.867572  0.683951  0.948185  0.969888  0.882016   \n",
              "53   0.961329  0.920996  0.993208  0.404483  0.987905  0.840959  0.981028   \n",
              "54   0.986553  0.993310  0.871046  0.586456  0.982728  0.932942  0.982758   \n",
              "55   0.991594  0.858342  0.988115  0.619244  0.981002  0.979933  0.979310   \n",
              "56   0.904712  0.883332  0.860236  0.760513  0.931246  0.889246  0.863110   \n",
              "57   0.802038  0.383186  0.927360  0.903856  0.965721  0.938161  0.916500   \n",
              "58   0.761469  0.924799  0.923908  0.940433  0.805888  0.906504  0.848353   \n",
              "59   0.926402  0.948212  0.950973  0.942226  0.977716  0.973283  0.935165   \n",
              "60   0.914947  0.770478  0.834088  0.750260  0.928081  0.899598  0.882343   \n",
              "61   0.871649  0.542705  0.920602  0.673940  0.967438  0.688021  0.916407   \n",
              "62   0.899356  0.838727  0.905250  0.916859  0.951998  0.875332  0.895597   \n",
              "63   0.878868  0.668109  0.861400  0.906107  0.943445  0.826702  0.895936   \n",
              "64   0.632650  0.806734  0.740883  0.463444  0.657224  0.670451  0.656725   \n",
              "65   0.412312  0.672520  0.871620  0.527200  0.771379  0.532546  0.791988   \n",
              "66   0.812253  0.984999  0.940874  0.854983  0.888526  0.886782  0.819913   \n",
              "67   0.898299  0.969973  0.944230  0.916940  0.905788  0.943052  0.899260   \n",
              "68   0.867992  0.904463  0.853816  0.781013  0.875397  0.878277  0.837753   \n",
              "69   0.885671  0.355178  0.907050  0.881669  0.922780  0.922999  0.904259   \n",
              "70   0.838727  0.981664  0.925553  0.960288  0.929596  0.914644  0.902296   \n",
              "71   0.943227  0.988333  0.939133  0.967505  0.943465  0.946552  0.914272   \n",
              "72   0.706686  0.782835  0.753498  0.495834  0.680546  0.751414  0.691832   \n",
              "73   0.897758  0.358748  0.918762  0.871267  0.938269  0.924571  0.926369   \n",
              "74   0.555606  0.878788  0.922123  0.925886  0.761128  0.760885  0.693175   \n",
              "75   0.959956  0.995000  0.939120  0.976532  0.934855  0.963270  0.936803   \n",
              "76   0.530048  0.875015  0.647655  0.332527  0.821116  0.519080  0.604306   \n",
              "77   0.332589  0.615361  0.647033  0.332527  0.496365  0.340351  0.484719   \n",
              "78   0.427667  0.984771  0.854020  0.382439  0.730500  0.414468  0.538322   \n",
              "79   0.570539  0.986464  0.916035  0.370507  0.502410  0.367375  0.582594   \n",
              "80   0.606131  0.767321  0.607818  0.471954  0.642337  0.642806  0.673385   \n",
              "81   0.512291  0.577647  0.399889  0.347268  0.426150  0.604990  0.416472   \n",
              "82   0.742819  0.983329  0.486039  0.341306  0.339394  0.625486  0.361007   \n",
              "83   0.855203  0.991666  0.554446  0.345255  0.547415  0.773418  0.616838   \n",
              "84   0.542001  0.870710  0.507309  0.333333  0.441679  0.537946  0.415349   \n",
              "85   0.658299  0.359651  0.561295  0.374969  0.387302  0.837362  0.437818   \n",
              "86   0.511446  0.937863  0.576746  0.333333  0.392633  0.622961  0.398365   \n",
              "87   0.727486  0.989965  0.755102  0.337332  0.394444  0.863783  0.443158   \n",
              "88   0.699821  0.647573  0.808713  0.724933  0.816685  0.715688  0.764468   \n",
              "89   0.637560  0.355178  0.862949  0.683093  0.869563  0.798742  0.799616   \n",
              "90   0.439919  0.687666  0.583750  0.345255  0.518355  0.484494  0.513155   \n",
              "91   0.443979  0.742229  0.711712  0.377834  0.567945  0.589670  0.569955   \n",
              "92   0.362297  0.572402  0.454921  0.333333  0.475122  0.354810  0.372091   \n",
              "93   0.340698  0.865946  0.857633  0.372237  0.618774  0.366299  0.549896   \n",
              "94   0.797985  0.938098  0.903297  0.813637  0.902333  0.875988  0.846442   \n",
              "95   0.752577  0.927963  0.939120  0.857196  0.929760  0.697895  0.841560   \n",
              "96   0.913332  0.887860  0.913839  0.965698  0.940064  0.916148  0.911300   \n",
              "97   0.883328  0.648040  0.928988  0.958483  0.931506  0.821486  0.933449   \n",
              "98   0.806245  0.890609  0.872700  0.907484  0.864293  0.804425  0.836376   \n",
              "99   0.928309  0.959972  0.957769  0.985559  0.958900  0.948084  0.952183   \n",
              "100  0.974999  0.963332  0.939072  0.981949  0.977734  0.981635  0.950596   \n",
              "101  0.919799  0.355178  0.896361  0.980143  0.946918  0.956594  0.936968   \n",
              "102  0.871004  0.976666  0.957769  0.974728  0.928041  0.897707  0.903930   \n",
              "103  0.985000  0.988332  0.942473  0.990974  0.984581  0.979967  0.972736   \n",
              "104  0.956608  0.953308  0.925458  0.992780  0.948512  0.968277  0.945291   \n",
              "105  0.944988  0.340698  0.901432  0.992779  0.964017  0.924571  0.948731   \n",
              "106  0.931459  0.988333  0.979728  0.945747  0.950183  0.958217  0.919479   \n",
              "107  0.973326  0.985000  0.969595  0.994585  0.969142  0.981636  0.982950   \n",
              "108  0.864468  0.928762  0.786738  0.907606  0.987940  0.783536  0.944907   \n",
              "109  0.885282  0.950910  0.905754  0.929281  0.986227  0.719234  0.969059   \n",
              "110  0.888725  0.983078  0.806141  0.900354  0.982770  0.645039  0.953522   \n",
              "111  0.801332  0.937253  0.882691  0.898359  0.991388  0.467301  0.944806   \n",
              "112  0.721942  0.767449  0.728228  0.871580  0.809685  0.688022  0.825600   \n",
              "113  0.844007  0.801006  0.818552  0.904262  0.862601  0.759110  0.801186   \n",
              "114  0.939626  1.000000  0.897798  0.847868  0.884706  0.876344  0.864558   \n",
              "115  0.951433  0.996667  0.834821  0.887941  0.869471  0.895853  0.921224   \n",
              "116  0.932673  0.959830  0.838827  0.920568  0.921907  0.900615  0.922375   \n",
              "117  0.959650  0.924281  0.952422  0.956678  0.979273  0.956452  0.970687   \n",
              "118  0.988235  1.000000  0.957543  0.920511  0.967184  0.973225  0.981028   \n",
              "119  0.996638  1.000000  0.993208  0.905427  0.968910  0.991639  0.970689   \n",
              "120  0.958319  0.934904  0.952694  0.963876  0.963975  0.961587  0.935083   \n",
              "121  0.684687  0.333333  0.972973  0.980143  0.974306  0.959930  0.943779   \n",
              "122  0.853584  0.951569  0.911799  0.896429  0.844561  0.875486  0.889139   \n",
              "123  0.793827  0.887007  0.935667  0.969302  0.929794  0.964938  0.906194   \n",
              "124  0.953291  0.896251  0.890177  0.960288  0.948608  0.959933  0.942030   \n",
              "125  0.862044  0.509959  0.903105  0.980139  0.939827  0.890662  0.935210   \n",
              "126  0.807752  0.899057  0.920361  0.890658  0.841004  0.859559  0.872052   \n",
              "127  0.951656  0.934778  0.959448  0.981949  0.970876  0.917866  0.921323   \n",
              "128  0.874293  0.859961  0.905336  0.983754  0.761689  0.907913  0.857861   \n",
              "129  0.762298  0.495798  0.885029  0.980143  0.736009  0.859440  0.865471   \n",
              "130  0.687197  0.809567  0.852901  0.938548  0.635581  0.788842  0.734008   \n",
              "131  0.837792  0.878184  0.942514  0.987364  0.698462  0.900725  0.787154   \n",
              "132  0.917785  0.938332  0.925675  0.981949  0.871843  0.949876  0.929827   \n",
              "133  0.908333  0.333333  0.904880  0.987364  0.887388  0.963271  0.928358   \n",
              "134  0.735275  0.815809  0.878317  0.925760  0.702010  0.826702  0.783397   \n",
              "135  0.979999  0.974997  0.940817  0.983752  0.919230  0.954912  0.938500   \n",
              "136  0.900893  0.919999  0.930727  0.996390  0.732342  0.938175  0.838871   \n",
              "137  0.929888  0.333333  0.910430  0.992780  0.795138  0.931403  0.907192   \n",
              "138  0.724894  0.907612  0.867904  0.629051  0.650597  0.802330  0.706365   \n",
              "139  0.909319  0.956643  0.964522  0.994585  0.828572  0.924821  0.880757   \n",
              "140  0.641660  0.797071  0.701859  0.921855  0.805686  0.637019  0.687980   \n",
              "141  0.361560  0.776111  0.715696  0.914707  0.551698  0.459106  0.561643   \n",
              "142  0.664557  0.927212  0.756037  0.895919  0.866191  0.682070  0.717597   \n",
              "143  0.407351  0.841352  0.719821  0.938366  0.639744  0.407951  0.553549   \n",
              "144  0.524086  0.684992  0.678879  0.907918  0.555801  0.527395  0.637663   \n",
              "145  0.589050  0.570750  0.656421  0.918517  0.560484  0.582609  0.644910   \n",
              "146  0.431624  0.531647  0.625698  0.771954  0.471266  0.418367  0.389667   \n",
              "147  0.408885  0.602140  0.675034  0.920452  0.587124  0.420474  0.593539   \n",
              "148  0.637249  0.708583  0.700335  0.954856  0.632731  0.670498  0.644652   \n",
              "149  0.566327  0.573060  0.636859  0.972924  0.588083  0.590092  0.495213   \n",
              "150  0.861843  0.921165  0.815227  0.972922  0.772649  0.855827  0.739090   \n",
              "151  0.739327  0.742273  0.812958  0.971110  0.755262  0.619457  0.693278   \n",
              "152  0.906366  0.881461  0.895068  0.969313  0.798652  0.911471  0.863244   \n",
              "153  0.893237  0.333333  0.870103  0.989169  0.848392  0.938219  0.907865   \n",
              "154  0.767995  0.893975  0.889506  0.907435  0.640090  0.874799  0.772539   \n",
              "155  0.904195  0.526789  0.901578  0.985559  0.832714  0.948226  0.887163   \n",
              "156  0.882074  0.836601  0.840988  0.987365  0.786081  0.938175  0.821761   \n",
              "157  0.907860  0.488887  0.825447  0.990975  0.696255  0.916054  0.804291   \n",
              "158  0.786145  0.698312  0.822525  0.792751  0.614244  0.880938  0.706436   \n",
              "159  0.900893  0.764844  0.932370  0.989170  0.676076  0.861264  0.744415   \n",
              "\n",
              "           ru        uk        zh  \\\n",
              "0    0.837419  0.809057  0.553690   \n",
              "1    0.816224  0.821046  0.462625   \n",
              "2    0.798806  0.756265  0.309948   \n",
              "3    0.873298  0.831866  0.447399   \n",
              "4    0.918328  0.896247  0.893291   \n",
              "5    0.901331  0.907969  0.561038   \n",
              "6    0.648425  0.764939  0.673301   \n",
              "7    0.883877  0.629046  0.515536   \n",
              "8    0.792921  0.710398  0.836037   \n",
              "9    0.800774  0.784926  0.473412   \n",
              "10   0.839545  0.766841  0.570567   \n",
              "11   0.767590  0.456707  0.358748   \n",
              "12   0.703173  0.702042  0.519355   \n",
              "13   0.466942  0.477949  0.523263   \n",
              "14   0.710270  0.669289  0.438383   \n",
              "15   0.689892  0.628996  0.489185   \n",
              "16   0.587605  0.559436  0.504128   \n",
              "17   0.441578  0.464481  0.416126   \n",
              "18   0.579892  0.601623  0.561681   \n",
              "19   0.670640  0.696108  0.535815   \n",
              "20   0.549435  0.533343  0.439036   \n",
              "21   0.482872  0.492465  0.578835   \n",
              "22   0.947054  0.956963  0.701631   \n",
              "23   0.748330  0.758673  0.623693   \n",
              "24   0.716103  0.665660  0.840924   \n",
              "25   0.771285  0.693195  0.524630   \n",
              "26   0.696867  0.601455  0.777948   \n",
              "27   0.576880  0.473909  0.437676   \n",
              "28   0.830933  0.813668  0.666693   \n",
              "29   0.634704  0.708828  0.594102   \n",
              "30   0.412107  0.463450  0.588810   \n",
              "31   0.778696  0.489713  0.437038   \n",
              "32   0.859986  0.818227  0.771335   \n",
              "33   0.836644  0.887960  0.569114   \n",
              "34   0.931657  0.909553  0.795438   \n",
              "35   0.884512  0.827786  0.616377   \n",
              "36   0.894916  0.872153  0.876469   \n",
              "37   0.894236  0.919624  0.678108   \n",
              "38   0.888962  0.912794  0.941620   \n",
              "39   0.909319  0.736821  0.850401   \n",
              "40   0.891630  0.879208  0.879892   \n",
              "41   0.901645  0.845394  0.664903   \n",
              "42   0.854453  0.909577  0.904510   \n",
              "43   0.598705  0.399356  0.400083   \n",
              "44   0.822676  0.929002  0.639578   \n",
              "45   0.540075  0.834843  0.373068   \n",
              "46   0.976510  0.984847  0.353950   \n",
              "47   0.941188  0.967987  0.684155   \n",
              "48   0.674798  0.754343  0.822457   \n",
              "49   0.571312  0.642585  0.455412   \n",
              "50   0.969586  0.962696  0.699728   \n",
              "51   0.810581  0.840842  0.538099   \n",
              "52   0.902707  0.970712  0.952752   \n",
              "53   0.897030  0.984507  0.583843   \n",
              "54   0.994877  0.993109  0.874422   \n",
              "55   0.757796  0.706033  0.818718   \n",
              "56   0.802661  0.740548  0.839972   \n",
              "57   0.857829  0.716456  0.766293   \n",
              "58   0.859653  0.744615  0.916644   \n",
              "59   0.894400  0.792063  0.876797   \n",
              "60   0.822568  0.786864  0.851378   \n",
              "61   0.720465  0.789295  0.619095   \n",
              "62   0.859492  0.856181  0.829064   \n",
              "63   0.777281  0.640005  0.647270   \n",
              "64   0.909936  0.882879  0.669646   \n",
              "65   0.934978  0.912926  0.490640   \n",
              "66   0.961667  0.961536  0.823733   \n",
              "67   0.943282  0.939718  0.852248   \n",
              "68   0.956666  0.933107  0.866630   \n",
              "69   0.953325  0.924428  0.625838   \n",
              "70   0.928300  0.948157  0.832907   \n",
              "71   0.976663  0.961514  0.904835   \n",
              "72   0.933297  0.921399  0.812230   \n",
              "73   0.956649  0.856972  0.707815   \n",
              "74   0.830468  0.894049  0.779666   \n",
              "75   0.956667  0.944703  0.921368   \n",
              "76   0.984894  0.976421  0.476467   \n",
              "77   0.978185  0.935939  0.405639   \n",
              "78   0.988253  0.981477  0.562238   \n",
              "79   0.993287  0.988215  0.631352   \n",
              "80   0.933984  0.925416  0.640839   \n",
              "81   0.966202  0.981352  0.764907   \n",
              "82   0.984788  0.993219  0.764493   \n",
              "83   0.984794  0.986441  0.852985   \n",
              "84   0.979482  0.982761  0.824275   \n",
              "85   0.994876  0.981029  0.767807   \n",
              "86   0.922422  0.970651  0.833545   \n",
              "87   0.996585  0.994832  0.933835   \n",
              "88   0.914614  0.907869  0.791094   \n",
              "89   0.931540  0.819250  0.641026   \n",
              "90   0.938054  0.959758  0.735809   \n",
              "91   0.938186  0.949698  0.774999   \n",
              "92   0.926421  0.840089  0.347210   \n",
              "93   0.954849  0.897842  0.544260   \n",
              "94   0.956463  0.958152  0.859437   \n",
              "95   0.961481  0.927846  0.899810   \n",
              "96   0.919999  0.892832  0.747233   \n",
              "97   0.938332  0.912946  0.555541   \n",
              "98   0.928324  0.933074  0.711756   \n",
              "99   0.936622  0.924572  0.819962   \n",
              "100  0.954994  0.931318  0.914828   \n",
              "101  0.938284  0.893877  0.569309   \n",
              "102  0.923248  0.939775  0.852572   \n",
              "103  0.984999  0.946371  0.938168   \n",
              "104  0.939983  0.936452  0.904807   \n",
              "105  0.946666  0.921219  0.723636   \n",
              "106  0.938295  0.949787  0.885362   \n",
              "107  0.973332  0.983277  0.936548   \n",
              "108  0.978180  0.978111  0.658036   \n",
              "109  0.979860  0.973061  0.533715   \n",
              "110  0.989933  0.966299  0.597399   \n",
              "111  0.989932  0.979792  0.599157   \n",
              "112  0.916831  0.899965  0.773006   \n",
              "113  0.974662  0.972880  0.692877   \n",
              "114  0.988175  0.983051  0.807321   \n",
              "115  0.974624  0.996610  0.780601   \n",
              "116  0.986342  0.984503  0.949368   \n",
              "117  0.998292  0.996555  0.768063   \n",
              "118  1.000000  0.996555  0.799917   \n",
              "119  1.000000  0.998278  0.807778   \n",
              "120  0.909843  0.877524  0.918315   \n",
              "121  0.949885  0.839593  0.844200   \n",
              "122  0.934889  0.924432  0.933065   \n",
              "123  0.904119  0.833569  0.671403   \n",
              "124  0.939775  0.892804  0.717279   \n",
              "125  0.941314  0.845123  0.652242   \n",
              "126  0.936295  0.936409  0.701151   \n",
              "127  0.974913  0.956520  0.744186   \n",
              "128  0.846113  0.814381  0.571886   \n",
              "129  0.820896  0.804347  0.426802   \n",
              "130  0.733177  0.682376  0.417706   \n",
              "131  0.829411  0.852366  0.598436   \n",
              "132  0.910000  0.894599  0.908321   \n",
              "133  0.923248  0.886038  0.549381   \n",
              "134  0.676881  0.734812  0.729492   \n",
              "135  0.951660  0.946450  0.796341   \n",
              "136  0.876579  0.866167  0.807944   \n",
              "137  0.879774  0.806716  0.657042   \n",
              "138  0.779360  0.827100  0.693959   \n",
              "139  0.906563  0.929765  0.861447   \n",
              "140  0.701551  0.803135  0.565878   \n",
              "141  0.484584  0.513736  0.555561   \n",
              "142  0.773435  0.853296  0.590222   \n",
              "143  0.637093  0.655976  0.448793   \n",
              "144  0.610414  0.647736  0.532262   \n",
              "145  0.446283  0.498090  0.446363   \n",
              "146  0.550282  0.471574  0.530702   \n",
              "147  0.588398  0.622024  0.517499   \n",
              "148  0.550129  0.637987  0.518781   \n",
              "149  0.256258  0.253658  0.415553   \n",
              "150  0.754218  0.847703  0.732787   \n",
              "151  0.568899  0.597704  0.616519   \n",
              "152  0.777323  0.733287  0.888281   \n",
              "153  0.693971  0.568728  0.590378   \n",
              "154  0.734054  0.612957  0.719948   \n",
              "155  0.598078  0.393205  0.398708   \n",
              "156  0.833505  0.837781  0.637889   \n",
              "157  0.636147  0.721517  0.677753   \n",
              "158  0.603137  0.676501  0.548784   \n",
              "159  0.761834  0.786622  0.759474   \n",
              "\n",
              "                                        subject_id within  \n",
              "0     bert-base-multilingual-cased_alpaca-lora-30b     en  \n",
              "1                             mGPT_alpaca-lora-30b     en  \n",
              "2                 mdeberta-v3-base_alpaca-lora-30b     en  \n",
              "3                xlm-roberta-large_alpaca-lora-30b     en  \n",
              "4       bert-base-multilingual-cased_gpt-3.5-turbo     en  \n",
              "5                               mGPT_gpt-3.5-turbo     en  \n",
              "6                   mdeberta-v3-base_gpt-3.5-turbo     en  \n",
              "7                  xlm-roberta-large_gpt-3.5-turbo     en  \n",
              "8               bert-base-multilingual-cased_gpt-4     en  \n",
              "9                                       mGPT_gpt-4     en  \n",
              "10                          mdeberta-v3-base_gpt-4     en  \n",
              "11                         xlm-roberta-large_gpt-4     en  \n",
              "12          bert-base-multilingual-cased_llama-65b     en  \n",
              "13                                  mGPT_llama-65b     en  \n",
              "14                      mdeberta-v3-base_llama-65b     en  \n",
              "15                     xlm-roberta-large_llama-65b     en  \n",
              "16            bert-base-multilingual-cased_opt-66b     en  \n",
              "17                                    mGPT_opt-66b     en  \n",
              "18                        mdeberta-v3-base_opt-66b     en  \n",
              "19                       xlm-roberta-large_opt-66b     en  \n",
              "20   bert-base-multilingual-cased_opt-iml-max-1.3b     en  \n",
              "21                           mGPT_opt-iml-max-1.3b     en  \n",
              "22               mdeberta-v3-base_opt-iml-max-1.3b     en  \n",
              "23              xlm-roberta-large_opt-iml-max-1.3b     en  \n",
              "24   bert-base-multilingual-cased_text-davinci-003     en  \n",
              "25                           mGPT_text-davinci-003     en  \n",
              "26               mdeberta-v3-base_text-davinci-003     en  \n",
              "27              xlm-roberta-large_text-davinci-003     en  \n",
              "28         bert-base-multilingual-cased_vicuna-13b     en  \n",
              "29                                 mGPT_vicuna-13b     en  \n",
              "30                     mdeberta-v3-base_vicuna-13b     en  \n",
              "31                    xlm-roberta-large_vicuna-13b     en  \n",
              "32    bert-base-multilingual-cased_alpaca-lora-30b     es  \n",
              "33                            mGPT_alpaca-lora-30b     es  \n",
              "34                mdeberta-v3-base_alpaca-lora-30b     es  \n",
              "35               xlm-roberta-large_alpaca-lora-30b     es  \n",
              "36      bert-base-multilingual-cased_gpt-3.5-turbo     es  \n",
              "37                              mGPT_gpt-3.5-turbo     es  \n",
              "38                  mdeberta-v3-base_gpt-3.5-turbo     es  \n",
              "39                 xlm-roberta-large_gpt-3.5-turbo     es  \n",
              "40              bert-base-multilingual-cased_gpt-4     es  \n",
              "41                                      mGPT_gpt-4     es  \n",
              "42                          mdeberta-v3-base_gpt-4     es  \n",
              "43                         xlm-roberta-large_gpt-4     es  \n",
              "44          bert-base-multilingual-cased_llama-65b     es  \n",
              "45                                  mGPT_llama-65b     es  \n",
              "46                      mdeberta-v3-base_llama-65b     es  \n",
              "47                     xlm-roberta-large_llama-65b     es  \n",
              "48            bert-base-multilingual-cased_opt-66b     es  \n",
              "49                                    mGPT_opt-66b     es  \n",
              "50                        mdeberta-v3-base_opt-66b     es  \n",
              "51                       xlm-roberta-large_opt-66b     es  \n",
              "52   bert-base-multilingual-cased_opt-iml-max-1.3b     es  \n",
              "53                           mGPT_opt-iml-max-1.3b     es  \n",
              "54               mdeberta-v3-base_opt-iml-max-1.3b     es  \n",
              "55              xlm-roberta-large_opt-iml-max-1.3b     es  \n",
              "56   bert-base-multilingual-cased_text-davinci-003     es  \n",
              "57                           mGPT_text-davinci-003     es  \n",
              "58               mdeberta-v3-base_text-davinci-003     es  \n",
              "59              xlm-roberta-large_text-davinci-003     es  \n",
              "60         bert-base-multilingual-cased_vicuna-13b     es  \n",
              "61                                 mGPT_vicuna-13b     es  \n",
              "62                     mdeberta-v3-base_vicuna-13b     es  \n",
              "63                    xlm-roberta-large_vicuna-13b     es  \n",
              "64    bert-base-multilingual-cased_alpaca-lora-30b     ru  \n",
              "65                            mGPT_alpaca-lora-30b     ru  \n",
              "66                mdeberta-v3-base_alpaca-lora-30b     ru  \n",
              "67               xlm-roberta-large_alpaca-lora-30b     ru  \n",
              "68      bert-base-multilingual-cased_gpt-3.5-turbo     ru  \n",
              "69                              mGPT_gpt-3.5-turbo     ru  \n",
              "70                  mdeberta-v3-base_gpt-3.5-turbo     ru  \n",
              "71                 xlm-roberta-large_gpt-3.5-turbo     ru  \n",
              "72              bert-base-multilingual-cased_gpt-4     ru  \n",
              "73                                      mGPT_gpt-4     ru  \n",
              "74                          mdeberta-v3-base_gpt-4     ru  \n",
              "75                         xlm-roberta-large_gpt-4     ru  \n",
              "76          bert-base-multilingual-cased_llama-65b     ru  \n",
              "77                                  mGPT_llama-65b     ru  \n",
              "78                      mdeberta-v3-base_llama-65b     ru  \n",
              "79                     xlm-roberta-large_llama-65b     ru  \n",
              "80            bert-base-multilingual-cased_opt-66b     ru  \n",
              "81                                    mGPT_opt-66b     ru  \n",
              "82                        mdeberta-v3-base_opt-66b     ru  \n",
              "83                       xlm-roberta-large_opt-66b     ru  \n",
              "84   bert-base-multilingual-cased_opt-iml-max-1.3b     ru  \n",
              "85                           mGPT_opt-iml-max-1.3b     ru  \n",
              "86               mdeberta-v3-base_opt-iml-max-1.3b     ru  \n",
              "87              xlm-roberta-large_opt-iml-max-1.3b     ru  \n",
              "88   bert-base-multilingual-cased_text-davinci-003     ru  \n",
              "89                           mGPT_text-davinci-003     ru  \n",
              "90               mdeberta-v3-base_text-davinci-003     ru  \n",
              "91              xlm-roberta-large_text-davinci-003     ru  \n",
              "92         bert-base-multilingual-cased_vicuna-13b     ru  \n",
              "93                                 mGPT_vicuna-13b     ru  \n",
              "94                     mdeberta-v3-base_vicuna-13b     ru  \n",
              "95                    xlm-roberta-large_vicuna-13b     ru  \n",
              "96    bert-base-multilingual-cased_alpaca-lora-30b    all  \n",
              "97                            mGPT_alpaca-lora-30b    all  \n",
              "98                mdeberta-v3-base_alpaca-lora-30b    all  \n",
              "99               xlm-roberta-large_alpaca-lora-30b    all  \n",
              "100     bert-base-multilingual-cased_gpt-3.5-turbo    all  \n",
              "101                             mGPT_gpt-3.5-turbo    all  \n",
              "102                 mdeberta-v3-base_gpt-3.5-turbo    all  \n",
              "103                xlm-roberta-large_gpt-3.5-turbo    all  \n",
              "104             bert-base-multilingual-cased_gpt-4    all  \n",
              "105                                     mGPT_gpt-4    all  \n",
              "106                         mdeberta-v3-base_gpt-4    all  \n",
              "107                        xlm-roberta-large_gpt-4    all  \n",
              "108         bert-base-multilingual-cased_llama-65b    all  \n",
              "109                                 mGPT_llama-65b    all  \n",
              "110                     mdeberta-v3-base_llama-65b    all  \n",
              "111                    xlm-roberta-large_llama-65b    all  \n",
              "112           bert-base-multilingual-cased_opt-66b    all  \n",
              "113                                   mGPT_opt-66b    all  \n",
              "114                       mdeberta-v3-base_opt-66b    all  \n",
              "115                      xlm-roberta-large_opt-66b    all  \n",
              "116  bert-base-multilingual-cased_opt-iml-max-1.3b    all  \n",
              "117                          mGPT_opt-iml-max-1.3b    all  \n",
              "118              mdeberta-v3-base_opt-iml-max-1.3b    all  \n",
              "119             xlm-roberta-large_opt-iml-max-1.3b    all  \n",
              "120  bert-base-multilingual-cased_text-davinci-003    all  \n",
              "121                          mGPT_text-davinci-003    all  \n",
              "122              mdeberta-v3-base_text-davinci-003    all  \n",
              "123             xlm-roberta-large_text-davinci-003    all  \n",
              "124        bert-base-multilingual-cased_vicuna-13b    all  \n",
              "125                                mGPT_vicuna-13b    all  \n",
              "126                    mdeberta-v3-base_vicuna-13b    all  \n",
              "127                   xlm-roberta-large_vicuna-13b    all  \n",
              "128   bert-base-multilingual-cased_alpaca-lora-30b    en3  \n",
              "129                           mGPT_alpaca-lora-30b    en3  \n",
              "130               mdeberta-v3-base_alpaca-lora-30b    en3  \n",
              "131              xlm-roberta-large_alpaca-lora-30b    en3  \n",
              "132     bert-base-multilingual-cased_gpt-3.5-turbo    en3  \n",
              "133                             mGPT_gpt-3.5-turbo    en3  \n",
              "134                 mdeberta-v3-base_gpt-3.5-turbo    en3  \n",
              "135                xlm-roberta-large_gpt-3.5-turbo    en3  \n",
              "136             bert-base-multilingual-cased_gpt-4    en3  \n",
              "137                                     mGPT_gpt-4    en3  \n",
              "138                         mdeberta-v3-base_gpt-4    en3  \n",
              "139                        xlm-roberta-large_gpt-4    en3  \n",
              "140         bert-base-multilingual-cased_llama-65b    en3  \n",
              "141                                 mGPT_llama-65b    en3  \n",
              "142                     mdeberta-v3-base_llama-65b    en3  \n",
              "143                    xlm-roberta-large_llama-65b    en3  \n",
              "144           bert-base-multilingual-cased_opt-66b    en3  \n",
              "145                                   mGPT_opt-66b    en3  \n",
              "146                       mdeberta-v3-base_opt-66b    en3  \n",
              "147                      xlm-roberta-large_opt-66b    en3  \n",
              "148  bert-base-multilingual-cased_opt-iml-max-1.3b    en3  \n",
              "149                          mGPT_opt-iml-max-1.3b    en3  \n",
              "150              mdeberta-v3-base_opt-iml-max-1.3b    en3  \n",
              "151             xlm-roberta-large_opt-iml-max-1.3b    en3  \n",
              "152  bert-base-multilingual-cased_text-davinci-003    en3  \n",
              "153                          mGPT_text-davinci-003    en3  \n",
              "154              mdeberta-v3-base_text-davinci-003    en3  \n",
              "155             xlm-roberta-large_text-davinci-003    en3  \n",
              "156        bert-base-multilingual-cased_vicuna-13b    en3  \n",
              "157                                mGPT_vicuna-13b    en3  \n",
              "158                    mdeberta-v3-base_vicuna-13b    en3  \n",
              "159                   xlm-roberta-large_vicuna-13b    en3  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11359e73-14a0-4270-bc1f-c0932325e515\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train Language</th>\n",
              "      <th>Train LLM</th>\n",
              "      <th>Model</th>\n",
              "      <th>ar</th>\n",
              "      <th>ca</th>\n",
              "      <th>cs</th>\n",
              "      <th>de</th>\n",
              "      <th>en</th>\n",
              "      <th>es</th>\n",
              "      <th>nl</th>\n",
              "      <th>pt</th>\n",
              "      <th>ru</th>\n",
              "      <th>uk</th>\n",
              "      <th>zh</th>\n",
              "      <th>subject_id</th>\n",
              "      <th>within</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>en</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.537542</td>\n",
              "      <td>0.827078</td>\n",
              "      <td>0.854560</td>\n",
              "      <td>0.891714</td>\n",
              "      <td>0.956665</td>\n",
              "      <td>0.751676</td>\n",
              "      <td>0.856264</td>\n",
              "      <td>0.805481</td>\n",
              "      <td>0.837419</td>\n",
              "      <td>0.809057</td>\n",
              "      <td>0.553690</td>\n",
              "      <td>bert-base-multilingual-cased_alpaca-lora-30b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>en</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.402388</td>\n",
              "      <td>0.808877</td>\n",
              "      <td>0.613177</td>\n",
              "      <td>0.876305</td>\n",
              "      <td>0.963891</td>\n",
              "      <td>0.738976</td>\n",
              "      <td>0.879058</td>\n",
              "      <td>0.833282</td>\n",
              "      <td>0.816224</td>\n",
              "      <td>0.821046</td>\n",
              "      <td>0.462625</td>\n",
              "      <td>mGPT_alpaca-lora-30b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>en</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.207984</td>\n",
              "      <td>0.859243</td>\n",
              "      <td>0.769121</td>\n",
              "      <td>0.900331</td>\n",
              "      <td>0.943947</td>\n",
              "      <td>0.774384</td>\n",
              "      <td>0.897682</td>\n",
              "      <td>0.857008</td>\n",
              "      <td>0.798806</td>\n",
              "      <td>0.756265</td>\n",
              "      <td>0.309948</td>\n",
              "      <td>mdeberta-v3-base_alpaca-lora-30b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>en</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.447388</td>\n",
              "      <td>0.848775</td>\n",
              "      <td>0.871277</td>\n",
              "      <td>0.932405</td>\n",
              "      <td>0.980144</td>\n",
              "      <td>0.631892</td>\n",
              "      <td>0.770589</td>\n",
              "      <td>0.712993</td>\n",
              "      <td>0.873298</td>\n",
              "      <td>0.831866</td>\n",
              "      <td>0.447399</td>\n",
              "      <td>xlm-roberta-large_alpaca-lora-30b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>en</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.921465</td>\n",
              "      <td>0.890380</td>\n",
              "      <td>0.914981</td>\n",
              "      <td>0.902026</td>\n",
              "      <td>0.978339</td>\n",
              "      <td>0.854525</td>\n",
              "      <td>0.934819</td>\n",
              "      <td>0.912430</td>\n",
              "      <td>0.918328</td>\n",
              "      <td>0.896247</td>\n",
              "      <td>0.893291</td>\n",
              "      <td>bert-base-multilingual-cased_gpt-3.5-turbo</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>en</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.700916</td>\n",
              "      <td>0.926614</td>\n",
              "      <td>0.337026</td>\n",
              "      <td>0.907073</td>\n",
              "      <td>0.989169</td>\n",
              "      <td>0.850593</td>\n",
              "      <td>0.943226</td>\n",
              "      <td>0.914732</td>\n",
              "      <td>0.901331</td>\n",
              "      <td>0.907969</td>\n",
              "      <td>0.561038</td>\n",
              "      <td>mGPT_gpt-3.5-turbo</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>en</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.787647</td>\n",
              "      <td>0.559878</td>\n",
              "      <td>0.780318</td>\n",
              "      <td>0.805155</td>\n",
              "      <td>0.717787</td>\n",
              "      <td>0.602325</td>\n",
              "      <td>0.779041</td>\n",
              "      <td>0.678157</td>\n",
              "      <td>0.648425</td>\n",
              "      <td>0.764939</td>\n",
              "      <td>0.673301</td>\n",
              "      <td>mdeberta-v3-base_gpt-3.5-turbo</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>en</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.721820</td>\n",
              "      <td>0.973319</td>\n",
              "      <td>0.721150</td>\n",
              "      <td>0.939164</td>\n",
              "      <td>0.983754</td>\n",
              "      <td>0.786081</td>\n",
              "      <td>0.922878</td>\n",
              "      <td>0.928099</td>\n",
              "      <td>0.883877</td>\n",
              "      <td>0.629046</td>\n",
              "      <td>0.515536</td>\n",
              "      <td>xlm-roberta-large_gpt-3.5-turbo</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>en</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.841926</td>\n",
              "      <td>0.926529</td>\n",
              "      <td>0.759176</td>\n",
              "      <td>0.816308</td>\n",
              "      <td>0.976534</td>\n",
              "      <td>0.826112</td>\n",
              "      <td>0.926544</td>\n",
              "      <td>0.870419</td>\n",
              "      <td>0.792921</td>\n",
              "      <td>0.710398</td>\n",
              "      <td>0.836037</td>\n",
              "      <td>bert-base-multilingual-cased_gpt-4</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>en</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.694497</td>\n",
              "      <td>0.788771</td>\n",
              "      <td>0.386602</td>\n",
              "      <td>0.836477</td>\n",
              "      <td>0.990975</td>\n",
              "      <td>0.795370</td>\n",
              "      <td>0.833370</td>\n",
              "      <td>0.865765</td>\n",
              "      <td>0.800774</td>\n",
              "      <td>0.784926</td>\n",
              "      <td>0.473412</td>\n",
              "      <td>mGPT_gpt-4</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>en</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.719857</td>\n",
              "      <td>0.973331</td>\n",
              "      <td>0.862618</td>\n",
              "      <td>0.894392</td>\n",
              "      <td>0.825513</td>\n",
              "      <td>0.943480</td>\n",
              "      <td>0.933106</td>\n",
              "      <td>0.931852</td>\n",
              "      <td>0.839545</td>\n",
              "      <td>0.766841</td>\n",
              "      <td>0.570567</td>\n",
              "      <td>mdeberta-v3-base_gpt-4</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>en</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.565875</td>\n",
              "      <td>0.934778</td>\n",
              "      <td>0.498656</td>\n",
              "      <td>0.892721</td>\n",
              "      <td>0.989170</td>\n",
              "      <td>0.859221</td>\n",
              "      <td>0.949913</td>\n",
              "      <td>0.926661</td>\n",
              "      <td>0.767590</td>\n",
              "      <td>0.456707</td>\n",
              "      <td>0.358748</td>\n",
              "      <td>xlm-roberta-large_gpt-4</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>en</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.589577</td>\n",
              "      <td>0.606979</td>\n",
              "      <td>0.695458</td>\n",
              "      <td>0.689542</td>\n",
              "      <td>0.901658</td>\n",
              "      <td>0.799832</td>\n",
              "      <td>0.577084</td>\n",
              "      <td>0.675244</td>\n",
              "      <td>0.703173</td>\n",
              "      <td>0.702042</td>\n",
              "      <td>0.519355</td>\n",
              "      <td>bert-base-multilingual-cased_llama-65b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>en</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.384803</td>\n",
              "      <td>0.350849</td>\n",
              "      <td>0.689755</td>\n",
              "      <td>0.711503</td>\n",
              "      <td>0.903682</td>\n",
              "      <td>0.524759</td>\n",
              "      <td>0.410240</td>\n",
              "      <td>0.483544</td>\n",
              "      <td>0.466942</td>\n",
              "      <td>0.477949</td>\n",
              "      <td>0.523263</td>\n",
              "      <td>mGPT_llama-65b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>en</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.511518</td>\n",
              "      <td>0.405992</td>\n",
              "      <td>0.830677</td>\n",
              "      <td>0.716159</td>\n",
              "      <td>0.900289</td>\n",
              "      <td>0.607432</td>\n",
              "      <td>0.459106</td>\n",
              "      <td>0.541285</td>\n",
              "      <td>0.710270</td>\n",
              "      <td>0.669289</td>\n",
              "      <td>0.438383</td>\n",
              "      <td>mdeberta-v3-base_llama-65b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>en</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.477913</td>\n",
              "      <td>0.489240</td>\n",
              "      <td>0.850697</td>\n",
              "      <td>0.798271</td>\n",
              "      <td>0.916509</td>\n",
              "      <td>0.724079</td>\n",
              "      <td>0.436761</td>\n",
              "      <td>0.608223</td>\n",
              "      <td>0.689892</td>\n",
              "      <td>0.628996</td>\n",
              "      <td>0.489185</td>\n",
              "      <td>xlm-roberta-large_llama-65b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>en</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.455733</td>\n",
              "      <td>0.458569</td>\n",
              "      <td>0.610699</td>\n",
              "      <td>0.588965</td>\n",
              "      <td>0.859176</td>\n",
              "      <td>0.539436</td>\n",
              "      <td>0.426829</td>\n",
              "      <td>0.563455</td>\n",
              "      <td>0.587605</td>\n",
              "      <td>0.559436</td>\n",
              "      <td>0.504128</td>\n",
              "      <td>bert-base-multilingual-cased_opt-66b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>en</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.295934</td>\n",
              "      <td>0.504866</td>\n",
              "      <td>0.395400</td>\n",
              "      <td>0.635131</td>\n",
              "      <td>0.893075</td>\n",
              "      <td>0.506420</td>\n",
              "      <td>0.494248</td>\n",
              "      <td>0.601702</td>\n",
              "      <td>0.441578</td>\n",
              "      <td>0.464481</td>\n",
              "      <td>0.416126</td>\n",
              "      <td>mGPT_opt-66b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>en</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.390341</td>\n",
              "      <td>0.518131</td>\n",
              "      <td>0.627795</td>\n",
              "      <td>0.656829</td>\n",
              "      <td>0.853764</td>\n",
              "      <td>0.605320</td>\n",
              "      <td>0.492703</td>\n",
              "      <td>0.643090</td>\n",
              "      <td>0.579892</td>\n",
              "      <td>0.601623</td>\n",
              "      <td>0.561681</td>\n",
              "      <td>mdeberta-v3-base_opt-66b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>en</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.490988</td>\n",
              "      <td>0.461322</td>\n",
              "      <td>0.549451</td>\n",
              "      <td>0.673485</td>\n",
              "      <td>0.882653</td>\n",
              "      <td>0.561258</td>\n",
              "      <td>0.418667</td>\n",
              "      <td>0.589688</td>\n",
              "      <td>0.670640</td>\n",
              "      <td>0.696108</td>\n",
              "      <td>0.535815</td>\n",
              "      <td>xlm-roberta-large_opt-66b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>en</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.459169</td>\n",
              "      <td>0.517135</td>\n",
              "      <td>0.579223</td>\n",
              "      <td>0.591575</td>\n",
              "      <td>0.906132</td>\n",
              "      <td>0.514289</td>\n",
              "      <td>0.513633</td>\n",
              "      <td>0.472087</td>\n",
              "      <td>0.549435</td>\n",
              "      <td>0.533343</td>\n",
              "      <td>0.439036</td>\n",
              "      <td>bert-base-multilingual-cased_opt-iml-max-1.3b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>en</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.338008</td>\n",
              "      <td>0.513427</td>\n",
              "      <td>0.641027</td>\n",
              "      <td>0.745042</td>\n",
              "      <td>0.958480</td>\n",
              "      <td>0.528383</td>\n",
              "      <td>0.695527</td>\n",
              "      <td>0.521178</td>\n",
              "      <td>0.482872</td>\n",
              "      <td>0.492465</td>\n",
              "      <td>0.578835</td>\n",
              "      <td>mGPT_opt-iml-max-1.3b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>en</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.793226</td>\n",
              "      <td>0.973103</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.960929</td>\n",
              "      <td>0.925661</td>\n",
              "      <td>0.902335</td>\n",
              "      <td>0.961500</td>\n",
              "      <td>0.902473</td>\n",
              "      <td>0.947054</td>\n",
              "      <td>0.956963</td>\n",
              "      <td>0.701631</td>\n",
              "      <td>mdeberta-v3-base_opt-iml-max-1.3b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>en</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.580995</td>\n",
              "      <td>0.856666</td>\n",
              "      <td>0.865734</td>\n",
              "      <td>0.841577</td>\n",
              "      <td>0.943910</td>\n",
              "      <td>0.726786</td>\n",
              "      <td>0.768422</td>\n",
              "      <td>0.699108</td>\n",
              "      <td>0.748330</td>\n",
              "      <td>0.758673</td>\n",
              "      <td>0.623693</td>\n",
              "      <td>xlm-roberta-large_opt-iml-max-1.3b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>en</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.764651</td>\n",
              "      <td>0.901368</td>\n",
              "      <td>0.853679</td>\n",
              "      <td>0.855428</td>\n",
              "      <td>0.969314</td>\n",
              "      <td>0.774992</td>\n",
              "      <td>0.903165</td>\n",
              "      <td>0.840752</td>\n",
              "      <td>0.716103</td>\n",
              "      <td>0.665660</td>\n",
              "      <td>0.840924</td>\n",
              "      <td>bert-base-multilingual-cased_text-davinci-003</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>en</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.539324</td>\n",
              "      <td>0.919928</td>\n",
              "      <td>0.362297</td>\n",
              "      <td>0.893556</td>\n",
              "      <td>0.978335</td>\n",
              "      <td>0.767779</td>\n",
              "      <td>0.902670</td>\n",
              "      <td>0.838866</td>\n",
              "      <td>0.771285</td>\n",
              "      <td>0.693195</td>\n",
              "      <td>0.524630</td>\n",
              "      <td>mGPT_text-davinci-003</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>en</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.543187</td>\n",
              "      <td>0.639753</td>\n",
              "      <td>0.823919</td>\n",
              "      <td>0.856322</td>\n",
              "      <td>0.951251</td>\n",
              "      <td>0.577358</td>\n",
              "      <td>0.759612</td>\n",
              "      <td>0.633527</td>\n",
              "      <td>0.696867</td>\n",
              "      <td>0.601455</td>\n",
              "      <td>0.777948</td>\n",
              "      <td>mdeberta-v3-base_text-davinci-003</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>en</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.503535</td>\n",
              "      <td>0.921334</td>\n",
              "      <td>0.696479</td>\n",
              "      <td>0.891345</td>\n",
              "      <td>0.985559</td>\n",
              "      <td>0.771487</td>\n",
              "      <td>0.897266</td>\n",
              "      <td>0.823210</td>\n",
              "      <td>0.576880</td>\n",
              "      <td>0.473909</td>\n",
              "      <td>0.437676</td>\n",
              "      <td>xlm-roberta-large_text-davinci-003</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>en</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.791310</td>\n",
              "      <td>0.885598</td>\n",
              "      <td>0.801202</td>\n",
              "      <td>0.844550</td>\n",
              "      <td>0.974726</td>\n",
              "      <td>0.785887</td>\n",
              "      <td>0.936540</td>\n",
              "      <td>0.851997</td>\n",
              "      <td>0.830933</td>\n",
              "      <td>0.813668</td>\n",
              "      <td>0.666693</td>\n",
              "      <td>bert-base-multilingual-cased_vicuna-13b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>en</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.408638</td>\n",
              "      <td>0.788775</td>\n",
              "      <td>0.504333</td>\n",
              "      <td>0.851064</td>\n",
              "      <td>0.976534</td>\n",
              "      <td>0.663255</td>\n",
              "      <td>0.887594</td>\n",
              "      <td>0.807120</td>\n",
              "      <td>0.634704</td>\n",
              "      <td>0.708828</td>\n",
              "      <td>0.594102</td>\n",
              "      <td>mGPT_vicuna-13b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>en</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.178446</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.501832</td>\n",
              "      <td>0.629463</td>\n",
              "      <td>0.777510</td>\n",
              "      <td>0.475434</td>\n",
              "      <td>0.619061</td>\n",
              "      <td>0.507817</td>\n",
              "      <td>0.412107</td>\n",
              "      <td>0.463450</td>\n",
              "      <td>0.588810</td>\n",
              "      <td>mdeberta-v3-base_vicuna-13b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>en</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.386417</td>\n",
              "      <td>0.941659</td>\n",
              "      <td>0.489440</td>\n",
              "      <td>0.908579</td>\n",
              "      <td>0.980144</td>\n",
              "      <td>0.686643</td>\n",
              "      <td>0.840000</td>\n",
              "      <td>0.716308</td>\n",
              "      <td>0.778696</td>\n",
              "      <td>0.489713</td>\n",
              "      <td>0.437038</td>\n",
              "      <td>xlm-roberta-large_vicuna-13b</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>es</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.601879</td>\n",
              "      <td>0.894965</td>\n",
              "      <td>0.810758</td>\n",
              "      <td>0.876534</td>\n",
              "      <td>0.751693</td>\n",
              "      <td>0.909150</td>\n",
              "      <td>0.894101</td>\n",
              "      <td>0.894326</td>\n",
              "      <td>0.859986</td>\n",
              "      <td>0.818227</td>\n",
              "      <td>0.771335</td>\n",
              "      <td>bert-base-multilingual-cased_alpaca-lora-30b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>es</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.697994</td>\n",
              "      <td>0.863938</td>\n",
              "      <td>0.698896</td>\n",
              "      <td>0.900098</td>\n",
              "      <td>0.658906</td>\n",
              "      <td>0.945195</td>\n",
              "      <td>0.708766</td>\n",
              "      <td>0.940350</td>\n",
              "      <td>0.836644</td>\n",
              "      <td>0.887960</td>\n",
              "      <td>0.569114</td>\n",
              "      <td>mGPT_alpaca-lora-30b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>es</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.825526</td>\n",
              "      <td>0.733757</td>\n",
              "      <td>0.911024</td>\n",
              "      <td>0.917144</td>\n",
              "      <td>0.877062</td>\n",
              "      <td>0.825619</td>\n",
              "      <td>0.803750</td>\n",
              "      <td>0.777745</td>\n",
              "      <td>0.931657</td>\n",
              "      <td>0.909553</td>\n",
              "      <td>0.795438</td>\n",
              "      <td>mdeberta-v3-base_alpaca-lora-30b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>es</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.699932</td>\n",
              "      <td>0.834861</td>\n",
              "      <td>0.900833</td>\n",
              "      <td>0.949315</td>\n",
              "      <td>0.929459</td>\n",
              "      <td>0.928069</td>\n",
              "      <td>0.953177</td>\n",
              "      <td>0.930124</td>\n",
              "      <td>0.884512</td>\n",
              "      <td>0.827786</td>\n",
              "      <td>0.616377</td>\n",
              "      <td>xlm-roberta-large_alpaca-lora-30b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>es</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.889624</td>\n",
              "      <td>0.936548</td>\n",
              "      <td>0.899715</td>\n",
              "      <td>0.890037</td>\n",
              "      <td>0.916880</td>\n",
              "      <td>0.926344</td>\n",
              "      <td>0.923169</td>\n",
              "      <td>0.885839</td>\n",
              "      <td>0.894916</td>\n",
              "      <td>0.872153</td>\n",
              "      <td>0.876469</td>\n",
              "      <td>bert-base-multilingual-cased_gpt-3.5-turbo</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>es</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.894145</td>\n",
              "      <td>0.964984</td>\n",
              "      <td>0.355178</td>\n",
              "      <td>0.930710</td>\n",
              "      <td>0.839701</td>\n",
              "      <td>0.969155</td>\n",
              "      <td>0.897390</td>\n",
              "      <td>0.955613</td>\n",
              "      <td>0.894236</td>\n",
              "      <td>0.919624</td>\n",
              "      <td>0.678108</td>\n",
              "      <td>mGPT_gpt-3.5-turbo</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>es</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.928117</td>\n",
              "      <td>0.881723</td>\n",
              "      <td>0.966649</td>\n",
              "      <td>0.957743</td>\n",
              "      <td>0.936777</td>\n",
              "      <td>0.957037</td>\n",
              "      <td>0.910947</td>\n",
              "      <td>0.908940</td>\n",
              "      <td>0.888962</td>\n",
              "      <td>0.912794</td>\n",
              "      <td>0.941620</td>\n",
              "      <td>mdeberta-v3-base_gpt-3.5-turbo</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>es</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.767102</td>\n",
              "      <td>0.954955</td>\n",
              "      <td>0.860860</td>\n",
              "      <td>0.853948</td>\n",
              "      <td>0.980144</td>\n",
              "      <td>0.974282</td>\n",
              "      <td>0.961602</td>\n",
              "      <td>0.967608</td>\n",
              "      <td>0.909319</td>\n",
              "      <td>0.736821</td>\n",
              "      <td>0.850401</td>\n",
              "      <td>xlm-roberta-large_gpt-3.5-turbo</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>es</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.876361</td>\n",
              "      <td>0.885192</td>\n",
              "      <td>0.929950</td>\n",
              "      <td>0.880837</td>\n",
              "      <td>0.730575</td>\n",
              "      <td>0.927672</td>\n",
              "      <td>0.907483</td>\n",
              "      <td>0.867887</td>\n",
              "      <td>0.891630</td>\n",
              "      <td>0.879208</td>\n",
              "      <td>0.879892</td>\n",
              "      <td>bert-base-multilingual-cased_gpt-4</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>es</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.849110</td>\n",
              "      <td>0.870498</td>\n",
              "      <td>0.337026</td>\n",
              "      <td>0.918737</td>\n",
              "      <td>0.947633</td>\n",
              "      <td>0.982867</td>\n",
              "      <td>0.924467</td>\n",
              "      <td>0.982956</td>\n",
              "      <td>0.901645</td>\n",
              "      <td>0.845394</td>\n",
              "      <td>0.664903</td>\n",
              "      <td>mGPT_gpt-4</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>es</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.933106</td>\n",
              "      <td>0.889724</td>\n",
              "      <td>0.954909</td>\n",
              "      <td>0.935623</td>\n",
              "      <td>0.882561</td>\n",
              "      <td>0.919452</td>\n",
              "      <td>0.933140</td>\n",
              "      <td>0.902705</td>\n",
              "      <td>0.854453</td>\n",
              "      <td>0.909577</td>\n",
              "      <td>0.904510</td>\n",
              "      <td>mdeberta-v3-base_gpt-4</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>es</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.512410</td>\n",
              "      <td>0.959946</td>\n",
              "      <td>0.568764</td>\n",
              "      <td>0.801054</td>\n",
              "      <td>0.972914</td>\n",
              "      <td>0.981155</td>\n",
              "      <td>0.961602</td>\n",
              "      <td>0.967626</td>\n",
              "      <td>0.598705</td>\n",
              "      <td>0.399356</td>\n",
              "      <td>0.400083</td>\n",
              "      <td>xlm-roberta-large_gpt-4</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>es</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.795561</td>\n",
              "      <td>0.917710</td>\n",
              "      <td>0.964457</td>\n",
              "      <td>0.771126</td>\n",
              "      <td>0.361111</td>\n",
              "      <td>0.991388</td>\n",
              "      <td>0.666485</td>\n",
              "      <td>0.979375</td>\n",
              "      <td>0.822676</td>\n",
              "      <td>0.929002</td>\n",
              "      <td>0.639578</td>\n",
              "      <td>bert-base-multilingual-cased_llama-65b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>es</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.591239</td>\n",
              "      <td>0.689859</td>\n",
              "      <td>0.860581</td>\n",
              "      <td>0.754042</td>\n",
              "      <td>0.350532</td>\n",
              "      <td>0.967288</td>\n",
              "      <td>0.360353</td>\n",
              "      <td>0.950147</td>\n",
              "      <td>0.540075</td>\n",
              "      <td>0.834843</td>\n",
              "      <td>0.373068</td>\n",
              "      <td>mGPT_llama-65b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>es</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.913549</td>\n",
              "      <td>0.926183</td>\n",
              "      <td>0.988152</td>\n",
              "      <td>0.960824</td>\n",
              "      <td>0.625957</td>\n",
              "      <td>0.986219</td>\n",
              "      <td>0.659130</td>\n",
              "      <td>0.977658</td>\n",
              "      <td>0.976510</td>\n",
              "      <td>0.984847</td>\n",
              "      <td>0.353950</td>\n",
              "      <td>mdeberta-v3-base_llama-65b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>es</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.846051</td>\n",
              "      <td>0.830698</td>\n",
              "      <td>0.983077</td>\n",
              "      <td>0.955682</td>\n",
              "      <td>0.668031</td>\n",
              "      <td>0.987945</td>\n",
              "      <td>0.458293</td>\n",
              "      <td>0.972497</td>\n",
              "      <td>0.941188</td>\n",
              "      <td>0.967987</td>\n",
              "      <td>0.684155</td>\n",
              "      <td>xlm-roberta-large_llama-65b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>es</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.737808</td>\n",
              "      <td>0.760815</td>\n",
              "      <td>0.644272</td>\n",
              "      <td>0.587242</td>\n",
              "      <td>0.470484</td>\n",
              "      <td>0.765752</td>\n",
              "      <td>0.703752</td>\n",
              "      <td>0.766234</td>\n",
              "      <td>0.674798</td>\n",
              "      <td>0.754343</td>\n",
              "      <td>0.822457</td>\n",
              "      <td>bert-base-multilingual-cased_opt-66b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>es</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.390615</td>\n",
              "      <td>0.729738</td>\n",
              "      <td>0.455011</td>\n",
              "      <td>0.695724</td>\n",
              "      <td>0.372230</td>\n",
              "      <td>0.799220</td>\n",
              "      <td>0.386565</td>\n",
              "      <td>0.594937</td>\n",
              "      <td>0.571312</td>\n",
              "      <td>0.642585</td>\n",
              "      <td>0.455412</td>\n",
              "      <td>mGPT_opt-66b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>es</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.913856</td>\n",
              "      <td>0.850023</td>\n",
              "      <td>0.988333</td>\n",
              "      <td>0.949266</td>\n",
              "      <td>0.624038</td>\n",
              "      <td>0.876242</td>\n",
              "      <td>0.853753</td>\n",
              "      <td>0.787190</td>\n",
              "      <td>0.969586</td>\n",
              "      <td>0.962696</td>\n",
              "      <td>0.699728</td>\n",
              "      <td>mdeberta-v3-base_opt-66b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>es</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.605757</td>\n",
              "      <td>0.745234</td>\n",
              "      <td>0.876505</td>\n",
              "      <td>0.810358</td>\n",
              "      <td>0.444280</td>\n",
              "      <td>0.812310</td>\n",
              "      <td>0.470239</td>\n",
              "      <td>0.633545</td>\n",
              "      <td>0.810581</td>\n",
              "      <td>0.840842</td>\n",
              "      <td>0.538099</td>\n",
              "      <td>xlm-roberta-large_opt-66b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>es</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.953707</td>\n",
              "      <td>0.969710</td>\n",
              "      <td>0.963200</td>\n",
              "      <td>0.867572</td>\n",
              "      <td>0.683951</td>\n",
              "      <td>0.948185</td>\n",
              "      <td>0.969888</td>\n",
              "      <td>0.882016</td>\n",
              "      <td>0.902707</td>\n",
              "      <td>0.970712</td>\n",
              "      <td>0.952752</td>\n",
              "      <td>bert-base-multilingual-cased_opt-iml-max-1.3b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>es</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.943433</td>\n",
              "      <td>0.961329</td>\n",
              "      <td>0.920996</td>\n",
              "      <td>0.993208</td>\n",
              "      <td>0.404483</td>\n",
              "      <td>0.987905</td>\n",
              "      <td>0.840959</td>\n",
              "      <td>0.981028</td>\n",
              "      <td>0.897030</td>\n",
              "      <td>0.984507</td>\n",
              "      <td>0.583843</td>\n",
              "      <td>mGPT_opt-iml-max-1.3b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>es</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.993148</td>\n",
              "      <td>0.986553</td>\n",
              "      <td>0.993310</td>\n",
              "      <td>0.871046</td>\n",
              "      <td>0.586456</td>\n",
              "      <td>0.982728</td>\n",
              "      <td>0.932942</td>\n",
              "      <td>0.982758</td>\n",
              "      <td>0.994877</td>\n",
              "      <td>0.993109</td>\n",
              "      <td>0.874422</td>\n",
              "      <td>mdeberta-v3-base_opt-iml-max-1.3b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>es</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.979411</td>\n",
              "      <td>0.991594</td>\n",
              "      <td>0.858342</td>\n",
              "      <td>0.988115</td>\n",
              "      <td>0.619244</td>\n",
              "      <td>0.981002</td>\n",
              "      <td>0.979933</td>\n",
              "      <td>0.979310</td>\n",
              "      <td>0.757796</td>\n",
              "      <td>0.706033</td>\n",
              "      <td>0.818718</td>\n",
              "      <td>xlm-roberta-large_opt-iml-max-1.3b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>es</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.731949</td>\n",
              "      <td>0.904712</td>\n",
              "      <td>0.883332</td>\n",
              "      <td>0.860236</td>\n",
              "      <td>0.760513</td>\n",
              "      <td>0.931246</td>\n",
              "      <td>0.889246</td>\n",
              "      <td>0.863110</td>\n",
              "      <td>0.802661</td>\n",
              "      <td>0.740548</td>\n",
              "      <td>0.839972</td>\n",
              "      <td>bert-base-multilingual-cased_text-davinci-003</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>es</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.718775</td>\n",
              "      <td>0.802038</td>\n",
              "      <td>0.383186</td>\n",
              "      <td>0.927360</td>\n",
              "      <td>0.903856</td>\n",
              "      <td>0.965721</td>\n",
              "      <td>0.938161</td>\n",
              "      <td>0.916500</td>\n",
              "      <td>0.857829</td>\n",
              "      <td>0.716456</td>\n",
              "      <td>0.766293</td>\n",
              "      <td>mGPT_text-davinci-003</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>es</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.643584</td>\n",
              "      <td>0.761469</td>\n",
              "      <td>0.924799</td>\n",
              "      <td>0.923908</td>\n",
              "      <td>0.940433</td>\n",
              "      <td>0.805888</td>\n",
              "      <td>0.906504</td>\n",
              "      <td>0.848353</td>\n",
              "      <td>0.859653</td>\n",
              "      <td>0.744615</td>\n",
              "      <td>0.916644</td>\n",
              "      <td>mdeberta-v3-base_text-davinci-003</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>es</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.799302</td>\n",
              "      <td>0.926402</td>\n",
              "      <td>0.948212</td>\n",
              "      <td>0.950973</td>\n",
              "      <td>0.942226</td>\n",
              "      <td>0.977716</td>\n",
              "      <td>0.973283</td>\n",
              "      <td>0.935165</td>\n",
              "      <td>0.894400</td>\n",
              "      <td>0.792063</td>\n",
              "      <td>0.876797</td>\n",
              "      <td>xlm-roberta-large_text-davinci-003</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>es</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.862826</td>\n",
              "      <td>0.914947</td>\n",
              "      <td>0.770478</td>\n",
              "      <td>0.834088</td>\n",
              "      <td>0.750260</td>\n",
              "      <td>0.928081</td>\n",
              "      <td>0.899598</td>\n",
              "      <td>0.882343</td>\n",
              "      <td>0.822568</td>\n",
              "      <td>0.786864</td>\n",
              "      <td>0.851378</td>\n",
              "      <td>bert-base-multilingual-cased_vicuna-13b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>es</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.723426</td>\n",
              "      <td>0.871649</td>\n",
              "      <td>0.542705</td>\n",
              "      <td>0.920602</td>\n",
              "      <td>0.673940</td>\n",
              "      <td>0.967438</td>\n",
              "      <td>0.688021</td>\n",
              "      <td>0.916407</td>\n",
              "      <td>0.720465</td>\n",
              "      <td>0.789295</td>\n",
              "      <td>0.619095</td>\n",
              "      <td>mGPT_vicuna-13b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>es</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.678396</td>\n",
              "      <td>0.899356</td>\n",
              "      <td>0.838727</td>\n",
              "      <td>0.905250</td>\n",
              "      <td>0.916859</td>\n",
              "      <td>0.951998</td>\n",
              "      <td>0.875332</td>\n",
              "      <td>0.895597</td>\n",
              "      <td>0.859492</td>\n",
              "      <td>0.856181</td>\n",
              "      <td>0.829064</td>\n",
              "      <td>mdeberta-v3-base_vicuna-13b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>es</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.844734</td>\n",
              "      <td>0.878868</td>\n",
              "      <td>0.668109</td>\n",
              "      <td>0.861400</td>\n",
              "      <td>0.906107</td>\n",
              "      <td>0.943445</td>\n",
              "      <td>0.826702</td>\n",
              "      <td>0.895936</td>\n",
              "      <td>0.777281</td>\n",
              "      <td>0.640005</td>\n",
              "      <td>0.647270</td>\n",
              "      <td>xlm-roberta-large_vicuna-13b</td>\n",
              "      <td>es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>ru</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.602643</td>\n",
              "      <td>0.632650</td>\n",
              "      <td>0.806734</td>\n",
              "      <td>0.740883</td>\n",
              "      <td>0.463444</td>\n",
              "      <td>0.657224</td>\n",
              "      <td>0.670451</td>\n",
              "      <td>0.656725</td>\n",
              "      <td>0.909936</td>\n",
              "      <td>0.882879</td>\n",
              "      <td>0.669646</td>\n",
              "      <td>bert-base-multilingual-cased_alpaca-lora-30b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>ru</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.563543</td>\n",
              "      <td>0.412312</td>\n",
              "      <td>0.672520</td>\n",
              "      <td>0.871620</td>\n",
              "      <td>0.527200</td>\n",
              "      <td>0.771379</td>\n",
              "      <td>0.532546</td>\n",
              "      <td>0.791988</td>\n",
              "      <td>0.934978</td>\n",
              "      <td>0.912926</td>\n",
              "      <td>0.490640</td>\n",
              "      <td>mGPT_alpaca-lora-30b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>ru</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.971599</td>\n",
              "      <td>0.812253</td>\n",
              "      <td>0.984999</td>\n",
              "      <td>0.940874</td>\n",
              "      <td>0.854983</td>\n",
              "      <td>0.888526</td>\n",
              "      <td>0.886782</td>\n",
              "      <td>0.819913</td>\n",
              "      <td>0.961667</td>\n",
              "      <td>0.961536</td>\n",
              "      <td>0.823733</td>\n",
              "      <td>mdeberta-v3-base_alpaca-lora-30b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>ru</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.918018</td>\n",
              "      <td>0.898299</td>\n",
              "      <td>0.969973</td>\n",
              "      <td>0.944230</td>\n",
              "      <td>0.916940</td>\n",
              "      <td>0.905788</td>\n",
              "      <td>0.943052</td>\n",
              "      <td>0.899260</td>\n",
              "      <td>0.943282</td>\n",
              "      <td>0.939718</td>\n",
              "      <td>0.852248</td>\n",
              "      <td>xlm-roberta-large_alpaca-lora-30b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.941563</td>\n",
              "      <td>0.867992</td>\n",
              "      <td>0.904463</td>\n",
              "      <td>0.853816</td>\n",
              "      <td>0.781013</td>\n",
              "      <td>0.875397</td>\n",
              "      <td>0.878277</td>\n",
              "      <td>0.837753</td>\n",
              "      <td>0.956666</td>\n",
              "      <td>0.933107</td>\n",
              "      <td>0.866630</td>\n",
              "      <td>bert-base-multilingual-cased_gpt-3.5-turbo</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.784573</td>\n",
              "      <td>0.885671</td>\n",
              "      <td>0.355178</td>\n",
              "      <td>0.907050</td>\n",
              "      <td>0.881669</td>\n",
              "      <td>0.922780</td>\n",
              "      <td>0.922999</td>\n",
              "      <td>0.904259</td>\n",
              "      <td>0.953325</td>\n",
              "      <td>0.924428</td>\n",
              "      <td>0.625838</td>\n",
              "      <td>mGPT_gpt-3.5-turbo</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.936497</td>\n",
              "      <td>0.838727</td>\n",
              "      <td>0.981664</td>\n",
              "      <td>0.925553</td>\n",
              "      <td>0.960288</td>\n",
              "      <td>0.929596</td>\n",
              "      <td>0.914644</td>\n",
              "      <td>0.902296</td>\n",
              "      <td>0.928300</td>\n",
              "      <td>0.948157</td>\n",
              "      <td>0.832907</td>\n",
              "      <td>mdeberta-v3-base_gpt-3.5-turbo</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.933217</td>\n",
              "      <td>0.943227</td>\n",
              "      <td>0.988333</td>\n",
              "      <td>0.939133</td>\n",
              "      <td>0.967505</td>\n",
              "      <td>0.943465</td>\n",
              "      <td>0.946552</td>\n",
              "      <td>0.914272</td>\n",
              "      <td>0.976663</td>\n",
              "      <td>0.961514</td>\n",
              "      <td>0.904835</td>\n",
              "      <td>xlm-roberta-large_gpt-3.5-turbo</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.866685</td>\n",
              "      <td>0.706686</td>\n",
              "      <td>0.782835</td>\n",
              "      <td>0.753498</td>\n",
              "      <td>0.495834</td>\n",
              "      <td>0.680546</td>\n",
              "      <td>0.751414</td>\n",
              "      <td>0.691832</td>\n",
              "      <td>0.933297</td>\n",
              "      <td>0.921399</td>\n",
              "      <td>0.812230</td>\n",
              "      <td>bert-base-multilingual-cased_gpt-4</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.897834</td>\n",
              "      <td>0.897758</td>\n",
              "      <td>0.358748</td>\n",
              "      <td>0.918762</td>\n",
              "      <td>0.871267</td>\n",
              "      <td>0.938269</td>\n",
              "      <td>0.924571</td>\n",
              "      <td>0.926369</td>\n",
              "      <td>0.956649</td>\n",
              "      <td>0.856972</td>\n",
              "      <td>0.707815</td>\n",
              "      <td>mGPT_gpt-4</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.934891</td>\n",
              "      <td>0.555606</td>\n",
              "      <td>0.878788</td>\n",
              "      <td>0.922123</td>\n",
              "      <td>0.925886</td>\n",
              "      <td>0.761128</td>\n",
              "      <td>0.760885</td>\n",
              "      <td>0.693175</td>\n",
              "      <td>0.830468</td>\n",
              "      <td>0.894049</td>\n",
              "      <td>0.779666</td>\n",
              "      <td>mdeberta-v3-base_gpt-4</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>ru</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.921514</td>\n",
              "      <td>0.959956</td>\n",
              "      <td>0.995000</td>\n",
              "      <td>0.939120</td>\n",
              "      <td>0.976532</td>\n",
              "      <td>0.934855</td>\n",
              "      <td>0.963270</td>\n",
              "      <td>0.936803</td>\n",
              "      <td>0.956667</td>\n",
              "      <td>0.944703</td>\n",
              "      <td>0.921368</td>\n",
              "      <td>xlm-roberta-large_gpt-4</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>ru</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.707838</td>\n",
              "      <td>0.530048</td>\n",
              "      <td>0.875015</td>\n",
              "      <td>0.647655</td>\n",
              "      <td>0.332527</td>\n",
              "      <td>0.821116</td>\n",
              "      <td>0.519080</td>\n",
              "      <td>0.604306</td>\n",
              "      <td>0.984894</td>\n",
              "      <td>0.976421</td>\n",
              "      <td>0.476467</td>\n",
              "      <td>bert-base-multilingual-cased_llama-65b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>ru</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.534740</td>\n",
              "      <td>0.332589</td>\n",
              "      <td>0.615361</td>\n",
              "      <td>0.647033</td>\n",
              "      <td>0.332527</td>\n",
              "      <td>0.496365</td>\n",
              "      <td>0.340351</td>\n",
              "      <td>0.484719</td>\n",
              "      <td>0.978185</td>\n",
              "      <td>0.935939</td>\n",
              "      <td>0.405639</td>\n",
              "      <td>mGPT_llama-65b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>ru</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.762908</td>\n",
              "      <td>0.427667</td>\n",
              "      <td>0.984771</td>\n",
              "      <td>0.854020</td>\n",
              "      <td>0.382439</td>\n",
              "      <td>0.730500</td>\n",
              "      <td>0.414468</td>\n",
              "      <td>0.538322</td>\n",
              "      <td>0.988253</td>\n",
              "      <td>0.981477</td>\n",
              "      <td>0.562238</td>\n",
              "      <td>mdeberta-v3-base_llama-65b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>ru</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.885839</td>\n",
              "      <td>0.570539</td>\n",
              "      <td>0.986464</td>\n",
              "      <td>0.916035</td>\n",
              "      <td>0.370507</td>\n",
              "      <td>0.502410</td>\n",
              "      <td>0.367375</td>\n",
              "      <td>0.582594</td>\n",
              "      <td>0.993287</td>\n",
              "      <td>0.988215</td>\n",
              "      <td>0.631352</td>\n",
              "      <td>xlm-roberta-large_llama-65b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>ru</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.759294</td>\n",
              "      <td>0.606131</td>\n",
              "      <td>0.767321</td>\n",
              "      <td>0.607818</td>\n",
              "      <td>0.471954</td>\n",
              "      <td>0.642337</td>\n",
              "      <td>0.642806</td>\n",
              "      <td>0.673385</td>\n",
              "      <td>0.933984</td>\n",
              "      <td>0.925416</td>\n",
              "      <td>0.640839</td>\n",
              "      <td>bert-base-multilingual-cased_opt-66b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>ru</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.971423</td>\n",
              "      <td>0.512291</td>\n",
              "      <td>0.577647</td>\n",
              "      <td>0.399889</td>\n",
              "      <td>0.347268</td>\n",
              "      <td>0.426150</td>\n",
              "      <td>0.604990</td>\n",
              "      <td>0.416472</td>\n",
              "      <td>0.966202</td>\n",
              "      <td>0.981352</td>\n",
              "      <td>0.764907</td>\n",
              "      <td>mGPT_opt-66b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>ru</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.939331</td>\n",
              "      <td>0.742819</td>\n",
              "      <td>0.983329</td>\n",
              "      <td>0.486039</td>\n",
              "      <td>0.341306</td>\n",
              "      <td>0.339394</td>\n",
              "      <td>0.625486</td>\n",
              "      <td>0.361007</td>\n",
              "      <td>0.984788</td>\n",
              "      <td>0.993219</td>\n",
              "      <td>0.764493</td>\n",
              "      <td>mdeberta-v3-base_opt-66b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>ru</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.954589</td>\n",
              "      <td>0.855203</td>\n",
              "      <td>0.991666</td>\n",
              "      <td>0.554446</td>\n",
              "      <td>0.345255</td>\n",
              "      <td>0.547415</td>\n",
              "      <td>0.773418</td>\n",
              "      <td>0.616838</td>\n",
              "      <td>0.984794</td>\n",
              "      <td>0.986441</td>\n",
              "      <td>0.852985</td>\n",
              "      <td>xlm-roberta-large_opt-66b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>ru</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.958780</td>\n",
              "      <td>0.542001</td>\n",
              "      <td>0.870710</td>\n",
              "      <td>0.507309</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.441679</td>\n",
              "      <td>0.537946</td>\n",
              "      <td>0.415349</td>\n",
              "      <td>0.979482</td>\n",
              "      <td>0.982761</td>\n",
              "      <td>0.824275</td>\n",
              "      <td>bert-base-multilingual-cased_opt-iml-max-1.3b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>ru</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.963964</td>\n",
              "      <td>0.658299</td>\n",
              "      <td>0.359651</td>\n",
              "      <td>0.561295</td>\n",
              "      <td>0.374969</td>\n",
              "      <td>0.387302</td>\n",
              "      <td>0.837362</td>\n",
              "      <td>0.437818</td>\n",
              "      <td>0.994876</td>\n",
              "      <td>0.981029</td>\n",
              "      <td>0.767807</td>\n",
              "      <td>mGPT_opt-iml-max-1.3b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>ru</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.890215</td>\n",
              "      <td>0.511446</td>\n",
              "      <td>0.937863</td>\n",
              "      <td>0.576746</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.392633</td>\n",
              "      <td>0.622961</td>\n",
              "      <td>0.398365</td>\n",
              "      <td>0.922422</td>\n",
              "      <td>0.970651</td>\n",
              "      <td>0.833545</td>\n",
              "      <td>mdeberta-v3-base_opt-iml-max-1.3b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>ru</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.993146</td>\n",
              "      <td>0.727486</td>\n",
              "      <td>0.989965</td>\n",
              "      <td>0.755102</td>\n",
              "      <td>0.337332</td>\n",
              "      <td>0.394444</td>\n",
              "      <td>0.863783</td>\n",
              "      <td>0.443158</td>\n",
              "      <td>0.996585</td>\n",
              "      <td>0.994832</td>\n",
              "      <td>0.933835</td>\n",
              "      <td>xlm-roberta-large_opt-iml-max-1.3b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>ru</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.748031</td>\n",
              "      <td>0.699821</td>\n",
              "      <td>0.647573</td>\n",
              "      <td>0.808713</td>\n",
              "      <td>0.724933</td>\n",
              "      <td>0.816685</td>\n",
              "      <td>0.715688</td>\n",
              "      <td>0.764468</td>\n",
              "      <td>0.914614</td>\n",
              "      <td>0.907869</td>\n",
              "      <td>0.791094</td>\n",
              "      <td>bert-base-multilingual-cased_text-davinci-003</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>ru</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.879773</td>\n",
              "      <td>0.637560</td>\n",
              "      <td>0.355178</td>\n",
              "      <td>0.862949</td>\n",
              "      <td>0.683093</td>\n",
              "      <td>0.869563</td>\n",
              "      <td>0.798742</td>\n",
              "      <td>0.799616</td>\n",
              "      <td>0.931540</td>\n",
              "      <td>0.819250</td>\n",
              "      <td>0.641026</td>\n",
              "      <td>mGPT_text-davinci-003</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>ru</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.880012</td>\n",
              "      <td>0.439919</td>\n",
              "      <td>0.687666</td>\n",
              "      <td>0.583750</td>\n",
              "      <td>0.345255</td>\n",
              "      <td>0.518355</td>\n",
              "      <td>0.484494</td>\n",
              "      <td>0.513155</td>\n",
              "      <td>0.938054</td>\n",
              "      <td>0.959758</td>\n",
              "      <td>0.735809</td>\n",
              "      <td>mdeberta-v3-base_text-davinci-003</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>ru</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.866381</td>\n",
              "      <td>0.443979</td>\n",
              "      <td>0.742229</td>\n",
              "      <td>0.711712</td>\n",
              "      <td>0.377834</td>\n",
              "      <td>0.567945</td>\n",
              "      <td>0.589670</td>\n",
              "      <td>0.569955</td>\n",
              "      <td>0.938186</td>\n",
              "      <td>0.949698</td>\n",
              "      <td>0.774999</td>\n",
              "      <td>xlm-roberta-large_text-davinci-003</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>ru</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.545024</td>\n",
              "      <td>0.362297</td>\n",
              "      <td>0.572402</td>\n",
              "      <td>0.454921</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.475122</td>\n",
              "      <td>0.354810</td>\n",
              "      <td>0.372091</td>\n",
              "      <td>0.926421</td>\n",
              "      <td>0.840089</td>\n",
              "      <td>0.347210</td>\n",
              "      <td>bert-base-multilingual-cased_vicuna-13b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>ru</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.684569</td>\n",
              "      <td>0.340698</td>\n",
              "      <td>0.865946</td>\n",
              "      <td>0.857633</td>\n",
              "      <td>0.372237</td>\n",
              "      <td>0.618774</td>\n",
              "      <td>0.366299</td>\n",
              "      <td>0.549896</td>\n",
              "      <td>0.954849</td>\n",
              "      <td>0.897842</td>\n",
              "      <td>0.544260</td>\n",
              "      <td>mGPT_vicuna-13b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>ru</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.988313</td>\n",
              "      <td>0.797985</td>\n",
              "      <td>0.938098</td>\n",
              "      <td>0.903297</td>\n",
              "      <td>0.813637</td>\n",
              "      <td>0.902333</td>\n",
              "      <td>0.875988</td>\n",
              "      <td>0.846442</td>\n",
              "      <td>0.956463</td>\n",
              "      <td>0.958152</td>\n",
              "      <td>0.859437</td>\n",
              "      <td>mdeberta-v3-base_vicuna-13b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>ru</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.971604</td>\n",
              "      <td>0.752577</td>\n",
              "      <td>0.927963</td>\n",
              "      <td>0.939120</td>\n",
              "      <td>0.857196</td>\n",
              "      <td>0.929760</td>\n",
              "      <td>0.697895</td>\n",
              "      <td>0.841560</td>\n",
              "      <td>0.961481</td>\n",
              "      <td>0.927846</td>\n",
              "      <td>0.899810</td>\n",
              "      <td>xlm-roberta-large_vicuna-13b</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>all</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.509603</td>\n",
              "      <td>0.913332</td>\n",
              "      <td>0.887860</td>\n",
              "      <td>0.913839</td>\n",
              "      <td>0.965698</td>\n",
              "      <td>0.940064</td>\n",
              "      <td>0.916148</td>\n",
              "      <td>0.911300</td>\n",
              "      <td>0.919999</td>\n",
              "      <td>0.892832</td>\n",
              "      <td>0.747233</td>\n",
              "      <td>bert-base-multilingual-cased_alpaca-lora-30b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>all</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.695488</td>\n",
              "      <td>0.883328</td>\n",
              "      <td>0.648040</td>\n",
              "      <td>0.928988</td>\n",
              "      <td>0.958483</td>\n",
              "      <td>0.931506</td>\n",
              "      <td>0.821486</td>\n",
              "      <td>0.933449</td>\n",
              "      <td>0.938332</td>\n",
              "      <td>0.912946</td>\n",
              "      <td>0.555541</td>\n",
              "      <td>mGPT_alpaca-lora-30b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>all</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.843820</td>\n",
              "      <td>0.806245</td>\n",
              "      <td>0.890609</td>\n",
              "      <td>0.872700</td>\n",
              "      <td>0.907484</td>\n",
              "      <td>0.864293</td>\n",
              "      <td>0.804425</td>\n",
              "      <td>0.836376</td>\n",
              "      <td>0.928324</td>\n",
              "      <td>0.933074</td>\n",
              "      <td>0.711756</td>\n",
              "      <td>mdeberta-v3-base_alpaca-lora-30b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>all</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.774189</td>\n",
              "      <td>0.928309</td>\n",
              "      <td>0.959972</td>\n",
              "      <td>0.957769</td>\n",
              "      <td>0.985559</td>\n",
              "      <td>0.958900</td>\n",
              "      <td>0.948084</td>\n",
              "      <td>0.952183</td>\n",
              "      <td>0.936622</td>\n",
              "      <td>0.924572</td>\n",
              "      <td>0.819962</td>\n",
              "      <td>xlm-roberta-large_alpaca-lora-30b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>all</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.848292</td>\n",
              "      <td>0.974999</td>\n",
              "      <td>0.963332</td>\n",
              "      <td>0.939072</td>\n",
              "      <td>0.981949</td>\n",
              "      <td>0.977734</td>\n",
              "      <td>0.981635</td>\n",
              "      <td>0.950596</td>\n",
              "      <td>0.954994</td>\n",
              "      <td>0.931318</td>\n",
              "      <td>0.914828</td>\n",
              "      <td>bert-base-multilingual-cased_gpt-3.5-turbo</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>all</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.885649</td>\n",
              "      <td>0.919799</td>\n",
              "      <td>0.355178</td>\n",
              "      <td>0.896361</td>\n",
              "      <td>0.980143</td>\n",
              "      <td>0.946918</td>\n",
              "      <td>0.956594</td>\n",
              "      <td>0.936968</td>\n",
              "      <td>0.938284</td>\n",
              "      <td>0.893877</td>\n",
              "      <td>0.569309</td>\n",
              "      <td>mGPT_gpt-3.5-turbo</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>all</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.946576</td>\n",
              "      <td>0.871004</td>\n",
              "      <td>0.976666</td>\n",
              "      <td>0.957769</td>\n",
              "      <td>0.974728</td>\n",
              "      <td>0.928041</td>\n",
              "      <td>0.897707</td>\n",
              "      <td>0.903930</td>\n",
              "      <td>0.923248</td>\n",
              "      <td>0.939775</td>\n",
              "      <td>0.852572</td>\n",
              "      <td>mdeberta-v3-base_gpt-3.5-turbo</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>all</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.928008</td>\n",
              "      <td>0.985000</td>\n",
              "      <td>0.988332</td>\n",
              "      <td>0.942473</td>\n",
              "      <td>0.990974</td>\n",
              "      <td>0.984581</td>\n",
              "      <td>0.979967</td>\n",
              "      <td>0.972736</td>\n",
              "      <td>0.984999</td>\n",
              "      <td>0.946371</td>\n",
              "      <td>0.938168</td>\n",
              "      <td>xlm-roberta-large_gpt-3.5-turbo</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>all</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.941559</td>\n",
              "      <td>0.956608</td>\n",
              "      <td>0.953308</td>\n",
              "      <td>0.925458</td>\n",
              "      <td>0.992780</td>\n",
              "      <td>0.948512</td>\n",
              "      <td>0.968277</td>\n",
              "      <td>0.945291</td>\n",
              "      <td>0.939983</td>\n",
              "      <td>0.936452</td>\n",
              "      <td>0.904807</td>\n",
              "      <td>bert-base-multilingual-cased_gpt-4</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>all</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.891135</td>\n",
              "      <td>0.944988</td>\n",
              "      <td>0.340698</td>\n",
              "      <td>0.901432</td>\n",
              "      <td>0.992779</td>\n",
              "      <td>0.964017</td>\n",
              "      <td>0.924571</td>\n",
              "      <td>0.948731</td>\n",
              "      <td>0.946666</td>\n",
              "      <td>0.921219</td>\n",
              "      <td>0.723636</td>\n",
              "      <td>mGPT_gpt-4</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>all</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.938224</td>\n",
              "      <td>0.931459</td>\n",
              "      <td>0.988333</td>\n",
              "      <td>0.979728</td>\n",
              "      <td>0.945747</td>\n",
              "      <td>0.950183</td>\n",
              "      <td>0.958217</td>\n",
              "      <td>0.919479</td>\n",
              "      <td>0.938295</td>\n",
              "      <td>0.949787</td>\n",
              "      <td>0.885362</td>\n",
              "      <td>mdeberta-v3-base_gpt-4</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>all</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.959932</td>\n",
              "      <td>0.973326</td>\n",
              "      <td>0.985000</td>\n",
              "      <td>0.969595</td>\n",
              "      <td>0.994585</td>\n",
              "      <td>0.969142</td>\n",
              "      <td>0.981636</td>\n",
              "      <td>0.982950</td>\n",
              "      <td>0.973332</td>\n",
              "      <td>0.983277</td>\n",
              "      <td>0.936548</td>\n",
              "      <td>xlm-roberta-large_gpt-4</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>all</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.782089</td>\n",
              "      <td>0.864468</td>\n",
              "      <td>0.928762</td>\n",
              "      <td>0.786738</td>\n",
              "      <td>0.907606</td>\n",
              "      <td>0.987940</td>\n",
              "      <td>0.783536</td>\n",
              "      <td>0.944907</td>\n",
              "      <td>0.978180</td>\n",
              "      <td>0.978111</td>\n",
              "      <td>0.658036</td>\n",
              "      <td>bert-base-multilingual-cased_llama-65b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>all</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.885282</td>\n",
              "      <td>0.950910</td>\n",
              "      <td>0.905754</td>\n",
              "      <td>0.929281</td>\n",
              "      <td>0.986227</td>\n",
              "      <td>0.719234</td>\n",
              "      <td>0.969059</td>\n",
              "      <td>0.979860</td>\n",
              "      <td>0.973061</td>\n",
              "      <td>0.533715</td>\n",
              "      <td>mGPT_llama-65b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>all</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.800609</td>\n",
              "      <td>0.888725</td>\n",
              "      <td>0.983078</td>\n",
              "      <td>0.806141</td>\n",
              "      <td>0.900354</td>\n",
              "      <td>0.982770</td>\n",
              "      <td>0.645039</td>\n",
              "      <td>0.953522</td>\n",
              "      <td>0.989933</td>\n",
              "      <td>0.966299</td>\n",
              "      <td>0.597399</td>\n",
              "      <td>mdeberta-v3-base_llama-65b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>all</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.744559</td>\n",
              "      <td>0.801332</td>\n",
              "      <td>0.937253</td>\n",
              "      <td>0.882691</td>\n",
              "      <td>0.898359</td>\n",
              "      <td>0.991388</td>\n",
              "      <td>0.467301</td>\n",
              "      <td>0.944806</td>\n",
              "      <td>0.989932</td>\n",
              "      <td>0.979792</td>\n",
              "      <td>0.599157</td>\n",
              "      <td>xlm-roberta-large_llama-65b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>all</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.796683</td>\n",
              "      <td>0.721942</td>\n",
              "      <td>0.767449</td>\n",
              "      <td>0.728228</td>\n",
              "      <td>0.871580</td>\n",
              "      <td>0.809685</td>\n",
              "      <td>0.688022</td>\n",
              "      <td>0.825600</td>\n",
              "      <td>0.916831</td>\n",
              "      <td>0.899965</td>\n",
              "      <td>0.773006</td>\n",
              "      <td>bert-base-multilingual-cased_opt-66b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>all</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.906852</td>\n",
              "      <td>0.844007</td>\n",
              "      <td>0.801006</td>\n",
              "      <td>0.818552</td>\n",
              "      <td>0.904262</td>\n",
              "      <td>0.862601</td>\n",
              "      <td>0.759110</td>\n",
              "      <td>0.801186</td>\n",
              "      <td>0.974662</td>\n",
              "      <td>0.972880</td>\n",
              "      <td>0.692877</td>\n",
              "      <td>mGPT_opt-66b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>all</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.891294</td>\n",
              "      <td>0.939626</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.897798</td>\n",
              "      <td>0.847868</td>\n",
              "      <td>0.884706</td>\n",
              "      <td>0.876344</td>\n",
              "      <td>0.864558</td>\n",
              "      <td>0.988175</td>\n",
              "      <td>0.983051</td>\n",
              "      <td>0.807321</td>\n",
              "      <td>mdeberta-v3-base_opt-66b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>all</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.941006</td>\n",
              "      <td>0.951433</td>\n",
              "      <td>0.996667</td>\n",
              "      <td>0.834821</td>\n",
              "      <td>0.887941</td>\n",
              "      <td>0.869471</td>\n",
              "      <td>0.895853</td>\n",
              "      <td>0.921224</td>\n",
              "      <td>0.974624</td>\n",
              "      <td>0.996610</td>\n",
              "      <td>0.780601</td>\n",
              "      <td>xlm-roberta-large_opt-66b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>all</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.950318</td>\n",
              "      <td>0.932673</td>\n",
              "      <td>0.959830</td>\n",
              "      <td>0.838827</td>\n",
              "      <td>0.920568</td>\n",
              "      <td>0.921907</td>\n",
              "      <td>0.900615</td>\n",
              "      <td>0.922375</td>\n",
              "      <td>0.986342</td>\n",
              "      <td>0.984503</td>\n",
              "      <td>0.949368</td>\n",
              "      <td>bert-base-multilingual-cased_opt-iml-max-1.3b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>all</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.976027</td>\n",
              "      <td>0.959650</td>\n",
              "      <td>0.924281</td>\n",
              "      <td>0.952422</td>\n",
              "      <td>0.956678</td>\n",
              "      <td>0.979273</td>\n",
              "      <td>0.956452</td>\n",
              "      <td>0.970687</td>\n",
              "      <td>0.998292</td>\n",
              "      <td>0.996555</td>\n",
              "      <td>0.768063</td>\n",
              "      <td>mGPT_opt-iml-max-1.3b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>all</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.976027</td>\n",
              "      <td>0.988235</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.957543</td>\n",
              "      <td>0.920511</td>\n",
              "      <td>0.967184</td>\n",
              "      <td>0.973225</td>\n",
              "      <td>0.981028</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.996555</td>\n",
              "      <td>0.799917</td>\n",
              "      <td>mdeberta-v3-base_opt-iml-max-1.3b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>all</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.986297</td>\n",
              "      <td>0.996638</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.993208</td>\n",
              "      <td>0.905427</td>\n",
              "      <td>0.968910</td>\n",
              "      <td>0.991639</td>\n",
              "      <td>0.970689</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998278</td>\n",
              "      <td>0.807778</td>\n",
              "      <td>xlm-roberta-large_opt-iml-max-1.3b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>all</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.824558</td>\n",
              "      <td>0.958319</td>\n",
              "      <td>0.934904</td>\n",
              "      <td>0.952694</td>\n",
              "      <td>0.963876</td>\n",
              "      <td>0.963975</td>\n",
              "      <td>0.961587</td>\n",
              "      <td>0.935083</td>\n",
              "      <td>0.909843</td>\n",
              "      <td>0.877524</td>\n",
              "      <td>0.918315</td>\n",
              "      <td>bert-base-multilingual-cased_text-davinci-003</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>all</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.846222</td>\n",
              "      <td>0.684687</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.972973</td>\n",
              "      <td>0.980143</td>\n",
              "      <td>0.974306</td>\n",
              "      <td>0.959930</td>\n",
              "      <td>0.943779</td>\n",
              "      <td>0.949885</td>\n",
              "      <td>0.839593</td>\n",
              "      <td>0.844200</td>\n",
              "      <td>mGPT_text-davinci-003</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>all</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.759815</td>\n",
              "      <td>0.853584</td>\n",
              "      <td>0.951569</td>\n",
              "      <td>0.911799</td>\n",
              "      <td>0.896429</td>\n",
              "      <td>0.844561</td>\n",
              "      <td>0.875486</td>\n",
              "      <td>0.889139</td>\n",
              "      <td>0.934889</td>\n",
              "      <td>0.924432</td>\n",
              "      <td>0.933065</td>\n",
              "      <td>mdeberta-v3-base_text-davinci-003</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>all</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.670553</td>\n",
              "      <td>0.793827</td>\n",
              "      <td>0.887007</td>\n",
              "      <td>0.935667</td>\n",
              "      <td>0.969302</td>\n",
              "      <td>0.929794</td>\n",
              "      <td>0.964938</td>\n",
              "      <td>0.906194</td>\n",
              "      <td>0.904119</td>\n",
              "      <td>0.833569</td>\n",
              "      <td>0.671403</td>\n",
              "      <td>xlm-roberta-large_text-davinci-003</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>all</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.829439</td>\n",
              "      <td>0.953291</td>\n",
              "      <td>0.896251</td>\n",
              "      <td>0.890177</td>\n",
              "      <td>0.960288</td>\n",
              "      <td>0.948608</td>\n",
              "      <td>0.959933</td>\n",
              "      <td>0.942030</td>\n",
              "      <td>0.939775</td>\n",
              "      <td>0.892804</td>\n",
              "      <td>0.717279</td>\n",
              "      <td>bert-base-multilingual-cased_vicuna-13b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>all</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.846662</td>\n",
              "      <td>0.862044</td>\n",
              "      <td>0.509959</td>\n",
              "      <td>0.903105</td>\n",
              "      <td>0.980139</td>\n",
              "      <td>0.939827</td>\n",
              "      <td>0.890662</td>\n",
              "      <td>0.935210</td>\n",
              "      <td>0.941314</td>\n",
              "      <td>0.845123</td>\n",
              "      <td>0.652242</td>\n",
              "      <td>mGPT_vicuna-13b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>all</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.854263</td>\n",
              "      <td>0.807752</td>\n",
              "      <td>0.899057</td>\n",
              "      <td>0.920361</td>\n",
              "      <td>0.890658</td>\n",
              "      <td>0.841004</td>\n",
              "      <td>0.859559</td>\n",
              "      <td>0.872052</td>\n",
              "      <td>0.936295</td>\n",
              "      <td>0.936409</td>\n",
              "      <td>0.701151</td>\n",
              "      <td>mdeberta-v3-base_vicuna-13b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>all</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.880938</td>\n",
              "      <td>0.951656</td>\n",
              "      <td>0.934778</td>\n",
              "      <td>0.959448</td>\n",
              "      <td>0.981949</td>\n",
              "      <td>0.970876</td>\n",
              "      <td>0.917866</td>\n",
              "      <td>0.921323</td>\n",
              "      <td>0.974913</td>\n",
              "      <td>0.956520</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>xlm-roberta-large_vicuna-13b</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>en3</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.495071</td>\n",
              "      <td>0.874293</td>\n",
              "      <td>0.859961</td>\n",
              "      <td>0.905336</td>\n",
              "      <td>0.983754</td>\n",
              "      <td>0.761689</td>\n",
              "      <td>0.907913</td>\n",
              "      <td>0.857861</td>\n",
              "      <td>0.846113</td>\n",
              "      <td>0.814381</td>\n",
              "      <td>0.571886</td>\n",
              "      <td>bert-base-multilingual-cased_alpaca-lora-30b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>en3</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.293049</td>\n",
              "      <td>0.762298</td>\n",
              "      <td>0.495798</td>\n",
              "      <td>0.885029</td>\n",
              "      <td>0.980143</td>\n",
              "      <td>0.736009</td>\n",
              "      <td>0.859440</td>\n",
              "      <td>0.865471</td>\n",
              "      <td>0.820896</td>\n",
              "      <td>0.804347</td>\n",
              "      <td>0.426802</td>\n",
              "      <td>mGPT_alpaca-lora-30b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>en3</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.176374</td>\n",
              "      <td>0.687197</td>\n",
              "      <td>0.809567</td>\n",
              "      <td>0.852901</td>\n",
              "      <td>0.938548</td>\n",
              "      <td>0.635581</td>\n",
              "      <td>0.788842</td>\n",
              "      <td>0.734008</td>\n",
              "      <td>0.733177</td>\n",
              "      <td>0.682376</td>\n",
              "      <td>0.417706</td>\n",
              "      <td>mdeberta-v3-base_alpaca-lora-30b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>en3</td>\n",
              "      <td>alpaca-lora-30b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.330592</td>\n",
              "      <td>0.837792</td>\n",
              "      <td>0.878184</td>\n",
              "      <td>0.942514</td>\n",
              "      <td>0.987364</td>\n",
              "      <td>0.698462</td>\n",
              "      <td>0.900725</td>\n",
              "      <td>0.787154</td>\n",
              "      <td>0.829411</td>\n",
              "      <td>0.852366</td>\n",
              "      <td>0.598436</td>\n",
              "      <td>xlm-roberta-large_alpaca-lora-30b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.944893</td>\n",
              "      <td>0.917785</td>\n",
              "      <td>0.938332</td>\n",
              "      <td>0.925675</td>\n",
              "      <td>0.981949</td>\n",
              "      <td>0.871843</td>\n",
              "      <td>0.949876</td>\n",
              "      <td>0.929827</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.894599</td>\n",
              "      <td>0.908321</td>\n",
              "      <td>bert-base-multilingual-cased_gpt-3.5-turbo</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.678622</td>\n",
              "      <td>0.908333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.904880</td>\n",
              "      <td>0.987364</td>\n",
              "      <td>0.887388</td>\n",
              "      <td>0.963271</td>\n",
              "      <td>0.928358</td>\n",
              "      <td>0.923248</td>\n",
              "      <td>0.886038</td>\n",
              "      <td>0.549381</td>\n",
              "      <td>mGPT_gpt-3.5-turbo</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.814844</td>\n",
              "      <td>0.735275</td>\n",
              "      <td>0.815809</td>\n",
              "      <td>0.878317</td>\n",
              "      <td>0.925760</td>\n",
              "      <td>0.702010</td>\n",
              "      <td>0.826702</td>\n",
              "      <td>0.783397</td>\n",
              "      <td>0.676881</td>\n",
              "      <td>0.734812</td>\n",
              "      <td>0.729492</td>\n",
              "      <td>mdeberta-v3-base_gpt-3.5-turbo</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.919866</td>\n",
              "      <td>0.979999</td>\n",
              "      <td>0.974997</td>\n",
              "      <td>0.940817</td>\n",
              "      <td>0.983752</td>\n",
              "      <td>0.919230</td>\n",
              "      <td>0.954912</td>\n",
              "      <td>0.938500</td>\n",
              "      <td>0.951660</td>\n",
              "      <td>0.946450</td>\n",
              "      <td>0.796341</td>\n",
              "      <td>xlm-roberta-large_gpt-3.5-turbo</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.834971</td>\n",
              "      <td>0.900893</td>\n",
              "      <td>0.919999</td>\n",
              "      <td>0.930727</td>\n",
              "      <td>0.996390</td>\n",
              "      <td>0.732342</td>\n",
              "      <td>0.938175</td>\n",
              "      <td>0.838871</td>\n",
              "      <td>0.876579</td>\n",
              "      <td>0.866167</td>\n",
              "      <td>0.807944</td>\n",
              "      <td>bert-base-multilingual-cased_gpt-4</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.848069</td>\n",
              "      <td>0.929888</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.910430</td>\n",
              "      <td>0.992780</td>\n",
              "      <td>0.795138</td>\n",
              "      <td>0.931403</td>\n",
              "      <td>0.907192</td>\n",
              "      <td>0.879774</td>\n",
              "      <td>0.806716</td>\n",
              "      <td>0.657042</td>\n",
              "      <td>mGPT_gpt-4</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.819887</td>\n",
              "      <td>0.724894</td>\n",
              "      <td>0.907612</td>\n",
              "      <td>0.867904</td>\n",
              "      <td>0.629051</td>\n",
              "      <td>0.650597</td>\n",
              "      <td>0.802330</td>\n",
              "      <td>0.706365</td>\n",
              "      <td>0.779360</td>\n",
              "      <td>0.827100</td>\n",
              "      <td>0.693959</td>\n",
              "      <td>mdeberta-v3-base_gpt-4</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>en3</td>\n",
              "      <td>gpt-4</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.877966</td>\n",
              "      <td>0.909319</td>\n",
              "      <td>0.956643</td>\n",
              "      <td>0.964522</td>\n",
              "      <td>0.994585</td>\n",
              "      <td>0.828572</td>\n",
              "      <td>0.924821</td>\n",
              "      <td>0.880757</td>\n",
              "      <td>0.906563</td>\n",
              "      <td>0.929765</td>\n",
              "      <td>0.861447</td>\n",
              "      <td>xlm-roberta-large_gpt-4</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>en3</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.655237</td>\n",
              "      <td>0.641660</td>\n",
              "      <td>0.797071</td>\n",
              "      <td>0.701859</td>\n",
              "      <td>0.921855</td>\n",
              "      <td>0.805686</td>\n",
              "      <td>0.637019</td>\n",
              "      <td>0.687980</td>\n",
              "      <td>0.701551</td>\n",
              "      <td>0.803135</td>\n",
              "      <td>0.565878</td>\n",
              "      <td>bert-base-multilingual-cased_llama-65b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>en3</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.336877</td>\n",
              "      <td>0.361560</td>\n",
              "      <td>0.776111</td>\n",
              "      <td>0.715696</td>\n",
              "      <td>0.914707</td>\n",
              "      <td>0.551698</td>\n",
              "      <td>0.459106</td>\n",
              "      <td>0.561643</td>\n",
              "      <td>0.484584</td>\n",
              "      <td>0.513736</td>\n",
              "      <td>0.555561</td>\n",
              "      <td>mGPT_llama-65b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>en3</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.747725</td>\n",
              "      <td>0.664557</td>\n",
              "      <td>0.927212</td>\n",
              "      <td>0.756037</td>\n",
              "      <td>0.895919</td>\n",
              "      <td>0.866191</td>\n",
              "      <td>0.682070</td>\n",
              "      <td>0.717597</td>\n",
              "      <td>0.773435</td>\n",
              "      <td>0.853296</td>\n",
              "      <td>0.590222</td>\n",
              "      <td>mdeberta-v3-base_llama-65b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>en3</td>\n",
              "      <td>llama-65b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.453154</td>\n",
              "      <td>0.407351</td>\n",
              "      <td>0.841352</td>\n",
              "      <td>0.719821</td>\n",
              "      <td>0.938366</td>\n",
              "      <td>0.639744</td>\n",
              "      <td>0.407951</td>\n",
              "      <td>0.553549</td>\n",
              "      <td>0.637093</td>\n",
              "      <td>0.655976</td>\n",
              "      <td>0.448793</td>\n",
              "      <td>xlm-roberta-large_llama-65b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>en3</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.412144</td>\n",
              "      <td>0.524086</td>\n",
              "      <td>0.684992</td>\n",
              "      <td>0.678879</td>\n",
              "      <td>0.907918</td>\n",
              "      <td>0.555801</td>\n",
              "      <td>0.527395</td>\n",
              "      <td>0.637663</td>\n",
              "      <td>0.610414</td>\n",
              "      <td>0.647736</td>\n",
              "      <td>0.532262</td>\n",
              "      <td>bert-base-multilingual-cased_opt-66b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>en3</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.198296</td>\n",
              "      <td>0.589050</td>\n",
              "      <td>0.570750</td>\n",
              "      <td>0.656421</td>\n",
              "      <td>0.918517</td>\n",
              "      <td>0.560484</td>\n",
              "      <td>0.582609</td>\n",
              "      <td>0.644910</td>\n",
              "      <td>0.446283</td>\n",
              "      <td>0.498090</td>\n",
              "      <td>0.446363</td>\n",
              "      <td>mGPT_opt-66b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>en3</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.261175</td>\n",
              "      <td>0.431624</td>\n",
              "      <td>0.531647</td>\n",
              "      <td>0.625698</td>\n",
              "      <td>0.771954</td>\n",
              "      <td>0.471266</td>\n",
              "      <td>0.418367</td>\n",
              "      <td>0.389667</td>\n",
              "      <td>0.550282</td>\n",
              "      <td>0.471574</td>\n",
              "      <td>0.530702</td>\n",
              "      <td>mdeberta-v3-base_opt-66b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>en3</td>\n",
              "      <td>opt-66b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.418210</td>\n",
              "      <td>0.408885</td>\n",
              "      <td>0.602140</td>\n",
              "      <td>0.675034</td>\n",
              "      <td>0.920452</td>\n",
              "      <td>0.587124</td>\n",
              "      <td>0.420474</td>\n",
              "      <td>0.593539</td>\n",
              "      <td>0.588398</td>\n",
              "      <td>0.622024</td>\n",
              "      <td>0.517499</td>\n",
              "      <td>xlm-roberta-large_opt-66b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>en3</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.570502</td>\n",
              "      <td>0.637249</td>\n",
              "      <td>0.708583</td>\n",
              "      <td>0.700335</td>\n",
              "      <td>0.954856</td>\n",
              "      <td>0.632731</td>\n",
              "      <td>0.670498</td>\n",
              "      <td>0.644652</td>\n",
              "      <td>0.550129</td>\n",
              "      <td>0.637987</td>\n",
              "      <td>0.518781</td>\n",
              "      <td>bert-base-multilingual-cased_opt-iml-max-1.3b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>en3</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.260741</td>\n",
              "      <td>0.566327</td>\n",
              "      <td>0.573060</td>\n",
              "      <td>0.636859</td>\n",
              "      <td>0.972924</td>\n",
              "      <td>0.588083</td>\n",
              "      <td>0.590092</td>\n",
              "      <td>0.495213</td>\n",
              "      <td>0.256258</td>\n",
              "      <td>0.253658</td>\n",
              "      <td>0.415553</td>\n",
              "      <td>mGPT_opt-iml-max-1.3b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>en3</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.730906</td>\n",
              "      <td>0.861843</td>\n",
              "      <td>0.921165</td>\n",
              "      <td>0.815227</td>\n",
              "      <td>0.972922</td>\n",
              "      <td>0.772649</td>\n",
              "      <td>0.855827</td>\n",
              "      <td>0.739090</td>\n",
              "      <td>0.754218</td>\n",
              "      <td>0.847703</td>\n",
              "      <td>0.732787</td>\n",
              "      <td>mdeberta-v3-base_opt-iml-max-1.3b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>en3</td>\n",
              "      <td>opt-iml-max-1.3b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.669366</td>\n",
              "      <td>0.739327</td>\n",
              "      <td>0.742273</td>\n",
              "      <td>0.812958</td>\n",
              "      <td>0.971110</td>\n",
              "      <td>0.755262</td>\n",
              "      <td>0.619457</td>\n",
              "      <td>0.693278</td>\n",
              "      <td>0.568899</td>\n",
              "      <td>0.597704</td>\n",
              "      <td>0.616519</td>\n",
              "      <td>xlm-roberta-large_opt-iml-max-1.3b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>en3</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.785305</td>\n",
              "      <td>0.906366</td>\n",
              "      <td>0.881461</td>\n",
              "      <td>0.895068</td>\n",
              "      <td>0.969313</td>\n",
              "      <td>0.798652</td>\n",
              "      <td>0.911471</td>\n",
              "      <td>0.863244</td>\n",
              "      <td>0.777323</td>\n",
              "      <td>0.733287</td>\n",
              "      <td>0.888281</td>\n",
              "      <td>bert-base-multilingual-cased_text-davinci-003</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>en3</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.613629</td>\n",
              "      <td>0.893237</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.870103</td>\n",
              "      <td>0.989169</td>\n",
              "      <td>0.848392</td>\n",
              "      <td>0.938219</td>\n",
              "      <td>0.907865</td>\n",
              "      <td>0.693971</td>\n",
              "      <td>0.568728</td>\n",
              "      <td>0.590378</td>\n",
              "      <td>mGPT_text-davinci-003</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>en3</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.579101</td>\n",
              "      <td>0.767995</td>\n",
              "      <td>0.893975</td>\n",
              "      <td>0.889506</td>\n",
              "      <td>0.907435</td>\n",
              "      <td>0.640090</td>\n",
              "      <td>0.874799</td>\n",
              "      <td>0.772539</td>\n",
              "      <td>0.734054</td>\n",
              "      <td>0.612957</td>\n",
              "      <td>0.719948</td>\n",
              "      <td>mdeberta-v3-base_text-davinci-003</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>en3</td>\n",
              "      <td>text-davinci-003</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.389863</td>\n",
              "      <td>0.904195</td>\n",
              "      <td>0.526789</td>\n",
              "      <td>0.901578</td>\n",
              "      <td>0.985559</td>\n",
              "      <td>0.832714</td>\n",
              "      <td>0.948226</td>\n",
              "      <td>0.887163</td>\n",
              "      <td>0.598078</td>\n",
              "      <td>0.393205</td>\n",
              "      <td>0.398708</td>\n",
              "      <td>xlm-roberta-large_text-davinci-003</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>en3</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>bert-base-multilingual-cased</td>\n",
              "      <td>0.767944</td>\n",
              "      <td>0.882074</td>\n",
              "      <td>0.836601</td>\n",
              "      <td>0.840988</td>\n",
              "      <td>0.987365</td>\n",
              "      <td>0.786081</td>\n",
              "      <td>0.938175</td>\n",
              "      <td>0.821761</td>\n",
              "      <td>0.833505</td>\n",
              "      <td>0.837781</td>\n",
              "      <td>0.637889</td>\n",
              "      <td>bert-base-multilingual-cased_vicuna-13b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>en3</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mGPT</td>\n",
              "      <td>0.388148</td>\n",
              "      <td>0.907860</td>\n",
              "      <td>0.488887</td>\n",
              "      <td>0.825447</td>\n",
              "      <td>0.990975</td>\n",
              "      <td>0.696255</td>\n",
              "      <td>0.916054</td>\n",
              "      <td>0.804291</td>\n",
              "      <td>0.636147</td>\n",
              "      <td>0.721517</td>\n",
              "      <td>0.677753</td>\n",
              "      <td>mGPT_vicuna-13b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>en3</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>mdeberta-v3-base</td>\n",
              "      <td>0.257780</td>\n",
              "      <td>0.786145</td>\n",
              "      <td>0.698312</td>\n",
              "      <td>0.822525</td>\n",
              "      <td>0.792751</td>\n",
              "      <td>0.614244</td>\n",
              "      <td>0.880938</td>\n",
              "      <td>0.706436</td>\n",
              "      <td>0.603137</td>\n",
              "      <td>0.676501</td>\n",
              "      <td>0.548784</td>\n",
              "      <td>mdeberta-v3-base_vicuna-13b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>en3</td>\n",
              "      <td>vicuna-13b</td>\n",
              "      <td>xlm-roberta-large</td>\n",
              "      <td>0.406231</td>\n",
              "      <td>0.900893</td>\n",
              "      <td>0.764844</td>\n",
              "      <td>0.932370</td>\n",
              "      <td>0.989170</td>\n",
              "      <td>0.676076</td>\n",
              "      <td>0.861264</td>\n",
              "      <td>0.744415</td>\n",
              "      <td>0.761834</td>\n",
              "      <td>0.786622</td>\n",
              "      <td>0.759474</td>\n",
              "      <td>xlm-roberta-large_vicuna-13b</td>\n",
              "      <td>en3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11359e73-14a0-4270-bc1f-c0932325e515')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-11359e73-14a0-4270-bc1f-c0932325e515 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-11359e73-14a0-4270-bc1f-c0932325e515');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2408a81a-3a5b-403c-bb8e-93e1d0b6cafe\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2408a81a-3a5b-403c-bb8e-93e1d0b6cafe')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2408a81a-3a5b-403c-bb8e-93e1d0b6cafe button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#T-test for all combinations of train languages\n",
        "for (src, trg) in itertools.combinations_with_replacement(['en', 'es', 'ru', 'all', 'en3'], 2):\n",
        "  for lang in sorted_languages:\n",
        "   if src == trg: continue\n",
        "   print(f\"Test language: {lang}, ({trg}, {src})\")\n",
        "   res = stats.ttest_rel(temp[temp[\"Train Language\"] == trg][lang], temp[temp[\"Train Language\"] == src][lang])\n",
        "   print(stats.bayes_mvs(temp[temp[\"Train Language\"] == trg][lang])[0])\n",
        "   print(stats.bayes_mvs(temp[temp[\"Train Language\"] == src][lang])[0])\n",
        "   print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5HZSSL4V7RU",
        "outputId": "001a9330-8a78-4cd9-95d2-e359f49b8b32"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test language: en, (es, en)\n",
            "Mean(statistic=0.7321654431554827, minmax=(0.6716763103740118, 0.7926545759369537))\n",
            "Mean(statistic=0.9291952750903931, minmax=(0.9094486936553241, 0.9489418565254621))\n",
            "TtestResult(statistic=-5.579616536059947, pvalue=4.0759617309541465e-06, df=31)\n",
            "Test language: de, (es, en)\n",
            "Mean(statistic=0.881243868426579, minmax=(0.8551394981190868, 0.9073482387340711))\n",
            "Mean(statistic=0.8104183690222857, minmax=(0.7773688548598876, 0.8434678831846838))\n",
            "TtestResult(statistic=3.749296999557661, pvalue=0.0007294359059912015, df=31)\n",
            "Test language: nl, (es, en)\n",
            "Mean(statistic=0.8143354015523292, minmax=(0.7608956217257862, 0.8677751813788723))\n",
            "Mean(statistic=0.7508473921257937, minmax=(0.692139277952657, 0.8095555062989304))\n",
            "TtestResult(statistic=2.4678731210511757, pvalue=0.01931370019530848, df=31)\n",
            "Test language: es, (es, en)\n",
            "Mean(statistic=0.9313660269095141, minmax=(0.9124199669949031, 0.950312086824125))\n",
            "Mean(statistic=0.7018062196286831, minmax=(0.6628381900645105, 0.7407742491928557))\n",
            "TtestResult(statistic=10.587053020689872, pvalue=8.064440430291651e-12, df=31)\n",
            "Test language: pt, (es, en)\n",
            "Mean(statistic=0.8944320427093911, minmax=(0.8660521919121572, 0.9228118935066252))\n",
            "Mean(statistic=0.7361736019636431, minmax=(0.69121708891829, 0.7811301150089963))\n",
            "TtestResult(statistic=5.690913711651713, pvalue=2.964095757878827e-06, df=31)\n",
            "Test language: ca, (es, en)\n",
            "Mean(statistic=0.8746867839339207, minmax=(0.8502145786497324, 0.8991589892181091))\n",
            "Mean(statistic=0.7335083001418823, minmax=(0.6719409589258563, 0.7950756413579083))\n",
            "TtestResult(statistic=4.309945492021276, pvalue=0.0001533028892494897, df=31)\n",
            "Test language: cs, (es, en)\n",
            "Mean(statistic=0.8015733756474701, minmax=(0.741357744989389, 0.8617890063055512))\n",
            "Mean(statistic=0.6793049905538366, minmax=(0.6256165087304075, 0.7329934723772658))\n",
            "TtestResult(statistic=5.524752767264836, pvalue=4.769607076355003e-06, df=31)\n",
            "Test language: ru, (es, en)\n",
            "Mean(statistic=0.8375047249011152, minmax=(0.804023668196691, 0.8709857816055393))\n",
            "Mean(statistic=0.7148061806120866, minmax=(0.6715355935069887, 0.7580767677171845))\n",
            "TtestResult(statistic=4.819976525328107, pvalue=3.5960067161821157e-05, df=31)\n",
            "Test language: uk, (es, en)\n",
            "Mean(statistic=0.8298745402842684, minmax=(0.7919382946001892, 0.8678107859683477))\n",
            "Mean(statistic=0.674579669361251, minmax=(0.6313730673236849, 0.7177862713988171))\n",
            "TtestResult(statistic=5.80424619339245, pvalue=2.1440165150830406e-06, df=31)\n",
            "Test language: ar, (es, en)\n",
            "Mean(statistic=0.7857010927218487, minmax=(0.7421797024368003, 0.8292224830068972))\n",
            "Mean(statistic=0.5447817580646505, minmax=(0.4890131231521517, 0.6005503929771493))\n",
            "TtestResult(statistic=7.316157781658352, pvalue=3.107285681912764e-08, df=31)\n",
            "Test language: zh, (es, en)\n",
            "Mean(statistic=0.7215919850277607, minmax=(0.6703537994169798, 0.7728301706385415))\n",
            "Mean(statistic=0.5579543885344403, minmax=(0.5165259292153798, 0.5993828478535009))\n",
            "TtestResult(statistic=5.915629636418454, pvalue=1.5603658420536169e-06, df=31)\n",
            "Test language: en, (ru, en)\n",
            "Mean(statistic=0.5760259533382235, minmax=(0.5004876745245197, 0.6515642321519273))\n",
            "Mean(statistic=0.9291952750903931, minmax=(0.9094486936553241, 0.9489418565254621))\n",
            "TtestResult(statistic=-7.677292016109306, pvalue=1.1684829894819127e-08, df=31)\n",
            "Test language: de, (ru, en)\n",
            "Mean(statistic=0.7591294340362715, minmax=(0.7074880498822876, 0.8107708181902554))\n",
            "Mean(statistic=0.8104183690222857, minmax=(0.7773688548598876, 0.8434678831846838))\n",
            "TtestResult(statistic=-2.016026017954804, pvalue=0.05253412814807076, df=31)\n",
            "Test language: nl, (ru, en)\n",
            "Mean(statistic=0.6915329002535896, minmax=(0.6320598881927483, 0.751005912314431))\n",
            "Mean(statistic=0.7508473921257937, minmax=(0.692139277952657, 0.8095555062989304))\n",
            "TtestResult(statistic=-1.5580379318232784, pvalue=0.1293759666006121, df=31)\n",
            "Test language: es, (ru, en)\n",
            "Mean(statistic=0.6884135445954014, minmax=(0.6267347019692713, 0.7500923872215316))\n",
            "Mean(statistic=0.7018062196286831, minmax=(0.6628381900645105, 0.7407742491928557))\n",
            "TtestResult(statistic=-0.37635146769881617, pvalue=0.7092192433603035, df=31)\n",
            "Test language: pt, (ru, en)\n",
            "Mean(statistic=0.662630155197885, minmax=(0.6057298982135462, 0.719530412182224))\n",
            "Mean(statistic=0.7361736019636431, minmax=(0.69121708891829, 0.7811301150089963))\n",
            "TtestResult(statistic=-2.3041452333691836, pvalue=0.028079956003742607, df=31)\n",
            "Test language: ca, (ru, en)\n",
            "Mean(statistic=0.653202923386915, minmax=(0.5962775897825391, 0.7101282569912909))\n",
            "Mean(statistic=0.7335083001418823, minmax=(0.6719409589258563, 0.7950756413579083))\n",
            "TtestResult(statistic=-1.8909829003751648, pvalue=0.0680043182887038, df=31)\n",
            "Test language: cs, (ru, en)\n",
            "Mean(statistic=0.7923766286248519, minmax=(0.7285407013749885, 0.8562125558747153))\n",
            "Mean(statistic=0.6793049905538366, minmax=(0.6256165087304075, 0.7329934723772658))\n",
            "TtestResult(statistic=3.141201355914121, pvalue=0.003685416680757236, df=31)\n",
            "Test language: ru, (ru, en)\n",
            "Mean(statistic=0.952226851526951, minmax=(0.9422311792108217, 0.9622225238430803))\n",
            "Mean(statistic=0.7148061806120866, minmax=(0.6715355935069887, 0.7580767677171845))\n",
            "TtestResult(statistic=8.411548010474371, pvalue=1.679225838917204e-09, df=31)\n",
            "Test language: uk, (ru, en)\n",
            "Mean(statistic=0.9387388763780348, minmax=(0.9252598981291698, 0.9522178546268998))\n",
            "Mean(statistic=0.674579669361251, minmax=(0.6313730673236849, 0.7177862713988171))\n",
            "TtestResult(statistic=9.111528640829242, pvalue=2.8168834241347484e-10, df=31)\n",
            "Test language: ar, (ru, en)\n",
            "Mean(statistic=0.8486985093329653, minmax=(0.8073979665707111, 0.8899990520952196))\n",
            "Mean(statistic=0.5447817580646505, minmax=(0.4890131231521517, 0.6005503929771493))\n",
            "TtestResult(statistic=7.103207190420006, pvalue=5.5697794464486856e-08, df=31)\n",
            "Test language: zh, (ru, en)\n",
            "Mean(statistic=0.7293618331981047, minmax=(0.6831336252686233, 0.775590041127586))\n",
            "Mean(statistic=0.5579543885344403, minmax=(0.5165259292153798, 0.5993828478535009))\n",
            "TtestResult(statistic=4.820626760200575, pvalue=3.5893266317916216e-05, df=31)\n",
            "Test language: en, (all, en)\n",
            "Mean(statistic=0.9420039505320452, minmax=(0.929404367593004, 0.9546035334710864))\n",
            "Mean(statistic=0.9291952750903931, minmax=(0.9094486936553241, 0.9489418565254621))\n",
            "TtestResult(statistic=1.298336466758957, pvalue=0.20374810169826074, df=31)\n",
            "Test language: de, (all, en)\n",
            "Mean(statistic=0.9073165968715574, minmax=(0.8889539831617989, 0.925679210581316))\n",
            "Mean(statistic=0.8104183690222857, minmax=(0.7773688548598876, 0.8434678831846838))\n",
            "TtestResult(statistic=6.861805848654858, pvalue=1.0855988714537022e-07, df=31)\n",
            "Test language: nl, (all, en)\n",
            "Mean(statistic=0.8807837195488317, minmax=(0.8452271804005478, 0.9163402586971157))\n",
            "Mean(statistic=0.7508473921257937, minmax=(0.692139277952657, 0.8095555062989304))\n",
            "TtestResult(statistic=5.364171730304759, pvalue=7.558057333901195e-06, df=31)\n",
            "Test language: es, (all, en)\n",
            "Mean(statistic=0.9371533103000859, minmax=(0.9222880408649127, 0.952018579735259))\n",
            "Mean(statistic=0.7018062196286831, minmax=(0.6628381900645105, 0.7407742491928557))\n",
            "TtestResult(statistic=11.73801497044155, pvalue=6.116465412225636e-13, df=31)\n",
            "Test language: pt, (all, en)\n",
            "Mean(statistic=0.9252637474775207, minmax=(0.9118830962457521, 0.9386443987092893))\n",
            "Mean(statistic=0.7361736019636431, minmax=(0.69121708891829, 0.7811301150089963))\n",
            "TtestResult(statistic=7.440614725968179, pvalue=2.2144234263930078e-08, df=31)\n",
            "Test language: ca, (all, en)\n",
            "Mean(statistic=0.8977365917999118, minmax=(0.8744976445011716, 0.920975539098652))\n",
            "Mean(statistic=0.7335083001418823, minmax=(0.6719409589258563, 0.7950756413579083))\n",
            "TtestResult(statistic=4.735153611825911, pvalue=4.582321418422923e-05, df=31)\n",
            "Test language: cs, (all, en)\n",
            "Mean(statistic=0.8604194298303537, minmax=(0.8008739573194851, 0.9199649023412223))\n",
            "Mean(statistic=0.6793049905538366, minmax=(0.6256165087304075, 0.7329934723772658))\n",
            "TtestResult(statistic=6.679603056064252, pvalue=1.803353769797916e-07, df=31)\n",
            "Test language: ru, (all, en)\n",
            "Mean(statistic=0.9559668901788397, minmax=(0.9473409071566536, 0.9645928732010258))\n",
            "Mean(statistic=0.7148061806120866, minmax=(0.6715355935069887, 0.7580767677171845))\n",
            "TtestResult(statistic=9.088184324535739, pvalue=2.9866081354346015e-10, df=31)\n",
            "Test language: uk, (all, en)\n",
            "Mean(statistic=0.9374104109281147, minmax=(0.9232685377834912, 0.9515522840727383))\n",
            "Mean(statistic=0.674579669361251, minmax=(0.6313730673236849, 0.7177862713988171))\n",
            "TtestResult(statistic=9.350558224811385, pvalue=1.5537603614699837e-10, df=31)\n",
            "Test language: ar, (all, en)\n",
            "Mean(statistic=0.853705571011186, minmax=(0.8227567426639387, 0.8846543993584334))\n",
            "Mean(statistic=0.5447817580646505, minmax=(0.4890131231521517, 0.6005503929771493))\n",
            "TtestResult(statistic=8.853791247756575, pvalue=5.395435206899178e-10, df=31)\n",
            "Test language: zh, (all, en)\n",
            "Mean(statistic=0.7659000551030682, minmax=(0.7292006732978095, 0.802599436908327))\n",
            "Mean(statistic=0.5579543885344403, minmax=(0.5165259292153798, 0.5993828478535009))\n",
            "TtestResult(statistic=8.189611144301754, pvalue=2.9969540587040044e-09, df=31)\n",
            "Test language: en, (en3, en)\n",
            "Mean(statistic=0.9392086264648767, minmax=(0.915790180634485, 0.9626270722952683))\n",
            "Mean(statistic=0.9291952750903931, minmax=(0.9094486936553241, 0.9489418565254621))\n",
            "TtestResult(statistic=0.9942908653681876, pvalue=0.32777941241314623, df=31)\n",
            "Test language: de, (en3, en)\n",
            "Mean(statistic=0.8244206925330154, minmax=(0.7934006634771519, 0.855440721588879))\n",
            "Mean(statistic=0.8104183690222857, minmax=(0.7773688548598876, 0.8434678831846838))\n",
            "TtestResult(statistic=1.216096265321655, pvalue=0.2331275627546626, df=31)\n",
            "Test language: nl, (en3, en)\n",
            "Mean(statistic=0.7777631416443065, minmax=(0.7224555056059276, 0.8330707776826853))\n",
            "Mean(statistic=0.7508473921257937, minmax=(0.692139277952657, 0.8095555062989304))\n",
            "TtestResult(statistic=1.6132031128902375, pvalue=0.11683555440518287, df=31)\n",
            "Test language: es, (en3, en)\n",
            "Mean(statistic=0.7155651001967807, minmax=(0.6812343298520154, 0.7498958705415459))\n",
            "Mean(statistic=0.7018062196286831, minmax=(0.6628381900645105, 0.7407742491928557))\n",
            "TtestResult(statistic=0.778730745490349, pvalue=0.44204240535277495, df=31)\n",
            "Test language: pt, (en3, en)\n",
            "Mean(statistic=0.7507579833454906, minmax=(0.7096758861738924, 0.7918400805170888))\n",
            "Mean(statistic=0.7361736019636431, minmax=(0.69121708891829, 0.7811301150089963))\n",
            "TtestResult(statistic=0.807109176266628, pvalue=0.42575309590900645, df=31)\n",
            "Test language: ca, (en3, en)\n",
            "Mean(statistic=0.748445286823147, minmax=(0.6948491452044648, 0.8020414284418294))\n",
            "Mean(statistic=0.7335083001418823, minmax=(0.6719409589258563, 0.7950756413579083))\n",
            "TtestResult(statistic=0.7467646325203624, pvalue=0.4608321843873219, df=31)\n",
            "Test language: cs, (en3, en)\n",
            "Mean(statistic=0.7288789986347208, minmax=(0.6707136221994145, 0.7870443750700271))\n",
            "Mean(statistic=0.6793049905538366, minmax=(0.6256165087304075, 0.7329934723772658))\n",
            "TtestResult(statistic=2.179703376772624, pvalue=0.03699703499096669, df=31)\n",
            "Test language: ru, (en3, en)\n",
            "Mean(statistic=0.7091641620369513, minmax=(0.6617244589607233, 0.7566038651131793))\n",
            "Mean(statistic=0.7148061806120866, minmax=(0.6715355935069887, 0.7580767677171845))\n",
            "TtestResult(statistic=-0.36909581831426413, pvalue=0.7145654220475024, df=31)\n",
            "Test language: uk, (en3, en)\n",
            "Mean(statistic=0.7118229239459797, minmax=(0.6624604514849884, 0.7611853964069709))\n",
            "Mean(statistic=0.674579669361251, minmax=(0.6313730673236849, 0.7177862713988171))\n",
            "TtestResult(statistic=1.443518206298436, pvalue=0.15890496033509743, df=31)\n",
            "Test language: ar, (en3, en)\n",
            "Mean(statistic=0.5605166829642224, minmax=(0.48974530601969557, 0.6312880599087493))\n",
            "Mean(statistic=0.5447817580646505, minmax=(0.4890131231521517, 0.6005503929771493))\n",
            "TtestResult(statistic=0.8415255416288064, pvalue=0.4064986229081, df=31)\n",
            "Test language: zh, (en3, en)\n",
            "Mean(statistic=0.6159654059144679, minmax=(0.5732184266019944, 0.6587123852269414))\n",
            "Mean(statistic=0.5579543885344403, minmax=(0.5165259292153798, 0.5993828478535009))\n",
            "TtestResult(statistic=2.5763646772966693, pvalue=0.014970770722845472, df=31)\n",
            "Test language: en, (ru, es)\n",
            "Mean(statistic=0.5760259533382235, minmax=(0.5004876745245197, 0.6515642321519273))\n",
            "Mean(statistic=0.7321654431554827, minmax=(0.6716763103740118, 0.7926545759369537))\n",
            "TtestResult(statistic=-5.1494344965048136, pvalue=1.3993259904808516e-05, df=31)\n",
            "Test language: de, (ru, es)\n",
            "Mean(statistic=0.7591294340362715, minmax=(0.7074880498822876, 0.8107708181902554))\n",
            "Mean(statistic=0.881243868426579, minmax=(0.8551394981190868, 0.9073482387340711))\n",
            "TtestResult(statistic=-4.336260677846687, pvalue=0.00014232972067985372, df=31)\n",
            "Test language: nl, (ru, es)\n",
            "Mean(statistic=0.6915329002535896, minmax=(0.6320598881927483, 0.751005912314431))\n",
            "Mean(statistic=0.8143354015523292, minmax=(0.7608956217257862, 0.8677751813788723))\n",
            "TtestResult(statistic=-3.7999837949269057, pvalue=0.0006348420588762876, df=31)\n",
            "Test language: es, (ru, es)\n",
            "Mean(statistic=0.6884135445954014, minmax=(0.6267347019692713, 0.7500923872215316))\n",
            "Mean(statistic=0.9313660269095141, minmax=(0.9124199669949031, 0.950312086824125))\n",
            "TtestResult(statistic=-6.7566680557592775, pvalue=1.4544152393419664e-07, df=31)\n",
            "Test language: pt, (ru, es)\n",
            "Mean(statistic=0.662630155197885, minmax=(0.6057298982135462, 0.719530412182224))\n",
            "Mean(statistic=0.8944320427093911, minmax=(0.8660521919121572, 0.9228118935066252))\n",
            "TtestResult(statistic=-6.708481357978561, pvalue=1.663628064249509e-07, df=31)\n",
            "Test language: ca, (ru, es)\n",
            "Mean(statistic=0.653202923386915, minmax=(0.5962775897825391, 0.7101282569912909))\n",
            "Mean(statistic=0.8746867839339207, minmax=(0.8502145786497324, 0.8991589892181091))\n",
            "TtestResult(statistic=-6.509855447743591, pvalue=2.9014328333742974e-07, df=31)\n",
            "Test language: cs, (ru, es)\n",
            "Mean(statistic=0.7923766286248519, minmax=(0.7285407013749885, 0.8562125558747153))\n",
            "Mean(statistic=0.8015733756474701, minmax=(0.741357744989389, 0.8617890063055512))\n",
            "TtestResult(statistic=-0.2805281529609695, pvalue=0.7809360307982424, df=31)\n",
            "Test language: ru, (ru, es)\n",
            "Mean(statistic=0.952226851526951, minmax=(0.9422311792108217, 0.9622225238430803))\n",
            "Mean(statistic=0.8375047249011152, minmax=(0.804023668196691, 0.8709857816055393))\n",
            "TtestResult(statistic=5.511178211471291, pvalue=4.958780853583753e-06, df=31)\n",
            "Test language: uk, (ru, es)\n",
            "Mean(statistic=0.9387388763780348, minmax=(0.9252598981291698, 0.9522178546268998))\n",
            "Mean(statistic=0.8298745402842684, minmax=(0.7919382946001892, 0.8678107859683477))\n",
            "TtestResult(statistic=4.967166198909871, pvalue=2.3596503910777186e-05, df=31)\n",
            "Test language: ar, (ru, es)\n",
            "Mean(statistic=0.8486985093329653, minmax=(0.8073979665707111, 0.8899990520952196))\n",
            "Mean(statistic=0.7857010927218487, minmax=(0.7421797024368003, 0.8292224830068972))\n",
            "TtestResult(statistic=2.004373677780636, pvalue=0.053832623467061004, df=31)\n",
            "Test language: zh, (ru, es)\n",
            "Mean(statistic=0.7293618331981047, minmax=(0.6831336252686233, 0.775590041127586))\n",
            "Mean(statistic=0.7215919850277607, minmax=(0.6703537994169798, 0.7728301706385415))\n",
            "TtestResult(statistic=0.22927968915821348, pvalue=0.8201575500730953, df=31)\n",
            "Test language: en, (all, es)\n",
            "Mean(statistic=0.9420039505320452, minmax=(0.929404367593004, 0.9546035334710864))\n",
            "Mean(statistic=0.7321654431554827, minmax=(0.6716763103740118, 0.7926545759369537))\n",
            "TtestResult(statistic=6.503505019903646, pvalue=2.953657383908717e-07, df=31)\n",
            "Test language: de, (all, es)\n",
            "Mean(statistic=0.9073165968715574, minmax=(0.8889539831617989, 0.925679210581316))\n",
            "Mean(statistic=0.881243868426579, minmax=(0.8551394981190868, 0.9073482387340711))\n",
            "TtestResult(statistic=2.1090108660510274, pvalue=0.04312071711310055, df=31)\n",
            "Test language: nl, (all, es)\n",
            "Mean(statistic=0.8807837195488317, minmax=(0.8452271804005478, 0.9163402586971157))\n",
            "Mean(statistic=0.8143354015523292, minmax=(0.7608956217257862, 0.8677751813788723))\n",
            "TtestResult(statistic=3.2010509728804, pvalue=0.0031562119031367333, df=31)\n",
            "Test language: es, (all, es)\n",
            "Mean(statistic=0.9371533103000859, minmax=(0.9222880408649127, 0.952018579735259))\n",
            "Mean(statistic=0.9313660269095141, minmax=(0.9124199669949031, 0.950312086824125))\n",
            "TtestResult(statistic=0.9289080723222147, pvalue=0.36011376998029443, df=31)\n",
            "Test language: pt, (all, es)\n",
            "Mean(statistic=0.9252637474775207, minmax=(0.9118830962457521, 0.9386443987092893))\n",
            "Mean(statistic=0.8944320427093911, minmax=(0.8660521919121572, 0.9228118935066252))\n",
            "TtestResult(statistic=2.6030373441013266, pvalue=0.014051003426649158, df=31)\n",
            "Test language: ca, (all, es)\n",
            "Mean(statistic=0.8977365917999118, minmax=(0.8744976445011716, 0.920975539098652))\n",
            "Mean(statistic=0.8746867839339207, minmax=(0.8502145786497324, 0.8991589892181091))\n",
            "TtestResult(statistic=1.6990450812951798, pvalue=0.09932656857426864, df=31)\n",
            "Test language: cs, (all, es)\n",
            "Mean(statistic=0.8604194298303537, minmax=(0.8008739573194851, 0.9199649023412223))\n",
            "Mean(statistic=0.8015733756474701, minmax=(0.741357744989389, 0.8617890063055512))\n",
            "TtestResult(statistic=3.00916592254779, pvalue=0.005167220196250202, df=31)\n",
            "Test language: ru, (all, es)\n",
            "Mean(statistic=0.9559668901788397, minmax=(0.9473409071566536, 0.9645928732010258))\n",
            "Mean(statistic=0.8375047249011152, minmax=(0.804023668196691, 0.8709857816055393))\n",
            "TtestResult(statistic=5.829863590151675, pvalue=1.9928097987152172e-06, df=31)\n",
            "Test language: uk, (all, es)\n",
            "Mean(statistic=0.9374104109281147, minmax=(0.9232685377834912, 0.9515522840727383))\n",
            "Mean(statistic=0.8298745402842684, minmax=(0.7919382946001892, 0.8678107859683477))\n",
            "TtestResult(statistic=4.798778380167474, pvalue=3.820687298889795e-05, df=31)\n",
            "Test language: ar, (all, es)\n",
            "Mean(statistic=0.853705571011186, minmax=(0.8227567426639387, 0.8846543993584334))\n",
            "Mean(statistic=0.7857010927218487, minmax=(0.7421797024368003, 0.8292224830068972))\n",
            "TtestResult(statistic=2.556245577192191, pvalue=0.01570100384805994, df=31)\n",
            "Test language: zh, (all, es)\n",
            "Mean(statistic=0.7659000551030682, minmax=(0.7292006732978095, 0.802599436908327))\n",
            "Mean(statistic=0.7215919850277607, minmax=(0.6703537994169798, 0.7728301706385415))\n",
            "TtestResult(statistic=1.7067684752553514, pvalue=0.09786485393305064, df=31)\n",
            "Test language: en, (en3, es)\n",
            "Mean(statistic=0.9392086264648767, minmax=(0.915790180634485, 0.9626270722952683))\n",
            "Mean(statistic=0.7321654431554827, minmax=(0.6716763103740118, 0.7926545759369537))\n",
            "TtestResult(statistic=5.559776339300997, pvalue=4.314277006698865e-06, df=31)\n",
            "Test language: de, (en3, es)\n",
            "Mean(statistic=0.8244206925330154, minmax=(0.7934006634771519, 0.855440721588879))\n",
            "Mean(statistic=0.881243868426579, minmax=(0.8551394981190868, 0.9073482387340711))\n",
            "TtestResult(statistic=-2.8081016058801844, pvalue=0.008545852384516679, df=31)\n",
            "Test language: nl, (en3, es)\n",
            "Mean(statistic=0.7777631416443065, minmax=(0.7224555056059276, 0.8330707776826853))\n",
            "Mean(statistic=0.8143354015523292, minmax=(0.7608956217257862, 0.8677751813788723))\n",
            "TtestResult(statistic=-1.4452056637366524, pvalue=0.1584338616029379, df=31)\n",
            "Test language: es, (en3, es)\n",
            "Mean(statistic=0.7155651001967807, minmax=(0.6812343298520154, 0.7498958705415459))\n",
            "Mean(statistic=0.9313660269095141, minmax=(0.9124199669949031, 0.950312086824125))\n",
            "TtestResult(statistic=-12.77632580057317, pvalue=6.840620283203676e-14, df=31)\n",
            "Test language: pt, (en3, es)\n",
            "Mean(statistic=0.7507579833454906, minmax=(0.7096758861738924, 0.7918400805170888))\n",
            "Mean(statistic=0.8944320427093911, minmax=(0.8660521919121572, 0.9228118935066252))\n",
            "TtestResult(statistic=-5.770253743810693, pvalue=2.3626209054997367e-06, df=31)\n",
            "Test language: ca, (en3, es)\n",
            "Mean(statistic=0.748445286823147, minmax=(0.6948491452044648, 0.8020414284418294))\n",
            "Mean(statistic=0.8746867839339207, minmax=(0.8502145786497324, 0.8991589892181091))\n",
            "TtestResult(statistic=-4.659843788139324, pvalue=5.6807287997750756e-05, df=31)\n",
            "Test language: cs, (en3, es)\n",
            "Mean(statistic=0.7288789986347208, minmax=(0.6707136221994145, 0.7870443750700271))\n",
            "Mean(statistic=0.8015733756474701, minmax=(0.741357744989389, 0.8617890063055512))\n",
            "TtestResult(statistic=-2.4751016057158317, pvalue=0.018991779523353795, df=31)\n",
            "Test language: ru, (en3, es)\n",
            "Mean(statistic=0.7091641620369513, minmax=(0.6617244589607233, 0.7566038651131793))\n",
            "Mean(statistic=0.8375047249011152, minmax=(0.804023668196691, 0.8709857816055393))\n",
            "TtestResult(statistic=-4.27531081173326, pvalue=0.00016902536444491598, df=31)\n",
            "Test language: uk, (en3, es)\n",
            "Mean(statistic=0.7118229239459797, minmax=(0.6624604514849884, 0.7611853964069709))\n",
            "Mean(statistic=0.8298745402842684, minmax=(0.7919382946001892, 0.8678107859683477))\n",
            "TtestResult(statistic=-3.0643483730306933, pvalue=0.004489697350461722, df=31)\n",
            "Test language: ar, (en3, es)\n",
            "Mean(statistic=0.5605166829642224, minmax=(0.48974530601969557, 0.6312880599087493))\n",
            "Mean(statistic=0.7857010927218487, minmax=(0.7421797024368003, 0.8292224830068972))\n",
            "TtestResult(statistic=-5.489953151380483, pvalue=5.269764935114437e-06, df=31)\n",
            "Test language: zh, (en3, es)\n",
            "Mean(statistic=0.6159654059144679, minmax=(0.5732184266019944, 0.6587123852269414))\n",
            "Mean(statistic=0.7215919850277607, minmax=(0.6703537994169798, 0.7728301706385415))\n",
            "TtestResult(statistic=-3.0948956506891996, pvalue=0.004151831816044922, df=31)\n",
            "Test language: en, (all, ru)\n",
            "Mean(statistic=0.9420039505320452, minmax=(0.929404367593004, 0.9546035334710864))\n",
            "Mean(statistic=0.5760259533382235, minmax=(0.5004876745245197, 0.6515642321519273))\n",
            "TtestResult(statistic=8.942568867953328, pvalue=4.3089952365115937e-10, df=31)\n",
            "Test language: de, (all, ru)\n",
            "Mean(statistic=0.9073165968715574, minmax=(0.8889539831617989, 0.925679210581316))\n",
            "Mean(statistic=0.7591294340362715, minmax=(0.7074880498822876, 0.8107708181902554))\n",
            "TtestResult(statistic=5.378898660388387, pvalue=7.245485515088971e-06, df=31)\n",
            "Test language: nl, (all, ru)\n",
            "Mean(statistic=0.8807837195488317, minmax=(0.8452271804005478, 0.9163402586971157))\n",
            "Mean(statistic=0.6915329002535896, minmax=(0.6320598881927483, 0.751005912314431))\n",
            "TtestResult(statistic=6.507048103499295, pvalue=2.9244037495481405e-07, df=31)\n",
            "Test language: es, (all, ru)\n",
            "Mean(statistic=0.9371533103000859, minmax=(0.9222880408649127, 0.952018579735259))\n",
            "Mean(statistic=0.6884135445954014, minmax=(0.6267347019692713, 0.7500923872215316))\n",
            "TtestResult(statistic=6.910310182032351, pvalue=9.489167658163848e-08, df=31)\n",
            "Test language: pt, (all, ru)\n",
            "Mean(statistic=0.9252637474775207, minmax=(0.9118830962457521, 0.9386443987092893))\n",
            "Mean(statistic=0.662630155197885, minmax=(0.6057298982135462, 0.719530412182224))\n",
            "TtestResult(statistic=7.7782518374850005, pvalue=8.913848267750024e-09, df=31)\n",
            "Test language: ca, (all, ru)\n",
            "Mean(statistic=0.8977365917999118, minmax=(0.8744976445011716, 0.920975539098652))\n",
            "Mean(statistic=0.653202923386915, minmax=(0.5962775897825391, 0.7101282569912909))\n",
            "TtestResult(statistic=7.5860130242847035, pvalue=1.4940370751196045e-08, df=31)\n",
            "Test language: cs, (all, ru)\n",
            "Mean(statistic=0.8604194298303537, minmax=(0.8008739573194851, 0.9199649023412223))\n",
            "Mean(statistic=0.7923766286248519, minmax=(0.7285407013749885, 0.8562125558747153))\n",
            "TtestResult(statistic=2.368598551964823, pvalue=0.024269874392827684, df=31)\n",
            "Test language: ru, (all, ru)\n",
            "Mean(statistic=0.9559668901788397, minmax=(0.9473409071566536, 0.9645928732010258))\n",
            "Mean(statistic=0.952226851526951, minmax=(0.9422311792108217, 0.9622225238430803))\n",
            "TtestResult(statistic=0.7887557024662336, pvalue=0.4362457110442327, df=31)\n",
            "Test language: uk, (all, ru)\n",
            "Mean(statistic=0.9374104109281147, minmax=(0.9232685377834912, 0.9515522840727383))\n",
            "Mean(statistic=0.9387388763780348, minmax=(0.9252598981291698, 0.9522178546268998))\n",
            "TtestResult(statistic=-0.21542459210003934, pvalue=0.8308473881448167, df=31)\n",
            "Test language: ar, (all, ru)\n",
            "Mean(statistic=0.853705571011186, minmax=(0.8227567426639387, 0.8846543993584334))\n",
            "Mean(statistic=0.8486985093329653, minmax=(0.8073979665707111, 0.8899990520952196))\n",
            "TtestResult(statistic=0.23463757369157173, pvalue=0.8160329567045386, df=31)\n",
            "Test language: zh, (all, ru)\n",
            "Mean(statistic=0.7659000551030682, minmax=(0.7292006732978095, 0.802599436908327))\n",
            "Mean(statistic=0.7293618331981047, minmax=(0.6831336252686233, 0.775590041127586))\n",
            "TtestResult(statistic=1.760367458544829, pvalue=0.08821006386613482, df=31)\n",
            "Test language: en, (en3, ru)\n",
            "Mean(statistic=0.9392086264648767, minmax=(0.915790180634485, 0.9626270722952683))\n",
            "Mean(statistic=0.5760259533382235, minmax=(0.5004876745245197, 0.6515642321519273))\n",
            "TtestResult(statistic=7.710970908745724, pvalue=1.0674507294084145e-08, df=31)\n",
            "Test language: de, (en3, ru)\n",
            "Mean(statistic=0.8244206925330154, minmax=(0.7934006634771519, 0.855440721588879))\n",
            "Mean(statistic=0.7591294340362715, minmax=(0.7074880498822876, 0.8107708181902554))\n",
            "TtestResult(statistic=2.912394514599586, pvalue=0.006594892934434883, df=31)\n",
            "Test language: nl, (en3, ru)\n",
            "Mean(statistic=0.7777631416443065, minmax=(0.7224555056059276, 0.8330707776826853))\n",
            "Mean(statistic=0.6915329002535896, minmax=(0.6320598881927483, 0.751005912314431))\n",
            "TtestResult(statistic=2.2381413890198862, pvalue=0.032534211150352425, df=31)\n",
            "Test language: es, (en3, ru)\n",
            "Mean(statistic=0.7155651001967807, minmax=(0.6812343298520154, 0.7498958705415459))\n",
            "Mean(statistic=0.6884135445954014, minmax=(0.6267347019692713, 0.7500923872215316))\n",
            "TtestResult(statistic=0.8691396938436036, pvalue=0.39145044546736896, df=31)\n",
            "Test language: pt, (en3, ru)\n",
            "Mean(statistic=0.7507579833454906, minmax=(0.7096758861738924, 0.7918400805170888))\n",
            "Mean(statistic=0.662630155197885, minmax=(0.6057298982135462, 0.719530412182224))\n",
            "TtestResult(statistic=3.3489391756114815, pvalue=0.0021421509336904403, df=31)\n",
            "Test language: ca, (en3, ru)\n",
            "Mean(statistic=0.748445286823147, minmax=(0.6948491452044648, 0.8020414284418294))\n",
            "Mean(statistic=0.653202923386915, minmax=(0.5962775897825391, 0.7101282569912909))\n",
            "TtestResult(statistic=2.385327123558788, pvalue=0.023360948697565054, df=31)\n",
            "Test language: cs, (en3, ru)\n",
            "Mean(statistic=0.7288789986347208, minmax=(0.6707136221994145, 0.7870443750700271))\n",
            "Mean(statistic=0.7923766286248519, minmax=(0.7285407013749885, 0.8562125558747153))\n",
            "TtestResult(statistic=-2.012164469978886, pvalue=0.05296137108582532, df=31)\n",
            "Test language: ru, (en3, ru)\n",
            "Mean(statistic=0.7091641620369513, minmax=(0.6617244589607233, 0.7566038651131793))\n",
            "Mean(statistic=0.952226851526951, minmax=(0.9422311792108217, 0.9622225238430803))\n",
            "TtestResult(statistic=-7.898734569835106, pvalue=6.463628775373967e-09, df=31)\n",
            "Test language: uk, (en3, ru)\n",
            "Mean(statistic=0.7118229239459797, minmax=(0.6624604514849884, 0.7611853964069709))\n",
            "Mean(statistic=0.9387388763780348, minmax=(0.9252598981291698, 0.9522178546268998))\n",
            "TtestResult(statistic=-7.0138968113608335, pvalue=7.124650782939048e-08, df=31)\n",
            "Test language: ar, (en3, ru)\n",
            "Mean(statistic=0.5605166829642224, minmax=(0.48974530601969557, 0.6312880599087493))\n",
            "Mean(statistic=0.8486985093329653, minmax=(0.8073979665707111, 0.8899990520952196))\n",
            "TtestResult(statistic=-5.9837906722028755, pvalue=1.2850283793558116e-06, df=31)\n",
            "Test language: zh, (en3, ru)\n",
            "Mean(statistic=0.6159654059144679, minmax=(0.5732184266019944, 0.6587123852269414))\n",
            "Mean(statistic=0.7293618331981047, minmax=(0.6831336252686233, 0.775590041127586))\n",
            "TtestResult(statistic=-3.7222023813773024, pvalue=0.000785492035336379, df=31)\n",
            "Test language: en, (en3, all)\n",
            "Mean(statistic=0.9392086264648767, minmax=(0.915790180634485, 0.9626270722952683))\n",
            "Mean(statistic=0.9420039505320452, minmax=(0.929404367593004, 0.9546035334710864))\n",
            "TtestResult(statistic=-0.24034118303200563, pvalue=0.8116480885413118, df=31)\n",
            "Test language: de, (en3, all)\n",
            "Mean(statistic=0.8244206925330154, minmax=(0.7934006634771519, 0.855440721588879))\n",
            "Mean(statistic=0.9073165968715574, minmax=(0.8889539831617989, 0.925679210581316))\n",
            "TtestResult(statistic=-5.7190138491573554, pvalue=2.7352338324277665e-06, df=31)\n",
            "Test language: nl, (en3, all)\n",
            "Mean(statistic=0.7777631416443065, minmax=(0.7224555056059276, 0.8330707776826853))\n",
            "Mean(statistic=0.8807837195488317, minmax=(0.8452271804005478, 0.9163402586971157))\n",
            "TtestResult(statistic=-4.075712316681768, pvalue=0.0002958447918556002, df=31)\n",
            "Test language: es, (en3, all)\n",
            "Mean(statistic=0.7155651001967807, minmax=(0.6812343298520154, 0.7498958705415459))\n",
            "Mean(statistic=0.9371533103000859, minmax=(0.9222880408649127, 0.952018579735259))\n",
            "TtestResult(statistic=-13.234294158665767, pvalue=2.705336029065224e-14, df=31)\n",
            "Test language: pt, (en3, all)\n",
            "Mean(statistic=0.7507579833454906, minmax=(0.7096758861738924, 0.7918400805170888))\n",
            "Mean(statistic=0.9252637474775207, minmax=(0.9118830962457521, 0.9386443987092893))\n",
            "TtestResult(statistic=-7.4887760564714165, pvalue=1.943276936734318e-08, df=31)\n",
            "Test language: ca, (en3, all)\n",
            "Mean(statistic=0.748445286823147, minmax=(0.6948491452044648, 0.8020414284418294))\n",
            "Mean(statistic=0.8977365917999118, minmax=(0.8744976445011716, 0.920975539098652))\n",
            "TtestResult(statistic=-4.741777255785575, pvalue=4.496465897686431e-05, df=31)\n",
            "Test language: cs, (en3, all)\n",
            "Mean(statistic=0.7288789986347208, minmax=(0.6707136221994145, 0.7870443750700271))\n",
            "Mean(statistic=0.8604194298303537, minmax=(0.8008739573194851, 0.9199649023412223))\n",
            "TtestResult(statistic=-5.964858437511356, pvalue=1.3561895714241687e-06, df=31)\n",
            "Test language: ru, (en3, all)\n",
            "Mean(statistic=0.7091641620369513, minmax=(0.6617244589607233, 0.7566038651131793))\n",
            "Mean(statistic=0.9559668901788397, minmax=(0.9473409071566536, 0.9645928732010258))\n",
            "TtestResult(statistic=-8.220621848562475, pvalue=2.7629005992963065e-09, df=31)\n",
            "Test language: uk, (en3, all)\n",
            "Mean(statistic=0.7118229239459797, minmax=(0.6624604514849884, 0.7611853964069709))\n",
            "Mean(statistic=0.9374104109281147, minmax=(0.9232685377834912, 0.9515522840727383))\n",
            "TtestResult(statistic=-7.322385765326277, pvalue=3.0549287587751914e-08, df=31)\n",
            "Test language: ar, (en3, all)\n",
            "Mean(statistic=0.5605166829642224, minmax=(0.48974530601969557, 0.6312880599087493))\n",
            "Mean(statistic=0.853705571011186, minmax=(0.8227567426639387, 0.8846543993584334))\n",
            "TtestResult(statistic=-7.149838285638863, pvalue=4.899533130968515e-08, df=31)\n",
            "Test language: zh, (en3, all)\n",
            "Mean(statistic=0.6159654059144679, minmax=(0.5732184266019944, 0.6587123852269414))\n",
            "Mean(statistic=0.7659000551030682, minmax=(0.7292006732978095, 0.802599436908327))\n",
            "TtestResult(statistic=-7.307226982546001, pvalue=3.183958273503821e-08, df=31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#T-test results for only combinations with pvalue >= 0.05 ~ differences between combinations not statistically significant\n",
        "for (src, trg) in itertools.combinations_with_replacement(['en', 'es', 'ru', 'all', 'en3'], 2):\n",
        "  for lang in sorted_languages:\n",
        "   if src == trg: continue\n",
        "   res = stats.ttest_rel(temp[temp[\"Train Language\"] == trg][lang], temp[temp[\"Train Language\"] == src][lang])\n",
        "   if (res.pvalue < 0.05): continue\n",
        "   print(f\"Test language: {lang}, ({trg}, {src})\")\n",
        "   print(stats.bayes_mvs(temp[temp[\"Train Language\"] == trg][lang])[0])\n",
        "   print(stats.bayes_mvs(temp[temp[\"Train Language\"] == src][lang])[0])\n",
        "   print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaNk71OVj2Vr",
        "outputId": "52334b25-60a8-425f-805b-2b1907eba809"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test language: de, (ru, en)\n",
            "Mean(statistic=0.7591294340362715, minmax=(0.7074880498822876, 0.8107708181902554))\n",
            "Mean(statistic=0.8104183690222857, minmax=(0.7773688548598876, 0.8434678831846838))\n",
            "TtestResult(statistic=-2.016026017954804, pvalue=0.05253412814807076, df=31)\n",
            "Test language: nl, (ru, en)\n",
            "Mean(statistic=0.6915329002535896, minmax=(0.6320598881927483, 0.751005912314431))\n",
            "Mean(statistic=0.7508473921257937, minmax=(0.692139277952657, 0.8095555062989304))\n",
            "TtestResult(statistic=-1.5580379318232784, pvalue=0.1293759666006121, df=31)\n",
            "Test language: es, (ru, en)\n",
            "Mean(statistic=0.6884135445954014, minmax=(0.6267347019692713, 0.7500923872215316))\n",
            "Mean(statistic=0.7018062196286831, minmax=(0.6628381900645105, 0.7407742491928557))\n",
            "TtestResult(statistic=-0.37635146769881617, pvalue=0.7092192433603035, df=31)\n",
            "Test language: ca, (ru, en)\n",
            "Mean(statistic=0.653202923386915, minmax=(0.5962775897825391, 0.7101282569912909))\n",
            "Mean(statistic=0.7335083001418823, minmax=(0.6719409589258563, 0.7950756413579083))\n",
            "TtestResult(statistic=-1.8909829003751648, pvalue=0.0680043182887038, df=31)\n",
            "Test language: en, (all, en)\n",
            "Mean(statistic=0.9420039505320452, minmax=(0.929404367593004, 0.9546035334710864))\n",
            "Mean(statistic=0.9291952750903931, minmax=(0.9094486936553241, 0.9489418565254621))\n",
            "TtestResult(statistic=1.298336466758957, pvalue=0.20374810169826074, df=31)\n",
            "Test language: en, (en3, en)\n",
            "Mean(statistic=0.9392086264648767, minmax=(0.915790180634485, 0.9626270722952683))\n",
            "Mean(statistic=0.9291952750903931, minmax=(0.9094486936553241, 0.9489418565254621))\n",
            "TtestResult(statistic=0.9942908653681876, pvalue=0.32777941241314623, df=31)\n",
            "Test language: de, (en3, en)\n",
            "Mean(statistic=0.8244206925330154, minmax=(0.7934006634771519, 0.855440721588879))\n",
            "Mean(statistic=0.8104183690222857, minmax=(0.7773688548598876, 0.8434678831846838))\n",
            "TtestResult(statistic=1.216096265321655, pvalue=0.2331275627546626, df=31)\n",
            "Test language: nl, (en3, en)\n",
            "Mean(statistic=0.7777631416443065, minmax=(0.7224555056059276, 0.8330707776826853))\n",
            "Mean(statistic=0.7508473921257937, minmax=(0.692139277952657, 0.8095555062989304))\n",
            "TtestResult(statistic=1.6132031128902375, pvalue=0.11683555440518287, df=31)\n",
            "Test language: es, (en3, en)\n",
            "Mean(statistic=0.7155651001967807, minmax=(0.6812343298520154, 0.7498958705415459))\n",
            "Mean(statistic=0.7018062196286831, minmax=(0.6628381900645105, 0.7407742491928557))\n",
            "TtestResult(statistic=0.778730745490349, pvalue=0.44204240535277495, df=31)\n",
            "Test language: pt, (en3, en)\n",
            "Mean(statistic=0.7507579833454906, minmax=(0.7096758861738924, 0.7918400805170888))\n",
            "Mean(statistic=0.7361736019636431, minmax=(0.69121708891829, 0.7811301150089963))\n",
            "TtestResult(statistic=0.807109176266628, pvalue=0.42575309590900645, df=31)\n",
            "Test language: ca, (en3, en)\n",
            "Mean(statistic=0.748445286823147, minmax=(0.6948491452044648, 0.8020414284418294))\n",
            "Mean(statistic=0.7335083001418823, minmax=(0.6719409589258563, 0.7950756413579083))\n",
            "TtestResult(statistic=0.7467646325203624, pvalue=0.4608321843873219, df=31)\n",
            "Test language: ru, (en3, en)\n",
            "Mean(statistic=0.7091641620369513, minmax=(0.6617244589607233, 0.7566038651131793))\n",
            "Mean(statistic=0.7148061806120866, minmax=(0.6715355935069887, 0.7580767677171845))\n",
            "TtestResult(statistic=-0.36909581831426413, pvalue=0.7145654220475024, df=31)\n",
            "Test language: uk, (en3, en)\n",
            "Mean(statistic=0.7118229239459797, minmax=(0.6624604514849884, 0.7611853964069709))\n",
            "Mean(statistic=0.674579669361251, minmax=(0.6313730673236849, 0.7177862713988171))\n",
            "TtestResult(statistic=1.443518206298436, pvalue=0.15890496033509743, df=31)\n",
            "Test language: ar, (en3, en)\n",
            "Mean(statistic=0.5605166829642224, minmax=(0.48974530601969557, 0.6312880599087493))\n",
            "Mean(statistic=0.5447817580646505, minmax=(0.4890131231521517, 0.6005503929771493))\n",
            "TtestResult(statistic=0.8415255416288064, pvalue=0.4064986229081, df=31)\n",
            "Test language: cs, (ru, es)\n",
            "Mean(statistic=0.7923766286248519, minmax=(0.7285407013749885, 0.8562125558747153))\n",
            "Mean(statistic=0.8015733756474701, minmax=(0.741357744989389, 0.8617890063055512))\n",
            "TtestResult(statistic=-0.2805281529609695, pvalue=0.7809360307982424, df=31)\n",
            "Test language: ar, (ru, es)\n",
            "Mean(statistic=0.8486985093329653, minmax=(0.8073979665707111, 0.8899990520952196))\n",
            "Mean(statistic=0.7857010927218487, minmax=(0.7421797024368003, 0.8292224830068972))\n",
            "TtestResult(statistic=2.004373677780636, pvalue=0.053832623467061004, df=31)\n",
            "Test language: zh, (ru, es)\n",
            "Mean(statistic=0.7293618331981047, minmax=(0.6831336252686233, 0.775590041127586))\n",
            "Mean(statistic=0.7215919850277607, minmax=(0.6703537994169798, 0.7728301706385415))\n",
            "TtestResult(statistic=0.22927968915821348, pvalue=0.8201575500730953, df=31)\n",
            "Test language: es, (all, es)\n",
            "Mean(statistic=0.9371533103000859, minmax=(0.9222880408649127, 0.952018579735259))\n",
            "Mean(statistic=0.9313660269095141, minmax=(0.9124199669949031, 0.950312086824125))\n",
            "TtestResult(statistic=0.9289080723222147, pvalue=0.36011376998029443, df=31)\n",
            "Test language: ca, (all, es)\n",
            "Mean(statistic=0.8977365917999118, minmax=(0.8744976445011716, 0.920975539098652))\n",
            "Mean(statistic=0.8746867839339207, minmax=(0.8502145786497324, 0.8991589892181091))\n",
            "TtestResult(statistic=1.6990450812951798, pvalue=0.09932656857426864, df=31)\n",
            "Test language: zh, (all, es)\n",
            "Mean(statistic=0.7659000551030682, minmax=(0.7292006732978095, 0.802599436908327))\n",
            "Mean(statistic=0.7215919850277607, minmax=(0.6703537994169798, 0.7728301706385415))\n",
            "TtestResult(statistic=1.7067684752553514, pvalue=0.09786485393305064, df=31)\n",
            "Test language: nl, (en3, es)\n",
            "Mean(statistic=0.7777631416443065, minmax=(0.7224555056059276, 0.8330707776826853))\n",
            "Mean(statistic=0.8143354015523292, minmax=(0.7608956217257862, 0.8677751813788723))\n",
            "TtestResult(statistic=-1.4452056637366524, pvalue=0.1584338616029379, df=31)\n",
            "Test language: ru, (all, ru)\n",
            "Mean(statistic=0.9559668901788397, minmax=(0.9473409071566536, 0.9645928732010258))\n",
            "Mean(statistic=0.952226851526951, minmax=(0.9422311792108217, 0.9622225238430803))\n",
            "TtestResult(statistic=0.7887557024662336, pvalue=0.4362457110442327, df=31)\n",
            "Test language: uk, (all, ru)\n",
            "Mean(statistic=0.9374104109281147, minmax=(0.9232685377834912, 0.9515522840727383))\n",
            "Mean(statistic=0.9387388763780348, minmax=(0.9252598981291698, 0.9522178546268998))\n",
            "TtestResult(statistic=-0.21542459210003934, pvalue=0.8308473881448167, df=31)\n",
            "Test language: ar, (all, ru)\n",
            "Mean(statistic=0.853705571011186, minmax=(0.8227567426639387, 0.8846543993584334))\n",
            "Mean(statistic=0.8486985093329653, minmax=(0.8073979665707111, 0.8899990520952196))\n",
            "TtestResult(statistic=0.23463757369157173, pvalue=0.8160329567045386, df=31)\n",
            "Test language: zh, (all, ru)\n",
            "Mean(statistic=0.7659000551030682, minmax=(0.7292006732978095, 0.802599436908327))\n",
            "Mean(statistic=0.7293618331981047, minmax=(0.6831336252686233, 0.775590041127586))\n",
            "TtestResult(statistic=1.760367458544829, pvalue=0.08821006386613482, df=31)\n",
            "Test language: es, (en3, ru)\n",
            "Mean(statistic=0.7155651001967807, minmax=(0.6812343298520154, 0.7498958705415459))\n",
            "Mean(statistic=0.6884135445954014, minmax=(0.6267347019692713, 0.7500923872215316))\n",
            "TtestResult(statistic=0.8691396938436036, pvalue=0.39145044546736896, df=31)\n",
            "Test language: cs, (en3, ru)\n",
            "Mean(statistic=0.7288789986347208, minmax=(0.6707136221994145, 0.7870443750700271))\n",
            "Mean(statistic=0.7923766286248519, minmax=(0.7285407013749885, 0.8562125558747153))\n",
            "TtestResult(statistic=-2.012164469978886, pvalue=0.05296137108582532, df=31)\n",
            "Test language: en, (en3, all)\n",
            "Mean(statistic=0.9392086264648767, minmax=(0.915790180634485, 0.9626270722952683))\n",
            "Mean(statistic=0.9420039505320452, minmax=(0.929404367593004, 0.9546035334710864))\n",
            "TtestResult(statistic=-0.24034118303200563, pvalue=0.8116480885413118, df=31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RQ4 Cross-Generator Generalization"
      ],
      "metadata": {
        "id": "yw-ewqeieh5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#How does a detectors trained on one LLM perform on different LLMs?\n",
        "def analyze_llm_for_train_language_per_llm(results_list, train_language, test_llm):\n",
        "  results = pd.DataFrame()\n",
        "  for detector in tqdm(results_list, total=len(results_list)):\n",
        "    for detector_name, detector_data in detector.items():\n",
        "      for llm in ['text-davinci-003', 'gpt-3.5-turbo', 'gpt-4', 'alpaca-lora-30b', 'vicuna-13b', 'llama-65b', 'opt-66b', 'opt-iml-max-1.3b', 'all']:\n",
        "        if f'-{train_language}-' not in detector_name: continue\n",
        "        if f'-{llm}.' not in detector_name and 'all' not in llm: continue\n",
        "        if 'all' in test_llm:\n",
        "          temp = detector_data\n",
        "        else:\n",
        "          temp = detector_data[(detector_data.multi_label.str.contains(test_llm) | detector_data.multi_label.str.contains('human'))]\n",
        "        if len(temp.label.unique()) < 2: continue\n",
        "        #results = pd.concat([results, cr2df(temp['label'], temp['predictions'], detector_name)], copy=False, ignore_index=True)\n",
        "        if optimize_threshold and 'prediction_probs' in temp.columns:\n",
        "          optimal_threshold = 0.5\n",
        "          labels = [label2id[x] for x in temp['label']]\n",
        "          predictions = [label2id[x] for x in temp['predictions']]\n",
        "          temp = temp.fillna(0.0)\n",
        "          temp['prediction_probs'] = temp['prediction_probs'].astype(float)\n",
        "          temp.loc[temp.predictions == 'human', 'prediction_probs'] = 1 - temp['prediction_probs']\n",
        "          if (optimize_threshold):\n",
        "            fpr, tpr, thresholds = roc_curve(labels, temp['prediction_probs'])\n",
        "            optimal_threshold = thresholds[np.argmax(tpr - fpr)]\n",
        "            #optimal_threshold = thresholds[fpr <= 0.05][-1] #get threshold for 5% FPR\n",
        "          preds = [\"machine\" if ((y > optimal_threshold)) else \"human\" for x,y in zip(temp['predictions'],temp['prediction_probs'])]\n",
        "        else:\n",
        "          preds = temp['predictions']\n",
        "        scores = cr2df(temp['label'], preds, detector_name)\n",
        "        results = pd.concat([results, scores], copy=False, ignore_index=True)\n",
        "  temp = results.sort_values(by=['Macro avg F1-score'], ascending=False).reset_index(drop=True)\n",
        "  return temp"
      ],
      "metadata": {
        "id": "UJfIYWrAem9p"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#How does a detectorstrained on one LLM perform on different LLMs?\n",
        "results_all = pd.DataFrame()\n",
        "for train_language in ['en', 'es', 'ru', 'all', 'en3']:\n",
        "  results = pd.DataFrame()\n",
        "  for test_llm in ['text-davinci-003', 'gpt-3.5-turbo', 'gpt-4', 'alpaca-lora-30b', 'vicuna-13b', 'llama-65b', 'opt-66b', 'opt-iml-max-1.3b', 'all']:\n",
        "    temp = analyze_llm_for_train_language_per_llm(test_results, train_language, test_llm)\n",
        "    #temp = temp[~temp['Train LLM'].str.contains('all')]\n",
        "    temp = temp[['Train Language', 'Train LLM', 'Model', 'Macro avg F1-score']]\n",
        "    temp = temp.sort_values(by=['Train Language', 'Train LLM', 'Model'])\n",
        "    temp = temp.set_index(['Train Language', 'Train LLM', 'Model'])\n",
        "    temp.rename(columns={'Macro avg F1-score': test_llm}, inplace=True)\n",
        "    if len(results) > 0: temp = temp[test_llm]\n",
        "    results = pd.concat([results, temp], copy=False, axis=1)\n",
        "    #break\n",
        "  results_all = pd.concat([results_all, results], copy=False)\n",
        "  #break"
      ],
      "metadata": {
        "id": "asHk1E9niIPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb160e0-9157-47b2-d86b-2c58e7b9e772"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 324/324 [00:40<00:00,  7.99it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.80it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.81it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.73it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.79it/s]\n",
            "100%|██████████| 324/324 [00:40<00:00,  7.98it/s]\n",
            "100%|██████████| 324/324 [00:43<00:00,  7.43it/s]\n",
            "100%|██████████| 324/324 [00:40<00:00,  8.01it/s]\n",
            "100%|██████████| 324/324 [03:03<00:00,  1.77it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.86it/s]\n",
            "100%|██████████| 324/324 [00:40<00:00,  8.05it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.77it/s]\n",
            "100%|██████████| 324/324 [00:44<00:00,  7.34it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.75it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.77it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.82it/s]\n",
            "100%|██████████| 324/324 [00:39<00:00,  8.16it/s]\n",
            "100%|██████████| 324/324 [03:05<00:00,  1.75it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.84it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.78it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.83it/s]\n",
            "100%|██████████| 324/324 [00:39<00:00,  8.14it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.80it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.79it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.81it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.82it/s]\n",
            "100%|██████████| 324/324 [03:02<00:00,  1.77it/s]\n",
            "100%|██████████| 324/324 [00:44<00:00,  7.28it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.77it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.80it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.82it/s]\n",
            "100%|██████████| 324/324 [00:42<00:00,  7.70it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.82it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.84it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.80it/s]\n",
            "100%|██████████| 324/324 [03:01<00:00,  1.78it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.76it/s]\n",
            "100%|██████████| 324/324 [00:44<00:00,  7.35it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.77it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.87it/s]\n",
            "100%|██████████| 324/324 [00:40<00:00,  8.07it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.75it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.80it/s]\n",
            "100%|██████████| 324/324 [00:41<00:00,  7.84it/s]\n",
            "100%|██████████| 324/324 [03:03<00:00,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 42min 21s, sys: 8.63 s, total: 42min 29s\n",
            "Wall time: 43min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rq4_results_all = results_all"
      ],
      "metadata": {
        "id": "6-594pN3mHlu"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_all = rq4_results_all"
      ],
      "metadata": {
        "id": "mAvDRDNs2JzA"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_order = ['gpt-4', 'gpt-3.5-turbo', 'text-davinci-003', 'vicuna-13b', 'alpaca-lora-30b', 'opt-iml-max-1.3b', 'llama-65b', 'opt-66b', 'all']\n",
        "results_all = results_all.reset_index().set_index(['Train Language', 'Train LLM', 'Model'])\n",
        "results_all = results_all[new_order]"
      ],
      "metadata": {
        "id": "jfDoie9ziIRo"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_all"
      ],
      "metadata": {
        "id": "vINFflETN03p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "198da5ac-21f3-4e92-8c21-2d532d153ba8"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                  gpt-4  \\\n",
              "Train Language Train LLM        Model                                     \n",
              "en             all              bert-base-multilingual-cased   0.613070   \n",
              "                                electra-large-discriminator    0.572043   \n",
              "                                gpt2-medium                    0.490022   \n",
              "                                mGPT                           0.517999   \n",
              "                                mdeberta-v3-base               0.575883   \n",
              "                                roberta-large-openai-detector  0.427121   \n",
              "                                xlm-roberta-large              0.447902   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased   0.796355   \n",
              "                                bert-base-multilingual-cased   0.796355   \n",
              "                                electra-large-discriminator    0.500101   \n",
              "                                electra-large-discriminator    0.500101   \n",
              "                                gpt2-medium                    0.443838   \n",
              "                                gpt2-medium                    0.443838   \n",
              "                                mGPT                           0.743686   \n",
              "                                mGPT                           0.743686   \n",
              "                                mdeberta-v3-base               0.824227   \n",
              "                                mdeberta-v3-base               0.824227   \n",
              "                                roberta-large-openai-detector  0.564559   \n",
              "                                roberta-large-openai-detector  0.564559   \n",
              "                                xlm-roberta-large              0.802856   \n",
              "                                xlm-roberta-large              0.802856   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased   0.884076   \n",
              "                                bert-base-multilingual-cased   0.884076   \n",
              "                                electra-large-discriminator    0.587872   \n",
              "                                electra-large-discriminator    0.587872   \n",
              "                                gpt2-medium                    0.438164   \n",
              "                                gpt2-medium                    0.438164   \n",
              "                                mGPT                           0.785794   \n",
              "                                mGPT                           0.785794   \n",
              "                                mdeberta-v3-base               0.731336   \n",
              "                                mdeberta-v3-base               0.731336   \n",
              "                                roberta-large-openai-detector  0.610473   \n",
              "                                roberta-large-openai-detector  0.610473   \n",
              "                                xlm-roberta-large              0.786173   \n",
              "                                xlm-roberta-large              0.786173   \n",
              "               gpt-4            bert-base-multilingual-cased   0.845805   \n",
              "                                bert-base-multilingual-cased   0.845805   \n",
              "                                electra-large-discriminator    0.536176   \n",
              "                                electra-large-discriminator    0.536176   \n",
              "                                gpt2-medium                    0.446006   \n",
              "                                gpt2-medium                    0.446006   \n",
              "                                mGPT                           0.765694   \n",
              "                                mGPT                           0.765694   \n",
              "                                mdeberta-v3-base               0.846920   \n",
              "                                mdeberta-v3-base               0.846920   \n",
              "                                roberta-large-openai-detector  0.613606   \n",
              "                                roberta-large-openai-detector  0.613606   \n",
              "                                xlm-roberta-large              0.774210   \n",
              "                                xlm-roberta-large              0.774210   \n",
              "               llama-65b        bert-base-multilingual-cased   0.432711   \n",
              "                                bert-base-multilingual-cased   0.432711   \n",
              "                                electra-large-discriminator    0.480630   \n",
              "                                electra-large-discriminator    0.480630   \n",
              "                                gpt2-medium                    0.409221   \n",
              "                                gpt2-medium                    0.409221   \n",
              "                                mGPT                           0.508570   \n",
              "                                mGPT                           0.508570   \n",
              "                                mdeberta-v3-base               0.562645   \n",
              "                                mdeberta-v3-base               0.562645   \n",
              "                                roberta-large-openai-detector  0.415707   \n",
              "                                roberta-large-openai-detector  0.415707   \n",
              "                                xlm-roberta-large              0.518337   \n",
              "                                xlm-roberta-large              0.518337   \n",
              "               opt-66b          bert-base-multilingual-cased   0.526402   \n",
              "                                bert-base-multilingual-cased   0.526402   \n",
              "                                electra-large-discriminator    0.521714   \n",
              "                                electra-large-discriminator    0.521714   \n",
              "                                gpt2-medium                    0.457371   \n",
              "                                gpt2-medium                    0.457371   \n",
              "                                mGPT                           0.624758   \n",
              "                                mGPT                           0.624758   \n",
              "                                mdeberta-v3-base               0.592428   \n",
              "                                mdeberta-v3-base               0.592428   \n",
              "                                roberta-large-openai-detector  0.478735   \n",
              "                                roberta-large-openai-detector  0.478735   \n",
              "                                xlm-roberta-large              0.554454   \n",
              "                                xlm-roberta-large              0.554454   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased   0.533043   \n",
              "                                bert-base-multilingual-cased   0.533043   \n",
              "                                electra-large-discriminator    0.478760   \n",
              "                                electra-large-discriminator    0.478760   \n",
              "                                gpt2-medium                    0.505258   \n",
              "                                gpt2-medium                    0.505258   \n",
              "                                mGPT                           0.606138   \n",
              "                                mGPT                           0.606138   \n",
              "                                mdeberta-v3-base               0.509315   \n",
              "                                mdeberta-v3-base               0.509315   \n",
              "                                roberta-large-openai-detector  0.649095   \n",
              "                                roberta-large-openai-detector  0.649095   \n",
              "                                xlm-roberta-large              0.519586   \n",
              "                                xlm-roberta-large              0.519586   \n",
              "               text-davinci-003 bert-base-multilingual-cased   0.811605   \n",
              "                                bert-base-multilingual-cased   0.811605   \n",
              "                                electra-large-discriminator    0.531004   \n",
              "                                electra-large-discriminator    0.531004   \n",
              "                                gpt2-medium                    0.437797   \n",
              "                                gpt2-medium                    0.437797   \n",
              "                                mGPT                           0.770905   \n",
              "                                mGPT                           0.770905   \n",
              "                                mdeberta-v3-base               0.753282   \n",
              "                                mdeberta-v3-base               0.753282   \n",
              "                                roberta-large-openai-detector  0.567119   \n",
              "                                roberta-large-openai-detector  0.567119   \n",
              "                                xlm-roberta-large              0.746851   \n",
              "                                xlm-roberta-large              0.746851   \n",
              "               vicuna-13b       bert-base-multilingual-cased   0.867224   \n",
              "                                bert-base-multilingual-cased   0.867224   \n",
              "                                electra-large-discriminator    0.566895   \n",
              "                                electra-large-discriminator    0.566895   \n",
              "                                gpt2-medium                    0.442135   \n",
              "                                gpt2-medium                    0.442135   \n",
              "                                mGPT                           0.736586   \n",
              "                                mGPT                           0.736586   \n",
              "                                mdeberta-v3-base               0.592511   \n",
              "                                mdeberta-v3-base               0.592511   \n",
              "                                roberta-large-openai-detector  0.609310   \n",
              "                                roberta-large-openai-detector  0.609310   \n",
              "                                xlm-roberta-large              0.747565   \n",
              "                                xlm-roberta-large              0.747565   \n",
              "es             all              bert-base-multilingual-cased   0.756407   \n",
              "                                electra-large-discriminator    0.408230   \n",
              "                                gpt2-medium                    0.569450   \n",
              "                                mGPT                           0.717659   \n",
              "                                mdeberta-v3-base               0.769217   \n",
              "                                roberta-large-openai-detector  0.655892   \n",
              "                                xlm-roberta-large              0.822754   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased   0.807827   \n",
              "                                bert-base-multilingual-cased   0.807827   \n",
              "                                electra-large-discriminator    0.510482   \n",
              "                                electra-large-discriminator    0.510482   \n",
              "                                gpt2-medium                    0.449078   \n",
              "                                gpt2-medium                    0.449078   \n",
              "                                mGPT                           0.696568   \n",
              "                                mGPT                           0.696568   \n",
              "                                mdeberta-v3-base               0.823715   \n",
              "                                mdeberta-v3-base               0.823715   \n",
              "                                roberta-large-openai-detector  0.645201   \n",
              "                                roberta-large-openai-detector  0.645201   \n",
              "                                xlm-roberta-large              0.645166   \n",
              "                                xlm-roberta-large              0.645166   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased   0.828953   \n",
              "                                bert-base-multilingual-cased   0.828953   \n",
              "                                electra-large-discriminator    0.561989   \n",
              "                                electra-large-discriminator    0.561989   \n",
              "                                gpt2-medium                    0.502223   \n",
              "                                gpt2-medium                    0.502223   \n",
              "                                mGPT                           0.803299   \n",
              "                                mGPT                           0.803299   \n",
              "                                mdeberta-v3-base               0.903984   \n",
              "                                mdeberta-v3-base               0.903984   \n",
              "                                roberta-large-openai-detector  0.687462   \n",
              "                                roberta-large-openai-detector  0.687462   \n",
              "                                xlm-roberta-large              0.801038   \n",
              "                                xlm-roberta-large              0.801038   \n",
              "               gpt-4            bert-base-multilingual-cased   0.880243   \n",
              "                                bert-base-multilingual-cased   0.880243   \n",
              "                                electra-large-discriminator    0.578472   \n",
              "                                electra-large-discriminator    0.578472   \n",
              "                                gpt2-medium                    0.555052   \n",
              "                                gpt2-medium                    0.555052   \n",
              "                                mGPT                           0.852589   \n",
              "                                mGPT                           0.852589   \n",
              "                                mdeberta-v3-base               0.911326   \n",
              "                                mdeberta-v3-base               0.911326   \n",
              "                                roberta-large-openai-detector  0.706333   \n",
              "                                roberta-large-openai-detector  0.706333   \n",
              "                                xlm-roberta-large              0.767969   \n",
              "                                xlm-roberta-large              0.767969   \n",
              "               llama-65b        bert-base-multilingual-cased   0.429948   \n",
              "                                bert-base-multilingual-cased   0.429948   \n",
              "                                electra-large-discriminator    0.480871   \n",
              "                                electra-large-discriminator    0.480871   \n",
              "                                gpt2-medium                    0.472059   \n",
              "                                gpt2-medium                    0.472059   \n",
              "                                mGPT                           0.519882   \n",
              "                                mGPT                           0.519882   \n",
              "                                mdeberta-v3-base               0.438188   \n",
              "                                mdeberta-v3-base               0.438188   \n",
              "                                roberta-large-openai-detector  0.628179   \n",
              "                                roberta-large-openai-detector  0.628179   \n",
              "                                xlm-roberta-large              0.445081   \n",
              "                                xlm-roberta-large              0.445081   \n",
              "               opt-66b          bert-base-multilingual-cased   0.536997   \n",
              "                                bert-base-multilingual-cased   0.536997   \n",
              "                                electra-large-discriminator    0.478527   \n",
              "                                electra-large-discriminator    0.478527   \n",
              "                                gpt2-medium                    0.457473   \n",
              "                                gpt2-medium                    0.457473   \n",
              "                                mGPT                           0.539005   \n",
              "                                mGPT                           0.539005   \n",
              "                                mdeberta-v3-base               0.400021   \n",
              "                                mdeberta-v3-base               0.400021   \n",
              "                                roberta-large-openai-detector  0.634111   \n",
              "                                roberta-large-openai-detector  0.634111   \n",
              "                                xlm-roberta-large              0.465354   \n",
              "                                xlm-roberta-large              0.465354   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased   0.345517   \n",
              "                                bert-base-multilingual-cased   0.345517   \n",
              "                                electra-large-discriminator    0.490858   \n",
              "                                electra-large-discriminator    0.490858   \n",
              "                                gpt2-medium                    0.439919   \n",
              "                                gpt2-medium                    0.439919   \n",
              "                                mGPT                           0.364142   \n",
              "                                mGPT                           0.364142   \n",
              "                                mdeberta-v3-base               0.335227   \n",
              "                                mdeberta-v3-base               0.335227   \n",
              "                                roberta-large-openai-detector  0.582879   \n",
              "                                roberta-large-openai-detector  0.582879   \n",
              "                                xlm-roberta-large              0.332034   \n",
              "                                xlm-roberta-large              0.332034   \n",
              "               text-davinci-003 bert-base-multilingual-cased   0.784046   \n",
              "                                bert-base-multilingual-cased   0.784046   \n",
              "                                electra-large-discriminator    0.542862   \n",
              "                                electra-large-discriminator    0.542862   \n",
              "                                gpt2-medium                    0.474970   \n",
              "                                gpt2-medium                    0.474970   \n",
              "                                mGPT                           0.772954   \n",
              "                                mGPT                           0.772954   \n",
              "                                mdeberta-v3-base               0.888102   \n",
              "                                mdeberta-v3-base               0.888102   \n",
              "                                roberta-large-openai-detector  0.660211   \n",
              "                                roberta-large-openai-detector  0.660211   \n",
              "                                xlm-roberta-large              0.856881   \n",
              "                                xlm-roberta-large              0.856881   \n",
              "               vicuna-13b       bert-base-multilingual-cased   0.844382   \n",
              "                                bert-base-multilingual-cased   0.844382   \n",
              "                                electra-large-discriminator    0.570722   \n",
              "                                electra-large-discriminator    0.570722   \n",
              "                                gpt2-medium                    0.581898   \n",
              "                                gpt2-medium                    0.581898   \n",
              "                                mGPT                           0.766373   \n",
              "                                mGPT                           0.766373   \n",
              "                                mdeberta-v3-base               0.863041   \n",
              "                                mdeberta-v3-base               0.863041   \n",
              "                                roberta-large-openai-detector  0.687239   \n",
              "                                roberta-large-openai-detector  0.687239   \n",
              "                                xlm-roberta-large              0.699092   \n",
              "                                xlm-roberta-large              0.699092   \n",
              "ru             all              bert-base-multilingual-cased   0.593363   \n",
              "                                electra-large-discriminator    0.334938   \n",
              "                                gpt2-medium                    0.546518   \n",
              "                                mGPT                           0.675996   \n",
              "                                mdeberta-v3-base               0.674903   \n",
              "                                roberta-large-openai-detector  0.444655   \n",
              "                                xlm-roberta-large              0.776748   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased   0.727572   \n",
              "                                bert-base-multilingual-cased   0.727572   \n",
              "                                electra-large-discriminator    0.490623   \n",
              "                                electra-large-discriminator    0.490623   \n",
              "                                gpt2-medium                    0.433799   \n",
              "                                gpt2-medium                    0.433799   \n",
              "                                mGPT                           0.660963   \n",
              "                                mGPT                           0.660963   \n",
              "                                mdeberta-v3-base               0.795880   \n",
              "                                mdeberta-v3-base               0.795880   \n",
              "                                roberta-large-openai-detector  0.513894   \n",
              "                                roberta-large-openai-detector  0.513894   \n",
              "                                xlm-roberta-large              0.776239   \n",
              "                                xlm-roberta-large              0.776239   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased   0.854759   \n",
              "                                bert-base-multilingual-cased   0.854759   \n",
              "                                electra-large-discriminator    0.498302   \n",
              "                                electra-large-discriminator    0.498302   \n",
              "                                gpt2-medium                    0.459909   \n",
              "                                gpt2-medium                    0.459909   \n",
              "                                mGPT                           0.784028   \n",
              "                                mGPT                           0.784028   \n",
              "                                mdeberta-v3-base               0.907412   \n",
              "                                mdeberta-v3-base               0.907412   \n",
              "                                roberta-large-openai-detector  0.524653   \n",
              "                                roberta-large-openai-detector  0.524653   \n",
              "                                xlm-roberta-large              0.931978   \n",
              "                                xlm-roberta-large              0.931978   \n",
              "               gpt-4            bert-base-multilingual-cased   0.774697   \n",
              "                                bert-base-multilingual-cased   0.774697   \n",
              "                                electra-large-discriminator    0.478839   \n",
              "                                electra-large-discriminator    0.478839   \n",
              "                                gpt2-medium                    0.500350   \n",
              "                                gpt2-medium                    0.500350   \n",
              "                                mGPT                           0.856636   \n",
              "                                mGPT                           0.856636   \n",
              "                                mdeberta-v3-base               0.813415   \n",
              "                                mdeberta-v3-base               0.813415   \n",
              "                                roberta-large-openai-detector  0.535498   \n",
              "                                roberta-large-openai-detector  0.535498   \n",
              "                                xlm-roberta-large              0.949946   \n",
              "                                xlm-roberta-large              0.949946   \n",
              "               llama-65b        bert-base-multilingual-cased   0.555053   \n",
              "                                bert-base-multilingual-cased   0.555053   \n",
              "                                electra-large-discriminator    0.454623   \n",
              "                                electra-large-discriminator    0.454623   \n",
              "                                gpt2-medium                    0.409295   \n",
              "                                gpt2-medium                    0.409295   \n",
              "                                mGPT                           0.489052   \n",
              "                                mGPT                           0.489052   \n",
              "                                mdeberta-v3-base               0.481618   \n",
              "                                mdeberta-v3-base               0.481618   \n",
              "                                roberta-large-openai-detector  0.448728   \n",
              "                                roberta-large-openai-detector  0.448728   \n",
              "                                xlm-roberta-large              0.472980   \n",
              "                                xlm-roberta-large              0.472980   \n",
              "               opt-66b          bert-base-multilingual-cased   0.359153   \n",
              "                                bert-base-multilingual-cased   0.359153   \n",
              "                                electra-large-discriminator    0.418851   \n",
              "                                electra-large-discriminator    0.418851   \n",
              "                                gpt2-medium                    0.395000   \n",
              "                                gpt2-medium                    0.395000   \n",
              "                                mGPT                           0.359644   \n",
              "                                mGPT                           0.359644   \n",
              "                                mdeberta-v3-base               0.337545   \n",
              "                                mdeberta-v3-base               0.337545   \n",
              "                                roberta-large-openai-detector  0.435946   \n",
              "                                roberta-large-openai-detector  0.435946   \n",
              "                                xlm-roberta-large              0.332034   \n",
              "                                xlm-roberta-large              0.332034   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased   0.332727   \n",
              "                                bert-base-multilingual-cased   0.332727   \n",
              "                                electra-large-discriminator    0.358508   \n",
              "                                electra-large-discriminator    0.358508   \n",
              "                                gpt2-medium                    0.373721   \n",
              "                                gpt2-medium                    0.373721   \n",
              "                                mGPT                           0.338130   \n",
              "                                mGPT                           0.338130   \n",
              "                                mdeberta-v3-base               0.337146   \n",
              "                                mdeberta-v3-base               0.337146   \n",
              "                                roberta-large-openai-detector  0.478941   \n",
              "                                roberta-large-openai-detector  0.478941   \n",
              "                                xlm-roberta-large              0.331246   \n",
              "                                xlm-roberta-large              0.331246   \n",
              "               text-davinci-003 bert-base-multilingual-cased   0.655340   \n",
              "                                bert-base-multilingual-cased   0.655340   \n",
              "                                electra-large-discriminator    0.364191   \n",
              "                                electra-large-discriminator    0.364191   \n",
              "                                gpt2-medium                    0.375325   \n",
              "                                gpt2-medium                    0.375325   \n",
              "                                mGPT                           0.657068   \n",
              "                                mGPT                           0.657068   \n",
              "                                mdeberta-v3-base               0.434200   \n",
              "                                mdeberta-v3-base               0.434200   \n",
              "                                roberta-large-openai-detector  0.490727   \n",
              "                                roberta-large-openai-detector  0.490727   \n",
              "                                xlm-roberta-large              0.462627   \n",
              "                                xlm-roberta-large              0.462627   \n",
              "               vicuna-13b       bert-base-multilingual-cased   0.541668   \n",
              "                                bert-base-multilingual-cased   0.541668   \n",
              "                                electra-large-discriminator    0.460503   \n",
              "                                electra-large-discriminator    0.460503   \n",
              "                                gpt2-medium                    0.480937   \n",
              "                                gpt2-medium                    0.480937   \n",
              "                                mGPT                           0.661594   \n",
              "                                mGPT                           0.661594   \n",
              "                                mdeberta-v3-base               0.758154   \n",
              "                                mdeberta-v3-base               0.758154   \n",
              "                                roberta-large-openai-detector  0.514584   \n",
              "                                roberta-large-openai-detector  0.514584   \n",
              "                                xlm-roberta-large              0.799182   \n",
              "                                xlm-roberta-large              0.799182   \n",
              "all            all              bert-base-multilingual-cased   0.742030   \n",
              "                                electra-large-discriminator    0.420135   \n",
              "                                gpt2-medium                    0.639329   \n",
              "                                mGPT                           0.718145   \n",
              "                                mdeberta-v3-base               0.855155   \n",
              "                                roberta-large-openai-detector  0.732333   \n",
              "                                xlm-roberta-large              0.780130   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased   0.883190   \n",
              "                                bert-base-multilingual-cased   0.883190   \n",
              "                                electra-large-discriminator    0.582197   \n",
              "                                electra-large-discriminator    0.582197   \n",
              "                                gpt2-medium                    0.521494   \n",
              "                                gpt2-medium                    0.521494   \n",
              "                                mGPT                           0.747279   \n",
              "                                mGPT                           0.747279   \n",
              "                                mdeberta-v3-base               0.860280   \n",
              "                                mdeberta-v3-base               0.860280   \n",
              "                                roberta-large-openai-detector  0.740567   \n",
              "                                roberta-large-openai-detector  0.740567   \n",
              "                                xlm-roberta-large              0.833294   \n",
              "                                xlm-roberta-large              0.833294   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased   0.895417   \n",
              "                                bert-base-multilingual-cased   0.895417   \n",
              "                                electra-large-discriminator    0.548083   \n",
              "                                electra-large-discriminator    0.548083   \n",
              "                                gpt2-medium                    0.670842   \n",
              "                                gpt2-medium                    0.670842   \n",
              "                                mGPT                           0.736664   \n",
              "                                mGPT                           0.736664   \n",
              "                                mdeberta-v3-base               0.926275   \n",
              "                                mdeberta-v3-base               0.926275   \n",
              "                                roberta-large-openai-detector  0.660923   \n",
              "                                roberta-large-openai-detector  0.660923   \n",
              "                                xlm-roberta-large              0.939394   \n",
              "                                xlm-roberta-large              0.939394   \n",
              "               gpt-4            bert-base-multilingual-cased   0.946400   \n",
              "                                bert-base-multilingual-cased   0.946400   \n",
              "                                electra-large-discriminator    0.666535   \n",
              "                                electra-large-discriminator    0.666535   \n",
              "                                gpt2-medium                    0.786658   \n",
              "                                gpt2-medium                    0.786658   \n",
              "                                mGPT                           0.878417   \n",
              "                                mGPT                           0.878417   \n",
              "                                mdeberta-v3-base               0.944254   \n",
              "                                mdeberta-v3-base               0.944254   \n",
              "                                roberta-large-openai-detector  0.843479   \n",
              "                                roberta-large-openai-detector  0.843479   \n",
              "                                xlm-roberta-large              0.973437   \n",
              "                                xlm-roberta-large              0.973437   \n",
              "               llama-65b        bert-base-multilingual-cased   0.522707   \n",
              "                                bert-base-multilingual-cased   0.522707   \n",
              "                                electra-large-discriminator    0.403537   \n",
              "                                electra-large-discriminator    0.403537   \n",
              "                                gpt2-medium                    0.434776   \n",
              "                                gpt2-medium                    0.434776   \n",
              "                                mGPT                           0.557785   \n",
              "                                mGPT                           0.557785   \n",
              "                                mdeberta-v3-base               0.533878   \n",
              "                                mdeberta-v3-base               0.533878   \n",
              "                                roberta-large-openai-detector  0.590498   \n",
              "                                roberta-large-openai-detector  0.590498   \n",
              "                                xlm-roberta-large              0.504345   \n",
              "                                xlm-roberta-large              0.504345   \n",
              "               opt-66b          bert-base-multilingual-cased   0.523638   \n",
              "                                bert-base-multilingual-cased   0.523638   \n",
              "                                electra-large-discriminator    0.539051   \n",
              "                                electra-large-discriminator    0.539051   \n",
              "                                gpt2-medium                    0.461834   \n",
              "                                gpt2-medium                    0.461834   \n",
              "                                mGPT                           0.498187   \n",
              "                                mGPT                           0.498187   \n",
              "                                mdeberta-v3-base               0.486604   \n",
              "                                mdeberta-v3-base               0.486604   \n",
              "                                roberta-large-openai-detector  0.513527   \n",
              "                                roberta-large-openai-detector  0.513527   \n",
              "                                xlm-roberta-large              0.434749   \n",
              "                                xlm-roberta-large              0.434749   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased   0.527543   \n",
              "                                bert-base-multilingual-cased   0.527543   \n",
              "                                electra-large-discriminator    0.368141   \n",
              "                                electra-large-discriminator    0.368141   \n",
              "                                gpt2-medium                    0.461409   \n",
              "                                gpt2-medium                    0.461409   \n",
              "                                mGPT                           0.426201   \n",
              "                                mGPT                           0.426201   \n",
              "                                mdeberta-v3-base               0.433971   \n",
              "                                mdeberta-v3-base               0.433971   \n",
              "                                roberta-large-openai-detector  0.524179   \n",
              "                                roberta-large-openai-detector  0.524179   \n",
              "                                xlm-roberta-large              0.380097   \n",
              "                                xlm-roberta-large              0.380097   \n",
              "               text-davinci-003 bert-base-multilingual-cased   0.878401   \n",
              "                                bert-base-multilingual-cased   0.878401   \n",
              "                                electra-large-discriminator    0.566864   \n",
              "                                electra-large-discriminator    0.566864   \n",
              "                                gpt2-medium                    0.524432   \n",
              "                                gpt2-medium                    0.524432   \n",
              "                                mGPT                           0.792974   \n",
              "                                mGPT                           0.792974   \n",
              "                                mdeberta-v3-base               0.890076   \n",
              "                                mdeberta-v3-base               0.890076   \n",
              "                                roberta-large-openai-detector  0.721004   \n",
              "                                roberta-large-openai-detector  0.721004   \n",
              "                                xlm-roberta-large              0.765965   \n",
              "                                xlm-roberta-large              0.765965   \n",
              "               vicuna-13b       bert-base-multilingual-cased   0.890515   \n",
              "                                bert-base-multilingual-cased   0.890515   \n",
              "                                electra-large-discriminator    0.602345   \n",
              "                                electra-large-discriminator    0.602345   \n",
              "                                gpt2-medium                    0.624563   \n",
              "                                gpt2-medium                    0.624563   \n",
              "                                mGPT                           0.823762   \n",
              "                                mGPT                           0.823762   \n",
              "                                mdeberta-v3-base               0.903552   \n",
              "                                mdeberta-v3-base               0.903552   \n",
              "                                roberta-large-openai-detector  0.817151   \n",
              "                                roberta-large-openai-detector  0.817151   \n",
              "                                xlm-roberta-large              0.920155   \n",
              "                                xlm-roberta-large              0.920155   \n",
              "en3            all              bert-base-multilingual-cased   0.396812   \n",
              "                                electra-large-discriminator    0.414982   \n",
              "                                gpt2-medium                    0.438186   \n",
              "                                mGPT                           0.569036   \n",
              "                                mdeberta-v3-base               0.456115   \n",
              "                                roberta-large-openai-detector  0.506289   \n",
              "                                xlm-roberta-large              0.509404   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased   0.815090   \n",
              "                                bert-base-multilingual-cased   0.815090   \n",
              "                                electra-large-discriminator    0.539969   \n",
              "                                electra-large-discriminator    0.539969   \n",
              "                                gpt2-medium                    0.434758   \n",
              "                                gpt2-medium                    0.434758   \n",
              "                                mGPT                           0.740861   \n",
              "                                mGPT                           0.740861   \n",
              "                                mdeberta-v3-base               0.732471   \n",
              "                                mdeberta-v3-base               0.732471   \n",
              "                                roberta-large-openai-detector  0.507571   \n",
              "                                roberta-large-openai-detector  0.507571   \n",
              "                                xlm-roberta-large              0.842186   \n",
              "                                xlm-roberta-large              0.842186   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased   0.897896   \n",
              "                                bert-base-multilingual-cased   0.897896   \n",
              "                                electra-large-discriminator    0.477612   \n",
              "                                electra-large-discriminator    0.477612   \n",
              "                                gpt2-medium                    0.430465   \n",
              "                                gpt2-medium                    0.430465   \n",
              "                                mGPT                           0.765838   \n",
              "                                mGPT                           0.765838   \n",
              "                                mdeberta-v3-base               0.795732   \n",
              "                                mdeberta-v3-base               0.795732   \n",
              "                                roberta-large-openai-detector  0.743814   \n",
              "                                roberta-large-openai-detector  0.743814   \n",
              "                                xlm-roberta-large              0.915388   \n",
              "                                xlm-roberta-large              0.915388   \n",
              "               gpt-4            bert-base-multilingual-cased   0.877989   \n",
              "                                bert-base-multilingual-cased   0.877989   \n",
              "                                electra-large-discriminator    0.580817   \n",
              "                                electra-large-discriminator    0.580817   \n",
              "                                gpt2-medium                    0.436012   \n",
              "                                gpt2-medium                    0.436012   \n",
              "                                mGPT                           0.835305   \n",
              "                                mGPT                           0.835305   \n",
              "                                mdeberta-v3-base               0.771539   \n",
              "                                mdeberta-v3-base               0.771539   \n",
              "                                roberta-large-openai-detector  0.650100   \n",
              "                                roberta-large-openai-detector  0.650100   \n",
              "                                xlm-roberta-large              0.912705   \n",
              "                                xlm-roberta-large              0.912705   \n",
              "               llama-65b        bert-base-multilingual-cased   0.503145   \n",
              "                                bert-base-multilingual-cased   0.503145   \n",
              "                                electra-large-discriminator    0.422044   \n",
              "                                electra-large-discriminator    0.422044   \n",
              "                                gpt2-medium                    0.416851   \n",
              "                                gpt2-medium                    0.416851   \n",
              "                                mGPT                           0.549666   \n",
              "                                mGPT                           0.549666   \n",
              "                                mdeberta-v3-base               0.508899   \n",
              "                                mdeberta-v3-base               0.508899   \n",
              "                                roberta-large-openai-detector  0.429331   \n",
              "                                roberta-large-openai-detector  0.429331   \n",
              "                                xlm-roberta-large              0.543955   \n",
              "                                xlm-roberta-large              0.543955   \n",
              "               opt-66b          bert-base-multilingual-cased   0.591382   \n",
              "                                bert-base-multilingual-cased   0.591382   \n",
              "                                electra-large-discriminator    0.593698   \n",
              "                                electra-large-discriminator    0.593698   \n",
              "                                gpt2-medium                    0.476345   \n",
              "                                gpt2-medium                    0.476345   \n",
              "                                mGPT                           0.647876   \n",
              "                                mGPT                           0.647876   \n",
              "                                mdeberta-v3-base               0.613564   \n",
              "                                mdeberta-v3-base               0.613564   \n",
              "                                roberta-large-openai-detector  0.523213   \n",
              "                                roberta-large-openai-detector  0.523213   \n",
              "                                xlm-roberta-large              0.573028   \n",
              "                                xlm-roberta-large              0.573028   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased   0.608342   \n",
              "                                bert-base-multilingual-cased   0.608342   \n",
              "                                electra-large-discriminator    0.496345   \n",
              "                                electra-large-discriminator    0.496345   \n",
              "                                gpt2-medium                    0.513750   \n",
              "                                gpt2-medium                    0.513750   \n",
              "                                mGPT                           0.620189   \n",
              "                                mGPT                           0.620189   \n",
              "                                mdeberta-v3-base               0.609107   \n",
              "                                mdeberta-v3-base               0.609107   \n",
              "                                roberta-large-openai-detector  0.642522   \n",
              "                                roberta-large-openai-detector  0.642522   \n",
              "                                xlm-roberta-large              0.585287   \n",
              "                                xlm-roberta-large              0.585287   \n",
              "               text-davinci-003 bert-base-multilingual-cased   0.823819   \n",
              "                                bert-base-multilingual-cased   0.823819   \n",
              "                                electra-large-discriminator    0.428300   \n",
              "                                electra-large-discriminator    0.428300   \n",
              "                                gpt2-medium                    0.435057   \n",
              "                                gpt2-medium                    0.435057   \n",
              "                                mGPT                           0.791793   \n",
              "                                mGPT                           0.791793   \n",
              "                                mdeberta-v3-base               0.810319   \n",
              "                                mdeberta-v3-base               0.810319   \n",
              "                                roberta-large-openai-detector  0.713313   \n",
              "                                roberta-large-openai-detector  0.713313   \n",
              "                                xlm-roberta-large              0.749457   \n",
              "                                xlm-roberta-large              0.749457   \n",
              "               vicuna-13b       bert-base-multilingual-cased   0.859931   \n",
              "                                bert-base-multilingual-cased   0.859931   \n",
              "                                electra-large-discriminator    0.603294   \n",
              "                                electra-large-discriminator    0.603294   \n",
              "                                gpt2-medium                    0.481501   \n",
              "                                gpt2-medium                    0.481501   \n",
              "                                mGPT                           0.767501   \n",
              "                                mGPT                           0.767501   \n",
              "                                mdeberta-v3-base               0.759918   \n",
              "                                mdeberta-v3-base               0.759918   \n",
              "                                roberta-large-openai-detector  0.498172   \n",
              "                                roberta-large-openai-detector  0.498172   \n",
              "                                xlm-roberta-large              0.867251   \n",
              "                                xlm-roberta-large              0.867251   \n",
              "\n",
              "                                                               gpt-3.5-turbo  \\\n",
              "Train Language Train LLM        Model                                          \n",
              "en             all              bert-base-multilingual-cased        0.613582   \n",
              "                                electra-large-discriminator         0.524899   \n",
              "                                gpt2-medium                         0.487222   \n",
              "                                mGPT                                0.519259   \n",
              "                                mdeberta-v3-base                    0.575150   \n",
              "                                roberta-large-openai-detector       0.427305   \n",
              "                                xlm-roberta-large                   0.447416   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased        0.823652   \n",
              "                                bert-base-multilingual-cased        0.823652   \n",
              "                                electra-large-discriminator         0.524322   \n",
              "                                electra-large-discriminator         0.524322   \n",
              "                                gpt2-medium                         0.458944   \n",
              "                                gpt2-medium                         0.458944   \n",
              "                                mGPT                                0.802225   \n",
              "                                mGPT                                0.802225   \n",
              "                                mdeberta-v3-base                    0.811764   \n",
              "                                mdeberta-v3-base                    0.811764   \n",
              "                                roberta-large-openai-detector       0.566007   \n",
              "                                roberta-large-openai-detector       0.566007   \n",
              "                                xlm-roberta-large                   0.826377   \n",
              "                                xlm-roberta-large                   0.826377   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased        0.910558   \n",
              "                                bert-base-multilingual-cased        0.910558   \n",
              "                                electra-large-discriminator         0.605227   \n",
              "                                electra-large-discriminator         0.605227   \n",
              "                                gpt2-medium                         0.430198   \n",
              "                                gpt2-medium                         0.430198   \n",
              "                                mGPT                                0.828625   \n",
              "                                mGPT                                0.828625   \n",
              "                                mdeberta-v3-base                    0.713590   \n",
              "                                mdeberta-v3-base                    0.713590   \n",
              "                                roberta-large-openai-detector       0.614087   \n",
              "                                roberta-large-openai-detector       0.614087   \n",
              "                                xlm-roberta-large                   0.830767   \n",
              "                                xlm-roberta-large                   0.830767   \n",
              "               gpt-4            bert-base-multilingual-cased        0.795424   \n",
              "                                bert-base-multilingual-cased        0.795424   \n",
              "                                electra-large-discriminator         0.522535   \n",
              "                                electra-large-discriminator         0.522535   \n",
              "                                gpt2-medium                         0.433410   \n",
              "                                gpt2-medium                         0.433410   \n",
              "                                mGPT                                0.761554   \n",
              "                                mGPT                                0.761554   \n",
              "                                mdeberta-v3-base                    0.807743   \n",
              "                                mdeberta-v3-base                    0.807743   \n",
              "                                roberta-large-openai-detector       0.613081   \n",
              "                                roberta-large-openai-detector       0.613081   \n",
              "                                xlm-roberta-large                   0.715108   \n",
              "                                xlm-roberta-large                   0.715108   \n",
              "               llama-65b        bert-base-multilingual-cased        0.317376   \n",
              "                                bert-base-multilingual-cased        0.317376   \n",
              "                                electra-large-discriminator         0.484301   \n",
              "                                electra-large-discriminator         0.484301   \n",
              "                                gpt2-medium                         0.422094   \n",
              "                                gpt2-medium                         0.422094   \n",
              "                                mGPT                                0.482354   \n",
              "                                mGPT                                0.482354   \n",
              "                                mdeberta-v3-base                    0.535639   \n",
              "                                mdeberta-v3-base                    0.535639   \n",
              "                                roberta-large-openai-detector       0.415258   \n",
              "                                roberta-large-openai-detector       0.415258   \n",
              "                                xlm-roberta-large                   0.469120   \n",
              "                                xlm-roberta-large                   0.469120   \n",
              "               opt-66b          bert-base-multilingual-cased        0.444345   \n",
              "                                bert-base-multilingual-cased        0.444345   \n",
              "                                electra-large-discriminator         0.470984   \n",
              "                                electra-large-discriminator         0.470984   \n",
              "                                gpt2-medium                         0.455639   \n",
              "                                gpt2-medium                         0.455639   \n",
              "                                mGPT                                0.618479   \n",
              "                                mGPT                                0.618479   \n",
              "                                mdeberta-v3-base                    0.520340   \n",
              "                                mdeberta-v3-base                    0.520340   \n",
              "                                roberta-large-openai-detector       0.477587   \n",
              "                                roberta-large-openai-detector       0.477587   \n",
              "                                xlm-roberta-large                   0.504792   \n",
              "                                xlm-roberta-large                   0.504792   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased        0.528943   \n",
              "                                bert-base-multilingual-cased        0.528943   \n",
              "                                electra-large-discriminator         0.461260   \n",
              "                                electra-large-discriminator         0.461260   \n",
              "                                gpt2-medium                         0.493618   \n",
              "                                gpt2-medium                         0.493618   \n",
              "                                mGPT                                0.620005   \n",
              "                                mGPT                                0.620005   \n",
              "                                mdeberta-v3-base                    0.537425   \n",
              "                                mdeberta-v3-base                    0.537425   \n",
              "                                roberta-large-openai-detector       0.659102   \n",
              "                                roberta-large-openai-detector       0.659102   \n",
              "                                xlm-roberta-large                   0.521796   \n",
              "                                xlm-roberta-large                   0.521796   \n",
              "               text-davinci-003 bert-base-multilingual-cased        0.869161   \n",
              "                                bert-base-multilingual-cased        0.869161   \n",
              "                                electra-large-discriminator         0.592415   \n",
              "                                electra-large-discriminator         0.592415   \n",
              "                                gpt2-medium                         0.428374   \n",
              "                                gpt2-medium                         0.428374   \n",
              "                                mGPT                                0.804355   \n",
              "                                mGPT                                0.804355   \n",
              "                                mdeberta-v3-base                    0.739783   \n",
              "                                mdeberta-v3-base                    0.739783   \n",
              "                                roberta-large-openai-detector       0.569066   \n",
              "                                roberta-large-openai-detector       0.569066   \n",
              "                                xlm-roberta-large                   0.799051   \n",
              "                                xlm-roberta-large                   0.799051   \n",
              "               vicuna-13b       bert-base-multilingual-cased        0.874497   \n",
              "                                bert-base-multilingual-cased        0.874497   \n",
              "                                electra-large-discriminator         0.576884   \n",
              "                                electra-large-discriminator         0.576884   \n",
              "                                gpt2-medium                         0.452078   \n",
              "                                gpt2-medium                         0.452078   \n",
              "                                mGPT                                0.756777   \n",
              "                                mGPT                                0.756777   \n",
              "                                mdeberta-v3-base                    0.577340   \n",
              "                                mdeberta-v3-base                    0.577340   \n",
              "                                roberta-large-openai-detector       0.614995   \n",
              "                                roberta-large-openai-detector       0.614995   \n",
              "                                xlm-roberta-large                   0.780721   \n",
              "                                xlm-roberta-large                   0.780721   \n",
              "es             all              bert-base-multilingual-cased        0.774542   \n",
              "                                electra-large-discriminator         0.413019   \n",
              "                                gpt2-medium                         0.610274   \n",
              "                                mGPT                                0.734052   \n",
              "                                mdeberta-v3-base                    0.773139   \n",
              "                                roberta-large-openai-detector       0.679865   \n",
              "                                xlm-roberta-large                   0.827960   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased        0.890533   \n",
              "                                bert-base-multilingual-cased        0.890533   \n",
              "                                electra-large-discriminator         0.541388   \n",
              "                                electra-large-discriminator         0.541388   \n",
              "                                gpt2-medium                         0.597503   \n",
              "                                gpt2-medium                         0.597503   \n",
              "                                mGPT                                0.800509   \n",
              "                                mGPT                                0.800509   \n",
              "                                mdeberta-v3-base                    0.850513   \n",
              "                                mdeberta-v3-base                    0.850513   \n",
              "                                roberta-large-openai-detector       0.684684   \n",
              "                                roberta-large-openai-detector       0.684684   \n",
              "                                xlm-roberta-large                   0.813569   \n",
              "                                xlm-roberta-large                   0.813569   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased        0.901102   \n",
              "                                bert-base-multilingual-cased        0.901102   \n",
              "                                electra-large-discriminator         0.595763   \n",
              "                                electra-large-discriminator         0.595763   \n",
              "                                gpt2-medium                         0.578518   \n",
              "                                gpt2-medium                         0.578518   \n",
              "                                mGPT                                0.859639   \n",
              "                                mGPT                                0.859639   \n",
              "                                mdeberta-v3-base                    0.926604   \n",
              "                                mdeberta-v3-base                    0.926604   \n",
              "                                roberta-large-openai-detector       0.698075   \n",
              "                                roberta-large-openai-detector       0.698075   \n",
              "                                xlm-roberta-large                   0.893940   \n",
              "                                xlm-roberta-large                   0.893940   \n",
              "               gpt-4            bert-base-multilingual-cased        0.870233   \n",
              "                                bert-base-multilingual-cased        0.870233   \n",
              "                                electra-large-discriminator         0.553701   \n",
              "                                electra-large-discriminator         0.553701   \n",
              "                                gpt2-medium                         0.505868   \n",
              "                                gpt2-medium                         0.505868   \n",
              "                                mGPT                                0.842028   \n",
              "                                mGPT                                0.842028   \n",
              "                                mdeberta-v3-base                    0.909492   \n",
              "                                mdeberta-v3-base                    0.909492   \n",
              "                                roberta-large-openai-detector       0.689199   \n",
              "                                roberta-large-openai-detector       0.689199   \n",
              "                                xlm-roberta-large                   0.762675   \n",
              "                                xlm-roberta-large                   0.762675   \n",
              "               llama-65b        bert-base-multilingual-cased        0.381501   \n",
              "                                bert-base-multilingual-cased        0.381501   \n",
              "                                electra-large-discriminator         0.463229   \n",
              "                                electra-large-discriminator         0.463229   \n",
              "                                gpt2-medium                         0.453580   \n",
              "                                gpt2-medium                         0.453580   \n",
              "                                mGPT                                0.544659   \n",
              "                                mGPT                                0.544659   \n",
              "                                mdeberta-v3-base                    0.422407   \n",
              "                                mdeberta-v3-base                    0.422407   \n",
              "                                roberta-large-openai-detector       0.633109   \n",
              "                                roberta-large-openai-detector       0.633109   \n",
              "                                xlm-roberta-large                   0.428384   \n",
              "                                xlm-roberta-large                   0.428384   \n",
              "               opt-66b          bert-base-multilingual-cased        0.542004   \n",
              "                                bert-base-multilingual-cased        0.542004   \n",
              "                                electra-large-discriminator         0.461158   \n",
              "                                electra-large-discriminator         0.461158   \n",
              "                                gpt2-medium                         0.503340   \n",
              "                                gpt2-medium                         0.503340   \n",
              "                                mGPT                                0.575161   \n",
              "                                mGPT                                0.575161   \n",
              "                                mdeberta-v3-base                    0.401939   \n",
              "                                mdeberta-v3-base                    0.401939   \n",
              "                                roberta-large-openai-detector       0.647780   \n",
              "                                roberta-large-openai-detector       0.647780   \n",
              "                                xlm-roberta-large                   0.478076   \n",
              "                                xlm-roberta-large                   0.478076   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased        0.359474   \n",
              "                                bert-base-multilingual-cased        0.359474   \n",
              "                                electra-large-discriminator         0.525103   \n",
              "                                electra-large-discriminator         0.525103   \n",
              "                                gpt2-medium                         0.466809   \n",
              "                                gpt2-medium                         0.466809   \n",
              "                                mGPT                                0.406000   \n",
              "                                mGPT                                0.406000   \n",
              "                                mdeberta-v3-base                    0.345571   \n",
              "                                mdeberta-v3-base                    0.345571   \n",
              "                                roberta-large-openai-detector       0.623194   \n",
              "                                roberta-large-openai-detector       0.623194   \n",
              "                                xlm-roberta-large                   0.331025   \n",
              "                                xlm-roberta-large                   0.331025   \n",
              "               text-davinci-003 bert-base-multilingual-cased        0.870012   \n",
              "                                bert-base-multilingual-cased        0.870012   \n",
              "                                electra-large-discriminator         0.580711   \n",
              "                                electra-large-discriminator         0.580711   \n",
              "                                gpt2-medium                         0.658208   \n",
              "                                gpt2-medium                         0.658208   \n",
              "                                mGPT                                0.860416   \n",
              "                                mGPT                                0.860416   \n",
              "                                mdeberta-v3-base                    0.889804   \n",
              "                                mdeberta-v3-base                    0.889804   \n",
              "                                roberta-large-openai-detector       0.692269   \n",
              "                                roberta-large-openai-detector       0.692269   \n",
              "                                xlm-roberta-large                   0.934541   \n",
              "                                xlm-roberta-large                   0.934541   \n",
              "               vicuna-13b       bert-base-multilingual-cased        0.895358   \n",
              "                                bert-base-multilingual-cased        0.895358   \n",
              "                                electra-large-discriminator         0.595000   \n",
              "                                electra-large-discriminator         0.595000   \n",
              "                                gpt2-medium                         0.688590   \n",
              "                                gpt2-medium                         0.688590   \n",
              "                                mGPT                                0.796709   \n",
              "                                mGPT                                0.796709   \n",
              "                                mdeberta-v3-base                    0.900852   \n",
              "                                mdeberta-v3-base                    0.900852   \n",
              "                                roberta-large-openai-detector       0.730351   \n",
              "                                roberta-large-openai-detector       0.730351   \n",
              "                                xlm-roberta-large                   0.792443   \n",
              "                                xlm-roberta-large                   0.792443   \n",
              "ru             all              bert-base-multilingual-cased        0.595614   \n",
              "                                electra-large-discriminator         0.334597   \n",
              "                                gpt2-medium                         0.562153   \n",
              "                                mGPT                                0.680912   \n",
              "                                mdeberta-v3-base                    0.673815   \n",
              "                                roberta-large-openai-detector       0.446701   \n",
              "                                xlm-roberta-large                   0.779663   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased        0.742415   \n",
              "                                bert-base-multilingual-cased        0.742415   \n",
              "                                electra-large-discriminator         0.473657   \n",
              "                                electra-large-discriminator         0.473657   \n",
              "                                gpt2-medium                         0.497830   \n",
              "                                gpt2-medium                         0.497830   \n",
              "                                mGPT                                0.708923   \n",
              "                                mGPT                                0.708923   \n",
              "                                mdeberta-v3-base                    0.836616   \n",
              "                                mdeberta-v3-base                    0.836616   \n",
              "                                roberta-large-openai-detector       0.523997   \n",
              "                                roberta-large-openai-detector       0.523997   \n",
              "                                xlm-roberta-large                   0.850243   \n",
              "                                xlm-roberta-large                   0.850243   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased        0.883391   \n",
              "                                bert-base-multilingual-cased        0.883391   \n",
              "                                electra-large-discriminator         0.488864   \n",
              "                                electra-large-discriminator         0.488864   \n",
              "                                gpt2-medium                         0.487876   \n",
              "                                gpt2-medium                         0.487876   \n",
              "                                mGPT                                0.838748   \n",
              "                                mGPT                                0.838748   \n",
              "                                mdeberta-v3-base                    0.918469   \n",
              "                                mdeberta-v3-base                    0.918469   \n",
              "                                roberta-large-openai-detector       0.525704   \n",
              "                                roberta-large-openai-detector       0.525704   \n",
              "                                xlm-roberta-large                   0.947180   \n",
              "                                xlm-roberta-large                   0.947180   \n",
              "               gpt-4            bert-base-multilingual-cased        0.767134   \n",
              "                                bert-base-multilingual-cased        0.767134   \n",
              "                                electra-large-discriminator         0.478184   \n",
              "                                electra-large-discriminator         0.478184   \n",
              "                                gpt2-medium                         0.493490   \n",
              "                                gpt2-medium                         0.493490   \n",
              "                                mGPT                                0.845817   \n",
              "                                mGPT                                0.845817   \n",
              "                                mdeberta-v3-base                    0.812637   \n",
              "                                mdeberta-v3-base                    0.812637   \n",
              "                                roberta-large-openai-detector       0.498346   \n",
              "                                roberta-large-openai-detector       0.498346   \n",
              "                                xlm-roberta-large                   0.937806   \n",
              "                                xlm-roberta-large                   0.937806   \n",
              "               llama-65b        bert-base-multilingual-cased        0.496416   \n",
              "                                bert-base-multilingual-cased        0.496416   \n",
              "                                electra-large-discriminator         0.457380   \n",
              "                                electra-large-discriminator         0.457380   \n",
              "                                gpt2-medium                         0.418240   \n",
              "                                gpt2-medium                         0.418240   \n",
              "                                mGPT                                0.483550   \n",
              "                                mGPT                                0.483550   \n",
              "                                mdeberta-v3-base                    0.423566   \n",
              "                                mdeberta-v3-base                    0.423566   \n",
              "                                roberta-large-openai-detector       0.432759   \n",
              "                                roberta-large-openai-detector       0.432759   \n",
              "                                xlm-roberta-large                   0.447753   \n",
              "                                xlm-roberta-large                   0.447753   \n",
              "               opt-66b          bert-base-multilingual-cased        0.334250   \n",
              "                                bert-base-multilingual-cased        0.334250   \n",
              "                                electra-large-discriminator         0.420189   \n",
              "                                electra-large-discriminator         0.420189   \n",
              "                                gpt2-medium                         0.422160   \n",
              "                                gpt2-medium                         0.422160   \n",
              "                                mGPT                                0.360391   \n",
              "                                mGPT                                0.360391   \n",
              "                                mdeberta-v3-base                    0.339834   \n",
              "                                mdeberta-v3-base                    0.339834   \n",
              "                                roberta-large-openai-detector       0.420931   \n",
              "                                roberta-large-openai-detector       0.420931   \n",
              "                                xlm-roberta-large                   0.330367   \n",
              "                                xlm-roberta-large                   0.330367   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased        0.332053   \n",
              "                                bert-base-multilingual-cased        0.332053   \n",
              "                                electra-large-discriminator         0.435283   \n",
              "                                electra-large-discriminator         0.435283   \n",
              "                                gpt2-medium                         0.392233   \n",
              "                                gpt2-medium                         0.392233   \n",
              "                                mGPT                                0.325670   \n",
              "                                mGPT                                0.325670   \n",
              "                                mdeberta-v3-base                    0.340148   \n",
              "                                mdeberta-v3-base                    0.340148   \n",
              "                                roberta-large-openai-detector       0.483521   \n",
              "                                roberta-large-openai-detector       0.483521   \n",
              "                                xlm-roberta-large                   0.332259   \n",
              "                                xlm-roberta-large                   0.332259   \n",
              "               text-davinci-003 bert-base-multilingual-cased        0.794014   \n",
              "                                bert-base-multilingual-cased        0.794014   \n",
              "                                electra-large-discriminator         0.388732   \n",
              "                                electra-large-discriminator         0.388732   \n",
              "                                gpt2-medium                         0.387423   \n",
              "                                gpt2-medium                         0.387423   \n",
              "                                mGPT                                0.732203   \n",
              "                                mGPT                                0.732203   \n",
              "                                mdeberta-v3-base                    0.505115   \n",
              "                                mdeberta-v3-base                    0.505115   \n",
              "                                roberta-large-openai-detector       0.484581   \n",
              "                                roberta-large-openai-detector       0.484581   \n",
              "                                xlm-roberta-large                   0.592280   \n",
              "                                xlm-roberta-large                   0.592280   \n",
              "               vicuna-13b       bert-base-multilingual-cased        0.544359   \n",
              "                                bert-base-multilingual-cased        0.544359   \n",
              "                                electra-large-discriminator         0.459935   \n",
              "                                electra-large-discriminator         0.459935   \n",
              "                                gpt2-medium                         0.496502   \n",
              "                                gpt2-medium                         0.496502   \n",
              "                                mGPT                                0.676031   \n",
              "                                mGPT                                0.676031   \n",
              "                                mdeberta-v3-base                    0.800096   \n",
              "                                mdeberta-v3-base                    0.800096   \n",
              "                                roberta-large-openai-detector       0.513713   \n",
              "                                roberta-large-openai-detector       0.513713   \n",
              "                                xlm-roberta-large                   0.831837   \n",
              "                                xlm-roberta-large                   0.831837   \n",
              "all            all              bert-base-multilingual-cased        0.743030   \n",
              "                                electra-large-discriminator         0.417417   \n",
              "                                gpt2-medium                         0.662633   \n",
              "                                mGPT                                0.731644   \n",
              "                                mdeberta-v3-base                    0.844191   \n",
              "                                roberta-large-openai-detector       0.740233   \n",
              "                                xlm-roberta-large                   0.780568   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased        0.928132   \n",
              "                                bert-base-multilingual-cased        0.928132   \n",
              "                                electra-large-discriminator         0.627844   \n",
              "                                electra-large-discriminator         0.627844   \n",
              "                                gpt2-medium                         0.672544   \n",
              "                                gpt2-medium                         0.672544   \n",
              "                                mGPT                                0.836006   \n",
              "                                mGPT                                0.836006   \n",
              "                                mdeberta-v3-base                    0.868372   \n",
              "                                mdeberta-v3-base                    0.868372   \n",
              "                                roberta-large-openai-detector       0.825424   \n",
              "                                roberta-large-openai-detector       0.825424   \n",
              "                                xlm-roberta-large                   0.914097   \n",
              "                                xlm-roberta-large                   0.914097   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased        0.947168   \n",
              "                                bert-base-multilingual-cased        0.947168   \n",
              "                                electra-large-discriminator         0.602245   \n",
              "                                electra-large-discriminator         0.602245   \n",
              "                                gpt2-medium                         0.765369   \n",
              "                                gpt2-medium                         0.765369   \n",
              "                                mGPT                                0.856212   \n",
              "                                mGPT                                0.856212   \n",
              "                                mdeberta-v3-base                    0.924743   \n",
              "                                mdeberta-v3-base                    0.924743   \n",
              "                                roberta-large-openai-detector       0.737495   \n",
              "                                roberta-large-openai-detector       0.737495   \n",
              "                                xlm-roberta-large                   0.967289   \n",
              "                                xlm-roberta-large                   0.967289   \n",
              "               gpt-4            bert-base-multilingual-cased        0.927683   \n",
              "                                bert-base-multilingual-cased        0.927683   \n",
              "                                electra-large-discriminator         0.645391   \n",
              "                                electra-large-discriminator         0.645391   \n",
              "                                gpt2-medium                         0.722624   \n",
              "                                gpt2-medium                         0.722624   \n",
              "                                mGPT                                0.871056   \n",
              "                                mGPT                                0.871056   \n",
              "                                mdeberta-v3-base                    0.929526   \n",
              "                                mdeberta-v3-base                    0.929526   \n",
              "                                roberta-large-openai-detector       0.811925   \n",
              "                                roberta-large-openai-detector       0.811925   \n",
              "                                xlm-roberta-large                   0.953157   \n",
              "                                xlm-roberta-large                   0.953157   \n",
              "               llama-65b        bert-base-multilingual-cased        0.397352   \n",
              "                                bert-base-multilingual-cased        0.397352   \n",
              "                                electra-large-discriminator         0.376820   \n",
              "                                electra-large-discriminator         0.376820   \n",
              "                                gpt2-medium                         0.427964   \n",
              "                                gpt2-medium                         0.427964   \n",
              "                                mGPT                                0.482048   \n",
              "                                mGPT                                0.482048   \n",
              "                                mdeberta-v3-base                    0.454116   \n",
              "                                mdeberta-v3-base                    0.454116   \n",
              "                                roberta-large-openai-detector       0.565367   \n",
              "                                roberta-large-openai-detector       0.565367   \n",
              "                                xlm-roberta-large                   0.413110   \n",
              "                                xlm-roberta-large                   0.413110   \n",
              "               opt-66b          bert-base-multilingual-cased        0.494671   \n",
              "                                bert-base-multilingual-cased        0.494671   \n",
              "                                electra-large-discriminator         0.476240   \n",
              "                                electra-large-discriminator         0.476240   \n",
              "                                gpt2-medium                         0.506001   \n",
              "                                gpt2-medium                         0.506001   \n",
              "                                mGPT                                0.529334   \n",
              "                                mGPT                                0.529334   \n",
              "                                mdeberta-v3-base                    0.441337   \n",
              "                                mdeberta-v3-base                    0.441337   \n",
              "                                roberta-large-openai-detector       0.553044   \n",
              "                                roberta-large-openai-detector       0.553044   \n",
              "                                xlm-roberta-large                   0.378145   \n",
              "                                xlm-roberta-large                   0.378145   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased        0.521568   \n",
              "                                bert-base-multilingual-cased        0.521568   \n",
              "                                electra-large-discriminator         0.400034   \n",
              "                                electra-large-discriminator         0.400034   \n",
              "                                gpt2-medium                         0.500207   \n",
              "                                gpt2-medium                         0.500207   \n",
              "                                mGPT                                0.469781   \n",
              "                                mGPT                                0.469781   \n",
              "                                mdeberta-v3-base                    0.464366   \n",
              "                                mdeberta-v3-base                    0.464366   \n",
              "                                roberta-large-openai-detector       0.568078   \n",
              "                                roberta-large-openai-detector       0.568078   \n",
              "                                xlm-roberta-large                   0.364368   \n",
              "                                xlm-roberta-large                   0.364368   \n",
              "               text-davinci-003 bert-base-multilingual-cased        0.942565   \n",
              "                                bert-base-multilingual-cased        0.942565   \n",
              "                                electra-large-discriminator         0.634362   \n",
              "                                electra-large-discriminator         0.634362   \n",
              "                                gpt2-medium                         0.694682   \n",
              "                                gpt2-medium                         0.694682   \n",
              "                                mGPT                                0.880861   \n",
              "                                mGPT                                0.880861   \n",
              "                                mdeberta-v3-base                    0.885835   \n",
              "                                mdeberta-v3-base                    0.885835   \n",
              "                                roberta-large-openai-detector       0.762168   \n",
              "                                roberta-large-openai-detector       0.762168   \n",
              "                                xlm-roberta-large                   0.866400   \n",
              "                                xlm-roberta-large                   0.866400   \n",
              "               vicuna-13b       bert-base-multilingual-cased        0.922273   \n",
              "                                bert-base-multilingual-cased        0.922273   \n",
              "                                electra-large-discriminator         0.638156   \n",
              "                                electra-large-discriminator         0.638156   \n",
              "                                gpt2-medium                         0.711251   \n",
              "                                gpt2-medium                         0.711251   \n",
              "                                mGPT                                0.865952   \n",
              "                                mGPT                                0.865952   \n",
              "                                mdeberta-v3-base                    0.883311   \n",
              "                                mdeberta-v3-base                    0.883311   \n",
              "                                roberta-large-openai-detector       0.862368   \n",
              "                                roberta-large-openai-detector       0.862368   \n",
              "                                xlm-roberta-large                   0.933794   \n",
              "                                xlm-roberta-large                   0.933794   \n",
              "en3            all              bert-base-multilingual-cased        0.396643   \n",
              "                                electra-large-discriminator         0.413729   \n",
              "                                gpt2-medium                         0.423047   \n",
              "                                mGPT                                0.564114   \n",
              "                                mdeberta-v3-base                    0.455917   \n",
              "                                roberta-large-openai-detector       0.504309   \n",
              "                                xlm-roberta-large                   0.509294   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased        0.854275   \n",
              "                                bert-base-multilingual-cased        0.854275   \n",
              "                                electra-large-discriminator         0.547081   \n",
              "                                electra-large-discriminator         0.547081   \n",
              "                                gpt2-medium                         0.429913   \n",
              "                                gpt2-medium                         0.429913   \n",
              "                                mGPT                                0.774562   \n",
              "                                mGPT                                0.774562   \n",
              "                                mdeberta-v3-base                    0.724337   \n",
              "                                mdeberta-v3-base                    0.724337   \n",
              "                                roberta-large-openai-detector       0.508672   \n",
              "                                roberta-large-openai-detector       0.508672   \n",
              "                                xlm-roberta-large                   0.848953   \n",
              "                                xlm-roberta-large                   0.848953   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased        0.924718   \n",
              "                                bert-base-multilingual-cased        0.924718   \n",
              "                                electra-large-discriminator         0.500790   \n",
              "                                electra-large-discriminator         0.500790   \n",
              "                                gpt2-medium                         0.428445   \n",
              "                                gpt2-medium                         0.428445   \n",
              "                                mGPT                                0.830221   \n",
              "                                mGPT                                0.830221   \n",
              "                                mdeberta-v3-base                    0.786177   \n",
              "                                mdeberta-v3-base                    0.786177   \n",
              "                                roberta-large-openai-detector       0.768797   \n",
              "                                roberta-large-openai-detector       0.768797   \n",
              "                                xlm-roberta-large                   0.936736   \n",
              "                                xlm-roberta-large                   0.936736   \n",
              "               gpt-4            bert-base-multilingual-cased        0.839875   \n",
              "                                bert-base-multilingual-cased        0.839875   \n",
              "                                electra-large-discriminator         0.544298   \n",
              "                                electra-large-discriminator         0.544298   \n",
              "                                gpt2-medium                         0.427382   \n",
              "                                gpt2-medium                         0.427382   \n",
              "                                mGPT                                0.797218   \n",
              "                                mGPT                                0.797218   \n",
              "                                mdeberta-v3-base                    0.793915   \n",
              "                                mdeberta-v3-base                    0.793915   \n",
              "                                roberta-large-openai-detector       0.647093   \n",
              "                                roberta-large-openai-detector       0.647093   \n",
              "                                xlm-roberta-large                   0.887759   \n",
              "                                xlm-roberta-large                   0.887759   \n",
              "               llama-65b        bert-base-multilingual-cased        0.367020   \n",
              "                                bert-base-multilingual-cased        0.367020   \n",
              "                                electra-large-discriminator         0.419384   \n",
              "                                electra-large-discriminator         0.419384   \n",
              "                                gpt2-medium                         0.416172   \n",
              "                                gpt2-medium                         0.416172   \n",
              "                                mGPT                                0.517200   \n",
              "                                mGPT                                0.517200   \n",
              "                                mdeberta-v3-base                    0.428285   \n",
              "                                mdeberta-v3-base                    0.428285   \n",
              "                                roberta-large-openai-detector       0.427935   \n",
              "                                roberta-large-openai-detector       0.427935   \n",
              "                                xlm-roberta-large                   0.512116   \n",
              "                                xlm-roberta-large                   0.512116   \n",
              "               opt-66b          bert-base-multilingual-cased        0.554463   \n",
              "                                bert-base-multilingual-cased        0.554463   \n",
              "                                electra-large-discriminator         0.532817   \n",
              "                                electra-large-discriminator         0.532817   \n",
              "                                gpt2-medium                         0.469929   \n",
              "                                gpt2-medium                         0.469929   \n",
              "                                mGPT                                0.634054   \n",
              "                                mGPT                                0.634054   \n",
              "                                mdeberta-v3-base                    0.610537   \n",
              "                                mdeberta-v3-base                    0.610537   \n",
              "                                roberta-large-openai-detector       0.520497   \n",
              "                                roberta-large-openai-detector       0.520497   \n",
              "                                xlm-roberta-large                   0.513684   \n",
              "                                xlm-roberta-large                   0.513684   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased        0.603939   \n",
              "                                bert-base-multilingual-cased        0.603939   \n",
              "                                electra-large-discriminator         0.461284   \n",
              "                                electra-large-discriminator         0.461284   \n",
              "                                gpt2-medium                         0.513391   \n",
              "                                gpt2-medium                         0.513391   \n",
              "                                mGPT                                0.623164   \n",
              "                                mGPT                                0.623164   \n",
              "                                mdeberta-v3-base                    0.643656   \n",
              "                                mdeberta-v3-base                    0.643656   \n",
              "                                roberta-large-openai-detector       0.657189   \n",
              "                                roberta-large-openai-detector       0.657189   \n",
              "                                xlm-roberta-large                   0.568583   \n",
              "                                xlm-roberta-large                   0.568583   \n",
              "               text-davinci-003 bert-base-multilingual-cased        0.877743   \n",
              "                                bert-base-multilingual-cased        0.877743   \n",
              "                                electra-large-discriminator         0.431995   \n",
              "                                electra-large-discriminator         0.431995   \n",
              "                                gpt2-medium                         0.426725   \n",
              "                                gpt2-medium                         0.426725   \n",
              "                                mGPT                                0.841265   \n",
              "                                mGPT                                0.841265   \n",
              "                                mdeberta-v3-base                    0.789113   \n",
              "                                mdeberta-v3-base                    0.789113   \n",
              "                                roberta-large-openai-detector       0.753398   \n",
              "                                roberta-large-openai-detector       0.753398   \n",
              "                                xlm-roberta-large                   0.806275   \n",
              "                                xlm-roberta-large                   0.806275   \n",
              "               vicuna-13b       bert-base-multilingual-cased        0.861890   \n",
              "                                bert-base-multilingual-cased        0.861890   \n",
              "                                electra-large-discriminator         0.608145   \n",
              "                                electra-large-discriminator         0.608145   \n",
              "                                gpt2-medium                         0.502959   \n",
              "                                gpt2-medium                         0.502959   \n",
              "                                mGPT                                0.769260   \n",
              "                                mGPT                                0.769260   \n",
              "                                mdeberta-v3-base                    0.745103   \n",
              "                                mdeberta-v3-base                    0.745103   \n",
              "                                roberta-large-openai-detector       0.498064   \n",
              "                                roberta-large-openai-detector       0.498064   \n",
              "                                xlm-roberta-large                   0.867704   \n",
              "                                xlm-roberta-large                   0.867704   \n",
              "\n",
              "                                                               text-davinci-003  \\\n",
              "Train Language Train LLM        Model                                             \n",
              "en             all              bert-base-multilingual-cased           0.605591   \n",
              "                                electra-large-discriminator            0.499486   \n",
              "                                gpt2-medium                            0.485969   \n",
              "                                mGPT                                   0.515067   \n",
              "                                mdeberta-v3-base                       0.566313   \n",
              "                                roberta-large-openai-detector          0.427240   \n",
              "                                xlm-roberta-large                      0.447934   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased           0.810178   \n",
              "                                bert-base-multilingual-cased           0.810178   \n",
              "                                electra-large-discriminator            0.535580   \n",
              "                                electra-large-discriminator            0.535580   \n",
              "                                gpt2-medium                            0.526691   \n",
              "                                gpt2-medium                            0.526691   \n",
              "                                mGPT                                   0.765636   \n",
              "                                mGPT                                   0.765636   \n",
              "                                mdeberta-v3-base                       0.779460   \n",
              "                                mdeberta-v3-base                       0.779460   \n",
              "                                roberta-large-openai-detector          0.566308   \n",
              "                                roberta-large-openai-detector          0.566308   \n",
              "                                xlm-roberta-large                      0.794029   \n",
              "                                xlm-roberta-large                      0.794029   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased           0.851200   \n",
              "                                bert-base-multilingual-cased           0.851200   \n",
              "                                electra-large-discriminator            0.593694   \n",
              "                                electra-large-discriminator            0.593694   \n",
              "                                gpt2-medium                            0.414659   \n",
              "                                gpt2-medium                            0.414659   \n",
              "                                mGPT                                   0.757231   \n",
              "                                mGPT                                   0.757231   \n",
              "                                mdeberta-v3-base                       0.696568   \n",
              "                                mdeberta-v3-base                       0.696568   \n",
              "                                roberta-large-openai-detector          0.616739   \n",
              "                                roberta-large-openai-detector          0.616739   \n",
              "                                xlm-roberta-large                      0.719763   \n",
              "                                xlm-roberta-large                      0.719763   \n",
              "               gpt-4            bert-base-multilingual-cased           0.708960   \n",
              "                                bert-base-multilingual-cased           0.708960   \n",
              "                                electra-large-discriminator            0.497165   \n",
              "                                electra-large-discriminator            0.497165   \n",
              "                                gpt2-medium                            0.403710   \n",
              "                                gpt2-medium                            0.403710   \n",
              "                                mGPT                                   0.705776   \n",
              "                                mGPT                                   0.705776   \n",
              "                                mdeberta-v3-base                       0.620146   \n",
              "                                mdeberta-v3-base                       0.620146   \n",
              "                                roberta-large-openai-detector          0.606976   \n",
              "                                roberta-large-openai-detector          0.606976   \n",
              "                                xlm-roberta-large                      0.652566   \n",
              "                                xlm-roberta-large                      0.652566   \n",
              "               llama-65b        bert-base-multilingual-cased           0.296864   \n",
              "                                bert-base-multilingual-cased           0.296864   \n",
              "                                electra-large-discriminator            0.509602   \n",
              "                                electra-large-discriminator            0.509602   \n",
              "                                gpt2-medium                            0.453134   \n",
              "                                gpt2-medium                            0.453134   \n",
              "                                mGPT                                   0.484928   \n",
              "                                mGPT                                   0.484928   \n",
              "                                mdeberta-v3-base                       0.543878   \n",
              "                                mdeberta-v3-base                       0.543878   \n",
              "                                roberta-large-openai-detector          0.415731   \n",
              "                                roberta-large-openai-detector          0.415731   \n",
              "                                xlm-roberta-large                      0.481723   \n",
              "                                xlm-roberta-large                      0.481723   \n",
              "               opt-66b          bert-base-multilingual-cased           0.414439   \n",
              "                                bert-base-multilingual-cased           0.414439   \n",
              "                                electra-large-discriminator            0.473351   \n",
              "                                electra-large-discriminator            0.473351   \n",
              "                                gpt2-medium                            0.520984   \n",
              "                                gpt2-medium                            0.520984   \n",
              "                                mGPT                                   0.583475   \n",
              "                                mGPT                                   0.583475   \n",
              "                                mdeberta-v3-base                       0.458583   \n",
              "                                mdeberta-v3-base                       0.458583   \n",
              "                                roberta-large-openai-detector          0.478462   \n",
              "                                roberta-large-openai-detector          0.478462   \n",
              "                                xlm-roberta-large                      0.518040   \n",
              "                                xlm-roberta-large                      0.518040   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased           0.510768   \n",
              "                                bert-base-multilingual-cased           0.510768   \n",
              "                                electra-large-discriminator            0.474519   \n",
              "                                electra-large-discriminator            0.474519   \n",
              "                                gpt2-medium                            0.522037   \n",
              "                                gpt2-medium                            0.522037   \n",
              "                                mGPT                                   0.613530   \n",
              "                                mGPT                                   0.613530   \n",
              "                                mdeberta-v3-base                       0.597235   \n",
              "                                mdeberta-v3-base                       0.597235   \n",
              "                                roberta-large-openai-detector          0.654303   \n",
              "                                roberta-large-openai-detector          0.654303   \n",
              "                                xlm-roberta-large                      0.557576   \n",
              "                                xlm-roberta-large                      0.557576   \n",
              "               text-davinci-003 bert-base-multilingual-cased           0.828877   \n",
              "                                bert-base-multilingual-cased           0.828877   \n",
              "                                electra-large-discriminator            0.587801   \n",
              "                                electra-large-discriminator            0.587801   \n",
              "                                gpt2-medium                            0.417170   \n",
              "                                gpt2-medium                            0.417170   \n",
              "                                mGPT                                   0.763631   \n",
              "                                mGPT                                   0.763631   \n",
              "                                mdeberta-v3-base                       0.719272   \n",
              "                                mdeberta-v3-base                       0.719272   \n",
              "                                roberta-large-openai-detector          0.572172   \n",
              "                                roberta-large-openai-detector          0.572172   \n",
              "                                xlm-roberta-large                      0.742989   \n",
              "                                xlm-roberta-large                      0.742989   \n",
              "               vicuna-13b       bert-base-multilingual-cased           0.821160   \n",
              "                                bert-base-multilingual-cased           0.821160   \n",
              "                                electra-large-discriminator            0.565475   \n",
              "                                electra-large-discriminator            0.565475   \n",
              "                                gpt2-medium                            0.555839   \n",
              "                                gpt2-medium                            0.555839   \n",
              "                                mGPT                                   0.724204   \n",
              "                                mGPT                                   0.724204   \n",
              "                                mdeberta-v3-base                       0.553817   \n",
              "                                mdeberta-v3-base                       0.553817   \n",
              "                                roberta-large-openai-detector          0.617004   \n",
              "                                roberta-large-openai-detector          0.617004   \n",
              "                                xlm-roberta-large                      0.703831   \n",
              "                                xlm-roberta-large                      0.703831   \n",
              "es             all              bert-base-multilingual-cased           0.762005   \n",
              "                                electra-large-discriminator            0.435535   \n",
              "                                gpt2-medium                            0.629649   \n",
              "                                mGPT                                   0.719308   \n",
              "                                mdeberta-v3-base                       0.772076   \n",
              "                                roberta-large-openai-detector          0.668878   \n",
              "                                xlm-roberta-large                      0.824201   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased           0.858084   \n",
              "                                bert-base-multilingual-cased           0.858084   \n",
              "                                electra-large-discriminator            0.547345   \n",
              "                                electra-large-discriminator            0.547345   \n",
              "                                gpt2-medium                            0.663043   \n",
              "                                gpt2-medium                            0.663043   \n",
              "                                mGPT                                   0.774817   \n",
              "                                mGPT                                   0.774817   \n",
              "                                mdeberta-v3-base                       0.863198   \n",
              "                                mdeberta-v3-base                       0.863198   \n",
              "                                roberta-large-openai-detector          0.666668   \n",
              "                                roberta-large-openai-detector          0.666668   \n",
              "                                xlm-roberta-large                      0.816889   \n",
              "                                xlm-roberta-large                      0.816889   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased           0.836356   \n",
              "                                bert-base-multilingual-cased           0.836356   \n",
              "                                electra-large-discriminator            0.545695   \n",
              "                                electra-large-discriminator            0.545695   \n",
              "                                gpt2-medium                            0.560992   \n",
              "                                gpt2-medium                            0.560992   \n",
              "                                mGPT                                   0.797670   \n",
              "                                mGPT                                   0.797670   \n",
              "                                mdeberta-v3-base                       0.845145   \n",
              "                                mdeberta-v3-base                       0.845145   \n",
              "                                roberta-large-openai-detector          0.666956   \n",
              "                                roberta-large-openai-detector          0.666956   \n",
              "                                xlm-roberta-large                      0.812715   \n",
              "                                xlm-roberta-large                      0.812715   \n",
              "               gpt-4            bert-base-multilingual-cased           0.823954   \n",
              "                                bert-base-multilingual-cased           0.823954   \n",
              "                                electra-large-discriminator            0.545626   \n",
              "                                electra-large-discriminator            0.545626   \n",
              "                                gpt2-medium                            0.410940   \n",
              "                                gpt2-medium                            0.410940   \n",
              "                                mGPT                                   0.742801   \n",
              "                                mGPT                                   0.742801   \n",
              "                                mdeberta-v3-base                       0.797744   \n",
              "                                mdeberta-v3-base                       0.797744   \n",
              "                                roberta-large-openai-detector          0.642154   \n",
              "                                roberta-large-openai-detector          0.642154   \n",
              "                                xlm-roberta-large                      0.678276   \n",
              "                                xlm-roberta-large                      0.678276   \n",
              "               llama-65b        bert-base-multilingual-cased           0.345809   \n",
              "                                bert-base-multilingual-cased           0.345809   \n",
              "                                electra-large-discriminator            0.484928   \n",
              "                                electra-large-discriminator            0.484928   \n",
              "                                gpt2-medium                            0.483164   \n",
              "                                gpt2-medium                            0.483164   \n",
              "                                mGPT                                   0.526209   \n",
              "                                mGPT                                   0.526209   \n",
              "                                mdeberta-v3-base                       0.474644   \n",
              "                                mdeberta-v3-base                       0.474644   \n",
              "                                roberta-large-openai-detector          0.611076   \n",
              "                                roberta-large-openai-detector          0.611076   \n",
              "                                xlm-roberta-large                      0.425906   \n",
              "                                xlm-roberta-large                      0.425906   \n",
              "               opt-66b          bert-base-multilingual-cased           0.518158   \n",
              "                                bert-base-multilingual-cased           0.518158   \n",
              "                                electra-large-discriminator            0.460408   \n",
              "                                electra-large-discriminator            0.460408   \n",
              "                                gpt2-medium                            0.574086   \n",
              "                                gpt2-medium                            0.574086   \n",
              "                                mGPT                                   0.568385   \n",
              "                                mGPT                                   0.568385   \n",
              "                                mdeberta-v3-base                       0.607262   \n",
              "                                mdeberta-v3-base                       0.607262   \n",
              "                                roberta-large-openai-detector          0.650938   \n",
              "                                roberta-large-openai-detector          0.650938   \n",
              "                                xlm-roberta-large                      0.588880   \n",
              "                                xlm-roberta-large                      0.588880   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased           0.350286   \n",
              "                                bert-base-multilingual-cased           0.350286   \n",
              "                                electra-large-discriminator            0.528393   \n",
              "                                electra-large-discriminator            0.528393   \n",
              "                                gpt2-medium                            0.521470   \n",
              "                                gpt2-medium                            0.521470   \n",
              "                                mGPT                                   0.480591   \n",
              "                                mGPT                                   0.480591   \n",
              "                                mdeberta-v3-base                       0.469239   \n",
              "                                mdeberta-v3-base                       0.469239   \n",
              "                                roberta-large-openai-detector          0.630301   \n",
              "                                roberta-large-openai-detector          0.630301   \n",
              "                                xlm-roberta-large                      0.343768   \n",
              "                                xlm-roberta-large                      0.343768   \n",
              "               text-davinci-003 bert-base-multilingual-cased           0.839783   \n",
              "                                bert-base-multilingual-cased           0.839783   \n",
              "                                electra-large-discriminator            0.590808   \n",
              "                                electra-large-discriminator            0.590808   \n",
              "                                gpt2-medium                            0.727539   \n",
              "                                gpt2-medium                            0.727539   \n",
              "                                mGPT                                   0.822961   \n",
              "                                mGPT                                   0.822961   \n",
              "                                mdeberta-v3-base                       0.845859   \n",
              "                                mdeberta-v3-base                       0.845859   \n",
              "                                roberta-large-openai-detector          0.664114   \n",
              "                                roberta-large-openai-detector          0.664114   \n",
              "                                xlm-roberta-large                      0.911300   \n",
              "                                xlm-roberta-large                      0.911300   \n",
              "               vicuna-13b       bert-base-multilingual-cased           0.852523   \n",
              "                                bert-base-multilingual-cased           0.852523   \n",
              "                                electra-large-discriminator            0.541493   \n",
              "                                electra-large-discriminator            0.541493   \n",
              "                                gpt2-medium                            0.708367   \n",
              "                                gpt2-medium                            0.708367   \n",
              "                                mGPT                                   0.782388   \n",
              "                                mGPT                                   0.782388   \n",
              "                                mdeberta-v3-base                       0.851754   \n",
              "                                mdeberta-v3-base                       0.851754   \n",
              "                                roberta-large-openai-detector          0.696573   \n",
              "                                roberta-large-openai-detector          0.696573   \n",
              "                                xlm-roberta-large                      0.774209   \n",
              "                                xlm-roberta-large                      0.774209   \n",
              "ru             all              bert-base-multilingual-cased           0.591678   \n",
              "                                electra-large-discriminator            0.334734   \n",
              "                                gpt2-medium                            0.537755   \n",
              "                                mGPT                                   0.678807   \n",
              "                                mdeberta-v3-base                       0.675664   \n",
              "                                roberta-large-openai-detector          0.445661   \n",
              "                                xlm-roberta-large                      0.776265   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased           0.727944   \n",
              "                                bert-base-multilingual-cased           0.727944   \n",
              "                                electra-large-discriminator            0.474564   \n",
              "                                electra-large-discriminator            0.474564   \n",
              "                                gpt2-medium                            0.415352   \n",
              "                                gpt2-medium                            0.415352   \n",
              "                                mGPT                                   0.688471   \n",
              "                                mGPT                                   0.688471   \n",
              "                                mdeberta-v3-base                       0.870000   \n",
              "                                mdeberta-v3-base                       0.870000   \n",
              "                                roberta-large-openai-detector          0.513443   \n",
              "                                roberta-large-openai-detector          0.513443   \n",
              "                                xlm-roberta-large                      0.873881   \n",
              "                                xlm-roberta-large                      0.873881   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased           0.835752   \n",
              "                                bert-base-multilingual-cased           0.835752   \n",
              "                                electra-large-discriminator            0.434711   \n",
              "                                electra-large-discriminator            0.434711   \n",
              "                                gpt2-medium                            0.423387   \n",
              "                                gpt2-medium                            0.423387   \n",
              "                                mGPT                                   0.771182   \n",
              "                                mGPT                                   0.771182   \n",
              "                                mdeberta-v3-base                       0.860763   \n",
              "                                mdeberta-v3-base                       0.860763   \n",
              "                                roberta-large-openai-detector          0.499794   \n",
              "                                roberta-large-openai-detector          0.499794   \n",
              "                                xlm-roberta-large                      0.905740   \n",
              "                                xlm-roberta-large                      0.905740   \n",
              "               gpt-4            bert-base-multilingual-cased           0.735353   \n",
              "                                bert-base-multilingual-cased           0.735353   \n",
              "                                electra-large-discriminator            0.429674   \n",
              "                                electra-large-discriminator            0.429674   \n",
              "                                gpt2-medium                            0.454937   \n",
              "                                gpt2-medium                            0.454937   \n",
              "                                mGPT                                   0.758187   \n",
              "                                mGPT                                   0.758187   \n",
              "                                mdeberta-v3-base                       0.780542   \n",
              "                                mdeberta-v3-base                       0.780542   \n",
              "                                roberta-large-openai-detector          0.481389   \n",
              "                                roberta-large-openai-detector          0.481389   \n",
              "                                xlm-roberta-large                      0.874254   \n",
              "                                xlm-roberta-large                      0.874254   \n",
              "               llama-65b        bert-base-multilingual-cased           0.444447   \n",
              "                                bert-base-multilingual-cased           0.444447   \n",
              "                                electra-large-discriminator            0.466246   \n",
              "                                electra-large-discriminator            0.466246   \n",
              "                                gpt2-medium                            0.399905   \n",
              "                                gpt2-medium                            0.399905   \n",
              "                                mGPT                                   0.484449   \n",
              "                                mGPT                                   0.484449   \n",
              "                                mdeberta-v3-base                       0.554618   \n",
              "                                mdeberta-v3-base                       0.554618   \n",
              "                                roberta-large-openai-detector          0.437915   \n",
              "                                roberta-large-openai-detector          0.437915   \n",
              "                                xlm-roberta-large                      0.476672   \n",
              "                                xlm-roberta-large                      0.476672   \n",
              "               opt-66b          bert-base-multilingual-cased           0.335930   \n",
              "                                bert-base-multilingual-cased           0.335930   \n",
              "                                electra-large-discriminator            0.447306   \n",
              "                                electra-large-discriminator            0.447306   \n",
              "                                gpt2-medium                            0.490206   \n",
              "                                gpt2-medium                            0.490206   \n",
              "                                mGPT                                   0.395866   \n",
              "                                mGPT                                   0.395866   \n",
              "                                mdeberta-v3-base                       0.426238   \n",
              "                                mdeberta-v3-base                       0.426238   \n",
              "                                roberta-large-openai-detector          0.443540   \n",
              "                                roberta-large-openai-detector          0.443540   \n",
              "                                xlm-roberta-large                      0.393379   \n",
              "                                xlm-roberta-large                      0.393379   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased           0.332121   \n",
              "                                bert-base-multilingual-cased           0.332121   \n",
              "                                electra-large-discriminator            0.509972   \n",
              "                                electra-large-discriminator            0.509972   \n",
              "                                gpt2-medium                            0.443796   \n",
              "                                gpt2-medium                            0.443796   \n",
              "                                mGPT                                   0.334078   \n",
              "                                mGPT                                   0.334078   \n",
              "                                mdeberta-v3-base                       0.348160   \n",
              "                                mdeberta-v3-base                       0.348160   \n",
              "                                roberta-large-openai-detector          0.519878   \n",
              "                                roberta-large-openai-detector          0.519878   \n",
              "                                xlm-roberta-large                      0.336034   \n",
              "                                xlm-roberta-large                      0.336034   \n",
              "               text-davinci-003 bert-base-multilingual-cased           0.780767   \n",
              "                                bert-base-multilingual-cased           0.780767   \n",
              "                                electra-large-discriminator            0.437249   \n",
              "                                electra-large-discriminator            0.437249   \n",
              "                                gpt2-medium                            0.514701   \n",
              "                                gpt2-medium                            0.514701   \n",
              "                                mGPT                                   0.768223   \n",
              "                                mGPT                                   0.768223   \n",
              "                                mdeberta-v3-base                       0.672618   \n",
              "                                mdeberta-v3-base                       0.672618   \n",
              "                                roberta-large-openai-detector          0.543917   \n",
              "                                roberta-large-openai-detector          0.543917   \n",
              "                                xlm-roberta-large                      0.709935   \n",
              "                                xlm-roberta-large                      0.709935   \n",
              "               vicuna-13b       bert-base-multilingual-cased           0.532151   \n",
              "                                bert-base-multilingual-cased           0.532151   \n",
              "                                electra-large-discriminator            0.461325   \n",
              "                                electra-large-discriminator            0.461325   \n",
              "                                gpt2-medium                            0.439953   \n",
              "                                gpt2-medium                            0.439953   \n",
              "                                mGPT                                   0.659223   \n",
              "                                mGPT                                   0.659223   \n",
              "                                mdeberta-v3-base                       0.834161   \n",
              "                                mdeberta-v3-base                       0.834161   \n",
              "                                roberta-large-openai-detector          0.499775   \n",
              "                                roberta-large-openai-detector          0.499775   \n",
              "                                xlm-roberta-large                      0.842878   \n",
              "                                xlm-roberta-large                      0.842878   \n",
              "all            all              bert-base-multilingual-cased           0.739269   \n",
              "                                electra-large-discriminator            0.415117   \n",
              "                                gpt2-medium                            0.650695   \n",
              "                                mGPT                                   0.725047   \n",
              "                                mdeberta-v3-base                       0.850010   \n",
              "                                roberta-large-openai-detector          0.724094   \n",
              "                                xlm-roberta-large                      0.778774   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased           0.900453   \n",
              "                                bert-base-multilingual-cased           0.900453   \n",
              "                                electra-large-discriminator            0.600716   \n",
              "                                electra-large-discriminator            0.600716   \n",
              "                                gpt2-medium                            0.621441   \n",
              "                                gpt2-medium                            0.621441   \n",
              "                                mGPT                                   0.809960   \n",
              "                                mGPT                                   0.809960   \n",
              "                                mdeberta-v3-base                       0.870658   \n",
              "                                mdeberta-v3-base                       0.870658   \n",
              "                                roberta-large-openai-detector          0.790835   \n",
              "                                roberta-large-openai-detector          0.790835   \n",
              "                                xlm-roberta-large                      0.897182   \n",
              "                                xlm-roberta-large                      0.897182   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased           0.856150   \n",
              "                                bert-base-multilingual-cased           0.856150   \n",
              "                                electra-large-discriminator            0.550862   \n",
              "                                electra-large-discriminator            0.550862   \n",
              "                                gpt2-medium                            0.650399   \n",
              "                                gpt2-medium                            0.650399   \n",
              "                                mGPT                                   0.766684   \n",
              "                                mGPT                                   0.766684   \n",
              "                                mdeberta-v3-base                       0.851794   \n",
              "                                mdeberta-v3-base                       0.851794   \n",
              "                                roberta-large-openai-detector          0.651513   \n",
              "                                roberta-large-openai-detector          0.651513   \n",
              "                                xlm-roberta-large                      0.875907   \n",
              "                                xlm-roberta-large                      0.875907   \n",
              "               gpt-4            bert-base-multilingual-cased           0.847399   \n",
              "                                bert-base-multilingual-cased           0.847399   \n",
              "                                electra-large-discriminator            0.594819   \n",
              "                                electra-large-discriminator            0.594819   \n",
              "                                gpt2-medium                            0.581261   \n",
              "                                gpt2-medium                            0.581261   \n",
              "                                mGPT                                   0.793422   \n",
              "                                mGPT                                   0.793422   \n",
              "                                mdeberta-v3-base                       0.838802   \n",
              "                                mdeberta-v3-base                       0.838802   \n",
              "                                roberta-large-openai-detector          0.734350   \n",
              "                                roberta-large-openai-detector          0.734350   \n",
              "                                xlm-roberta-large                      0.861394   \n",
              "                                xlm-roberta-large                      0.861394   \n",
              "               llama-65b        bert-base-multilingual-cased           0.333450   \n",
              "                                bert-base-multilingual-cased           0.333450   \n",
              "                                electra-large-discriminator            0.374356   \n",
              "                                electra-large-discriminator            0.374356   \n",
              "                                gpt2-medium                            0.406685   \n",
              "                                gpt2-medium                            0.406685   \n",
              "                                mGPT                                   0.414811   \n",
              "                                mGPT                                   0.414811   \n",
              "                                mdeberta-v3-base                       0.482325   \n",
              "                                mdeberta-v3-base                       0.482325   \n",
              "                                roberta-large-openai-detector          0.546769   \n",
              "                                roberta-large-openai-detector          0.546769   \n",
              "                                xlm-roberta-large                      0.397285   \n",
              "                                xlm-roberta-large                      0.397285   \n",
              "               opt-66b          bert-base-multilingual-cased           0.461256   \n",
              "                                bert-base-multilingual-cased           0.461256   \n",
              "                                electra-large-discriminator            0.472782   \n",
              "                                electra-large-discriminator            0.472782   \n",
              "                                gpt2-medium                            0.577805   \n",
              "                                gpt2-medium                            0.577805   \n",
              "                                mGPT                                   0.590694   \n",
              "                                mGPT                                   0.590694   \n",
              "                                mdeberta-v3-base                       0.619535   \n",
              "                                mdeberta-v3-base                       0.619535   \n",
              "                                roberta-large-openai-detector          0.619316   \n",
              "                                roberta-large-openai-detector          0.619316   \n",
              "                                xlm-roberta-large                      0.440987   \n",
              "                                xlm-roberta-large                      0.440987   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased           0.482112   \n",
              "                                bert-base-multilingual-cased           0.482112   \n",
              "                                electra-large-discriminator            0.398083   \n",
              "                                electra-large-discriminator            0.398083   \n",
              "                                gpt2-medium                            0.516679   \n",
              "                                gpt2-medium                            0.516679   \n",
              "                                mGPT                                   0.491033   \n",
              "                                mGPT                                   0.491033   \n",
              "                                mdeberta-v3-base                       0.546656   \n",
              "                                mdeberta-v3-base                       0.546656   \n",
              "                                roberta-large-openai-detector          0.638827   \n",
              "                                roberta-large-openai-detector          0.638827   \n",
              "                                xlm-roberta-large                      0.382583   \n",
              "                                xlm-roberta-large                      0.382583   \n",
              "               text-davinci-003 bert-base-multilingual-cased           0.927200   \n",
              "                                bert-base-multilingual-cased           0.927200   \n",
              "                                electra-large-discriminator            0.607457   \n",
              "                                electra-large-discriminator            0.607457   \n",
              "                                gpt2-medium                            0.801445   \n",
              "                                gpt2-medium                            0.801445   \n",
              "                                mGPT                                   0.863465   \n",
              "                                mGPT                                   0.863465   \n",
              "                                mdeberta-v3-base                       0.889408   \n",
              "                                mdeberta-v3-base                       0.889408   \n",
              "                                roberta-large-openai-detector          0.827556   \n",
              "                                roberta-large-openai-detector          0.827556   \n",
              "                                xlm-roberta-large                      0.862075   \n",
              "                                xlm-roberta-large                      0.862075   \n",
              "               vicuna-13b       bert-base-multilingual-cased           0.859693   \n",
              "                                bert-base-multilingual-cased           0.859693   \n",
              "                                electra-large-discriminator            0.591192   \n",
              "                                electra-large-discriminator            0.591192   \n",
              "                                gpt2-medium                            0.649574   \n",
              "                                gpt2-medium                            0.649574   \n",
              "                                mGPT                                   0.817581   \n",
              "                                mGPT                                   0.817581   \n",
              "                                mdeberta-v3-base                       0.859736   \n",
              "                                mdeberta-v3-base                       0.859736   \n",
              "                                roberta-large-openai-detector          0.817243   \n",
              "                                roberta-large-openai-detector          0.817243   \n",
              "                                xlm-roberta-large                      0.893835   \n",
              "                                xlm-roberta-large                      0.893835   \n",
              "en3            all              bert-base-multilingual-cased           0.396746   \n",
              "                                electra-large-discriminator            0.415095   \n",
              "                                gpt2-medium                            0.413818   \n",
              "                                mGPT                                   0.551642   \n",
              "                                mdeberta-v3-base                       0.456844   \n",
              "                                roberta-large-openai-detector          0.500837   \n",
              "                                xlm-roberta-large                      0.509011   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased           0.835100   \n",
              "                                bert-base-multilingual-cased           0.835100   \n",
              "                                electra-large-discriminator            0.539625   \n",
              "                                electra-large-discriminator            0.539625   \n",
              "                                gpt2-medium                            0.424284   \n",
              "                                gpt2-medium                            0.424284   \n",
              "                                mGPT                                   0.743497   \n",
              "                                mGPT                                   0.743497   \n",
              "                                mdeberta-v3-base                       0.720857   \n",
              "                                mdeberta-v3-base                       0.720857   \n",
              "                                roberta-large-openai-detector          0.508389   \n",
              "                                roberta-large-openai-detector          0.508389   \n",
              "                                xlm-roberta-large                      0.834026   \n",
              "                                xlm-roberta-large                      0.834026   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased           0.858929   \n",
              "                                bert-base-multilingual-cased           0.858929   \n",
              "                                electra-large-discriminator            0.498706   \n",
              "                                electra-large-discriminator            0.498706   \n",
              "                                gpt2-medium                            0.411862   \n",
              "                                gpt2-medium                            0.411862   \n",
              "                                mGPT                                   0.737524   \n",
              "                                mGPT                                   0.737524   \n",
              "                                mdeberta-v3-base                       0.723222   \n",
              "                                mdeberta-v3-base                       0.723222   \n",
              "                                roberta-large-openai-detector          0.763752   \n",
              "                                roberta-large-openai-detector          0.763752   \n",
              "                                xlm-roberta-large                      0.828342   \n",
              "                                xlm-roberta-large                      0.828342   \n",
              "               gpt-4            bert-base-multilingual-cased           0.767030   \n",
              "                                bert-base-multilingual-cased           0.767030   \n",
              "                                electra-large-discriminator            0.530090   \n",
              "                                electra-large-discriminator            0.530090   \n",
              "                                gpt2-medium                            0.413139   \n",
              "                                gpt2-medium                            0.413139   \n",
              "                                mGPT                                   0.678715   \n",
              "                                mGPT                                   0.678715   \n",
              "                                mdeberta-v3-base                       0.717014   \n",
              "                                mdeberta-v3-base                       0.717014   \n",
              "                                roberta-large-openai-detector          0.639401   \n",
              "                                roberta-large-openai-detector          0.639401   \n",
              "                                xlm-roberta-large                      0.806010   \n",
              "                                xlm-roberta-large                      0.806010   \n",
              "               llama-65b        bert-base-multilingual-cased           0.318726   \n",
              "                                bert-base-multilingual-cased           0.318726   \n",
              "                                electra-large-discriminator            0.423078   \n",
              "                                electra-large-discriminator            0.423078   \n",
              "                                gpt2-medium                            0.435301   \n",
              "                                gpt2-medium                            0.435301   \n",
              "                                mGPT                                   0.501149   \n",
              "                                mGPT                                   0.501149   \n",
              "                                mdeberta-v3-base                       0.388016   \n",
              "                                mdeberta-v3-base                       0.388016   \n",
              "                                roberta-large-openai-detector          0.428986   \n",
              "                                roberta-large-openai-detector          0.428986   \n",
              "                                xlm-roberta-large                      0.509665   \n",
              "                                xlm-roberta-large                      0.509665   \n",
              "               opt-66b          bert-base-multilingual-cased           0.506736   \n",
              "                                bert-base-multilingual-cased           0.506736   \n",
              "                                electra-large-discriminator            0.506487   \n",
              "                                electra-large-discriminator            0.506487   \n",
              "                                gpt2-medium                            0.500059   \n",
              "                                gpt2-medium                            0.500059   \n",
              "                                mGPT                                   0.593213   \n",
              "                                mGPT                                   0.593213   \n",
              "                                mdeberta-v3-base                       0.573370   \n",
              "                                mdeberta-v3-base                       0.573370   \n",
              "                                roberta-large-openai-detector          0.519302   \n",
              "                                roberta-large-openai-detector          0.519302   \n",
              "                                xlm-roberta-large                      0.461860   \n",
              "                                xlm-roberta-large                      0.461860   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased           0.574315   \n",
              "                                bert-base-multilingual-cased           0.574315   \n",
              "                                electra-large-discriminator            0.471376   \n",
              "                                electra-large-discriminator            0.471376   \n",
              "                                gpt2-medium                            0.509593   \n",
              "                                gpt2-medium                            0.509593   \n",
              "                                mGPT                                   0.602370   \n",
              "                                mGPT                                   0.602370   \n",
              "                                mdeberta-v3-base                       0.686362   \n",
              "                                mdeberta-v3-base                       0.686362   \n",
              "                                roberta-large-openai-detector          0.647068   \n",
              "                                roberta-large-openai-detector          0.647068   \n",
              "                                xlm-roberta-large                      0.551929   \n",
              "                                xlm-roberta-large                      0.551929   \n",
              "               text-davinci-003 bert-base-multilingual-cased           0.856395   \n",
              "                                bert-base-multilingual-cased           0.856395   \n",
              "                                electra-large-discriminator            0.432967   \n",
              "                                electra-large-discriminator            0.432967   \n",
              "                                gpt2-medium                            0.417224   \n",
              "                                gpt2-medium                            0.417224   \n",
              "                                mGPT                                   0.768366   \n",
              "                                mGPT                                   0.768366   \n",
              "                                mdeberta-v3-base                       0.765471   \n",
              "                                mdeberta-v3-base                       0.765471   \n",
              "                                roberta-large-openai-detector          0.745457   \n",
              "                                roberta-large-openai-detector          0.745457   \n",
              "                                xlm-roberta-large                      0.737452   \n",
              "                                xlm-roberta-large                      0.737452   \n",
              "               vicuna-13b       bert-base-multilingual-cased           0.821508   \n",
              "                                bert-base-multilingual-cased           0.821508   \n",
              "                                electra-large-discriminator            0.578532   \n",
              "                                electra-large-discriminator            0.578532   \n",
              "                                gpt2-medium                            0.551711   \n",
              "                                gpt2-medium                            0.551711   \n",
              "                                mGPT                                   0.740019   \n",
              "                                mGPT                                   0.740019   \n",
              "                                mdeberta-v3-base                       0.690637   \n",
              "                                mdeberta-v3-base                       0.690637   \n",
              "                                roberta-large-openai-detector          0.497785   \n",
              "                                roberta-large-openai-detector          0.497785   \n",
              "                                xlm-roberta-large                      0.789901   \n",
              "                                xlm-roberta-large                      0.789901   \n",
              "\n",
              "                                                               vicuna-13b  \\\n",
              "Train Language Train LLM        Model                                       \n",
              "en             all              bert-base-multilingual-cased     0.604411   \n",
              "                                electra-large-discriminator      0.569871   \n",
              "                                gpt2-medium                      0.535650   \n",
              "                                mGPT                             0.521795   \n",
              "                                mdeberta-v3-base                 0.568594   \n",
              "                                roberta-large-openai-detector    0.426098   \n",
              "                                xlm-roberta-large                0.447804   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased     0.762162   \n",
              "                                bert-base-multilingual-cased     0.762162   \n",
              "                                electra-large-discriminator      0.545989   \n",
              "                                electra-large-discriminator      0.545989   \n",
              "                                gpt2-medium                      0.476839   \n",
              "                                gpt2-medium                      0.476839   \n",
              "                                mGPT                             0.739632   \n",
              "                                mGPT                             0.739632   \n",
              "                                mdeberta-v3-base                 0.740397   \n",
              "                                mdeberta-v3-base                 0.740397   \n",
              "                                roberta-large-openai-detector    0.555558   \n",
              "                                roberta-large-openai-detector    0.555558   \n",
              "                                xlm-roberta-large                0.754177   \n",
              "                                xlm-roberta-large                0.754177   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased     0.828103   \n",
              "                                bert-base-multilingual-cased     0.828103   \n",
              "                                electra-large-discriminator      0.608747   \n",
              "                                electra-large-discriminator      0.608747   \n",
              "                                gpt2-medium                      0.453172   \n",
              "                                gpt2-medium                      0.453172   \n",
              "                                mGPT                             0.750252   \n",
              "                                mGPT                             0.750252   \n",
              "                                mdeberta-v3-base                 0.670367   \n",
              "                                mdeberta-v3-base                 0.670367   \n",
              "                                roberta-large-openai-detector    0.594035   \n",
              "                                roberta-large-openai-detector    0.594035   \n",
              "                                xlm-roberta-large                0.721473   \n",
              "                                xlm-roberta-large                0.721473   \n",
              "               gpt-4            bert-base-multilingual-cased     0.663212   \n",
              "                                bert-base-multilingual-cased     0.663212   \n",
              "                                electra-large-discriminator      0.515616   \n",
              "                                electra-large-discriminator      0.515616   \n",
              "                                gpt2-medium                      0.438667   \n",
              "                                gpt2-medium                      0.438667   \n",
              "                                mGPT                             0.698069   \n",
              "                                mGPT                             0.698069   \n",
              "                                mdeberta-v3-base                 0.614968   \n",
              "                                mdeberta-v3-base                 0.614968   \n",
              "                                roberta-large-openai-detector    0.588854   \n",
              "                                roberta-large-openai-detector    0.588854   \n",
              "                                xlm-roberta-large                0.675404   \n",
              "                                xlm-roberta-large                0.675404   \n",
              "               llama-65b        bert-base-multilingual-cased     0.452214   \n",
              "                                bert-base-multilingual-cased     0.452214   \n",
              "                                electra-large-discriminator      0.492094   \n",
              "                                electra-large-discriminator      0.492094   \n",
              "                                gpt2-medium                      0.464423   \n",
              "                                gpt2-medium                      0.464423   \n",
              "                                mGPT                             0.530832   \n",
              "                                mGPT                             0.530832   \n",
              "                                mdeberta-v3-base                 0.605376   \n",
              "                                mdeberta-v3-base                 0.605376   \n",
              "                                roberta-large-openai-detector    0.413273   \n",
              "                                roberta-large-openai-detector    0.413273   \n",
              "                                xlm-roberta-large                0.575652   \n",
              "                                xlm-roberta-large                0.575652   \n",
              "               opt-66b          bert-base-multilingual-cased     0.503717   \n",
              "                                bert-base-multilingual-cased     0.503717   \n",
              "                                electra-large-discriminator      0.551458   \n",
              "                                electra-large-discriminator      0.551458   \n",
              "                                gpt2-medium                      0.497737   \n",
              "                                gpt2-medium                      0.497737   \n",
              "                                mGPT                             0.616905   \n",
              "                                mGPT                             0.616905   \n",
              "                                mdeberta-v3-base                 0.602537   \n",
              "                                mdeberta-v3-base                 0.602537   \n",
              "                                roberta-large-openai-detector    0.472583   \n",
              "                                roberta-large-openai-detector    0.472583   \n",
              "                                xlm-roberta-large                0.592877   \n",
              "                                xlm-roberta-large                0.592877   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased     0.544160   \n",
              "                                bert-base-multilingual-cased     0.544160   \n",
              "                                electra-large-discriminator      0.500856   \n",
              "                                electra-large-discriminator      0.500856   \n",
              "                                gpt2-medium                      0.543087   \n",
              "                                gpt2-medium                      0.543087   \n",
              "                                mGPT                             0.633120   \n",
              "                                mGPT                             0.633120   \n",
              "                                mdeberta-v3-base                 0.795252   \n",
              "                                mdeberta-v3-base                 0.795252   \n",
              "                                roberta-large-openai-detector    0.653459   \n",
              "                                roberta-large-openai-detector    0.653459   \n",
              "                                xlm-roberta-large                0.667456   \n",
              "                                xlm-roberta-large                0.667456   \n",
              "               text-davinci-003 bert-base-multilingual-cased     0.763961   \n",
              "                                bert-base-multilingual-cased     0.763961   \n",
              "                                electra-large-discriminator      0.594161   \n",
              "                                electra-large-discriminator      0.594161   \n",
              "                                gpt2-medium                      0.441097   \n",
              "                                gpt2-medium                      0.441097   \n",
              "                                mGPT                             0.730595   \n",
              "                                mGPT                             0.730595   \n",
              "                                mdeberta-v3-base                 0.687262   \n",
              "                                mdeberta-v3-base                 0.687262   \n",
              "                                roberta-large-openai-detector    0.553437   \n",
              "                                roberta-large-openai-detector    0.553437   \n",
              "                                xlm-roberta-large                0.706243   \n",
              "                                xlm-roberta-large                0.706243   \n",
              "               vicuna-13b       bert-base-multilingual-cased     0.835301   \n",
              "                                bert-base-multilingual-cased     0.835301   \n",
              "                                electra-large-discriminator      0.592047   \n",
              "                                electra-large-discriminator      0.592047   \n",
              "                                gpt2-medium                      0.520420   \n",
              "                                gpt2-medium                      0.520420   \n",
              "                                mGPT                             0.731212   \n",
              "                                mGPT                             0.731212   \n",
              "                                mdeberta-v3-base                 0.532847   \n",
              "                                mdeberta-v3-base                 0.532847   \n",
              "                                roberta-large-openai-detector    0.598000   \n",
              "                                roberta-large-openai-detector    0.598000   \n",
              "                                xlm-roberta-large                0.723371   \n",
              "                                xlm-roberta-large                0.723371   \n",
              "es             all              bert-base-multilingual-cased     0.770255   \n",
              "                                electra-large-discriminator      0.427387   \n",
              "                                gpt2-medium                      0.621749   \n",
              "                                mGPT                             0.724551   \n",
              "                                mdeberta-v3-base                 0.772198   \n",
              "                                roberta-large-openai-detector    0.681316   \n",
              "                                xlm-roberta-large                0.822695   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased     0.826751   \n",
              "                                bert-base-multilingual-cased     0.826751   \n",
              "                                electra-large-discriminator      0.562389   \n",
              "                                electra-large-discriminator      0.562389   \n",
              "                                gpt2-medium                      0.573020   \n",
              "                                gpt2-medium                      0.573020   \n",
              "                                mGPT                             0.750793   \n",
              "                                mGPT                             0.750793   \n",
              "                                mdeberta-v3-base                 0.848412   \n",
              "                                mdeberta-v3-base                 0.848412   \n",
              "                                roberta-large-openai-detector    0.662942   \n",
              "                                roberta-large-openai-detector    0.662942   \n",
              "                                xlm-roberta-large                0.765276   \n",
              "                                xlm-roberta-large                0.765276   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased     0.790922   \n",
              "                                bert-base-multilingual-cased     0.790922   \n",
              "                                electra-large-discriminator      0.573226   \n",
              "                                electra-large-discriminator      0.573226   \n",
              "                                gpt2-medium                      0.536628   \n",
              "                                gpt2-medium                      0.536628   \n",
              "                                mGPT                             0.772096   \n",
              "                                mGPT                             0.772096   \n",
              "                                mdeberta-v3-base                 0.802990   \n",
              "                                mdeberta-v3-base                 0.802990   \n",
              "                                roberta-large-openai-detector    0.655411   \n",
              "                                roberta-large-openai-detector    0.655411   \n",
              "                                xlm-roberta-large                0.773826   \n",
              "                                xlm-roberta-large                0.773826   \n",
              "               gpt-4            bert-base-multilingual-cased     0.796894   \n",
              "                                bert-base-multilingual-cased     0.796894   \n",
              "                                electra-large-discriminator      0.551024   \n",
              "                                electra-large-discriminator      0.551024   \n",
              "                                gpt2-medium                      0.468006   \n",
              "                                gpt2-medium                      0.468006   \n",
              "                                mGPT                             0.737230   \n",
              "                                mGPT                             0.737230   \n",
              "                                mdeberta-v3-base                 0.776773   \n",
              "                                mdeberta-v3-base                 0.776773   \n",
              "                                roberta-large-openai-detector    0.637546   \n",
              "                                roberta-large-openai-detector    0.637546   \n",
              "                                xlm-roberta-large                0.699953   \n",
              "                                xlm-roberta-large                0.699953   \n",
              "               llama-65b        bert-base-multilingual-cased     0.508116   \n",
              "                                bert-base-multilingual-cased     0.508116   \n",
              "                                electra-large-discriminator      0.523059   \n",
              "                                electra-large-discriminator      0.523059   \n",
              "                                gpt2-medium                      0.502418   \n",
              "                                gpt2-medium                      0.502418   \n",
              "                                mGPT                             0.574171   \n",
              "                                mGPT                             0.574171   \n",
              "                                mdeberta-v3-base                 0.644163   \n",
              "                                mdeberta-v3-base                 0.644163   \n",
              "                                roberta-large-openai-detector    0.622882   \n",
              "                                roberta-large-openai-detector    0.622882   \n",
              "                                xlm-roberta-large                0.616786   \n",
              "                                xlm-roberta-large                0.616786   \n",
              "               opt-66b          bert-base-multilingual-cased     0.639956   \n",
              "                                bert-base-multilingual-cased     0.639956   \n",
              "                                electra-large-discriminator      0.528197   \n",
              "                                electra-large-discriminator      0.528197   \n",
              "                                gpt2-medium                      0.564522   \n",
              "                                gpt2-medium                      0.564522   \n",
              "                                mGPT                             0.598629   \n",
              "                                mGPT                             0.598629   \n",
              "                                mdeberta-v3-base                 0.741862   \n",
              "                                mdeberta-v3-base                 0.741862   \n",
              "                                roberta-large-openai-detector    0.636044   \n",
              "                                roberta-large-openai-detector    0.636044   \n",
              "                                xlm-roberta-large                0.662631   \n",
              "                                xlm-roberta-large                0.662631   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased     0.501182   \n",
              "                                bert-base-multilingual-cased     0.501182   \n",
              "                                electra-large-discriminator      0.573783   \n",
              "                                electra-large-discriminator      0.573783   \n",
              "                                gpt2-medium                      0.511101   \n",
              "                                gpt2-medium                      0.511101   \n",
              "                                mGPT                             0.590208   \n",
              "                                mGPT                             0.590208   \n",
              "                                mdeberta-v3-base                 0.567421   \n",
              "                                mdeberta-v3-base                 0.567421   \n",
              "                                roberta-large-openai-detector    0.642404   \n",
              "                                roberta-large-openai-detector    0.642404   \n",
              "                                xlm-roberta-large                0.450039   \n",
              "                                xlm-roberta-large                0.450039   \n",
              "               text-davinci-003 bert-base-multilingual-cased     0.810548   \n",
              "                                bert-base-multilingual-cased     0.810548   \n",
              "                                electra-large-discriminator      0.574487   \n",
              "                                electra-large-discriminator      0.574487   \n",
              "                                gpt2-medium                      0.615896   \n",
              "                                gpt2-medium                      0.615896   \n",
              "                                mGPT                             0.782344   \n",
              "                                mGPT                             0.782344   \n",
              "                                mdeberta-v3-base                 0.813574   \n",
              "                                mdeberta-v3-base                 0.813574   \n",
              "                                roberta-large-openai-detector    0.645926   \n",
              "                                roberta-large-openai-detector    0.645926   \n",
              "                                xlm-roberta-large                0.862708   \n",
              "                                xlm-roberta-large                0.862708   \n",
              "               vicuna-13b       bert-base-multilingual-cased     0.848329   \n",
              "                                bert-base-multilingual-cased     0.848329   \n",
              "                                electra-large-discriminator      0.601691   \n",
              "                                electra-large-discriminator      0.601691   \n",
              "                                gpt2-medium                      0.706606   \n",
              "                                gpt2-medium                      0.706606   \n",
              "                                mGPT                             0.781468   \n",
              "                                mGPT                             0.781468   \n",
              "                                mdeberta-v3-base                 0.865591   \n",
              "                                mdeberta-v3-base                 0.865591   \n",
              "                                roberta-large-openai-detector    0.714052   \n",
              "                                roberta-large-openai-detector    0.714052   \n",
              "                                xlm-roberta-large                0.813266   \n",
              "                                xlm-roberta-large                0.813266   \n",
              "ru             all              bert-base-multilingual-cased     0.593899   \n",
              "                                electra-large-discriminator      0.335041   \n",
              "                                gpt2-medium                      0.562294   \n",
              "                                mGPT                             0.677413   \n",
              "                                mdeberta-v3-base                 0.674276   \n",
              "                                roberta-large-openai-detector    0.445628   \n",
              "                                xlm-roberta-large                0.774785   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased     0.707865   \n",
              "                                bert-base-multilingual-cased     0.707865   \n",
              "                                electra-large-discriminator      0.487297   \n",
              "                                electra-large-discriminator      0.487297   \n",
              "                                gpt2-medium                      0.415410   \n",
              "                                gpt2-medium                      0.415410   \n",
              "                                mGPT                             0.678443   \n",
              "                                mGPT                             0.678443   \n",
              "                                mdeberta-v3-base                 0.863713   \n",
              "                                mdeberta-v3-base                 0.863713   \n",
              "                                roberta-large-openai-detector    0.492256   \n",
              "                                roberta-large-openai-detector    0.492256   \n",
              "                                xlm-roberta-large                0.847002   \n",
              "                                xlm-roberta-large                0.847002   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased     0.817148   \n",
              "                                bert-base-multilingual-cased     0.817148   \n",
              "                                electra-large-discriminator      0.438362   \n",
              "                                electra-large-discriminator      0.438362   \n",
              "                                gpt2-medium                      0.432814   \n",
              "                                gpt2-medium                      0.432814   \n",
              "                                mGPT                             0.759379   \n",
              "                                mGPT                             0.759379   \n",
              "                                mdeberta-v3-base                 0.851493   \n",
              "                                mdeberta-v3-base                 0.851493   \n",
              "                                roberta-large-openai-detector    0.499337   \n",
              "                                roberta-large-openai-detector    0.499337   \n",
              "                                xlm-roberta-large                0.883186   \n",
              "                                xlm-roberta-large                0.883186   \n",
              "               gpt-4            bert-base-multilingual-cased     0.713914   \n",
              "                                bert-base-multilingual-cased     0.713914   \n",
              "                                electra-large-discriminator      0.452126   \n",
              "                                electra-large-discriminator      0.452126   \n",
              "                                gpt2-medium                      0.458278   \n",
              "                                gpt2-medium                      0.458278   \n",
              "                                mGPT                             0.749687   \n",
              "                                mGPT                             0.749687   \n",
              "                                mdeberta-v3-base                 0.759735   \n",
              "                                mdeberta-v3-base                 0.759735   \n",
              "                                roberta-large-openai-detector    0.479392   \n",
              "                                roberta-large-openai-detector    0.479392   \n",
              "                                xlm-roberta-large                0.834223   \n",
              "                                xlm-roberta-large                0.834223   \n",
              "               llama-65b        bert-base-multilingual-cased     0.547635   \n",
              "                                bert-base-multilingual-cased     0.547635   \n",
              "                                electra-large-discriminator      0.470846   \n",
              "                                electra-large-discriminator      0.470846   \n",
              "                                gpt2-medium                      0.437714   \n",
              "                                gpt2-medium                      0.437714   \n",
              "                                mGPT                             0.514634   \n",
              "                                mGPT                             0.514634   \n",
              "                                mdeberta-v3-base                 0.637247   \n",
              "                                mdeberta-v3-base                 0.637247   \n",
              "                                roberta-large-openai-detector    0.442989   \n",
              "                                roberta-large-openai-detector    0.442989   \n",
              "                                xlm-roberta-large                0.597346   \n",
              "                                xlm-roberta-large                0.597346   \n",
              "               opt-66b          bert-base-multilingual-cased     0.402940   \n",
              "                                bert-base-multilingual-cased     0.402940   \n",
              "                                electra-large-discriminator      0.440878   \n",
              "                                electra-large-discriminator      0.440878   \n",
              "                                gpt2-medium                      0.463408   \n",
              "                                gpt2-medium                      0.463408   \n",
              "                                mGPT                             0.486644   \n",
              "                                mGPT                             0.486644   \n",
              "                                mdeberta-v3-base                 0.480679   \n",
              "                                mdeberta-v3-base                 0.480679   \n",
              "                                roberta-large-openai-detector    0.429432   \n",
              "                                roberta-large-openai-detector    0.429432   \n",
              "                                xlm-roberta-large                0.455400   \n",
              "                                xlm-roberta-large                0.455400   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased     0.337201   \n",
              "                                bert-base-multilingual-cased     0.337201   \n",
              "                                electra-large-discriminator      0.455219   \n",
              "                                electra-large-discriminator      0.455219   \n",
              "                                gpt2-medium                      0.427904   \n",
              "                                gpt2-medium                      0.427904   \n",
              "                                mGPT                             0.365224   \n",
              "                                mGPT                             0.365224   \n",
              "                                mdeberta-v3-base                 0.384318   \n",
              "                                mdeberta-v3-base                 0.384318   \n",
              "                                roberta-large-openai-detector    0.492644   \n",
              "                                roberta-large-openai-detector    0.492644   \n",
              "                                xlm-roberta-large                0.362727   \n",
              "                                xlm-roberta-large                0.362727   \n",
              "               text-davinci-003 bert-base-multilingual-cased     0.684073   \n",
              "                                bert-base-multilingual-cased     0.684073   \n",
              "                                electra-large-discriminator      0.413060   \n",
              "                                electra-large-discriminator      0.413060   \n",
              "                                gpt2-medium                      0.414780   \n",
              "                                gpt2-medium                      0.414780   \n",
              "                                mGPT                             0.699299   \n",
              "                                mGPT                             0.699299   \n",
              "                                mdeberta-v3-base                 0.623038   \n",
              "                                mdeberta-v3-base                 0.623038   \n",
              "                                roberta-large-openai-detector    0.483043   \n",
              "                                roberta-large-openai-detector    0.483043   \n",
              "                                xlm-roberta-large                0.621001   \n",
              "                                xlm-roberta-large                0.621001   \n",
              "               vicuna-13b       bert-base-multilingual-cased     0.542860   \n",
              "                                bert-base-multilingual-cased     0.542860   \n",
              "                                electra-large-discriminator      0.467982   \n",
              "                                electra-large-discriminator      0.467982   \n",
              "                                gpt2-medium                      0.547681   \n",
              "                                gpt2-medium                      0.547681   \n",
              "                                mGPT                             0.676492   \n",
              "                                mGPT                             0.676492   \n",
              "                                mdeberta-v3-base                 0.895699   \n",
              "                                mdeberta-v3-base                 0.895699   \n",
              "                                roberta-large-openai-detector    0.526876   \n",
              "                                roberta-large-openai-detector    0.526876   \n",
              "                                xlm-roberta-large                0.885241   \n",
              "                                xlm-roberta-large                0.885241   \n",
              "all            all              bert-base-multilingual-cased     0.739102   \n",
              "                                electra-large-discriminator      0.422630   \n",
              "                                gpt2-medium                      0.650933   \n",
              "                                mGPT                             0.725872   \n",
              "                                mdeberta-v3-base                 0.843834   \n",
              "                                roberta-large-openai-detector    0.741445   \n",
              "                                xlm-roberta-large                0.779190   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased     0.864471   \n",
              "                                bert-base-multilingual-cased     0.864471   \n",
              "                                electra-large-discriminator      0.622260   \n",
              "                                electra-large-discriminator      0.622260   \n",
              "                                gpt2-medium                      0.598261   \n",
              "                                gpt2-medium                      0.598261   \n",
              "                                mGPT                             0.780015   \n",
              "                                mGPT                             0.780015   \n",
              "                                mdeberta-v3-base                 0.826217   \n",
              "                                mdeberta-v3-base                 0.826217   \n",
              "                                roberta-large-openai-detector    0.769592   \n",
              "                                roberta-large-openai-detector    0.769592   \n",
              "                                xlm-roberta-large                0.873733   \n",
              "                                xlm-roberta-large                0.873733   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased     0.853558   \n",
              "                                bert-base-multilingual-cased     0.853558   \n",
              "                                electra-large-discriminator      0.578723   \n",
              "                                electra-large-discriminator      0.578723   \n",
              "                                gpt2-medium                      0.669185   \n",
              "                                gpt2-medium                      0.669185   \n",
              "                                mGPT                             0.737011   \n",
              "                                mGPT                             0.737011   \n",
              "                                mdeberta-v3-base                 0.826534   \n",
              "                                mdeberta-v3-base                 0.826534   \n",
              "                                roberta-large-openai-detector    0.658047   \n",
              "                                roberta-large-openai-detector    0.658047   \n",
              "                                xlm-roberta-large                0.838572   \n",
              "                                xlm-roberta-large                0.838572   \n",
              "               gpt-4            bert-base-multilingual-cased     0.836558   \n",
              "                                bert-base-multilingual-cased     0.836558   \n",
              "                                electra-large-discriminator      0.631515   \n",
              "                                electra-large-discriminator      0.631515   \n",
              "                                gpt2-medium                      0.655184   \n",
              "                                gpt2-medium                      0.655184   \n",
              "                                mGPT                             0.780189   \n",
              "                                mGPT                             0.780189   \n",
              "                                mdeberta-v3-base                 0.810602   \n",
              "                                mdeberta-v3-base                 0.810602   \n",
              "                                roberta-large-openai-detector    0.738498   \n",
              "                                roberta-large-openai-detector    0.738498   \n",
              "                                xlm-roberta-large                0.838126   \n",
              "                                xlm-roberta-large                0.838126   \n",
              "               llama-65b        bert-base-multilingual-cased     0.526138   \n",
              "                                bert-base-multilingual-cased     0.526138   \n",
              "                                electra-large-discriminator      0.477141   \n",
              "                                electra-large-discriminator      0.477141   \n",
              "                                gpt2-medium                      0.487092   \n",
              "                                gpt2-medium                      0.487092   \n",
              "                                mGPT                             0.593470   \n",
              "                                mGPT                             0.593470   \n",
              "                                mdeberta-v3-base                 0.677106   \n",
              "                                mdeberta-v3-base                 0.677106   \n",
              "                                roberta-large-openai-detector    0.644447   \n",
              "                                roberta-large-openai-detector    0.644447   \n",
              "                                xlm-roberta-large                0.598527   \n",
              "                                xlm-roberta-large                0.598527   \n",
              "               opt-66b          bert-base-multilingual-cased     0.608878   \n",
              "                                bert-base-multilingual-cased     0.608878   \n",
              "                                electra-large-discriminator      0.568054   \n",
              "                                electra-large-discriminator      0.568054   \n",
              "                                gpt2-medium                      0.592408   \n",
              "                                gpt2-medium                      0.592408   \n",
              "                                mGPT                             0.700501   \n",
              "                                mGPT                             0.700501   \n",
              "                                mdeberta-v3-base                 0.770342   \n",
              "                                mdeberta-v3-base                 0.770342   \n",
              "                                roberta-large-openai-detector    0.642645   \n",
              "                                roberta-large-openai-detector    0.642645   \n",
              "                                xlm-roberta-large                0.576043   \n",
              "                                xlm-roberta-large                0.576043   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased     0.689920   \n",
              "                                bert-base-multilingual-cased     0.689920   \n",
              "                                electra-large-discriminator      0.516882   \n",
              "                                electra-large-discriminator      0.516882   \n",
              "                                gpt2-medium                      0.604245   \n",
              "                                gpt2-medium                      0.604245   \n",
              "                                mGPT                             0.676543   \n",
              "                                mGPT                             0.676543   \n",
              "                                mdeberta-v3-base                 0.741362   \n",
              "                                mdeberta-v3-base                 0.741362   \n",
              "                                roberta-large-openai-detector    0.622713   \n",
              "                                roberta-large-openai-detector    0.622713   \n",
              "                                xlm-roberta-large                0.527326   \n",
              "                                xlm-roberta-large                0.527326   \n",
              "               text-davinci-003 bert-base-multilingual-cased     0.837861   \n",
              "                                bert-base-multilingual-cased     0.837861   \n",
              "                                electra-large-discriminator      0.611247   \n",
              "                                electra-large-discriminator      0.611247   \n",
              "                                gpt2-medium                      0.682702   \n",
              "                                gpt2-medium                      0.682702   \n",
              "                                mGPT                             0.796022   \n",
              "                                mGPT                             0.796022   \n",
              "                                mdeberta-v3-base                 0.809454   \n",
              "                                mdeberta-v3-base                 0.809454   \n",
              "                                roberta-large-openai-detector    0.749570   \n",
              "                                roberta-large-openai-detector    0.749570   \n",
              "                                xlm-roberta-large                0.774199   \n",
              "                                xlm-roberta-large                0.774199   \n",
              "               vicuna-13b       bert-base-multilingual-cased     0.903072   \n",
              "                                bert-base-multilingual-cased     0.903072   \n",
              "                                electra-large-discriminator      0.658108   \n",
              "                                electra-large-discriminator      0.658108   \n",
              "                                gpt2-medium                      0.775774   \n",
              "                                gpt2-medium                      0.775774   \n",
              "                                mGPT                             0.852928   \n",
              "                                mGPT                             0.852928   \n",
              "                                mdeberta-v3-base                 0.865503   \n",
              "                                mdeberta-v3-base                 0.865503   \n",
              "                                roberta-large-openai-detector    0.871341   \n",
              "                                roberta-large-openai-detector    0.871341   \n",
              "                                xlm-roberta-large                0.926867   \n",
              "                                xlm-roberta-large                0.926867   \n",
              "en3            all              bert-base-multilingual-cased     0.396119   \n",
              "                                electra-large-discriminator      0.414973   \n",
              "                                gpt2-medium                      0.472503   \n",
              "                                mGPT                             0.568690   \n",
              "                                mdeberta-v3-base                 0.456315   \n",
              "                                roberta-large-openai-detector    0.503992   \n",
              "                                xlm-roberta-large                0.509199   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased     0.787890   \n",
              "                                bert-base-multilingual-cased     0.787890   \n",
              "                                electra-large-discriminator      0.554727   \n",
              "                                electra-large-discriminator      0.554727   \n",
              "                                gpt2-medium                      0.453424   \n",
              "                                gpt2-medium                      0.453424   \n",
              "                                mGPT                             0.727912   \n",
              "                                mGPT                             0.727912   \n",
              "                                mdeberta-v3-base                 0.693065   \n",
              "                                mdeberta-v3-base                 0.693065   \n",
              "                                roberta-large-openai-detector    0.501537   \n",
              "                                roberta-large-openai-detector    0.501537   \n",
              "                                xlm-roberta-large                0.797257   \n",
              "                                xlm-roberta-large                0.797257   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased     0.826905   \n",
              "                                bert-base-multilingual-cased     0.826905   \n",
              "                                electra-large-discriminator      0.527972   \n",
              "                                electra-large-discriminator      0.527972   \n",
              "                                gpt2-medium                      0.443775   \n",
              "                                gpt2-medium                      0.443775   \n",
              "                                mGPT                             0.714768   \n",
              "                                mGPT                             0.714768   \n",
              "                                mdeberta-v3-base                 0.705498   \n",
              "                                mdeberta-v3-base                 0.705498   \n",
              "                                roberta-large-openai-detector    0.744766   \n",
              "                                roberta-large-openai-detector    0.744766   \n",
              "                                xlm-roberta-large                0.811050   \n",
              "                                xlm-roberta-large                0.811050   \n",
              "               gpt-4            bert-base-multilingual-cased     0.748190   \n",
              "                                bert-base-multilingual-cased     0.748190   \n",
              "                                electra-large-discriminator      0.561800   \n",
              "                                electra-large-discriminator      0.561800   \n",
              "                                gpt2-medium                      0.440473   \n",
              "                                gpt2-medium                      0.440473   \n",
              "                                mGPT                             0.684853   \n",
              "                                mGPT                             0.684853   \n",
              "                                mdeberta-v3-base                 0.698062   \n",
              "                                mdeberta-v3-base                 0.698062   \n",
              "                                roberta-large-openai-detector    0.629417   \n",
              "                                roberta-large-openai-detector    0.629417   \n",
              "                                xlm-roberta-large                0.793684   \n",
              "                                xlm-roberta-large                0.793684   \n",
              "               llama-65b        bert-base-multilingual-cased     0.491762   \n",
              "                                bert-base-multilingual-cased     0.491762   \n",
              "                                electra-large-discriminator      0.420476   \n",
              "                                electra-large-discriminator      0.420476   \n",
              "                                gpt2-medium                      0.450519   \n",
              "                                gpt2-medium                      0.450519   \n",
              "                                mGPT                             0.550172   \n",
              "                                mGPT                             0.550172   \n",
              "                                mdeberta-v3-base                 0.611489   \n",
              "                                mdeberta-v3-base                 0.611489   \n",
              "                                roberta-large-openai-detector    0.426997   \n",
              "                                roberta-large-openai-detector    0.426997   \n",
              "                                xlm-roberta-large                0.562724   \n",
              "                                xlm-roberta-large                0.562724   \n",
              "               opt-66b          bert-base-multilingual-cased     0.605541   \n",
              "                                bert-base-multilingual-cased     0.605541   \n",
              "                                electra-large-discriminator      0.591001   \n",
              "                                electra-large-discriminator      0.591001   \n",
              "                                gpt2-medium                      0.526138   \n",
              "                                gpt2-medium                      0.526138   \n",
              "                                mGPT                             0.625004   \n",
              "                                mGPT                             0.625004   \n",
              "                                mdeberta-v3-base                 0.586491   \n",
              "                                mdeberta-v3-base                 0.586491   \n",
              "                                roberta-large-openai-detector    0.515992   \n",
              "                                roberta-large-openai-detector    0.515992   \n",
              "                                xlm-roberta-large                0.596003   \n",
              "                                xlm-roberta-large                0.596003   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased     0.656640   \n",
              "                                bert-base-multilingual-cased     0.656640   \n",
              "                                electra-large-discriminator      0.510125   \n",
              "                                electra-large-discriminator      0.510125   \n",
              "                                gpt2-medium                      0.554412   \n",
              "                                gpt2-medium                      0.554412   \n",
              "                                mGPT                             0.637804   \n",
              "                                mGPT                             0.637804   \n",
              "                                mdeberta-v3-base                 0.781939   \n",
              "                                mdeberta-v3-base                 0.781939   \n",
              "                                roberta-large-openai-detector    0.655868   \n",
              "                                roberta-large-openai-detector    0.655868   \n",
              "                                xlm-roberta-large                0.670401   \n",
              "                                xlm-roberta-large                0.670401   \n",
              "               text-davinci-003 bert-base-multilingual-cased     0.781326   \n",
              "                                bert-base-multilingual-cased     0.781326   \n",
              "                                electra-large-discriminator      0.439509   \n",
              "                                electra-large-discriminator      0.439509   \n",
              "                                gpt2-medium                      0.436099   \n",
              "                                gpt2-medium                      0.436099   \n",
              "                                mGPT                             0.726614   \n",
              "                                mGPT                             0.726614   \n",
              "                                mdeberta-v3-base                 0.723634   \n",
              "                                mdeberta-v3-base                 0.723634   \n",
              "                                roberta-large-openai-detector    0.736050   \n",
              "                                roberta-large-openai-detector    0.736050   \n",
              "                                xlm-roberta-large                0.704876   \n",
              "                                xlm-roberta-large                0.704876   \n",
              "               vicuna-13b       bert-base-multilingual-cased     0.834733   \n",
              "                                bert-base-multilingual-cased     0.834733   \n",
              "                                electra-large-discriminator      0.632847   \n",
              "                                electra-large-discriminator      0.632847   \n",
              "                                gpt2-medium                      0.549010   \n",
              "                                gpt2-medium                      0.549010   \n",
              "                                mGPT                             0.746739   \n",
              "                                mGPT                             0.746739   \n",
              "                                mdeberta-v3-base                 0.682103   \n",
              "                                mdeberta-v3-base                 0.682103   \n",
              "                                roberta-large-openai-detector    0.496890   \n",
              "                                roberta-large-openai-detector    0.496890   \n",
              "                                xlm-roberta-large                0.790022   \n",
              "                                xlm-roberta-large                0.790022   \n",
              "\n",
              "                                                               alpaca-lora-30b  \\\n",
              "Train Language Train LLM        Model                                            \n",
              "en             all              bert-base-multilingual-cased          0.591141   \n",
              "                                electra-large-discriminator           0.524444   \n",
              "                                gpt2-medium                           0.471993   \n",
              "                                mGPT                                  0.512592   \n",
              "                                mdeberta-v3-base                      0.543716   \n",
              "                                roberta-large-openai-detector         0.426897   \n",
              "                                xlm-roberta-large                     0.447578   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased          0.788933   \n",
              "                                bert-base-multilingual-cased          0.788933   \n",
              "                                electra-large-discriminator           0.569812   \n",
              "                                electra-large-discriminator           0.569812   \n",
              "                                gpt2-medium                           0.542632   \n",
              "                                gpt2-medium                           0.542632   \n",
              "                                mGPT                                  0.755631   \n",
              "                                mGPT                                  0.755631   \n",
              "                                mdeberta-v3-base                      0.742818   \n",
              "                                mdeberta-v3-base                      0.742818   \n",
              "                                roberta-large-openai-detector         0.562747   \n",
              "                                roberta-large-openai-detector         0.562747   \n",
              "                                xlm-roberta-large                     0.764315   \n",
              "                                xlm-roberta-large                     0.764315   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased          0.779677   \n",
              "                                bert-base-multilingual-cased          0.779677   \n",
              "                                electra-large-discriminator           0.601589   \n",
              "                                electra-large-discriminator           0.601589   \n",
              "                                gpt2-medium                           0.421494   \n",
              "                                gpt2-medium                           0.421494   \n",
              "                                mGPT                                  0.684079   \n",
              "                                mGPT                                  0.684079   \n",
              "                                mdeberta-v3-base                      0.658615   \n",
              "                                mdeberta-v3-base                      0.658615   \n",
              "                                roberta-large-openai-detector         0.601326   \n",
              "                                roberta-large-openai-detector         0.601326   \n",
              "                                xlm-roberta-large                     0.682943   \n",
              "                                xlm-roberta-large                     0.682943   \n",
              "               gpt-4            bert-base-multilingual-cased          0.642841   \n",
              "                                bert-base-multilingual-cased          0.642841   \n",
              "                                electra-large-discriminator           0.475945   \n",
              "                                electra-large-discriminator           0.475945   \n",
              "                                gpt2-medium                           0.419926   \n",
              "                                gpt2-medium                           0.419926   \n",
              "                                mGPT                                  0.638721   \n",
              "                                mGPT                                  0.638721   \n",
              "                                mdeberta-v3-base                      0.533108   \n",
              "                                mdeberta-v3-base                      0.533108   \n",
              "                                roberta-large-openai-detector         0.580427   \n",
              "                                roberta-large-openai-detector         0.580427   \n",
              "                                xlm-roberta-large                     0.570681   \n",
              "                                xlm-roberta-large                     0.570681   \n",
              "               llama-65b        bert-base-multilingual-cased          0.382769   \n",
              "                                bert-base-multilingual-cased          0.382769   \n",
              "                                electra-large-discriminator           0.493706   \n",
              "                                electra-large-discriminator           0.493706   \n",
              "                                gpt2-medium                           0.419304   \n",
              "                                gpt2-medium                           0.419304   \n",
              "                                mGPT                                  0.478274   \n",
              "                                mGPT                                  0.478274   \n",
              "                                mdeberta-v3-base                      0.547297   \n",
              "                                mdeberta-v3-base                      0.547297   \n",
              "                                roberta-large-openai-detector         0.413060   \n",
              "                                roberta-large-openai-detector         0.413060   \n",
              "                                xlm-roberta-large                     0.513304   \n",
              "                                xlm-roberta-large                     0.513304   \n",
              "               opt-66b          bert-base-multilingual-cased          0.458338   \n",
              "                                bert-base-multilingual-cased          0.458338   \n",
              "                                electra-large-discriminator           0.502842   \n",
              "                                electra-large-discriminator           0.502842   \n",
              "                                gpt2-medium                           0.476860   \n",
              "                                gpt2-medium                           0.476860   \n",
              "                                mGPT                                  0.486970   \n",
              "                                mGPT                                  0.486970   \n",
              "                                mdeberta-v3-base                      0.426629   \n",
              "                                mdeberta-v3-base                      0.426629   \n",
              "                                roberta-large-openai-detector         0.473071   \n",
              "                                roberta-large-openai-detector         0.473071   \n",
              "                                xlm-roberta-large                     0.470675   \n",
              "                                xlm-roberta-large                     0.470675   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased          0.517325   \n",
              "                                bert-base-multilingual-cased          0.517325   \n",
              "                                electra-large-discriminator           0.501064   \n",
              "                                electra-large-discriminator           0.501064   \n",
              "                                gpt2-medium                           0.519183   \n",
              "                                gpt2-medium                           0.519183   \n",
              "                                mGPT                                  0.600379   \n",
              "                                mGPT                                  0.600379   \n",
              "                                mdeberta-v3-base                      0.666393   \n",
              "                                mdeberta-v3-base                      0.666393   \n",
              "                                roberta-large-openai-detector         0.643833   \n",
              "                                roberta-large-openai-detector         0.643833   \n",
              "                                xlm-roberta-large                     0.573627   \n",
              "                                xlm-roberta-large                     0.573627   \n",
              "               text-davinci-003 bert-base-multilingual-cased          0.742691   \n",
              "                                bert-base-multilingual-cased          0.742691   \n",
              "                                electra-large-discriminator           0.577547   \n",
              "                                electra-large-discriminator           0.577547   \n",
              "                                gpt2-medium                           0.417578   \n",
              "                                gpt2-medium                           0.417578   \n",
              "                                mGPT                                  0.683438   \n",
              "                                mGPT                                  0.683438   \n",
              "                                mdeberta-v3-base                      0.660011   \n",
              "                                mdeberta-v3-base                      0.660011   \n",
              "                                roberta-large-openai-detector         0.561767   \n",
              "                                roberta-large-openai-detector         0.561767   \n",
              "                                xlm-roberta-large                     0.684713   \n",
              "                                xlm-roberta-large                     0.684713   \n",
              "               vicuna-13b       bert-base-multilingual-cased          0.755192   \n",
              "                                bert-base-multilingual-cased          0.755192   \n",
              "                                electra-large-discriminator           0.582279   \n",
              "                                electra-large-discriminator           0.582279   \n",
              "                                gpt2-medium                           0.511708   \n",
              "                                gpt2-medium                           0.511708   \n",
              "                                mGPT                                  0.674120   \n",
              "                                mGPT                                  0.674120   \n",
              "                                mdeberta-v3-base                      0.514061   \n",
              "                                mdeberta-v3-base                      0.514061   \n",
              "                                roberta-large-openai-detector         0.602860   \n",
              "                                roberta-large-openai-detector         0.602860   \n",
              "                                xlm-roberta-large                     0.659794   \n",
              "                                xlm-roberta-large                     0.659794   \n",
              "es             all              bert-base-multilingual-cased          0.747767   \n",
              "                                electra-large-discriminator           0.427450   \n",
              "                                gpt2-medium                           0.639213   \n",
              "                                mGPT                                  0.713553   \n",
              "                                mdeberta-v3-base                      0.767961   \n",
              "                                roberta-large-openai-detector         0.670930   \n",
              "                                xlm-roberta-large                     0.811687   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased          0.829515   \n",
              "                                bert-base-multilingual-cased          0.829515   \n",
              "                                electra-large-discriminator           0.556214   \n",
              "                                electra-large-discriminator           0.556214   \n",
              "                                gpt2-medium                           0.725897   \n",
              "                                gpt2-medium                           0.725897   \n",
              "                                mGPT                                  0.802501   \n",
              "                                mGPT                                  0.802501   \n",
              "                                mdeberta-v3-base                      0.846514   \n",
              "                                mdeberta-v3-base                      0.846514   \n",
              "                                roberta-large-openai-detector         0.680669   \n",
              "                                roberta-large-openai-detector         0.680669   \n",
              "                                xlm-roberta-large                     0.859926   \n",
              "                                xlm-roberta-large                     0.859926   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased          0.752371   \n",
              "                                bert-base-multilingual-cased          0.752371   \n",
              "                                electra-large-discriminator           0.524984   \n",
              "                                electra-large-discriminator           0.524984   \n",
              "                                gpt2-medium                           0.518531   \n",
              "                                gpt2-medium                           0.518531   \n",
              "                                mGPT                                  0.717230   \n",
              "                                mGPT                                  0.717230   \n",
              "                                mdeberta-v3-base                      0.763541   \n",
              "                                mdeberta-v3-base                      0.763541   \n",
              "                                roberta-large-openai-detector         0.620171   \n",
              "                                roberta-large-openai-detector         0.620171   \n",
              "                                xlm-roberta-large                     0.712522   \n",
              "                                xlm-roberta-large                     0.712522   \n",
              "               gpt-4            bert-base-multilingual-cased          0.741813   \n",
              "                                bert-base-multilingual-cased          0.741813   \n",
              "                                electra-large-discriminator           0.506006   \n",
              "                                electra-large-discriminator           0.506006   \n",
              "                                gpt2-medium                           0.423609   \n",
              "                                gpt2-medium                           0.423609   \n",
              "                                mGPT                                  0.630055   \n",
              "                                mGPT                                  0.630055   \n",
              "                                mdeberta-v3-base                      0.689545   \n",
              "                                mdeberta-v3-base                      0.689545   \n",
              "                                roberta-large-openai-detector         0.584510   \n",
              "                                roberta-large-openai-detector         0.584510   \n",
              "                                xlm-roberta-large                     0.623344   \n",
              "                                xlm-roberta-large                     0.623344   \n",
              "               llama-65b        bert-base-multilingual-cased          0.442373   \n",
              "                                bert-base-multilingual-cased          0.442373   \n",
              "                                electra-large-discriminator           0.504539   \n",
              "                                electra-large-discriminator           0.504539   \n",
              "                                gpt2-medium                           0.496316   \n",
              "                                gpt2-medium                           0.496316   \n",
              "                                mGPT                                  0.577591   \n",
              "                                mGPT                                  0.577591   \n",
              "                                mdeberta-v3-base                      0.588732   \n",
              "                                mdeberta-v3-base                      0.588732   \n",
              "                                roberta-large-openai-detector         0.579316   \n",
              "                                roberta-large-openai-detector         0.579316   \n",
              "                                xlm-roberta-large                     0.575689   \n",
              "                                xlm-roberta-large                     0.575689   \n",
              "               opt-66b          bert-base-multilingual-cased          0.537798   \n",
              "                                bert-base-multilingual-cased          0.537798   \n",
              "                                electra-large-discriminator           0.473990   \n",
              "                                electra-large-discriminator           0.473990   \n",
              "                                gpt2-medium                           0.537059   \n",
              "                                gpt2-medium                           0.537059   \n",
              "                                mGPT                                  0.569766   \n",
              "                                mGPT                                  0.569766   \n",
              "                                mdeberta-v3-base                      0.747028   \n",
              "                                mdeberta-v3-base                      0.747028   \n",
              "                                roberta-large-openai-detector         0.631064   \n",
              "                                roberta-large-openai-detector         0.631064   \n",
              "                                xlm-roberta-large                     0.634030   \n",
              "                                xlm-roberta-large                     0.634030   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased          0.434089   \n",
              "                                bert-base-multilingual-cased          0.434089   \n",
              "                                electra-large-discriminator           0.544521   \n",
              "                                electra-large-discriminator           0.544521   \n",
              "                                gpt2-medium                           0.476267   \n",
              "                                gpt2-medium                           0.476267   \n",
              "                                mGPT                                  0.600459   \n",
              "                                mGPT                                  0.600459   \n",
              "                                mdeberta-v3-base                      0.578770   \n",
              "                                mdeberta-v3-base                      0.578770   \n",
              "                                roberta-large-openai-detector         0.626819   \n",
              "                                roberta-large-openai-detector         0.626819   \n",
              "                                xlm-roberta-large                     0.473695   \n",
              "                                xlm-roberta-large                     0.473695   \n",
              "               text-davinci-003 bert-base-multilingual-cased          0.758460   \n",
              "                                bert-base-multilingual-cased          0.758460   \n",
              "                                electra-large-discriminator           0.544943   \n",
              "                                electra-large-discriminator           0.544943   \n",
              "                                gpt2-medium                           0.654878   \n",
              "                                gpt2-medium                           0.654878   \n",
              "                                mGPT                                  0.735110   \n",
              "                                mGPT                                  0.735110   \n",
              "                                mdeberta-v3-base                      0.778314   \n",
              "                                mdeberta-v3-base                      0.778314   \n",
              "                                roberta-large-openai-detector         0.614743   \n",
              "                                roberta-large-openai-detector         0.614743   \n",
              "                                xlm-roberta-large                     0.790945   \n",
              "                                xlm-roberta-large                     0.790945   \n",
              "               vicuna-13b       bert-base-multilingual-cased          0.781171   \n",
              "                                bert-base-multilingual-cased          0.781171   \n",
              "                                electra-large-discriminator           0.542844   \n",
              "                                electra-large-discriminator           0.542844   \n",
              "                                gpt2-medium                           0.667251   \n",
              "                                gpt2-medium                           0.667251   \n",
              "                                mGPT                                  0.749968   \n",
              "                                mGPT                                  0.749968   \n",
              "                                mdeberta-v3-base                      0.772824   \n",
              "                                mdeberta-v3-base                      0.772824   \n",
              "                                roberta-large-openai-detector         0.648929   \n",
              "                                roberta-large-openai-detector         0.648929   \n",
              "                                xlm-roberta-large                     0.708626   \n",
              "                                xlm-roberta-large                     0.708626   \n",
              "ru             all              bert-base-multilingual-cased          0.589619   \n",
              "                                electra-large-discriminator           0.335007   \n",
              "                                gpt2-medium                           0.570469   \n",
              "                                mGPT                                  0.677112   \n",
              "                                mdeberta-v3-base                      0.677108   \n",
              "                                roberta-large-openai-detector         0.445594   \n",
              "                                xlm-roberta-large                     0.771267   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased          0.712663   \n",
              "                                bert-base-multilingual-cased          0.712663   \n",
              "                                electra-large-discriminator           0.488869   \n",
              "                                electra-large-discriminator           0.488869   \n",
              "                                gpt2-medium                           0.609512   \n",
              "                                gpt2-medium                           0.609512   \n",
              "                                mGPT                                  0.707655   \n",
              "                                mGPT                                  0.707655   \n",
              "                                mdeberta-v3-base                      0.901845   \n",
              "                                mdeberta-v3-base                      0.901845   \n",
              "                                roberta-large-openai-detector         0.535886   \n",
              "                                roberta-large-openai-detector         0.535886   \n",
              "                                xlm-roberta-large                     0.921184   \n",
              "                                xlm-roberta-large                     0.921184   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased          0.761681   \n",
              "                                bert-base-multilingual-cased          0.761681   \n",
              "                                electra-large-discriminator           0.440807   \n",
              "                                electra-large-discriminator           0.440807   \n",
              "                                gpt2-medium                           0.453942   \n",
              "                                gpt2-medium                           0.453942   \n",
              "                                mGPT                                  0.710733   \n",
              "                                mGPT                                  0.710733   \n",
              "                                mdeberta-v3-base                      0.812685   \n",
              "                                mdeberta-v3-base                      0.812685   \n",
              "                                roberta-large-openai-detector         0.512572   \n",
              "                                roberta-large-openai-detector         0.512572   \n",
              "                                xlm-roberta-large                     0.852313   \n",
              "                                xlm-roberta-large                     0.852313   \n",
              "               gpt-4            bert-base-multilingual-cased          0.664545   \n",
              "                                bert-base-multilingual-cased          0.664545   \n",
              "                                electra-large-discriminator           0.456044   \n",
              "                                electra-large-discriminator           0.456044   \n",
              "                                gpt2-medium                           0.489256   \n",
              "                                gpt2-medium                           0.489256   \n",
              "                                mGPT                                  0.644358   \n",
              "                                mGPT                                  0.644358   \n",
              "                                mdeberta-v3-base                      0.719917   \n",
              "                                mdeberta-v3-base                      0.719917   \n",
              "                                roberta-large-openai-detector         0.481780   \n",
              "                                roberta-large-openai-detector         0.481780   \n",
              "                                xlm-roberta-large                     0.766843   \n",
              "                                xlm-roberta-large                     0.766843   \n",
              "               llama-65b        bert-base-multilingual-cased          0.445523   \n",
              "                                bert-base-multilingual-cased          0.445523   \n",
              "                                electra-large-discriminator           0.469011   \n",
              "                                electra-large-discriminator           0.469011   \n",
              "                                gpt2-medium                           0.442277   \n",
              "                                gpt2-medium                           0.442277   \n",
              "                                mGPT                                  0.528312   \n",
              "                                mGPT                                  0.528312   \n",
              "                                mdeberta-v3-base                      0.627281   \n",
              "                                mdeberta-v3-base                      0.627281   \n",
              "                                roberta-large-openai-detector         0.453975   \n",
              "                                roberta-large-openai-detector         0.453975   \n",
              "                                xlm-roberta-large                     0.580996   \n",
              "                                xlm-roberta-large                     0.580996   \n",
              "               opt-66b          bert-base-multilingual-cased          0.398935   \n",
              "                                bert-base-multilingual-cased          0.398935   \n",
              "                                electra-large-discriminator           0.442376   \n",
              "                                electra-large-discriminator           0.442376   \n",
              "                                gpt2-medium                           0.476458   \n",
              "                                gpt2-medium                           0.476458   \n",
              "                                mGPT                                  0.512730   \n",
              "                                mGPT                                  0.512730   \n",
              "                                mdeberta-v3-base                      0.512682   \n",
              "                                mdeberta-v3-base                      0.512682   \n",
              "                                roberta-large-openai-detector         0.438215   \n",
              "                                roberta-large-openai-detector         0.438215   \n",
              "                                xlm-roberta-large                     0.507385   \n",
              "                                xlm-roberta-large                     0.507385   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased          0.345562   \n",
              "                                bert-base-multilingual-cased          0.345562   \n",
              "                                electra-large-discriminator           0.560732   \n",
              "                                electra-large-discriminator           0.560732   \n",
              "                                gpt2-medium                           0.443109   \n",
              "                                gpt2-medium                           0.443109   \n",
              "                                mGPT                                  0.370714   \n",
              "                                mGPT                                  0.370714   \n",
              "                                mdeberta-v3-base                      0.448456   \n",
              "                                mdeberta-v3-base                      0.448456   \n",
              "                                roberta-large-openai-detector         0.499802   \n",
              "                                roberta-large-openai-detector         0.499802   \n",
              "                                xlm-roberta-large                     0.420958   \n",
              "                                xlm-roberta-large                     0.420958   \n",
              "               text-davinci-003 bert-base-multilingual-cased          0.718904   \n",
              "                                bert-base-multilingual-cased          0.718904   \n",
              "                                electra-large-discriminator           0.428917   \n",
              "                                electra-large-discriminator           0.428917   \n",
              "                                gpt2-medium                           0.448965   \n",
              "                                gpt2-medium                           0.448965   \n",
              "                                mGPT                                  0.727483   \n",
              "                                mGPT                                  0.727483   \n",
              "                                mdeberta-v3-base                      0.769534   \n",
              "                                mdeberta-v3-base                      0.769534   \n",
              "                                roberta-large-openai-detector         0.502528   \n",
              "                                roberta-large-openai-detector         0.502528   \n",
              "                                xlm-roberta-large                     0.723169   \n",
              "                                xlm-roberta-large                     0.723169   \n",
              "               vicuna-13b       bert-base-multilingual-cased          0.532085   \n",
              "                                bert-base-multilingual-cased          0.532085   \n",
              "                                electra-large-discriminator           0.470776   \n",
              "                                electra-large-discriminator           0.470776   \n",
              "                                gpt2-medium                           0.503075   \n",
              "                                gpt2-medium                           0.503075   \n",
              "                                mGPT                                  0.667321   \n",
              "                                mGPT                                  0.667321   \n",
              "                                mdeberta-v3-base                      0.851321   \n",
              "                                mdeberta-v3-base                      0.851321   \n",
              "                                roberta-large-openai-detector         0.508797   \n",
              "                                roberta-large-openai-detector         0.508797   \n",
              "                                xlm-roberta-large                     0.821978   \n",
              "                                xlm-roberta-large                     0.821978   \n",
              "all            all              bert-base-multilingual-cased          0.725558   \n",
              "                                electra-large-discriminator           0.420880   \n",
              "                                gpt2-medium                           0.659945   \n",
              "                                mGPT                                  0.722575   \n",
              "                                mdeberta-v3-base                      0.854785   \n",
              "                                roberta-large-openai-detector         0.727622   \n",
              "                                xlm-roberta-large                     0.775379   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased          0.869751   \n",
              "                                bert-base-multilingual-cased          0.869751   \n",
              "                                electra-large-discriminator           0.650700   \n",
              "                                electra-large-discriminator           0.650700   \n",
              "                                gpt2-medium                           0.796096   \n",
              "                                gpt2-medium                           0.796096   \n",
              "                                mGPT                                  0.841418   \n",
              "                                mGPT                                  0.841418   \n",
              "                                mdeberta-v3-base                      0.854249   \n",
              "                                mdeberta-v3-base                      0.854249   \n",
              "                                roberta-large-openai-detector         0.847530   \n",
              "                                roberta-large-openai-detector         0.847530   \n",
              "                                xlm-roberta-large                     0.922378   \n",
              "                                xlm-roberta-large                     0.922378   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased          0.759746   \n",
              "                                bert-base-multilingual-cased          0.759746   \n",
              "                                electra-large-discriminator           0.514294   \n",
              "                                electra-large-discriminator           0.514294   \n",
              "                                gpt2-medium                           0.610807   \n",
              "                                gpt2-medium                           0.610807   \n",
              "                                mGPT                                  0.702000   \n",
              "                                mGPT                                  0.702000   \n",
              "                                mdeberta-v3-base                      0.785708   \n",
              "                                mdeberta-v3-base                      0.785708   \n",
              "                                roberta-large-openai-detector         0.613129   \n",
              "                                roberta-large-openai-detector         0.613129   \n",
              "                                xlm-roberta-large                     0.762452   \n",
              "                                xlm-roberta-large                     0.762452   \n",
              "               gpt-4            bert-base-multilingual-cased          0.740100   \n",
              "                                bert-base-multilingual-cased          0.740100   \n",
              "                                electra-large-discriminator           0.556494   \n",
              "                                electra-large-discriminator           0.556494   \n",
              "                                gpt2-medium                           0.579252   \n",
              "                                gpt2-medium                           0.579252   \n",
              "                                mGPT                                  0.697109   \n",
              "                                mGPT                                  0.697109   \n",
              "                                mdeberta-v3-base                      0.734456   \n",
              "                                mdeberta-v3-base                      0.734456   \n",
              "                                roberta-large-openai-detector         0.651862   \n",
              "                                roberta-large-openai-detector         0.651862   \n",
              "                                xlm-roberta-large                     0.742782   \n",
              "                                xlm-roberta-large                     0.742782   \n",
              "               llama-65b        bert-base-multilingual-cased          0.453402   \n",
              "                                bert-base-multilingual-cased          0.453402   \n",
              "                                electra-large-discriminator           0.473302   \n",
              "                                electra-large-discriminator           0.473302   \n",
              "                                gpt2-medium                           0.481104   \n",
              "                                gpt2-medium                           0.481104   \n",
              "                                mGPT                                  0.515372   \n",
              "                                mGPT                                  0.515372   \n",
              "                                mdeberta-v3-base                      0.591946   \n",
              "                                mdeberta-v3-base                      0.591946   \n",
              "                                roberta-large-openai-detector         0.556488   \n",
              "                                roberta-large-openai-detector         0.556488   \n",
              "                                xlm-roberta-large                     0.520847   \n",
              "                                xlm-roberta-large                     0.520847   \n",
              "               opt-66b          bert-base-multilingual-cased          0.495871   \n",
              "                                bert-base-multilingual-cased          0.495871   \n",
              "                                electra-large-discriminator           0.514539   \n",
              "                                electra-large-discriminator           0.514539   \n",
              "                                gpt2-medium                           0.536317   \n",
              "                                gpt2-medium                           0.536317   \n",
              "                                mGPT                                  0.659507   \n",
              "                                mGPT                                  0.659507   \n",
              "                                mdeberta-v3-base                      0.736051   \n",
              "                                mdeberta-v3-base                      0.736051   \n",
              "                                roberta-large-openai-detector         0.619146   \n",
              "                                roberta-large-openai-detector         0.619146   \n",
              "                                xlm-roberta-large                     0.587883   \n",
              "                                xlm-roberta-large                     0.587883   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased          0.586596   \n",
              "                                bert-base-multilingual-cased          0.586596   \n",
              "                                electra-large-discriminator           0.468703   \n",
              "                                electra-large-discriminator           0.468703   \n",
              "                                gpt2-medium                           0.531220   \n",
              "                                gpt2-medium                           0.531220   \n",
              "                                mGPT                                  0.592099   \n",
              "                                mGPT                                  0.592099   \n",
              "                                mdeberta-v3-base                      0.658911   \n",
              "                                mdeberta-v3-base                      0.658911   \n",
              "                                roberta-large-openai-detector         0.589335   \n",
              "                                roberta-large-openai-detector         0.589335   \n",
              "                                xlm-roberta-large                     0.503676   \n",
              "                                xlm-roberta-large                     0.503676   \n",
              "               text-davinci-003 bert-base-multilingual-cased          0.814043   \n",
              "                                bert-base-multilingual-cased          0.814043   \n",
              "                                electra-large-discriminator           0.553411   \n",
              "                                electra-large-discriminator           0.553411   \n",
              "                                gpt2-medium                           0.703286   \n",
              "                                gpt2-medium                           0.703286   \n",
              "                                mGPT                                  0.740844   \n",
              "                                mGPT                                  0.740844   \n",
              "                                mdeberta-v3-base                      0.819020   \n",
              "                                mdeberta-v3-base                      0.819020   \n",
              "                                roberta-large-openai-detector         0.741933   \n",
              "                                roberta-large-openai-detector         0.741933   \n",
              "                                xlm-roberta-large                     0.756600   \n",
              "                                xlm-roberta-large                     0.756600   \n",
              "               vicuna-13b       bert-base-multilingual-cased          0.807397   \n",
              "                                bert-base-multilingual-cased          0.807397   \n",
              "                                electra-large-discriminator           0.589176   \n",
              "                                electra-large-discriminator           0.589176   \n",
              "                                gpt2-medium                           0.669699   \n",
              "                                gpt2-medium                           0.669699   \n",
              "                                mGPT                                  0.794869   \n",
              "                                mGPT                                  0.794869   \n",
              "                                mdeberta-v3-base                      0.821485   \n",
              "                                mdeberta-v3-base                      0.821485   \n",
              "                                roberta-large-openai-detector         0.796307   \n",
              "                                roberta-large-openai-detector         0.796307   \n",
              "                                xlm-roberta-large                     0.834466   \n",
              "                                xlm-roberta-large                     0.834466   \n",
              "en3            all              bert-base-multilingual-cased          0.396425   \n",
              "                                electra-large-discriminator           0.415208   \n",
              "                                gpt2-medium                           0.427483   \n",
              "                                mGPT                                  0.535509   \n",
              "                                mdeberta-v3-base                      0.455687   \n",
              "                                roberta-large-openai-detector         0.501980   \n",
              "                                xlm-roberta-large                     0.505425   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased          0.807579   \n",
              "                                bert-base-multilingual-cased          0.807579   \n",
              "                                electra-large-discriminator           0.565463   \n",
              "                                electra-large-discriminator           0.565463   \n",
              "                                gpt2-medium                           0.481300   \n",
              "                                gpt2-medium                           0.481300   \n",
              "                                mGPT                                  0.734528   \n",
              "                                mGPT                                  0.734528   \n",
              "                                mdeberta-v3-base                      0.681353   \n",
              "                                mdeberta-v3-base                      0.681353   \n",
              "                                roberta-large-openai-detector         0.507334   \n",
              "                                roberta-large-openai-detector         0.507334   \n",
              "                                xlm-roberta-large                     0.793299   \n",
              "                                xlm-roberta-large                     0.793299   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased          0.783661   \n",
              "                                bert-base-multilingual-cased          0.783661   \n",
              "                                electra-large-discriminator           0.508214   \n",
              "                                electra-large-discriminator           0.508214   \n",
              "                                gpt2-medium                           0.411083   \n",
              "                                gpt2-medium                           0.411083   \n",
              "                                mGPT                                  0.639292   \n",
              "                                mGPT                                  0.639292   \n",
              "                                mdeberta-v3-base                      0.674072   \n",
              "                                mdeberta-v3-base                      0.674072   \n",
              "                                roberta-large-openai-detector         0.721918   \n",
              "                                roberta-large-openai-detector         0.721918   \n",
              "                                xlm-roberta-large                     0.743945   \n",
              "                                xlm-roberta-large                     0.743945   \n",
              "               gpt-4            bert-base-multilingual-cased          0.694034   \n",
              "                                bert-base-multilingual-cased          0.694034   \n",
              "                                electra-large-discriminator           0.540311   \n",
              "                                electra-large-discriminator           0.540311   \n",
              "                                gpt2-medium                           0.416507   \n",
              "                                gpt2-medium                           0.416507   \n",
              "                                mGPT                                  0.600872   \n",
              "                                mGPT                                  0.600872   \n",
              "                                mdeberta-v3-base                      0.660488   \n",
              "                                mdeberta-v3-base                      0.660488   \n",
              "                                roberta-large-openai-detector         0.612845   \n",
              "                                roberta-large-openai-detector         0.612845   \n",
              "                                xlm-roberta-large                     0.758701   \n",
              "                                xlm-roberta-large                     0.758701   \n",
              "               llama-65b        bert-base-multilingual-cased          0.414377   \n",
              "                                bert-base-multilingual-cased          0.414377   \n",
              "                                electra-large-discriminator           0.422278   \n",
              "                                electra-large-discriminator           0.422278   \n",
              "                                gpt2-medium                           0.410406   \n",
              "                                gpt2-medium                           0.410406   \n",
              "                                mGPT                                  0.492332   \n",
              "                                mGPT                                  0.492332   \n",
              "                                mdeberta-v3-base                      0.468837   \n",
              "                                mdeberta-v3-base                      0.468837   \n",
              "                                roberta-large-openai-detector         0.426591   \n",
              "                                roberta-large-openai-detector         0.426591   \n",
              "                                xlm-roberta-large                     0.538878   \n",
              "                                xlm-roberta-large                     0.538878   \n",
              "               opt-66b          bert-base-multilingual-cased          0.532646   \n",
              "                                bert-base-multilingual-cased          0.532646   \n",
              "                                electra-large-discriminator           0.528278   \n",
              "                                electra-large-discriminator           0.528278   \n",
              "                                gpt2-medium                           0.485899   \n",
              "                                gpt2-medium                           0.485899   \n",
              "                                mGPT                                  0.500024   \n",
              "                                mGPT                                  0.500024   \n",
              "                                mdeberta-v3-base                      0.508379   \n",
              "                                mdeberta-v3-base                      0.508379   \n",
              "                                roberta-large-openai-detector         0.511218   \n",
              "                                roberta-large-openai-detector         0.511218   \n",
              "                                xlm-roberta-large                     0.446879   \n",
              "                                xlm-roberta-large                     0.446879   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased          0.578536   \n",
              "                                bert-base-multilingual-cased          0.578536   \n",
              "                                electra-large-discriminator           0.502396   \n",
              "                                electra-large-discriminator           0.502396   \n",
              "                                gpt2-medium                           0.505228   \n",
              "                                gpt2-medium                           0.505228   \n",
              "                                mGPT                                  0.568213   \n",
              "                                mGPT                                  0.568213   \n",
              "                                mdeberta-v3-base                      0.682897   \n",
              "                                mdeberta-v3-base                      0.682897   \n",
              "                                roberta-large-openai-detector         0.624747   \n",
              "                                roberta-large-openai-detector         0.624747   \n",
              "                                xlm-roberta-large                     0.564548   \n",
              "                                xlm-roberta-large                     0.564548   \n",
              "               text-davinci-003 bert-base-multilingual-cased          0.765857   \n",
              "                                bert-base-multilingual-cased          0.765857   \n",
              "                                electra-large-discriminator           0.429113   \n",
              "                                electra-large-discriminator           0.429113   \n",
              "                                gpt2-medium                           0.416163   \n",
              "                                gpt2-medium                           0.416163   \n",
              "                                mGPT                                  0.665491   \n",
              "                                mGPT                                  0.665491   \n",
              "                                mdeberta-v3-base                      0.705935   \n",
              "                                mdeberta-v3-base                      0.705935   \n",
              "                                roberta-large-openai-detector         0.714581   \n",
              "                                roberta-large-openai-detector         0.714581   \n",
              "                                xlm-roberta-large                     0.680123   \n",
              "                                xlm-roberta-large                     0.680123   \n",
              "               vicuna-13b       bert-base-multilingual-cased          0.768132   \n",
              "                                bert-base-multilingual-cased          0.768132   \n",
              "                                electra-large-discriminator           0.607712   \n",
              "                                electra-large-discriminator           0.607712   \n",
              "                                gpt2-medium                           0.544429   \n",
              "                                gpt2-medium                           0.544429   \n",
              "                                mGPT                                  0.680461   \n",
              "                                mGPT                                  0.680461   \n",
              "                                mdeberta-v3-base                      0.636707   \n",
              "                                mdeberta-v3-base                      0.636707   \n",
              "                                roberta-large-openai-detector         0.496210   \n",
              "                                roberta-large-openai-detector         0.496210   \n",
              "                                xlm-roberta-large                     0.708935   \n",
              "                                xlm-roberta-large                     0.708935   \n",
              "\n",
              "                                                               opt-iml-max-1.3b  \\\n",
              "Train Language Train LLM        Model                                             \n",
              "en             all              bert-base-multilingual-cased           0.521586   \n",
              "                                electra-large-discriminator            0.597828   \n",
              "                                gpt2-medium                            0.567807   \n",
              "                                mGPT                                   0.455756   \n",
              "                                mdeberta-v3-base                       0.509931   \n",
              "                                roberta-large-openai-detector          0.424688   \n",
              "                                xlm-roberta-large                      0.440570   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased           0.486282   \n",
              "                                bert-base-multilingual-cased           0.486282   \n",
              "                                electra-large-discriminator            0.516792   \n",
              "                                electra-large-discriminator            0.516792   \n",
              "                                gpt2-medium                            0.493881   \n",
              "                                gpt2-medium                            0.493881   \n",
              "                                mGPT                                   0.362395   \n",
              "                                mGPT                                   0.362395   \n",
              "                                mdeberta-v3-base                       0.317518   \n",
              "                                mdeberta-v3-base                       0.317518   \n",
              "                                roberta-large-openai-detector          0.506063   \n",
              "                                roberta-large-openai-detector          0.506063   \n",
              "                                xlm-roberta-large                      0.510659   \n",
              "                                xlm-roberta-large                      0.510659   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased           0.395933   \n",
              "                                bert-base-multilingual-cased           0.395933   \n",
              "                                electra-large-discriminator            0.486370   \n",
              "                                electra-large-discriminator            0.486370   \n",
              "                                gpt2-medium                            0.416237   \n",
              "                                gpt2-medium                            0.416237   \n",
              "                                mGPT                                   0.345741   \n",
              "                                mGPT                                   0.345741   \n",
              "                                mdeberta-v3-base                       0.272867   \n",
              "                                mdeberta-v3-base                       0.272867   \n",
              "                                roberta-large-openai-detector          0.471355   \n",
              "                                roberta-large-openai-detector          0.471355   \n",
              "                                xlm-roberta-large                      0.388087   \n",
              "                                xlm-roberta-large                      0.388087   \n",
              "               gpt-4            bert-base-multilingual-cased           0.358506   \n",
              "                                bert-base-multilingual-cased           0.358506   \n",
              "                                electra-large-discriminator            0.375501   \n",
              "                                electra-large-discriminator            0.375501   \n",
              "                                gpt2-medium                            0.403546   \n",
              "                                gpt2-medium                            0.403546   \n",
              "                                mGPT                                   0.355808   \n",
              "                                mGPT                                   0.355808   \n",
              "                                mdeberta-v3-base                       0.335712   \n",
              "                                mdeberta-v3-base                       0.335712   \n",
              "                                roberta-large-openai-detector          0.377659   \n",
              "                                roberta-large-openai-detector          0.377659   \n",
              "                                xlm-roberta-large                      0.374332   \n",
              "                                xlm-roberta-large                      0.374332   \n",
              "               llama-65b        bert-base-multilingual-cased           0.640336   \n",
              "                                bert-base-multilingual-cased           0.640336   \n",
              "                                electra-large-discriminator            0.509512   \n",
              "                                electra-large-discriminator            0.509512   \n",
              "                                gpt2-medium                            0.495358   \n",
              "                                gpt2-medium                            0.495358   \n",
              "                                mGPT                                   0.540967   \n",
              "                                mGPT                                   0.540967   \n",
              "                                mdeberta-v3-base                       0.631759   \n",
              "                                mdeberta-v3-base                       0.631759   \n",
              "                                roberta-large-openai-detector          0.406047   \n",
              "                                roberta-large-openai-detector          0.406047   \n",
              "                                xlm-roberta-large                      0.655350   \n",
              "                                xlm-roberta-large                      0.655350   \n",
              "               opt-66b          bert-base-multilingual-cased           0.576559   \n",
              "                                bert-base-multilingual-cased           0.576559   \n",
              "                                electra-large-discriminator            0.628491   \n",
              "                                electra-large-discriminator            0.628491   \n",
              "                                gpt2-medium                            0.567559   \n",
              "                                gpt2-medium                            0.567559   \n",
              "                                mGPT                                   0.438807   \n",
              "                                mGPT                                   0.438807   \n",
              "                                mdeberta-v3-base                       0.574441   \n",
              "                                mdeberta-v3-base                       0.574441   \n",
              "                                roberta-large-openai-detector          0.471897   \n",
              "                                roberta-large-openai-detector          0.471897   \n",
              "                                xlm-roberta-large                      0.676182   \n",
              "                                xlm-roberta-large                      0.676182   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased           0.562628   \n",
              "                                bert-base-multilingual-cased           0.562628   \n",
              "                                electra-large-discriminator            0.624022   \n",
              "                                electra-large-discriminator            0.624022   \n",
              "                                gpt2-medium                            0.614131   \n",
              "                                gpt2-medium                            0.614131   \n",
              "                                mGPT                                   0.617217   \n",
              "                                mGPT                                   0.617217   \n",
              "                                mdeberta-v3-base                       0.913757   \n",
              "                                mdeberta-v3-base                       0.913757   \n",
              "                                roberta-large-openai-detector          0.677238   \n",
              "                                roberta-large-openai-detector          0.677238   \n",
              "                                xlm-roberta-large                      0.773080   \n",
              "                                xlm-roberta-large                      0.773080   \n",
              "               text-davinci-003 bert-base-multilingual-cased           0.370307   \n",
              "                                bert-base-multilingual-cased           0.370307   \n",
              "                                electra-large-discriminator            0.530221   \n",
              "                                electra-large-discriminator            0.530221   \n",
              "                                gpt2-medium                            0.408826   \n",
              "                                gpt2-medium                            0.408826   \n",
              "                                mGPT                                   0.351834   \n",
              "                                mGPT                                   0.351834   \n",
              "                                mdeberta-v3-base                       0.264892   \n",
              "                                mdeberta-v3-base                       0.264892   \n",
              "                                roberta-large-openai-detector          0.481713   \n",
              "                                roberta-large-openai-detector          0.481713   \n",
              "                                xlm-roberta-large                      0.521610   \n",
              "                                xlm-roberta-large                      0.521610   \n",
              "               vicuna-13b       bert-base-multilingual-cased           0.425580   \n",
              "                                bert-base-multilingual-cased           0.425580   \n",
              "                                electra-large-discriminator            0.613638   \n",
              "                                electra-large-discriminator            0.613638   \n",
              "                                gpt2-medium                            0.532865   \n",
              "                                gpt2-medium                            0.532865   \n",
              "                                mGPT                                   0.366510   \n",
              "                                mGPT                                   0.366510   \n",
              "                                mdeberta-v3-base                       0.213053   \n",
              "                                mdeberta-v3-base                       0.213053   \n",
              "                                roberta-large-openai-detector          0.530956   \n",
              "                                roberta-large-openai-detector          0.530956   \n",
              "                                xlm-roberta-large                      0.511644   \n",
              "                                xlm-roberta-large                      0.511644   \n",
              "es             all              bert-base-multilingual-cased           0.746052   \n",
              "                                electra-large-discriminator            0.426456   \n",
              "                                gpt2-medium                            0.640059   \n",
              "                                mGPT                                   0.705262   \n",
              "                                mdeberta-v3-base                       0.776808   \n",
              "                                roberta-large-openai-detector          0.729118   \n",
              "                                xlm-roberta-large                      0.791989   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased           0.528624   \n",
              "                                bert-base-multilingual-cased           0.528624   \n",
              "                                electra-large-discriminator            0.479522   \n",
              "                                electra-large-discriminator            0.479522   \n",
              "                                gpt2-medium                            0.583718   \n",
              "                                gpt2-medium                            0.583718   \n",
              "                                mGPT                                   0.559165   \n",
              "                                mGPT                                   0.559165   \n",
              "                                mdeberta-v3-base                       0.600382   \n",
              "                                mdeberta-v3-base                       0.600382   \n",
              "                                roberta-large-openai-detector          0.375803   \n",
              "                                roberta-large-openai-detector          0.375803   \n",
              "                                xlm-roberta-large                      0.530708   \n",
              "                                xlm-roberta-large                      0.530708   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased           0.373587   \n",
              "                                bert-base-multilingual-cased           0.373587   \n",
              "                                electra-large-discriminator            0.427711   \n",
              "                                electra-large-discriminator            0.427711   \n",
              "                                gpt2-medium                            0.512145   \n",
              "                                gpt2-medium                            0.512145   \n",
              "                                mGPT                                   0.370160   \n",
              "                                mGPT                                   0.370160   \n",
              "                                mdeberta-v3-base                       0.326371   \n",
              "                                mdeberta-v3-base                       0.326371   \n",
              "                                roberta-large-openai-detector          0.332869   \n",
              "                                roberta-large-openai-detector          0.332869   \n",
              "                                xlm-roberta-large                      0.400503   \n",
              "                                xlm-roberta-large                      0.400503   \n",
              "               gpt-4            bert-base-multilingual-cased           0.379837   \n",
              "                                bert-base-multilingual-cased           0.379837   \n",
              "                                electra-large-discriminator            0.451181   \n",
              "                                electra-large-discriminator            0.451181   \n",
              "                                gpt2-medium                            0.430259   \n",
              "                                gpt2-medium                            0.430259   \n",
              "                                mGPT                                   0.361372   \n",
              "                                mGPT                                   0.361372   \n",
              "                                mdeberta-v3-base                       0.337111   \n",
              "                                mdeberta-v3-base                       0.337111   \n",
              "                                roberta-large-openai-detector          0.318732   \n",
              "                                roberta-large-openai-detector          0.318732   \n",
              "                                xlm-roberta-large                      0.378265   \n",
              "                                xlm-roberta-large                      0.378265   \n",
              "               llama-65b        bert-base-multilingual-cased           0.735577   \n",
              "                                bert-base-multilingual-cased           0.735577   \n",
              "                                electra-large-discriminator            0.615126   \n",
              "                                electra-large-discriminator            0.615126   \n",
              "                                gpt2-medium                            0.592352   \n",
              "                                gpt2-medium                            0.592352   \n",
              "                                mGPT                                   0.641868   \n",
              "                                mGPT                                   0.641868   \n",
              "                                mdeberta-v3-base                       0.857894   \n",
              "                                mdeberta-v3-base                       0.857894   \n",
              "                                roberta-large-openai-detector          0.493428   \n",
              "                                roberta-large-openai-detector          0.493428   \n",
              "                                xlm-roberta-large                      0.833164   \n",
              "                                xlm-roberta-large                      0.833164   \n",
              "               opt-66b          bert-base-multilingual-cased           0.739716   \n",
              "                                bert-base-multilingual-cased           0.739716   \n",
              "                                electra-large-discriminator            0.620457   \n",
              "                                electra-large-discriminator            0.620457   \n",
              "                                gpt2-medium                            0.650606   \n",
              "                                gpt2-medium                            0.650606   \n",
              "                                mGPT                                   0.603779   \n",
              "                                mGPT                                   0.603779   \n",
              "                                mdeberta-v3-base                       0.886105   \n",
              "                                mdeberta-v3-base                       0.886105   \n",
              "                                roberta-large-openai-detector          0.668285   \n",
              "                                roberta-large-openai-detector          0.668285   \n",
              "                                xlm-roberta-large                      0.708624   \n",
              "                                xlm-roberta-large                      0.708624   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased           0.917835   \n",
              "                                bert-base-multilingual-cased           0.917835   \n",
              "                                electra-large-discriminator            0.707548   \n",
              "                                electra-large-discriminator            0.707548   \n",
              "                                gpt2-medium                            0.677026   \n",
              "                                gpt2-medium                            0.677026   \n",
              "                                mGPT                                   0.878735   \n",
              "                                mGPT                                   0.878735   \n",
              "                                mdeberta-v3-base                       0.932284   \n",
              "                                mdeberta-v3-base                       0.932284   \n",
              "                                roberta-large-openai-detector          0.734720   \n",
              "                                roberta-large-openai-detector          0.734720   \n",
              "                                xlm-roberta-large                      0.885644   \n",
              "                                xlm-roberta-large                      0.885644   \n",
              "               text-davinci-003 bert-base-multilingual-cased           0.499226   \n",
              "                                bert-base-multilingual-cased           0.499226   \n",
              "                                electra-large-discriminator            0.459035   \n",
              "                                electra-large-discriminator            0.459035   \n",
              "                                gpt2-medium                            0.613219   \n",
              "                                gpt2-medium                            0.613219   \n",
              "                                mGPT                                   0.377743   \n",
              "                                mGPT                                   0.377743   \n",
              "                                mdeberta-v3-base                       0.351735   \n",
              "                                mdeberta-v3-base                       0.351735   \n",
              "                                roberta-large-openai-detector          0.337915   \n",
              "                                roberta-large-openai-detector          0.337915   \n",
              "                                xlm-roberta-large                      0.435786   \n",
              "                                xlm-roberta-large                      0.435786   \n",
              "               vicuna-13b       bert-base-multilingual-cased           0.499232   \n",
              "                                bert-base-multilingual-cased           0.499232   \n",
              "                                electra-large-discriminator            0.487593   \n",
              "                                electra-large-discriminator            0.487593   \n",
              "                                gpt2-medium                            0.661265   \n",
              "                                gpt2-medium                            0.661265   \n",
              "                                mGPT                                   0.482835   \n",
              "                                mGPT                                   0.482835   \n",
              "                                mdeberta-v3-base                       0.479366   \n",
              "                                mdeberta-v3-base                       0.479366   \n",
              "                                roberta-large-openai-detector          0.404100   \n",
              "                                roberta-large-openai-detector          0.404100   \n",
              "                                xlm-roberta-large                      0.667884   \n",
              "                                xlm-roberta-large                      0.667884   \n",
              "ru             all              bert-base-multilingual-cased           0.579198   \n",
              "                                electra-large-discriminator            0.331835   \n",
              "                                gpt2-medium                            0.570463   \n",
              "                                mGPT                                   0.677970   \n",
              "                                mdeberta-v3-base                       0.671397   \n",
              "                                roberta-large-openai-detector          0.441196   \n",
              "                                xlm-roberta-large                      0.783235   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased           0.507114   \n",
              "                                bert-base-multilingual-cased           0.507114   \n",
              "                                electra-large-discriminator            0.572538   \n",
              "                                electra-large-discriminator            0.572538   \n",
              "                                gpt2-medium                            0.466738   \n",
              "                                gpt2-medium                            0.466738   \n",
              "                                mGPT                                   0.498553   \n",
              "                                mGPT                                   0.498553   \n",
              "                                mdeberta-v3-base                       0.861113   \n",
              "                                mdeberta-v3-base                       0.861113   \n",
              "                                roberta-large-openai-detector          0.425370   \n",
              "                                roberta-large-openai-detector          0.425370   \n",
              "                                xlm-roberta-large                      0.800625   \n",
              "                                xlm-roberta-large                      0.800625   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased           0.425899   \n",
              "                                bert-base-multilingual-cased           0.425899   \n",
              "                                electra-large-discriminator            0.390803   \n",
              "                                electra-large-discriminator            0.390803   \n",
              "                                gpt2-medium                            0.432484   \n",
              "                                gpt2-medium                            0.432484   \n",
              "                                mGPT                                   0.366569   \n",
              "                                mGPT                                   0.366569   \n",
              "                                mdeberta-v3-base                       0.364402   \n",
              "                                mdeberta-v3-base                       0.364402   \n",
              "                                roberta-large-openai-detector          0.362901   \n",
              "                                roberta-large-openai-detector          0.362901   \n",
              "                                xlm-roberta-large                      0.412969   \n",
              "                                xlm-roberta-large                      0.412969   \n",
              "               gpt-4            bert-base-multilingual-cased           0.405104   \n",
              "                                bert-base-multilingual-cased           0.405104   \n",
              "                                electra-large-discriminator            0.432759   \n",
              "                                electra-large-discriminator            0.432759   \n",
              "                                gpt2-medium                            0.464348   \n",
              "                                gpt2-medium                            0.464348   \n",
              "                                mGPT                                   0.365638   \n",
              "                                mGPT                                   0.365638   \n",
              "                                mdeberta-v3-base                       0.342521   \n",
              "                                mdeberta-v3-base                       0.342521   \n",
              "                                roberta-large-openai-detector          0.353031   \n",
              "                                roberta-large-openai-detector          0.353031   \n",
              "                                xlm-roberta-large                      0.365146   \n",
              "                                xlm-roberta-large                      0.365146   \n",
              "               llama-65b        bert-base-multilingual-cased           0.651701   \n",
              "                                bert-base-multilingual-cased           0.651701   \n",
              "                                electra-large-discriminator            0.464780   \n",
              "                                electra-large-discriminator            0.464780   \n",
              "                                gpt2-medium                            0.485469   \n",
              "                                gpt2-medium                            0.485469   \n",
              "                                mGPT                                   0.579402   \n",
              "                                mGPT                                   0.579402   \n",
              "                                mdeberta-v3-base                       0.725998   \n",
              "                                mdeberta-v3-base                       0.725998   \n",
              "                                roberta-large-openai-detector          0.439600   \n",
              "                                roberta-large-openai-detector          0.439600   \n",
              "                                xlm-roberta-large                      0.741714   \n",
              "                                xlm-roberta-large                      0.741714   \n",
              "               opt-66b          bert-base-multilingual-cased           0.757279   \n",
              "                                bert-base-multilingual-cased           0.757279   \n",
              "                                electra-large-discriminator            0.467420   \n",
              "                                electra-large-discriminator            0.467420   \n",
              "                                gpt2-medium                            0.582652   \n",
              "                                gpt2-medium                            0.582652   \n",
              "                                mGPT                                   0.831490   \n",
              "                                mGPT                                   0.831490   \n",
              "                                mdeberta-v3-base                       0.866291   \n",
              "                                mdeberta-v3-base                       0.866291   \n",
              "                                roberta-large-openai-detector          0.414764   \n",
              "                                roberta-large-openai-detector          0.414764   \n",
              "                                xlm-roberta-large                      0.936430   \n",
              "                                xlm-roberta-large                      0.936430   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased           0.707558   \n",
              "                                bert-base-multilingual-cased           0.707558   \n",
              "                                electra-large-discriminator            0.590938   \n",
              "                                electra-large-discriminator            0.590938   \n",
              "                                gpt2-medium                            0.598311   \n",
              "                                gpt2-medium                            0.598311   \n",
              "                                mGPT                                   0.705426   \n",
              "                                mGPT                                   0.705426   \n",
              "                                mdeberta-v3-base                       0.706799   \n",
              "                                mdeberta-v3-base                       0.706799   \n",
              "                                roberta-large-openai-detector          0.519722   \n",
              "                                roberta-large-openai-detector          0.519722   \n",
              "                                xlm-roberta-large                      0.799578   \n",
              "                                xlm-roberta-large                      0.799578   \n",
              "               text-davinci-003 bert-base-multilingual-cased           0.375260   \n",
              "                                bert-base-multilingual-cased           0.375260   \n",
              "                                electra-large-discriminator            0.455254   \n",
              "                                electra-large-discriminator            0.455254   \n",
              "                                gpt2-medium                            0.507149   \n",
              "                                gpt2-medium                            0.507149   \n",
              "                                mGPT                                   0.391856   \n",
              "                                mGPT                                   0.391856   \n",
              "                                mdeberta-v3-base                       0.664845   \n",
              "                                mdeberta-v3-base                       0.664845   \n",
              "                                roberta-large-openai-detector          0.351229   \n",
              "                                roberta-large-openai-detector          0.351229   \n",
              "                                xlm-roberta-large                      0.635119   \n",
              "                                xlm-roberta-large                      0.635119   \n",
              "               vicuna-13b       bert-base-multilingual-cased           0.433768   \n",
              "                                bert-base-multilingual-cased           0.433768   \n",
              "                                electra-large-discriminator            0.470456   \n",
              "                                electra-large-discriminator            0.470456   \n",
              "                                gpt2-medium                            0.513854   \n",
              "                                gpt2-medium                            0.513854   \n",
              "                                mGPT                                   0.584928   \n",
              "                                mGPT                                   0.584928   \n",
              "                                mdeberta-v3-base                       0.908030   \n",
              "                                mdeberta-v3-base                       0.908030   \n",
              "                                roberta-large-openai-detector          0.446061   \n",
              "                                roberta-large-openai-detector          0.446061   \n",
              "                                xlm-roberta-large                      0.761966   \n",
              "                                xlm-roberta-large                      0.761966   \n",
              "all            all              bert-base-multilingual-cased           0.713254   \n",
              "                                electra-large-discriminator            0.421625   \n",
              "                                gpt2-medium                            0.650515   \n",
              "                                mGPT                                   0.713906   \n",
              "                                mdeberta-v3-base                       0.862336   \n",
              "                                roberta-large-openai-detector          0.765409   \n",
              "                                xlm-roberta-large                      0.777838   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased           0.481000   \n",
              "                                bert-base-multilingual-cased           0.481000   \n",
              "                                electra-large-discriminator            0.447202   \n",
              "                                electra-large-discriminator            0.447202   \n",
              "                                gpt2-medium                            0.572778   \n",
              "                                gpt2-medium                            0.572778   \n",
              "                                mGPT                                   0.463764   \n",
              "                                mGPT                                   0.463764   \n",
              "                                mdeberta-v3-base                       0.578029   \n",
              "                                mdeberta-v3-base                       0.578029   \n",
              "                                roberta-large-openai-detector          0.538735   \n",
              "                                roberta-large-openai-detector          0.538735   \n",
              "                                xlm-roberta-large                      0.501793   \n",
              "                                xlm-roberta-large                      0.501793   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased           0.372440   \n",
              "                                bert-base-multilingual-cased           0.372440   \n",
              "                                electra-large-discriminator            0.370216   \n",
              "                                electra-large-discriminator            0.370216   \n",
              "                                gpt2-medium                            0.491351   \n",
              "                                gpt2-medium                            0.491351   \n",
              "                                mGPT                                   0.341799   \n",
              "                                mGPT                                   0.341799   \n",
              "                                mdeberta-v3-base                       0.337009   \n",
              "                                mdeberta-v3-base                       0.337009   \n",
              "                                roberta-large-openai-detector          0.379544   \n",
              "                                roberta-large-openai-detector          0.379544   \n",
              "                                xlm-roberta-large                      0.358216   \n",
              "                                xlm-roberta-large                      0.358216   \n",
              "               gpt-4            bert-base-multilingual-cased           0.355474   \n",
              "                                bert-base-multilingual-cased           0.355474   \n",
              "                                electra-large-discriminator            0.379866   \n",
              "                                electra-large-discriminator            0.379866   \n",
              "                                gpt2-medium                            0.500542   \n",
              "                                gpt2-medium                            0.500542   \n",
              "                                mGPT                                   0.352665   \n",
              "                                mGPT                                   0.352665   \n",
              "                                mdeberta-v3-base                       0.341549   \n",
              "                                mdeberta-v3-base                       0.341549   \n",
              "                                roberta-large-openai-detector          0.389554   \n",
              "                                roberta-large-openai-detector          0.389554   \n",
              "                                xlm-roberta-large                      0.350308   \n",
              "                                xlm-roberta-large                      0.350308   \n",
              "               llama-65b        bert-base-multilingual-cased           0.765505   \n",
              "                                bert-base-multilingual-cased           0.765505   \n",
              "                                electra-large-discriminator            0.633728   \n",
              "                                electra-large-discriminator            0.633728   \n",
              "                                gpt2-medium                            0.682165   \n",
              "                                gpt2-medium                            0.682165   \n",
              "                                mGPT                                   0.796851   \n",
              "                                mGPT                                   0.796851   \n",
              "                                mdeberta-v3-base                       0.859332   \n",
              "                                mdeberta-v3-base                       0.859332   \n",
              "                                roberta-large-openai-detector          0.752134   \n",
              "                                roberta-large-openai-detector          0.752134   \n",
              "                                xlm-roberta-large                      0.813623   \n",
              "                                xlm-roberta-large                      0.813623   \n",
              "               opt-66b          bert-base-multilingual-cased           0.834210   \n",
              "                                bert-base-multilingual-cased           0.834210   \n",
              "                                electra-large-discriminator            0.712291   \n",
              "                                electra-large-discriminator            0.712291   \n",
              "                                gpt2-medium                            0.794005   \n",
              "                                gpt2-medium                            0.794005   \n",
              "                                mGPT                                   0.902213   \n",
              "                                mGPT                                   0.902213   \n",
              "                                mdeberta-v3-base                       0.914443   \n",
              "                                mdeberta-v3-base                       0.914443   \n",
              "                                roberta-large-openai-detector          0.847524   \n",
              "                                roberta-large-openai-detector          0.847524   \n",
              "                                xlm-roberta-large                      0.962542   \n",
              "                                xlm-roberta-large                      0.962542   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased           0.933727   \n",
              "                                bert-base-multilingual-cased           0.933727   \n",
              "                                electra-large-discriminator            0.769950   \n",
              "                                electra-large-discriminator            0.769950   \n",
              "                                gpt2-medium                            0.876682   \n",
              "                                gpt2-medium                            0.876682   \n",
              "                                mGPT                                   0.949505   \n",
              "                                mGPT                                   0.949505   \n",
              "                                mdeberta-v3-base                       0.960685   \n",
              "                                mdeberta-v3-base                       0.960685   \n",
              "                                roberta-large-openai-detector          0.903642   \n",
              "                                roberta-large-openai-detector          0.903642   \n",
              "                                xlm-roberta-large                      0.966133   \n",
              "                                xlm-roberta-large                      0.966133   \n",
              "               text-davinci-003 bert-base-multilingual-cased           0.391896   \n",
              "                                bert-base-multilingual-cased           0.391896   \n",
              "                                electra-large-discriminator            0.401165   \n",
              "                                electra-large-discriminator            0.401165   \n",
              "                                gpt2-medium                            0.639227   \n",
              "                                gpt2-medium                            0.639227   \n",
              "                                mGPT                                   0.376911   \n",
              "                                mGPT                                   0.376911   \n",
              "                                mdeberta-v3-base                       0.380513   \n",
              "                                mdeberta-v3-base                       0.380513   \n",
              "                                roberta-large-openai-detector          0.549049   \n",
              "                                roberta-large-openai-detector          0.549049   \n",
              "                                xlm-roberta-large                      0.424504   \n",
              "                                xlm-roberta-large                      0.424504   \n",
              "               vicuna-13b       bert-base-multilingual-cased           0.481618   \n",
              "                                bert-base-multilingual-cased           0.481618   \n",
              "                                electra-large-discriminator            0.456001   \n",
              "                                electra-large-discriminator            0.456001   \n",
              "                                gpt2-medium                            0.639953   \n",
              "                                gpt2-medium                            0.639953   \n",
              "                                mGPT                                   0.440699   \n",
              "                                mGPT                                   0.440699   \n",
              "                                mdeberta-v3-base                       0.491275   \n",
              "                                mdeberta-v3-base                       0.491275   \n",
              "                                roberta-large-openai-detector          0.526786   \n",
              "                                roberta-large-openai-detector          0.526786   \n",
              "                                xlm-roberta-large                      0.545461   \n",
              "                                xlm-roberta-large                      0.545461   \n",
              "en3            all              bert-base-multilingual-cased           0.393226   \n",
              "                                electra-large-discriminator            0.411713   \n",
              "                                gpt2-medium                            0.531722   \n",
              "                                mGPT                                   0.414920   \n",
              "                                mdeberta-v3-base                       0.451096   \n",
              "                                roberta-large-openai-detector          0.505160   \n",
              "                                xlm-roberta-large                      0.482085   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased           0.426781   \n",
              "                                bert-base-multilingual-cased           0.426781   \n",
              "                                electra-large-discriminator            0.520691   \n",
              "                                electra-large-discriminator            0.520691   \n",
              "                                gpt2-medium                            0.428447   \n",
              "                                gpt2-medium                            0.428447   \n",
              "                                mGPT                                   0.337669   \n",
              "                                mGPT                                   0.337669   \n",
              "                                mdeberta-v3-base                       0.301433   \n",
              "                                mdeberta-v3-base                       0.301433   \n",
              "                                roberta-large-openai-detector          0.480115   \n",
              "                                roberta-large-openai-detector          0.480115   \n",
              "                                xlm-roberta-large                      0.394107   \n",
              "                                xlm-roberta-large                      0.394107   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased           0.381844   \n",
              "                                bert-base-multilingual-cased           0.381844   \n",
              "                                electra-large-discriminator            0.440049   \n",
              "                                electra-large-discriminator            0.440049   \n",
              "                                gpt2-medium                            0.382515   \n",
              "                                gpt2-medium                            0.382515   \n",
              "                                mGPT                                   0.322327   \n",
              "                                mGPT                                   0.322327   \n",
              "                                mdeberta-v3-base                       0.271748   \n",
              "                                mdeberta-v3-base                       0.271748   \n",
              "                                roberta-large-openai-detector          0.515958   \n",
              "                                roberta-large-openai-detector          0.515958   \n",
              "                                xlm-roberta-large                      0.377967   \n",
              "                                xlm-roberta-large                      0.377967   \n",
              "               gpt-4            bert-base-multilingual-cased           0.349331   \n",
              "                                bert-base-multilingual-cased           0.349331   \n",
              "                                electra-large-discriminator            0.443509   \n",
              "                                electra-large-discriminator            0.443509   \n",
              "                                gpt2-medium                            0.389897   \n",
              "                                gpt2-medium                            0.389897   \n",
              "                                mGPT                                   0.331883   \n",
              "                                mGPT                                   0.331883   \n",
              "                                mdeberta-v3-base                       0.291053   \n",
              "                                mdeberta-v3-base                       0.291053   \n",
              "                                roberta-large-openai-detector          0.354317   \n",
              "                                roberta-large-openai-detector          0.354317   \n",
              "                                xlm-roberta-large                      0.372316   \n",
              "                                xlm-roberta-large                      0.372316   \n",
              "               llama-65b        bert-base-multilingual-cased           0.665641   \n",
              "                                bert-base-multilingual-cased           0.665641   \n",
              "                                electra-large-discriminator            0.419668   \n",
              "                                electra-large-discriminator            0.419668   \n",
              "                                gpt2-medium                            0.476261   \n",
              "                                gpt2-medium                            0.476261   \n",
              "                                mGPT                                   0.524732   \n",
              "                                mGPT                                   0.524732   \n",
              "                                mdeberta-v3-base                       0.753374   \n",
              "                                mdeberta-v3-base                       0.753374   \n",
              "                                roberta-large-openai-detector          0.425474   \n",
              "                                roberta-large-openai-detector          0.425474   \n",
              "                                xlm-roberta-large                      0.616834   \n",
              "                                xlm-roberta-large                      0.616834   \n",
              "               opt-66b          bert-base-multilingual-cased           0.598676   \n",
              "                                bert-base-multilingual-cased           0.598676   \n",
              "                                electra-large-discriminator            0.673953   \n",
              "                                electra-large-discriminator            0.673953   \n",
              "                                gpt2-medium                            0.597020   \n",
              "                                gpt2-medium                            0.597020   \n",
              "                                mGPT                                   0.460110   \n",
              "                                mGPT                                   0.460110   \n",
              "                                mdeberta-v3-base                       0.474023   \n",
              "                                mdeberta-v3-base                       0.474023   \n",
              "                                roberta-large-openai-detector          0.517334   \n",
              "                                roberta-large-openai-detector          0.517334   \n",
              "                                xlm-roberta-large                      0.548935   \n",
              "                                xlm-roberta-large                      0.548935   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased           0.660214   \n",
              "                                bert-base-multilingual-cased           0.660214   \n",
              "                                electra-large-discriminator            0.664909   \n",
              "                                electra-large-discriminator            0.664909   \n",
              "                                gpt2-medium                            0.655718   \n",
              "                                gpt2-medium                            0.655718   \n",
              "                                mGPT                                   0.530247   \n",
              "                                mGPT                                   0.530247   \n",
              "                                mdeberta-v3-base                       0.821387   \n",
              "                                mdeberta-v3-base                       0.821387   \n",
              "                                roberta-large-openai-detector          0.686816   \n",
              "                                roberta-large-openai-detector          0.686816   \n",
              "                                xlm-roberta-large                      0.714663   \n",
              "                                xlm-roberta-large                      0.714663   \n",
              "               text-davinci-003 bert-base-multilingual-cased           0.372565   \n",
              "                                bert-base-multilingual-cased           0.372565   \n",
              "                                electra-large-discriminator            0.370776   \n",
              "                                electra-large-discriminator            0.370776   \n",
              "                                gpt2-medium                            0.400234   \n",
              "                                gpt2-medium                            0.400234   \n",
              "                                mGPT                                   0.333927   \n",
              "                                mGPT                                   0.333927   \n",
              "                                mdeberta-v3-base                       0.291870   \n",
              "                                mdeberta-v3-base                       0.291870   \n",
              "                                roberta-large-openai-detector          0.496221   \n",
              "                                roberta-large-openai-detector          0.496221   \n",
              "                                xlm-roberta-large                      0.409468   \n",
              "                                xlm-roberta-large                      0.409468   \n",
              "               vicuna-13b       bert-base-multilingual-cased           0.459754   \n",
              "                                bert-base-multilingual-cased           0.459754   \n",
              "                                electra-large-discriminator            0.593568   \n",
              "                                electra-large-discriminator            0.593568   \n",
              "                                gpt2-medium                            0.543328   \n",
              "                                gpt2-medium                            0.543328   \n",
              "                                mGPT                                   0.350788   \n",
              "                                mGPT                                   0.350788   \n",
              "                                mdeberta-v3-base                       0.319401   \n",
              "                                mdeberta-v3-base                       0.319401   \n",
              "                                roberta-large-openai-detector          0.454638   \n",
              "                                roberta-large-openai-detector          0.454638   \n",
              "                                xlm-roberta-large                      0.393022   \n",
              "                                xlm-roberta-large                      0.393022   \n",
              "\n",
              "                                                               llama-65b  \\\n",
              "Train Language Train LLM        Model                                      \n",
              "en             all              bert-base-multilingual-cased    0.605250   \n",
              "                                electra-large-discriminator     0.627450   \n",
              "                                gpt2-medium                     0.634057   \n",
              "                                mGPT                            0.523695   \n",
              "                                mdeberta-v3-base                0.568337   \n",
              "                                roberta-large-openai-detector   0.424856   \n",
              "                                xlm-roberta-large               0.445515   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased    0.433616   \n",
              "                                bert-base-multilingual-cased    0.433616   \n",
              "                                electra-large-discriminator     0.405988   \n",
              "                                electra-large-discriminator     0.405988   \n",
              "                                gpt2-medium                     0.562619   \n",
              "                                gpt2-medium                     0.562619   \n",
              "                                mGPT                            0.442688   \n",
              "                                mGPT                            0.442688   \n",
              "                                mdeberta-v3-base                0.367561   \n",
              "                                mdeberta-v3-base                0.367561   \n",
              "                                roberta-large-openai-detector   0.459477   \n",
              "                                roberta-large-openai-detector   0.459477   \n",
              "                                xlm-roberta-large               0.491207   \n",
              "                                xlm-roberta-large               0.491207   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased    0.375808   \n",
              "                                bert-base-multilingual-cased    0.375808   \n",
              "                                electra-large-discriminator     0.438110   \n",
              "                                electra-large-discriminator     0.438110   \n",
              "                                gpt2-medium                     0.499467   \n",
              "                                gpt2-medium                     0.499467   \n",
              "                                mGPT                            0.372614   \n",
              "                                mGPT                            0.372614   \n",
              "                                mdeberta-v3-base                0.324795   \n",
              "                                mdeberta-v3-base                0.324795   \n",
              "                                roberta-large-openai-detector   0.421149   \n",
              "                                roberta-large-openai-detector   0.421149   \n",
              "                                xlm-roberta-large               0.396658   \n",
              "                                xlm-roberta-large               0.396658   \n",
              "               gpt-4            bert-base-multilingual-cased    0.374432   \n",
              "                                bert-base-multilingual-cased    0.374432   \n",
              "                                electra-large-discriminator     0.371973   \n",
              "                                electra-large-discriminator     0.371973   \n",
              "                                gpt2-medium                     0.520717   \n",
              "                                gpt2-medium                     0.520717   \n",
              "                                mGPT                            0.418632   \n",
              "                                mGPT                            0.418632   \n",
              "                                mdeberta-v3-base                0.334625   \n",
              "                                mdeberta-v3-base                0.334625   \n",
              "                                roberta-large-openai-detector   0.357731   \n",
              "                                roberta-large-openai-detector   0.357731   \n",
              "                                xlm-roberta-large               0.381520   \n",
              "                                xlm-roberta-large               0.381520   \n",
              "               llama-65b        bert-base-multilingual-cased    0.684084   \n",
              "                                bert-base-multilingual-cased    0.684084   \n",
              "                                electra-large-discriminator     0.511065   \n",
              "                                electra-large-discriminator     0.511065   \n",
              "                                gpt2-medium                     0.533748   \n",
              "                                gpt2-medium                     0.533748   \n",
              "                                mGPT                            0.560818   \n",
              "                                mGPT                            0.560818   \n",
              "                                mdeberta-v3-base                0.634777   \n",
              "                                mdeberta-v3-base                0.634777   \n",
              "                                roberta-large-openai-detector   0.412366   \n",
              "                                roberta-large-openai-detector   0.412366   \n",
              "                                xlm-roberta-large               0.662487   \n",
              "                                xlm-roberta-large               0.662487   \n",
              "               opt-66b          bert-base-multilingual-cased    0.629073   \n",
              "                                bert-base-multilingual-cased    0.629073   \n",
              "                                electra-large-discriminator     0.677941   \n",
              "                                electra-large-discriminator     0.677941   \n",
              "                                gpt2-medium                     0.665022   \n",
              "                                gpt2-medium                     0.665022   \n",
              "                                mGPT                            0.601623   \n",
              "                                mGPT                            0.601623   \n",
              "                                mdeberta-v3-base                0.655563   \n",
              "                                mdeberta-v3-base                0.655563   \n",
              "                                roberta-large-openai-detector   0.461880   \n",
              "                                roberta-large-openai-detector   0.461880   \n",
              "                                xlm-roberta-large               0.708483   \n",
              "                                xlm-roberta-large               0.708483   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased    0.563494   \n",
              "                                bert-base-multilingual-cased    0.563494   \n",
              "                                electra-large-discriminator     0.602123   \n",
              "                                electra-large-discriminator     0.602123   \n",
              "                                gpt2-medium                     0.689406   \n",
              "                                gpt2-medium                     0.689406   \n",
              "                                mGPT                            0.623846   \n",
              "                                mGPT                            0.623846   \n",
              "                                mdeberta-v3-base                0.823460   \n",
              "                                mdeberta-v3-base                0.823460   \n",
              "                                roberta-large-openai-detector   0.572192   \n",
              "                                roberta-large-openai-detector   0.572192   \n",
              "                                xlm-roberta-large               0.757210   \n",
              "                                xlm-roberta-large               0.757210   \n",
              "               text-davinci-003 bert-base-multilingual-cased    0.348985   \n",
              "                                bert-base-multilingual-cased    0.348985   \n",
              "                                electra-large-discriminator     0.401311   \n",
              "                                electra-large-discriminator     0.401311   \n",
              "                                gpt2-medium                     0.491612   \n",
              "                                gpt2-medium                     0.491612   \n",
              "                                mGPT                            0.362415   \n",
              "                                mGPT                            0.362415   \n",
              "                                mdeberta-v3-base                0.318991   \n",
              "                                mdeberta-v3-base                0.318991   \n",
              "                                roberta-large-openai-detector   0.402408   \n",
              "                                roberta-large-openai-detector   0.402408   \n",
              "                                xlm-roberta-large               0.415832   \n",
              "                                xlm-roberta-large               0.415832   \n",
              "               vicuna-13b       bert-base-multilingual-cased    0.441932   \n",
              "                                bert-base-multilingual-cased    0.441932   \n",
              "                                electra-large-discriminator     0.519718   \n",
              "                                electra-large-discriminator     0.519718   \n",
              "                                gpt2-medium                     0.535956   \n",
              "                                gpt2-medium                     0.535956   \n",
              "                                mGPT                            0.427459   \n",
              "                                mGPT                            0.427459   \n",
              "                                mdeberta-v3-base                0.401312   \n",
              "                                mdeberta-v3-base                0.401312   \n",
              "                                roberta-large-openai-detector   0.442067   \n",
              "                                roberta-large-openai-detector   0.442067   \n",
              "                                xlm-roberta-large               0.487835   \n",
              "                                xlm-roberta-large               0.487835   \n",
              "es             all              bert-base-multilingual-cased    0.762353   \n",
              "                                electra-large-discriminator     0.441124   \n",
              "                                gpt2-medium                     0.645790   \n",
              "                                mGPT                            0.728480   \n",
              "                                mdeberta-v3-base                0.761112   \n",
              "                                roberta-large-openai-detector   0.713048   \n",
              "                                xlm-roberta-large               0.813617   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased    0.515693   \n",
              "                                bert-base-multilingual-cased    0.515693   \n",
              "                                electra-large-discriminator     0.536341   \n",
              "                                electra-large-discriminator     0.536341   \n",
              "                                gpt2-medium                     0.571407   \n",
              "                                gpt2-medium                     0.571407   \n",
              "                                mGPT                            0.611429   \n",
              "                                mGPT                            0.611429   \n",
              "                                mdeberta-v3-base                0.476565   \n",
              "                                mdeberta-v3-base                0.476565   \n",
              "                                roberta-large-openai-detector   0.525403   \n",
              "                                roberta-large-openai-detector   0.525403   \n",
              "                                xlm-roberta-large               0.427914   \n",
              "                                xlm-roberta-large               0.427914   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased    0.347307   \n",
              "                                bert-base-multilingual-cased    0.347307   \n",
              "                                electra-large-discriminator     0.454531   \n",
              "                                electra-large-discriminator     0.454531   \n",
              "                                gpt2-medium                     0.529858   \n",
              "                                gpt2-medium                     0.529858   \n",
              "                                mGPT                            0.393437   \n",
              "                                mGPT                            0.393437   \n",
              "                                mdeberta-v3-base                0.347225   \n",
              "                                mdeberta-v3-base                0.347225   \n",
              "                                roberta-large-openai-detector   0.446844   \n",
              "                                roberta-large-openai-detector   0.446844   \n",
              "                                xlm-roberta-large               0.381634   \n",
              "                                xlm-roberta-large               0.381634   \n",
              "               gpt-4            bert-base-multilingual-cased    0.428534   \n",
              "                                bert-base-multilingual-cased    0.428534   \n",
              "                                electra-large-discriminator     0.455306   \n",
              "                                electra-large-discriminator     0.455306   \n",
              "                                gpt2-medium                     0.517202   \n",
              "                                gpt2-medium                     0.517202   \n",
              "                                mGPT                            0.419598   \n",
              "                                mGPT                            0.419598   \n",
              "                                mdeberta-v3-base                0.357258   \n",
              "                                mdeberta-v3-base                0.357258   \n",
              "                                roberta-large-openai-detector   0.423075   \n",
              "                                roberta-large-openai-detector   0.423075   \n",
              "                                xlm-roberta-large               0.369487   \n",
              "                                xlm-roberta-large               0.369487   \n",
              "               llama-65b        bert-base-multilingual-cased    0.821421   \n",
              "                                bert-base-multilingual-cased    0.821421   \n",
              "                                electra-large-discriminator     0.718008   \n",
              "                                electra-large-discriminator     0.718008   \n",
              "                                gpt2-medium                     0.740466   \n",
              "                                gpt2-medium                     0.740466   \n",
              "                                mGPT                            0.694319   \n",
              "                                mGPT                            0.694319   \n",
              "                                mdeberta-v3-base                0.867849   \n",
              "                                mdeberta-v3-base                0.867849   \n",
              "                                roberta-large-openai-detector   0.660175   \n",
              "                                roberta-large-openai-detector   0.660175   \n",
              "                                xlm-roberta-large               0.856514   \n",
              "                                xlm-roberta-large               0.856514   \n",
              "               opt-66b          bert-base-multilingual-cased    0.747646   \n",
              "                                bert-base-multilingual-cased    0.747646   \n",
              "                                electra-large-discriminator     0.650425   \n",
              "                                electra-large-discriminator     0.650425   \n",
              "                                gpt2-medium                     0.610230   \n",
              "                                gpt2-medium                     0.610230   \n",
              "                                mGPT                            0.569991   \n",
              "                                mGPT                            0.569991   \n",
              "                                mdeberta-v3-base                0.802689   \n",
              "                                mdeberta-v3-base                0.802689   \n",
              "                                roberta-large-openai-detector   0.607509   \n",
              "                                roberta-large-openai-detector   0.607509   \n",
              "                                xlm-roberta-large               0.706596   \n",
              "                                xlm-roberta-large               0.706596   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased    0.765078   \n",
              "                                bert-base-multilingual-cased    0.765078   \n",
              "                                electra-large-discriminator     0.655723   \n",
              "                                electra-large-discriminator     0.655723   \n",
              "                                gpt2-medium                     0.571864   \n",
              "                                gpt2-medium                     0.571864   \n",
              "                                mGPT                            0.738389   \n",
              "                                mGPT                            0.738389   \n",
              "                                mdeberta-v3-base                0.569028   \n",
              "                                mdeberta-v3-base                0.569028   \n",
              "                                roberta-large-openai-detector   0.571362   \n",
              "                                roberta-large-openai-detector   0.571362   \n",
              "                                xlm-roberta-large               0.521044   \n",
              "                                xlm-roberta-large               0.521044   \n",
              "               text-davinci-003 bert-base-multilingual-cased    0.383222   \n",
              "                                bert-base-multilingual-cased    0.383222   \n",
              "                                electra-large-discriminator     0.434165   \n",
              "                                electra-large-discriminator     0.434165   \n",
              "                                gpt2-medium                     0.490427   \n",
              "                                gpt2-medium                     0.490427   \n",
              "                                mGPT                            0.402494   \n",
              "                                mGPT                            0.402494   \n",
              "                                mdeberta-v3-base                0.361963   \n",
              "                                mdeberta-v3-base                0.361963   \n",
              "                                roberta-large-openai-detector   0.452305   \n",
              "                                roberta-large-openai-detector   0.452305   \n",
              "                                xlm-roberta-large               0.389762   \n",
              "                                xlm-roberta-large               0.389762   \n",
              "               vicuna-13b       bert-base-multilingual-cased    0.481860   \n",
              "                                bert-base-multilingual-cased    0.481860   \n",
              "                                electra-large-discriminator     0.558606   \n",
              "                                electra-large-discriminator     0.558606   \n",
              "                                gpt2-medium                     0.596212   \n",
              "                                gpt2-medium                     0.596212   \n",
              "                                mGPT                            0.600672   \n",
              "                                mGPT                            0.600672   \n",
              "                                mdeberta-v3-base                0.479286   \n",
              "                                mdeberta-v3-base                0.479286   \n",
              "                                roberta-large-openai-detector   0.536251   \n",
              "                                roberta-large-openai-detector   0.536251   \n",
              "                                xlm-roberta-large               0.509451   \n",
              "                                xlm-roberta-large               0.509451   \n",
              "ru             all              bert-base-multilingual-cased    0.591222   \n",
              "                                electra-large-discriminator     0.333641   \n",
              "                                gpt2-medium                     0.590907   \n",
              "                                mGPT                            0.682123   \n",
              "                                mdeberta-v3-base                0.674720   \n",
              "                                roberta-large-openai-detector   0.446650   \n",
              "                                xlm-roberta-large               0.778175   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased    0.490572   \n",
              "                                bert-base-multilingual-cased    0.490572   \n",
              "                                electra-large-discriminator     0.638808   \n",
              "                                electra-large-discriminator     0.638808   \n",
              "                                gpt2-medium                     0.574986   \n",
              "                                gpt2-medium                     0.574986   \n",
              "                                mGPT                            0.569842   \n",
              "                                mGPT                            0.569842   \n",
              "                                mdeberta-v3-base                0.578751   \n",
              "                                mdeberta-v3-base                0.578751   \n",
              "                                roberta-large-openai-detector   0.488967   \n",
              "                                roberta-large-openai-detector   0.488967   \n",
              "                                xlm-roberta-large               0.511200   \n",
              "                                xlm-roberta-large               0.511200   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased    0.380810   \n",
              "                                bert-base-multilingual-cased    0.380810   \n",
              "                                electra-large-discriminator     0.408386   \n",
              "                                electra-large-discriminator     0.408386   \n",
              "                                gpt2-medium                     0.439461   \n",
              "                                gpt2-medium                     0.439461   \n",
              "                                mGPT                            0.382755   \n",
              "                                mGPT                            0.382755   \n",
              "                                mdeberta-v3-base                0.402492   \n",
              "                                mdeberta-v3-base                0.402492   \n",
              "                                roberta-large-openai-detector   0.461201   \n",
              "                                roberta-large-openai-detector   0.461201   \n",
              "                                xlm-roberta-large               0.431128   \n",
              "                                xlm-roberta-large               0.431128   \n",
              "               gpt-4            bert-base-multilingual-cased    0.409385   \n",
              "                                bert-base-multilingual-cased    0.409385   \n",
              "                                electra-large-discriminator     0.482879   \n",
              "                                electra-large-discriminator     0.482879   \n",
              "                                gpt2-medium                     0.480221   \n",
              "                                gpt2-medium                     0.480221   \n",
              "                                mGPT                            0.399595   \n",
              "                                mGPT                            0.399595   \n",
              "                                mdeberta-v3-base                0.376977   \n",
              "                                mdeberta-v3-base                0.376977   \n",
              "                                roberta-large-openai-detector   0.432247   \n",
              "                                roberta-large-openai-detector   0.432247   \n",
              "                                xlm-roberta-large               0.385470   \n",
              "                                xlm-roberta-large               0.385470   \n",
              "               llama-65b        bert-base-multilingual-cased    0.707122   \n",
              "                                bert-base-multilingual-cased    0.707122   \n",
              "                                electra-large-discriminator     0.500342   \n",
              "                                electra-large-discriminator     0.500342   \n",
              "                                gpt2-medium                     0.650575   \n",
              "                                gpt2-medium                     0.650575   \n",
              "                                mGPT                            0.592451   \n",
              "                                mGPT                            0.592451   \n",
              "                                mdeberta-v3-base                0.726336   \n",
              "                                mdeberta-v3-base                0.726336   \n",
              "                                roberta-large-openai-detector   0.535198   \n",
              "                                roberta-large-openai-detector   0.535198   \n",
              "                                xlm-roberta-large               0.744336   \n",
              "                                xlm-roberta-large               0.744336   \n",
              "               opt-66b          bert-base-multilingual-cased    0.794267   \n",
              "                                bert-base-multilingual-cased    0.794267   \n",
              "                                electra-large-discriminator     0.461874   \n",
              "                                electra-large-discriminator     0.461874   \n",
              "                                gpt2-medium                     0.593218   \n",
              "                                gpt2-medium                     0.593218   \n",
              "                                mGPT                            0.648147   \n",
              "                                mGPT                            0.648147   \n",
              "                                mdeberta-v3-base                0.461495   \n",
              "                                mdeberta-v3-base                0.461495   \n",
              "                                roberta-large-openai-detector   0.451066   \n",
              "                                roberta-large-openai-detector   0.451066   \n",
              "                                xlm-roberta-large               0.609225   \n",
              "                                xlm-roberta-large               0.609225   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased    0.377582   \n",
              "                                bert-base-multilingual-cased    0.377582   \n",
              "                                electra-large-discriminator     0.510998   \n",
              "                                electra-large-discriminator     0.510998   \n",
              "                                gpt2-medium                     0.522873   \n",
              "                                gpt2-medium                     0.522873   \n",
              "                                mGPT                            0.432819   \n",
              "                                mGPT                            0.432819   \n",
              "                                mdeberta-v3-base                0.361107   \n",
              "                                mdeberta-v3-base                0.361107   \n",
              "                                roberta-large-openai-detector   0.477338   \n",
              "                                roberta-large-openai-detector   0.477338   \n",
              "                                xlm-roberta-large               0.379103   \n",
              "                                xlm-roberta-large               0.379103   \n",
              "               text-davinci-003 bert-base-multilingual-cased    0.345585   \n",
              "                                bert-base-multilingual-cased    0.345585   \n",
              "                                electra-large-discriminator     0.495071   \n",
              "                                electra-large-discriminator     0.495071   \n",
              "                                gpt2-medium                     0.485981   \n",
              "                                gpt2-medium                     0.485981   \n",
              "                                mGPT                            0.372177   \n",
              "                                mGPT                            0.372177   \n",
              "                                mdeberta-v3-base                0.361779   \n",
              "                                mdeberta-v3-base                0.361779   \n",
              "                                roberta-large-openai-detector   0.415617   \n",
              "                                roberta-large-openai-detector   0.415617   \n",
              "                                xlm-roberta-large               0.362306   \n",
              "                                xlm-roberta-large               0.362306   \n",
              "               vicuna-13b       bert-base-multilingual-cased    0.495575   \n",
              "                                bert-base-multilingual-cased    0.495575   \n",
              "                                electra-large-discriminator     0.485209   \n",
              "                                electra-large-discriminator     0.485209   \n",
              "                                gpt2-medium                     0.645866   \n",
              "                                gpt2-medium                     0.645866   \n",
              "                                mGPT                            0.587543   \n",
              "                                mGPT                            0.587543   \n",
              "                                mdeberta-v3-base                0.607534   \n",
              "                                mdeberta-v3-base                0.607534   \n",
              "                                roberta-large-openai-detector   0.520642   \n",
              "                                roberta-large-openai-detector   0.520642   \n",
              "                                xlm-roberta-large               0.533176   \n",
              "                                xlm-roberta-large               0.533176   \n",
              "all            all              bert-base-multilingual-cased    0.733861   \n",
              "                                electra-large-discriminator     0.439717   \n",
              "                                gpt2-medium                     0.662629   \n",
              "                                mGPT                            0.726538   \n",
              "                                mdeberta-v3-base                0.859953   \n",
              "                                roberta-large-openai-detector   0.766711   \n",
              "                                xlm-roberta-large               0.776027   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased    0.503212   \n",
              "                                bert-base-multilingual-cased    0.503212   \n",
              "                                electra-large-discriminator     0.448890   \n",
              "                                electra-large-discriminator     0.448890   \n",
              "                                gpt2-medium                     0.565056   \n",
              "                                gpt2-medium                     0.565056   \n",
              "                                mGPT                            0.573566   \n",
              "                                mGPT                            0.573566   \n",
              "                                mdeberta-v3-base                0.528942   \n",
              "                                mdeberta-v3-base                0.528942   \n",
              "                                roberta-large-openai-detector   0.513108   \n",
              "                                roberta-large-openai-detector   0.513108   \n",
              "                                xlm-roberta-large               0.464140   \n",
              "                                xlm-roberta-large               0.464140   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased    0.365711   \n",
              "                                bert-base-multilingual-cased    0.365711   \n",
              "                                electra-large-discriminator     0.359195   \n",
              "                                electra-large-discriminator     0.359195   \n",
              "                                gpt2-medium                     0.367576   \n",
              "                                gpt2-medium                     0.367576   \n",
              "                                mGPT                            0.359979   \n",
              "                                mGPT                            0.359979   \n",
              "                                mdeberta-v3-base                0.361438   \n",
              "                                mdeberta-v3-base                0.361438   \n",
              "                                roberta-large-openai-detector   0.365184   \n",
              "                                roberta-large-openai-detector   0.365184   \n",
              "                                xlm-roberta-large               0.359891   \n",
              "                                xlm-roberta-large               0.359891   \n",
              "               gpt-4            bert-base-multilingual-cased    0.377686   \n",
              "                                bert-base-multilingual-cased    0.377686   \n",
              "                                electra-large-discriminator     0.373655   \n",
              "                                electra-large-discriminator     0.373655   \n",
              "                                gpt2-medium                     0.410286   \n",
              "                                gpt2-medium                     0.410286   \n",
              "                                mGPT                            0.385132   \n",
              "                                mGPT                            0.385132   \n",
              "                                mdeberta-v3-base                0.356173   \n",
              "                                mdeberta-v3-base                0.356173   \n",
              "                                roberta-large-openai-detector   0.379720   \n",
              "                                roberta-large-openai-detector   0.379720   \n",
              "                                xlm-roberta-large               0.363593   \n",
              "                                xlm-roberta-large               0.363593   \n",
              "               llama-65b        bert-base-multilingual-cased    0.875430   \n",
              "                                bert-base-multilingual-cased    0.875430   \n",
              "                                electra-large-discriminator     0.894659   \n",
              "                                electra-large-discriminator     0.894659   \n",
              "                                gpt2-medium                     0.904397   \n",
              "                                gpt2-medium                     0.904397   \n",
              "                                mGPT                            0.890213   \n",
              "                                mGPT                            0.890213   \n",
              "                                mdeberta-v3-base                0.870969   \n",
              "                                mdeberta-v3-base                0.870969   \n",
              "                                roberta-large-openai-detector   0.892680   \n",
              "                                roberta-large-openai-detector   0.892680   \n",
              "                                xlm-roberta-large               0.850675   \n",
              "                                xlm-roberta-large               0.850675   \n",
              "               opt-66b          bert-base-multilingual-cased    0.858509   \n",
              "                                bert-base-multilingual-cased    0.858509   \n",
              "                                electra-large-discriminator     0.757972   \n",
              "                                electra-large-discriminator     0.757972   \n",
              "                                gpt2-medium                     0.647234   \n",
              "                                gpt2-medium                     0.647234   \n",
              "                                mGPT                            0.795716   \n",
              "                                mGPT                            0.795716   \n",
              "                                mdeberta-v3-base                0.875216   \n",
              "                                mdeberta-v3-base                0.875216   \n",
              "                                roberta-large-openai-detector   0.672685   \n",
              "                                roberta-large-openai-detector   0.672685   \n",
              "                                xlm-roberta-large               0.886844   \n",
              "                                xlm-roberta-large               0.886844   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased    0.856475   \n",
              "                                bert-base-multilingual-cased    0.856475   \n",
              "                                electra-large-discriminator     0.772747   \n",
              "                                electra-large-discriminator     0.772747   \n",
              "                                gpt2-medium                     0.634695   \n",
              "                                gpt2-medium                     0.634695   \n",
              "                                mGPT                            0.774886   \n",
              "                                mGPT                            0.774886   \n",
              "                                mdeberta-v3-base                0.818042   \n",
              "                                mdeberta-v3-base                0.818042   \n",
              "                                roberta-large-openai-detector   0.540688   \n",
              "                                roberta-large-openai-detector   0.540688   \n",
              "                                xlm-roberta-large               0.572232   \n",
              "                                xlm-roberta-large               0.572232   \n",
              "               text-davinci-003 bert-base-multilingual-cased    0.356377   \n",
              "                                bert-base-multilingual-cased    0.356377   \n",
              "                                electra-large-discriminator     0.385329   \n",
              "                                electra-large-discriminator     0.385329   \n",
              "                                gpt2-medium                     0.443965   \n",
              "                                gpt2-medium                     0.443965   \n",
              "                                mGPT                            0.364982   \n",
              "                                mGPT                            0.364982   \n",
              "                                mdeberta-v3-base                0.364060   \n",
              "                                mdeberta-v3-base                0.364060   \n",
              "                                roberta-large-openai-detector   0.447690   \n",
              "                                roberta-large-openai-detector   0.447690   \n",
              "                                xlm-roberta-large               0.369830   \n",
              "                                xlm-roberta-large               0.369830   \n",
              "               vicuna-13b       bert-base-multilingual-cased    0.486941   \n",
              "                                bert-base-multilingual-cased    0.486941   \n",
              "                                electra-large-discriminator     0.502353   \n",
              "                                electra-large-discriminator     0.502353   \n",
              "                                gpt2-medium                     0.475326   \n",
              "                                gpt2-medium                     0.475326   \n",
              "                                mGPT                            0.552195   \n",
              "                                mGPT                            0.552195   \n",
              "                                mdeberta-v3-base                0.485060   \n",
              "                                mdeberta-v3-base                0.485060   \n",
              "                                roberta-large-openai-detector   0.566316   \n",
              "                                roberta-large-openai-detector   0.566316   \n",
              "                                xlm-roberta-large               0.532225   \n",
              "                                xlm-roberta-large               0.532225   \n",
              "en3            all              bert-base-multilingual-cased    0.394164   \n",
              "                                electra-large-discriminator     0.412369   \n",
              "                                gpt2-medium                     0.590560   \n",
              "                                mGPT                            0.570724   \n",
              "                                mdeberta-v3-base                0.452209   \n",
              "                                roberta-large-openai-detector   0.503919   \n",
              "                                xlm-roberta-large               0.504289   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased    0.496862   \n",
              "                                bert-base-multilingual-cased    0.496862   \n",
              "                                electra-large-discriminator     0.465029   \n",
              "                                electra-large-discriminator     0.465029   \n",
              "                                gpt2-medium                     0.519389   \n",
              "                                gpt2-medium                     0.519389   \n",
              "                                mGPT                            0.512682   \n",
              "                                mGPT                            0.512682   \n",
              "                                mdeberta-v3-base                0.417425   \n",
              "                                mdeberta-v3-base                0.417425   \n",
              "                                roberta-large-openai-detector   0.434277   \n",
              "                                roberta-large-openai-detector   0.434277   \n",
              "                                xlm-roberta-large               0.453519   \n",
              "                                xlm-roberta-large               0.453519   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased    0.374024   \n",
              "                                bert-base-multilingual-cased    0.374024   \n",
              "                                electra-large-discriminator     0.397803   \n",
              "                                electra-large-discriminator     0.397803   \n",
              "                                gpt2-medium                     0.440530   \n",
              "                                gpt2-medium                     0.440530   \n",
              "                                mGPT                            0.351787   \n",
              "                                mGPT                            0.351787   \n",
              "                                mdeberta-v3-base                0.344195   \n",
              "                                mdeberta-v3-base                0.344195   \n",
              "                                roberta-large-openai-detector   0.464334   \n",
              "                                roberta-large-openai-detector   0.464334   \n",
              "                                xlm-roberta-large               0.372926   \n",
              "                                xlm-roberta-large               0.372926   \n",
              "               gpt-4            bert-base-multilingual-cased    0.372454   \n",
              "                                bert-base-multilingual-cased    0.372454   \n",
              "                                electra-large-discriminator     0.399115   \n",
              "                                electra-large-discriminator     0.399115   \n",
              "                                gpt2-medium                     0.464101   \n",
              "                                gpt2-medium                     0.464101   \n",
              "                                mGPT                            0.358096   \n",
              "                                mGPT                            0.358096   \n",
              "                                mdeberta-v3-base                0.349302   \n",
              "                                mdeberta-v3-base                0.349302   \n",
              "                                roberta-large-openai-detector   0.396930   \n",
              "                                roberta-large-openai-detector   0.396930   \n",
              "                                xlm-roberta-large               0.399539   \n",
              "                                xlm-roberta-large               0.399539   \n",
              "               llama-65b        bert-base-multilingual-cased    0.724555   \n",
              "                                bert-base-multilingual-cased    0.724555   \n",
              "                                electra-large-discriminator     0.420318   \n",
              "                                electra-large-discriminator     0.420318   \n",
              "                                gpt2-medium                     0.498253   \n",
              "                                gpt2-medium                     0.498253   \n",
              "                                mGPT                            0.589714   \n",
              "                                mGPT                            0.589714   \n",
              "                                mdeberta-v3-base                0.775922   \n",
              "                                mdeberta-v3-base                0.775922   \n",
              "                                roberta-large-openai-detector   0.426586   \n",
              "                                roberta-large-openai-detector   0.426586   \n",
              "                                xlm-roberta-large               0.628387   \n",
              "                                xlm-roberta-large               0.628387   \n",
              "               opt-66b          bert-base-multilingual-cased    0.697701   \n",
              "                                bert-base-multilingual-cased    0.697701   \n",
              "                                electra-large-discriminator     0.707458   \n",
              "                                electra-large-discriminator     0.707458   \n",
              "                                gpt2-medium                     0.705766   \n",
              "                                gpt2-medium                     0.705766   \n",
              "                                mGPT                            0.637743   \n",
              "                                mGPT                            0.637743   \n",
              "                                mdeberta-v3-base                0.582629   \n",
              "                                mdeberta-v3-base                0.582629   \n",
              "                                roberta-large-openai-detector   0.509336   \n",
              "                                roberta-large-openai-detector   0.509336   \n",
              "                                xlm-roberta-large               0.688343   \n",
              "                                xlm-roberta-large               0.688343   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased    0.701585   \n",
              "                                bert-base-multilingual-cased    0.701585   \n",
              "                                electra-large-discriminator     0.641222   \n",
              "                                electra-large-discriminator     0.641222   \n",
              "                                gpt2-medium                     0.678519   \n",
              "                                gpt2-medium                     0.678519   \n",
              "                                mGPT                            0.634501   \n",
              "                                mGPT                            0.634501   \n",
              "                                mdeberta-v3-base                0.746017   \n",
              "                                mdeberta-v3-base                0.746017   \n",
              "                                roberta-large-openai-detector   0.592852   \n",
              "                                roberta-large-openai-detector   0.592852   \n",
              "                                xlm-roberta-large               0.746239   \n",
              "                                xlm-roberta-large               0.746239   \n",
              "               text-davinci-003 bert-base-multilingual-cased    0.352382   \n",
              "                                bert-base-multilingual-cased    0.352382   \n",
              "                                electra-large-discriminator     0.360770   \n",
              "                                electra-large-discriminator     0.360770   \n",
              "                                gpt2-medium                     0.475738   \n",
              "                                gpt2-medium                     0.475738   \n",
              "                                mGPT                            0.353695   \n",
              "                                mGPT                            0.353695   \n",
              "                                mdeberta-v3-base                0.320987   \n",
              "                                mdeberta-v3-base                0.320987   \n",
              "                                roberta-large-openai-detector   0.401836   \n",
              "                                roberta-large-openai-detector   0.401836   \n",
              "                                xlm-roberta-large               0.367216   \n",
              "                                xlm-roberta-large               0.367216   \n",
              "               vicuna-13b       bert-base-multilingual-cased    0.538549   \n",
              "                                bert-base-multilingual-cased    0.538549   \n",
              "                                electra-large-discriminator     0.529225   \n",
              "                                electra-large-discriminator     0.529225   \n",
              "                                gpt2-medium                     0.545911   \n",
              "                                gpt2-medium                     0.545911   \n",
              "                                mGPT                            0.487444   \n",
              "                                mGPT                            0.487444   \n",
              "                                mdeberta-v3-base                0.448960   \n",
              "                                mdeberta-v3-base                0.448960   \n",
              "                                roberta-large-openai-detector   0.432668   \n",
              "                                roberta-large-openai-detector   0.432668   \n",
              "                                xlm-roberta-large               0.437325   \n",
              "                                xlm-roberta-large               0.437325   \n",
              "\n",
              "                                                                opt-66b  \\\n",
              "Train Language Train LLM        Model                                     \n",
              "en             all              bert-base-multilingual-cased   0.545696   \n",
              "                                electra-large-discriminator    0.560828   \n",
              "                                gpt2-medium                    0.555311   \n",
              "                                mGPT                           0.496428   \n",
              "                                mdeberta-v3-base               0.520285   \n",
              "                                roberta-large-openai-detector  0.425894   \n",
              "                                xlm-roberta-large              0.437878   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased   0.431465   \n",
              "                                bert-base-multilingual-cased   0.431465   \n",
              "                                electra-large-discriminator    0.459469   \n",
              "                                electra-large-discriminator    0.459469   \n",
              "                                gpt2-medium                    0.440140   \n",
              "                                gpt2-medium                    0.440140   \n",
              "                                mGPT                           0.438631   \n",
              "                                mGPT                           0.438631   \n",
              "                                mdeberta-v3-base               0.328532   \n",
              "                                mdeberta-v3-base               0.328532   \n",
              "                                roberta-large-openai-detector  0.527190   \n",
              "                                roberta-large-openai-detector  0.527190   \n",
              "                                xlm-roberta-large              0.465985   \n",
              "                                xlm-roberta-large              0.465985   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased   0.402321   \n",
              "                                bert-base-multilingual-cased   0.402321   \n",
              "                                electra-large-discriminator    0.451089   \n",
              "                                electra-large-discriminator    0.451089   \n",
              "                                gpt2-medium                    0.367236   \n",
              "                                gpt2-medium                    0.367236   \n",
              "                                mGPT                           0.387935   \n",
              "                                mGPT                           0.387935   \n",
              "                                mdeberta-v3-base               0.297651   \n",
              "                                mdeberta-v3-base               0.297651   \n",
              "                                roberta-large-openai-detector  0.546130   \n",
              "                                roberta-large-openai-detector  0.546130   \n",
              "                                xlm-roberta-large              0.405500   \n",
              "                                xlm-roberta-large              0.405500   \n",
              "               gpt-4            bert-base-multilingual-cased   0.374916   \n",
              "                                bert-base-multilingual-cased   0.374916   \n",
              "                                electra-large-discriminator    0.375672   \n",
              "                                electra-large-discriminator    0.375672   \n",
              "                                gpt2-medium                    0.362785   \n",
              "                                gpt2-medium                    0.362785   \n",
              "                                mGPT                           0.425517   \n",
              "                                mGPT                           0.425517   \n",
              "                                mdeberta-v3-base               0.338509   \n",
              "                                mdeberta-v3-base               0.338509   \n",
              "                                roberta-large-openai-detector  0.508470   \n",
              "                                roberta-large-openai-detector  0.508470   \n",
              "                                xlm-roberta-large              0.382052   \n",
              "                                xlm-roberta-large              0.382052   \n",
              "               llama-65b        bert-base-multilingual-cased   0.601528   \n",
              "                                bert-base-multilingual-cased   0.601528   \n",
              "                                electra-large-discriminator    0.510785   \n",
              "                                electra-large-discriminator    0.510785   \n",
              "                                gpt2-medium                    0.495689   \n",
              "                                gpt2-medium                    0.495689   \n",
              "                                mGPT                           0.533706   \n",
              "                                mGPT                           0.533706   \n",
              "                                mdeberta-v3-base               0.607457   \n",
              "                                mdeberta-v3-base               0.607457   \n",
              "                                roberta-large-openai-detector  0.409882   \n",
              "                                roberta-large-openai-detector  0.409882   \n",
              "                                xlm-roberta-large              0.614469   \n",
              "                                xlm-roberta-large              0.614469   \n",
              "               opt-66b          bert-base-multilingual-cased   0.562338   \n",
              "                                bert-base-multilingual-cased   0.562338   \n",
              "                                electra-large-discriminator    0.597358   \n",
              "                                electra-large-discriminator    0.597358   \n",
              "                                gpt2-medium                    0.537868   \n",
              "                                gpt2-medium                    0.537868   \n",
              "                                mGPT                           0.534366   \n",
              "                                mGPT                           0.534366   \n",
              "                                mdeberta-v3-base               0.596935   \n",
              "                                mdeberta-v3-base               0.596935   \n",
              "                                roberta-large-openai-detector  0.470761   \n",
              "                                roberta-large-openai-detector  0.470761   \n",
              "                                xlm-roberta-large              0.611580   \n",
              "                                xlm-roberta-large              0.611580   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased   0.545035   \n",
              "                                bert-base-multilingual-cased   0.545035   \n",
              "                                electra-large-discriminator    0.565670   \n",
              "                                electra-large-discriminator    0.565670   \n",
              "                                gpt2-medium                    0.549681   \n",
              "                                gpt2-medium                    0.549681   \n",
              "                                mGPT                           0.590380   \n",
              "                                mGPT                           0.590380   \n",
              "                                mdeberta-v3-base               0.813907   \n",
              "                                mdeberta-v3-base               0.813907   \n",
              "                                roberta-large-openai-detector  0.648384   \n",
              "                                roberta-large-openai-detector  0.648384   \n",
              "                                xlm-roberta-large              0.682991   \n",
              "                                xlm-roberta-large              0.682991   \n",
              "               text-davinci-003 bert-base-multilingual-cased   0.387589   \n",
              "                                bert-base-multilingual-cased   0.387589   \n",
              "                                electra-large-discriminator    0.461366   \n",
              "                                electra-large-discriminator    0.461366   \n",
              "                                gpt2-medium                    0.365072   \n",
              "                                gpt2-medium                    0.365072   \n",
              "                                mGPT                           0.410279   \n",
              "                                mGPT                           0.410279   \n",
              "                                mdeberta-v3-base               0.324835   \n",
              "                                mdeberta-v3-base               0.324835   \n",
              "                                roberta-large-openai-detector  0.521642   \n",
              "                                roberta-large-openai-detector  0.521642   \n",
              "                                xlm-roberta-large              0.465613   \n",
              "                                xlm-roberta-large              0.465613   \n",
              "               vicuna-13b       bert-base-multilingual-cased   0.416745   \n",
              "                                bert-base-multilingual-cased   0.416745   \n",
              "                                electra-large-discriminator    0.516087   \n",
              "                                electra-large-discriminator    0.516087   \n",
              "                                gpt2-medium                    0.499351   \n",
              "                                gpt2-medium                    0.499351   \n",
              "                                mGPT                           0.470568   \n",
              "                                mGPT                           0.470568   \n",
              "                                mdeberta-v3-base               0.279484   \n",
              "                                mdeberta-v3-base               0.279484   \n",
              "                                roberta-large-openai-detector  0.562276   \n",
              "                                roberta-large-openai-detector  0.562276   \n",
              "                                xlm-roberta-large              0.489583   \n",
              "                                xlm-roberta-large              0.489583   \n",
              "es             all              bert-base-multilingual-cased   0.683472   \n",
              "                                electra-large-discriminator    0.435071   \n",
              "                                gpt2-medium                    0.565513   \n",
              "                                mGPT                           0.641124   \n",
              "                                mdeberta-v3-base               0.737539   \n",
              "                                roberta-large-openai-detector  0.655543   \n",
              "                                xlm-roberta-large              0.761340   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased   0.452312   \n",
              "                                bert-base-multilingual-cased   0.452312   \n",
              "                                electra-large-discriminator    0.468060   \n",
              "                                electra-large-discriminator    0.468060   \n",
              "                                gpt2-medium                    0.491556   \n",
              "                                gpt2-medium                    0.491556   \n",
              "                                mGPT                           0.531391   \n",
              "                                mGPT                           0.531391   \n",
              "                                mdeberta-v3-base               0.517264   \n",
              "                                mdeberta-v3-base               0.517264   \n",
              "                                roberta-large-openai-detector  0.476398   \n",
              "                                roberta-large-openai-detector  0.476398   \n",
              "                                xlm-roberta-large              0.435287   \n",
              "                                xlm-roberta-large              0.435287   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased   0.384162   \n",
              "                                bert-base-multilingual-cased   0.384162   \n",
              "                                electra-large-discriminator    0.445054   \n",
              "                                electra-large-discriminator    0.445054   \n",
              "                                gpt2-medium                    0.396593   \n",
              "                                gpt2-medium                    0.396593   \n",
              "                                mGPT                           0.409314   \n",
              "                                mGPT                           0.409314   \n",
              "                                mdeberta-v3-base               0.340518   \n",
              "                                mdeberta-v3-base               0.340518   \n",
              "                                roberta-large-openai-detector  0.425731   \n",
              "                                roberta-large-openai-detector  0.425731   \n",
              "                                xlm-roberta-large              0.381594   \n",
              "                                xlm-roberta-large              0.381594   \n",
              "               gpt-4            bert-base-multilingual-cased   0.425133   \n",
              "                                bert-base-multilingual-cased   0.425133   \n",
              "                                electra-large-discriminator    0.463157   \n",
              "                                electra-large-discriminator    0.463157   \n",
              "                                gpt2-medium                    0.371317   \n",
              "                                gpt2-medium                    0.371317   \n",
              "                                mGPT                           0.382275   \n",
              "                                mGPT                           0.382275   \n",
              "                                mdeberta-v3-base               0.347109   \n",
              "                                mdeberta-v3-base               0.347109   \n",
              "                                roberta-large-openai-detector  0.386238   \n",
              "                                roberta-large-openai-detector  0.386238   \n",
              "                                xlm-roberta-large              0.366985   \n",
              "                                xlm-roberta-large              0.366985   \n",
              "               llama-65b        bert-base-multilingual-cased   0.668702   \n",
              "                                bert-base-multilingual-cased   0.668702   \n",
              "                                electra-large-discriminator    0.569875   \n",
              "                                electra-large-discriminator    0.569875   \n",
              "                                gpt2-medium                    0.532473   \n",
              "                                gpt2-medium                    0.532473   \n",
              "                                mGPT                           0.554199   \n",
              "                                mGPT                           0.554199   \n",
              "                                mdeberta-v3-base               0.771485   \n",
              "                                mdeberta-v3-base               0.771485   \n",
              "                                roberta-large-openai-detector  0.542222   \n",
              "                                roberta-large-openai-detector  0.542222   \n",
              "                                xlm-roberta-large              0.723967   \n",
              "                                xlm-roberta-large              0.723967   \n",
              "               opt-66b          bert-base-multilingual-cased   0.707425   \n",
              "                                bert-base-multilingual-cased   0.707425   \n",
              "                                electra-large-discriminator    0.600901   \n",
              "                                electra-large-discriminator    0.600901   \n",
              "                                gpt2-medium                    0.601931   \n",
              "                                gpt2-medium                    0.601931   \n",
              "                                mGPT                           0.597176   \n",
              "                                mGPT                           0.597176   \n",
              "                                mdeberta-v3-base               0.865094   \n",
              "                                mdeberta-v3-base               0.865094   \n",
              "                                roberta-large-openai-detector  0.668286   \n",
              "                                roberta-large-openai-detector  0.668286   \n",
              "                                xlm-roberta-large              0.705301   \n",
              "                                xlm-roberta-large              0.705301   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased   0.694672   \n",
              "                                bert-base-multilingual-cased   0.694672   \n",
              "                                electra-large-discriminator    0.584892   \n",
              "                                electra-large-discriminator    0.584892   \n",
              "                                gpt2-medium                    0.497204   \n",
              "                                gpt2-medium                    0.497204   \n",
              "                                mGPT                           0.677734   \n",
              "                                mGPT                           0.677734   \n",
              "                                mdeberta-v3-base               0.804597   \n",
              "                                mdeberta-v3-base               0.804597   \n",
              "                                roberta-large-openai-detector  0.649810   \n",
              "                                roberta-large-openai-detector  0.649810   \n",
              "                                xlm-roberta-large              0.675749   \n",
              "                                xlm-roberta-large              0.675749   \n",
              "               text-davinci-003 bert-base-multilingual-cased   0.445866   \n",
              "                                bert-base-multilingual-cased   0.445866   \n",
              "                                electra-large-discriminator    0.471284   \n",
              "                                electra-large-discriminator    0.471284   \n",
              "                                gpt2-medium                    0.482187   \n",
              "                                gpt2-medium                    0.482187   \n",
              "                                mGPT                           0.402151   \n",
              "                                mGPT                           0.402151   \n",
              "                                mdeberta-v3-base               0.355353   \n",
              "                                mdeberta-v3-base               0.355353   \n",
              "                                roberta-large-openai-detector  0.415002   \n",
              "                                roberta-large-openai-detector  0.415002   \n",
              "                                xlm-roberta-large              0.400079   \n",
              "                                xlm-roberta-large              0.400079   \n",
              "               vicuna-13b       bert-base-multilingual-cased   0.445165   \n",
              "                                bert-base-multilingual-cased   0.445165   \n",
              "                                electra-large-discriminator    0.474528   \n",
              "                                electra-large-discriminator    0.474528   \n",
              "                                gpt2-medium                    0.504030   \n",
              "                                gpt2-medium                    0.504030   \n",
              "                                mGPT                           0.511069   \n",
              "                                mGPT                           0.511069   \n",
              "                                mdeberta-v3-base               0.408273   \n",
              "                                mdeberta-v3-base               0.408273   \n",
              "                                roberta-large-openai-detector  0.478893   \n",
              "                                roberta-large-openai-detector  0.478893   \n",
              "                                xlm-roberta-large              0.506038   \n",
              "                                xlm-roberta-large              0.506038   \n",
              "ru             all              bert-base-multilingual-cased   0.570235   \n",
              "                                electra-large-discriminator    0.334533   \n",
              "                                gpt2-medium                    0.548742   \n",
              "                                mGPT                           0.619571   \n",
              "                                mdeberta-v3-base               0.666684   \n",
              "                                roberta-large-openai-detector  0.443981   \n",
              "                                xlm-roberta-large              0.733798   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased   0.489059   \n",
              "                                bert-base-multilingual-cased   0.489059   \n",
              "                                electra-large-discriminator    0.526478   \n",
              "                                electra-large-discriminator    0.526478   \n",
              "                                gpt2-medium                    0.385549   \n",
              "                                gpt2-medium                    0.385549   \n",
              "                                mGPT                           0.500403   \n",
              "                                mGPT                           0.500403   \n",
              "                                mdeberta-v3-base               0.743632   \n",
              "                                mdeberta-v3-base               0.743632   \n",
              "                                roberta-large-openai-detector  0.472350   \n",
              "                                roberta-large-openai-detector  0.472350   \n",
              "                                xlm-roberta-large              0.574964   \n",
              "                                xlm-roberta-large              0.574964   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased   0.436265   \n",
              "                                bert-base-multilingual-cased   0.436265   \n",
              "                                electra-large-discriminator    0.385726   \n",
              "                                electra-large-discriminator    0.385726   \n",
              "                                gpt2-medium                    0.362316   \n",
              "                                gpt2-medium                    0.362316   \n",
              "                                mGPT                           0.428041   \n",
              "                                mGPT                           0.428041   \n",
              "                                mdeberta-v3-base               0.379878   \n",
              "                                mdeberta-v3-base               0.379878   \n",
              "                                roberta-large-openai-detector  0.478841   \n",
              "                                roberta-large-openai-detector  0.478841   \n",
              "                                xlm-roberta-large              0.419648   \n",
              "                                xlm-roberta-large              0.419648   \n",
              "               gpt-4            bert-base-multilingual-cased   0.464901   \n",
              "                                bert-base-multilingual-cased   0.464901   \n",
              "                                electra-large-discriminator    0.424568   \n",
              "                                electra-large-discriminator    0.424568   \n",
              "                                gpt2-medium                    0.410579   \n",
              "                                gpt2-medium                    0.410579   \n",
              "                                mGPT                           0.408986   \n",
              "                                mGPT                           0.408986   \n",
              "                                mdeberta-v3-base               0.383364   \n",
              "                                mdeberta-v3-base               0.383364   \n",
              "                                roberta-large-openai-detector  0.460050   \n",
              "                                roberta-large-openai-detector  0.460050   \n",
              "                                xlm-roberta-large              0.377855   \n",
              "                                xlm-roberta-large              0.377855   \n",
              "               llama-65b        bert-base-multilingual-cased   0.624746   \n",
              "                                bert-base-multilingual-cased   0.624746   \n",
              "                                electra-large-discriminator    0.475367   \n",
              "                                electra-large-discriminator    0.475367   \n",
              "                                gpt2-medium                    0.479126   \n",
              "                                gpt2-medium                    0.479126   \n",
              "                                mGPT                           0.517087   \n",
              "                                mGPT                           0.517087   \n",
              "                                mdeberta-v3-base               0.712269   \n",
              "                                mdeberta-v3-base               0.712269   \n",
              "                                roberta-large-openai-detector  0.477820   \n",
              "                                roberta-large-openai-detector  0.477820   \n",
              "                                xlm-roberta-large              0.701213   \n",
              "                                xlm-roberta-large              0.701213   \n",
              "               opt-66b          bert-base-multilingual-cased   0.709071   \n",
              "                                bert-base-multilingual-cased   0.709071   \n",
              "                                electra-large-discriminator    0.475864   \n",
              "                                electra-large-discriminator    0.475864   \n",
              "                                gpt2-medium                    0.552630   \n",
              "                                gpt2-medium                    0.552630   \n",
              "                                mGPT                           0.679307   \n",
              "                                mGPT                           0.679307   \n",
              "                                mdeberta-v3-base               0.728873   \n",
              "                                mdeberta-v3-base               0.728873   \n",
              "                                roberta-large-openai-detector  0.488984   \n",
              "                                roberta-large-openai-detector  0.488984   \n",
              "                                xlm-roberta-large              0.794838   \n",
              "                                xlm-roberta-large              0.794838   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased   0.430485   \n",
              "                                bert-base-multilingual-cased   0.430485   \n",
              "                                electra-large-discriminator    0.550968   \n",
              "                                electra-large-discriminator    0.550968   \n",
              "                                gpt2-medium                    0.493475   \n",
              "                                gpt2-medium                    0.493475   \n",
              "                                mGPT                           0.476578   \n",
              "                                mGPT                           0.476578   \n",
              "                                mdeberta-v3-base               0.540236   \n",
              "                                mdeberta-v3-base               0.540236   \n",
              "                                roberta-large-openai-detector  0.519294   \n",
              "                                roberta-large-openai-detector  0.519294   \n",
              "                                xlm-roberta-large              0.559416   \n",
              "                                xlm-roberta-large              0.559416   \n",
              "               text-davinci-003 bert-base-multilingual-cased   0.379790   \n",
              "                                bert-base-multilingual-cased   0.379790   \n",
              "                                electra-large-discriminator    0.451824   \n",
              "                                electra-large-discriminator    0.451824   \n",
              "                                gpt2-medium                    0.445593   \n",
              "                                gpt2-medium                    0.445593   \n",
              "                                mGPT                           0.415079   \n",
              "                                mGPT                           0.415079   \n",
              "                                mdeberta-v3-base               0.603316   \n",
              "                                mdeberta-v3-base               0.603316   \n",
              "                                roberta-large-openai-detector  0.484497   \n",
              "                                roberta-large-openai-detector  0.484497   \n",
              "                                xlm-roberta-large              0.479156   \n",
              "                                xlm-roberta-large              0.479156   \n",
              "               vicuna-13b       bert-base-multilingual-cased   0.456960   \n",
              "                                bert-base-multilingual-cased   0.456960   \n",
              "                                electra-large-discriminator    0.477359   \n",
              "                                electra-large-discriminator    0.477359   \n",
              "                                gpt2-medium                    0.451901   \n",
              "                                gpt2-medium                    0.451901   \n",
              "                                mGPT                           0.546205   \n",
              "                                mGPT                           0.546205   \n",
              "                                mdeberta-v3-base               0.728884   \n",
              "                                mdeberta-v3-base               0.728884   \n",
              "                                roberta-large-openai-detector  0.487261   \n",
              "                                roberta-large-openai-detector  0.487261   \n",
              "                                xlm-roberta-large              0.593499   \n",
              "                                xlm-roberta-large              0.593499   \n",
              "all            all              bert-base-multilingual-cased   0.685665   \n",
              "                                electra-large-discriminator    0.427767   \n",
              "                                gpt2-medium                    0.590778   \n",
              "                                mGPT                           0.614153   \n",
              "                                mdeberta-v3-base               0.829534   \n",
              "                                roberta-large-openai-detector  0.736193   \n",
              "                                xlm-roberta-large              0.748398   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased   0.421225   \n",
              "                                bert-base-multilingual-cased   0.421225   \n",
              "                                electra-large-discriminator    0.403770   \n",
              "                                electra-large-discriminator    0.403770   \n",
              "                                gpt2-medium                    0.450887   \n",
              "                                gpt2-medium                    0.450887   \n",
              "                                mGPT                           0.462548   \n",
              "                                mGPT                           0.462548   \n",
              "                                mdeberta-v3-base               0.520888   \n",
              "                                mdeberta-v3-base               0.520888   \n",
              "                                roberta-large-openai-detector  0.545008   \n",
              "                                roberta-large-openai-detector  0.545008   \n",
              "                                xlm-roberta-large              0.431615   \n",
              "                                xlm-roberta-large              0.431615   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased   0.360731   \n",
              "                                bert-base-multilingual-cased   0.360731   \n",
              "                                electra-large-discriminator    0.360602   \n",
              "                                electra-large-discriminator    0.360602   \n",
              "                                gpt2-medium                    0.375207   \n",
              "                                gpt2-medium                    0.375207   \n",
              "                                mGPT                           0.362091   \n",
              "                                mGPT                           0.362091   \n",
              "                                mdeberta-v3-base               0.345353   \n",
              "                                mdeberta-v3-base               0.345353   \n",
              "                                roberta-large-openai-detector  0.376208   \n",
              "                                roberta-large-openai-detector  0.376208   \n",
              "                                xlm-roberta-large              0.358718   \n",
              "                                xlm-roberta-large              0.358718   \n",
              "               gpt-4            bert-base-multilingual-cased   0.368102   \n",
              "                                bert-base-multilingual-cased   0.368102   \n",
              "                                electra-large-discriminator    0.373050   \n",
              "                                electra-large-discriminator    0.373050   \n",
              "                                gpt2-medium                    0.417728   \n",
              "                                gpt2-medium                    0.417728   \n",
              "                                mGPT                           0.382936   \n",
              "                                mGPT                           0.382936   \n",
              "                                mdeberta-v3-base               0.352552   \n",
              "                                mdeberta-v3-base               0.352552   \n",
              "                                roberta-large-openai-detector  0.403679   \n",
              "                                roberta-large-openai-detector  0.403679   \n",
              "                                xlm-roberta-large              0.356027   \n",
              "                                xlm-roberta-large              0.356027   \n",
              "               llama-65b        bert-base-multilingual-cased   0.709120   \n",
              "                                bert-base-multilingual-cased   0.709120   \n",
              "                                electra-large-discriminator    0.594474   \n",
              "                                electra-large-discriminator    0.594474   \n",
              "                                gpt2-medium                    0.629567   \n",
              "                                gpt2-medium                    0.629567   \n",
              "                                mGPT                           0.657316   \n",
              "                                mGPT                           0.657316   \n",
              "                                mdeberta-v3-base               0.792183   \n",
              "                                mdeberta-v3-base               0.792183   \n",
              "                                roberta-large-openai-detector  0.718293   \n",
              "                                roberta-large-openai-detector  0.718293   \n",
              "                                xlm-roberta-large              0.733263   \n",
              "                                xlm-roberta-large              0.733263   \n",
              "               opt-66b          bert-base-multilingual-cased   0.800106   \n",
              "                                bert-base-multilingual-cased   0.800106   \n",
              "                                electra-large-discriminator    0.666038   \n",
              "                                electra-large-discriminator    0.666038   \n",
              "                                gpt2-medium                    0.759248   \n",
              "                                gpt2-medium                    0.759248   \n",
              "                                mGPT                           0.851116   \n",
              "                                mGPT                           0.851116   \n",
              "                                mdeberta-v3-base               0.908434   \n",
              "                                mdeberta-v3-base               0.908434   \n",
              "                                roberta-large-openai-detector  0.814549   \n",
              "                                roberta-large-openai-detector  0.814549   \n",
              "                                xlm-roberta-large              0.914877   \n",
              "                                xlm-roberta-large              0.914877   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased   0.814461   \n",
              "                                bert-base-multilingual-cased   0.814461   \n",
              "                                electra-large-discriminator    0.612871   \n",
              "                                electra-large-discriminator    0.612871   \n",
              "                                gpt2-medium                    0.658790   \n",
              "                                gpt2-medium                    0.658790   \n",
              "                                mGPT                           0.664608   \n",
              "                                mGPT                           0.664608   \n",
              "                                mdeberta-v3-base               0.859511   \n",
              "                                mdeberta-v3-base               0.859511   \n",
              "                                roberta-large-openai-detector  0.684239   \n",
              "                                roberta-large-openai-detector  0.684239   \n",
              "                                xlm-roberta-large              0.696015   \n",
              "                                xlm-roberta-large              0.696015   \n",
              "               text-davinci-003 bert-base-multilingual-cased   0.386660   \n",
              "                                bert-base-multilingual-cased   0.386660   \n",
              "                                electra-large-discriminator    0.381847   \n",
              "                                electra-large-discriminator    0.381847   \n",
              "                                gpt2-medium                    0.493947   \n",
              "                                gpt2-medium                    0.493947   \n",
              "                                mGPT                           0.394864   \n",
              "                                mGPT                           0.394864   \n",
              "                                mdeberta-v3-base               0.383281   \n",
              "                                mdeberta-v3-base               0.383281   \n",
              "                                roberta-large-openai-detector  0.587390   \n",
              "                                roberta-large-openai-detector  0.587390   \n",
              "                                xlm-roberta-large              0.382837   \n",
              "                                xlm-roberta-large              0.382837   \n",
              "               vicuna-13b       bert-base-multilingual-cased   0.409251   \n",
              "                                bert-base-multilingual-cased   0.409251   \n",
              "                                electra-large-discriminator    0.410590   \n",
              "                                electra-large-discriminator    0.410590   \n",
              "                                gpt2-medium                    0.468093   \n",
              "                                gpt2-medium                    0.468093   \n",
              "                                mGPT                           0.439702   \n",
              "                                mGPT                           0.439702   \n",
              "                                mdeberta-v3-base               0.423140   \n",
              "                                mdeberta-v3-base               0.423140   \n",
              "                                roberta-large-openai-detector  0.518273   \n",
              "                                roberta-large-openai-detector  0.518273   \n",
              "                                xlm-roberta-large              0.464019   \n",
              "                                xlm-roberta-large              0.464019   \n",
              "en3            all              bert-base-multilingual-cased   0.394157   \n",
              "                                electra-large-discriminator    0.411422   \n",
              "                                gpt2-medium                    0.521381   \n",
              "                                mGPT                           0.504929   \n",
              "                                mdeberta-v3-base               0.445304   \n",
              "                                roberta-large-openai-detector  0.503466   \n",
              "                                xlm-roberta-large              0.487426   \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased   0.410618   \n",
              "                                bert-base-multilingual-cased   0.410618   \n",
              "                                electra-large-discriminator    0.488219   \n",
              "                                electra-large-discriminator    0.488219   \n",
              "                                gpt2-medium                    0.378684   \n",
              "                                gpt2-medium                    0.378684   \n",
              "                                mGPT                           0.433243   \n",
              "                                mGPT                           0.433243   \n",
              "                                mdeberta-v3-base               0.341440   \n",
              "                                mdeberta-v3-base               0.341440   \n",
              "                                roberta-large-openai-detector  0.479982   \n",
              "                                roberta-large-openai-detector  0.479982   \n",
              "                                xlm-roberta-large              0.413505   \n",
              "                                xlm-roberta-large              0.413505   \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased   0.391338   \n",
              "                                bert-base-multilingual-cased   0.391338   \n",
              "                                electra-large-discriminator    0.415385   \n",
              "                                electra-large-discriminator    0.415385   \n",
              "                                gpt2-medium                    0.349465   \n",
              "                                gpt2-medium                    0.349465   \n",
              "                                mGPT                           0.356470   \n",
              "                                mGPT                           0.356470   \n",
              "                                mdeberta-v3-base               0.302841   \n",
              "                                mdeberta-v3-base               0.302841   \n",
              "                                roberta-large-openai-detector  0.579071   \n",
              "                                roberta-large-openai-detector  0.579071   \n",
              "                                xlm-roberta-large              0.367510   \n",
              "                                xlm-roberta-large              0.367510   \n",
              "               gpt-4            bert-base-multilingual-cased   0.395701   \n",
              "                                bert-base-multilingual-cased   0.395701   \n",
              "                                electra-large-discriminator    0.418281   \n",
              "                                electra-large-discriminator    0.418281   \n",
              "                                gpt2-medium                    0.361000   \n",
              "                                gpt2-medium                    0.361000   \n",
              "                                mGPT                           0.373165   \n",
              "                                mGPT                           0.373165   \n",
              "                                mdeberta-v3-base               0.322611   \n",
              "                                mdeberta-v3-base               0.322611   \n",
              "                                roberta-large-openai-detector  0.527413   \n",
              "                                roberta-large-openai-detector  0.527413   \n",
              "                                xlm-roberta-large              0.402976   \n",
              "                                xlm-roberta-large              0.402976   \n",
              "               llama-65b        bert-base-multilingual-cased   0.632193   \n",
              "                                bert-base-multilingual-cased   0.632193   \n",
              "                                electra-large-discriminator    0.418780   \n",
              "                                electra-large-discriminator    0.418780   \n",
              "                                gpt2-medium                    0.477082   \n",
              "                                gpt2-medium                    0.477082   \n",
              "                                mGPT                           0.544761   \n",
              "                                mGPT                           0.544761   \n",
              "                                mdeberta-v3-base               0.683452   \n",
              "                                mdeberta-v3-base               0.683452   \n",
              "                                roberta-large-openai-detector  0.425292   \n",
              "                                roberta-large-openai-detector  0.425292   \n",
              "                                xlm-roberta-large              0.583703   \n",
              "                                xlm-roberta-large              0.583703   \n",
              "               opt-66b          bert-base-multilingual-cased   0.612799   \n",
              "                                bert-base-multilingual-cased   0.612799   \n",
              "                                electra-large-discriminator    0.628242   \n",
              "                                electra-large-discriminator    0.628242   \n",
              "                                gpt2-medium                    0.583681   \n",
              "                                gpt2-medium                    0.583681   \n",
              "                                mGPT                           0.563462   \n",
              "                                mGPT                           0.563462   \n",
              "                                mdeberta-v3-base               0.502744   \n",
              "                                mdeberta-v3-base               0.502744   \n",
              "                                roberta-large-openai-detector  0.517843   \n",
              "                                roberta-large-openai-detector  0.517843   \n",
              "                                xlm-roberta-large              0.582297   \n",
              "                                xlm-roberta-large              0.582297   \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased   0.645344   \n",
              "                                bert-base-multilingual-cased   0.645344   \n",
              "                                electra-large-discriminator    0.600095   \n",
              "                                electra-large-discriminator    0.600095   \n",
              "                                gpt2-medium                    0.574215   \n",
              "                                gpt2-medium                    0.574215   \n",
              "                                mGPT                           0.582643   \n",
              "                                mGPT                           0.582643   \n",
              "                                mdeberta-v3-base               0.770143   \n",
              "                                mdeberta-v3-base               0.770143   \n",
              "                                roberta-large-openai-detector  0.661270   \n",
              "                                roberta-large-openai-detector  0.661270   \n",
              "                                xlm-roberta-large              0.679977   \n",
              "                                xlm-roberta-large              0.679977   \n",
              "               text-davinci-003 bert-base-multilingual-cased   0.390528   \n",
              "                                bert-base-multilingual-cased   0.390528   \n",
              "                                electra-large-discriminator    0.361828   \n",
              "                                electra-large-discriminator    0.361828   \n",
              "                                gpt2-medium                    0.361108   \n",
              "                                gpt2-medium                    0.361108   \n",
              "                                mGPT                           0.366245   \n",
              "                                mGPT                           0.366245   \n",
              "                                mdeberta-v3-base               0.334326   \n",
              "                                mdeberta-v3-base               0.334326   \n",
              "                                roberta-large-openai-detector  0.543846   \n",
              "                                roberta-large-openai-detector  0.543846   \n",
              "                                xlm-roberta-large              0.404657   \n",
              "                                xlm-roberta-large              0.404657   \n",
              "               vicuna-13b       bert-base-multilingual-cased   0.449614   \n",
              "                                bert-base-multilingual-cased   0.449614   \n",
              "                                electra-large-discriminator    0.509856   \n",
              "                                electra-large-discriminator    0.509856   \n",
              "                                gpt2-medium                    0.511891   \n",
              "                                gpt2-medium                    0.511891   \n",
              "                                mGPT                           0.441330   \n",
              "                                mGPT                           0.441330   \n",
              "                                mdeberta-v3-base               0.339154   \n",
              "                                mdeberta-v3-base               0.339154   \n",
              "                                roberta-large-openai-detector  0.477027   \n",
              "                                roberta-large-openai-detector  0.477027   \n",
              "                                xlm-roberta-large              0.426190   \n",
              "                                xlm-roberta-large              0.426190   \n",
              "\n",
              "                                                                    all  \n",
              "Train Language Train LLM        Model                                    \n",
              "en             all              bert-base-multilingual-cased   0.628257  \n",
              "                                electra-large-discriminator    0.555896  \n",
              "                                gpt2-medium                    0.484911  \n",
              "                                mGPT                           0.572694  \n",
              "                                mdeberta-v3-base               0.614824  \n",
              "                                roberta-large-openai-detector  0.554132  \n",
              "                                xlm-roberta-large              0.567889  \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased   0.540083  \n",
              "                                bert-base-multilingual-cased   0.540083  \n",
              "                                electra-large-discriminator    0.314809  \n",
              "                                electra-large-discriminator    0.314809  \n",
              "                                gpt2-medium                    0.278989  \n",
              "                                gpt2-medium                    0.278989  \n",
              "                                mGPT                           0.498148  \n",
              "                                mGPT                           0.498148  \n",
              "                                mdeberta-v3-base               0.480343  \n",
              "                                mdeberta-v3-base               0.480343  \n",
              "                                roberta-large-openai-detector  0.594013  \n",
              "                                roberta-large-openai-detector  0.594013  \n",
              "                                xlm-roberta-large              0.552626  \n",
              "                                xlm-roberta-large              0.552626  \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased   0.510298  \n",
              "                                bert-base-multilingual-cased   0.510298  \n",
              "                                electra-large-discriminator    0.350254  \n",
              "                                electra-large-discriminator    0.350254  \n",
              "                                gpt2-medium                    0.197480  \n",
              "                                gpt2-medium                    0.197480  \n",
              "                                mGPT                           0.445926  \n",
              "                                mGPT                           0.445926  \n",
              "                                mdeberta-v3-base               0.437736  \n",
              "                                mdeberta-v3-base               0.437736  \n",
              "                                roberta-large-openai-detector  0.560357  \n",
              "                                roberta-large-openai-detector  0.560357  \n",
              "                                xlm-roberta-large              0.418850  \n",
              "                                xlm-roberta-large              0.418850  \n",
              "               gpt-4            bert-base-multilingual-cased   0.402692  \n",
              "                                bert-base-multilingual-cased   0.402692  \n",
              "                                electra-large-discriminator    0.243161  \n",
              "                                electra-large-discriminator    0.243161  \n",
              "                                gpt2-medium                    0.197699  \n",
              "                                gpt2-medium                    0.197699  \n",
              "                                mGPT                           0.442825  \n",
              "                                mGPT                           0.442825  \n",
              "                                mdeberta-v3-base               0.350923  \n",
              "                                mdeberta-v3-base               0.350923  \n",
              "                                roberta-large-openai-detector  0.503498  \n",
              "                                roberta-large-openai-detector  0.503498  \n",
              "                                xlm-roberta-large              0.354579  \n",
              "                                xlm-roberta-large              0.354579  \n",
              "               llama-65b        bert-base-multilingual-cased   0.411308  \n",
              "                                bert-base-multilingual-cased   0.411308  \n",
              "                                electra-large-discriminator    0.531405  \n",
              "                                electra-large-discriminator    0.531405  \n",
              "                                gpt2-medium                    0.466852  \n",
              "                                gpt2-medium                    0.466852  \n",
              "                                mGPT                           0.541660  \n",
              "                                mGPT                           0.541660  \n",
              "                                mdeberta-v3-base               0.588254  \n",
              "                                mdeberta-v3-base               0.588254  \n",
              "                                roberta-large-openai-detector  0.537760  \n",
              "                                roberta-large-openai-detector  0.537760  \n",
              "                                xlm-roberta-large              0.527563  \n",
              "                                xlm-roberta-large              0.527563  \n",
              "               opt-66b          bert-base-multilingual-cased   0.482495  \n",
              "                                bert-base-multilingual-cased   0.482495  \n",
              "                                electra-large-discriminator    0.470752  \n",
              "                                electra-large-discriminator    0.470752  \n",
              "                                gpt2-medium                    0.345167  \n",
              "                                gpt2-medium                    0.345167  \n",
              "                                mGPT                           0.477264  \n",
              "                                mGPT                           0.477264  \n",
              "                                mdeberta-v3-base               0.443604  \n",
              "                                mdeberta-v3-base               0.443604  \n",
              "                                roberta-large-openai-detector  0.579981  \n",
              "                                roberta-large-openai-detector  0.579981  \n",
              "                                xlm-roberta-large              0.498294  \n",
              "                                xlm-roberta-large              0.498294  \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased   0.579916  \n",
              "                                bert-base-multilingual-cased   0.579916  \n",
              "                                electra-large-discriminator    0.509822  \n",
              "                                electra-large-discriminator    0.509822  \n",
              "                                gpt2-medium                    0.413160  \n",
              "                                gpt2-medium                    0.413160  \n",
              "                                mGPT                           0.603522  \n",
              "                                mGPT                           0.603522  \n",
              "                                mdeberta-v3-base               0.533466  \n",
              "                                mdeberta-v3-base               0.533466  \n",
              "                                roberta-large-openai-detector  0.652380  \n",
              "                                roberta-large-openai-detector  0.652380  \n",
              "                                xlm-roberta-large              0.517943  \n",
              "                                xlm-roberta-large              0.517943  \n",
              "               text-davinci-003 bert-base-multilingual-cased   0.466336  \n",
              "                                bert-base-multilingual-cased   0.466336  \n",
              "                                electra-large-discriminator    0.352606  \n",
              "                                electra-large-discriminator    0.352606  \n",
              "                                gpt2-medium                    0.193042  \n",
              "                                gpt2-medium                    0.193042  \n",
              "                                mGPT                           0.462483  \n",
              "                                mGPT                           0.462483  \n",
              "                                mdeberta-v3-base               0.478074  \n",
              "                                mdeberta-v3-base               0.478074  \n",
              "                                roberta-large-openai-detector  0.564681  \n",
              "                                roberta-large-openai-detector  0.564681  \n",
              "                                xlm-roberta-large              0.452250  \n",
              "                                xlm-roberta-large              0.452250  \n",
              "               vicuna-13b       bert-base-multilingual-cased   0.529381  \n",
              "                                bert-base-multilingual-cased   0.529381  \n",
              "                                electra-large-discriminator    0.445433  \n",
              "                                electra-large-discriminator    0.445433  \n",
              "                                gpt2-medium                    0.313205  \n",
              "                                gpt2-medium                    0.313205  \n",
              "                                mGPT                           0.500709  \n",
              "                                mGPT                           0.500709  \n",
              "                                mdeberta-v3-base               0.453134  \n",
              "                                mdeberta-v3-base               0.453134  \n",
              "                                roberta-large-openai-detector  0.579163  \n",
              "                                roberta-large-openai-detector  0.579163  \n",
              "                                xlm-roberta-large              0.465287  \n",
              "                                xlm-roberta-large              0.465287  \n",
              "es             all              bert-base-multilingual-cased   0.740026  \n",
              "                                electra-large-discriminator    0.491472  \n",
              "                                gpt2-medium                    0.567999  \n",
              "                                mGPT                           0.684915  \n",
              "                                mdeberta-v3-base               0.796051  \n",
              "                                roberta-large-openai-detector  0.634903  \n",
              "                                xlm-roberta-large              0.811016  \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased   0.544745  \n",
              "                                bert-base-multilingual-cased   0.544745  \n",
              "                                electra-large-discriminator    0.335094  \n",
              "                                electra-large-discriminator    0.335094  \n",
              "                                gpt2-medium                    0.396313  \n",
              "                                gpt2-medium                    0.396313  \n",
              "                                mGPT                           0.555471  \n",
              "                                mGPT                           0.555471  \n",
              "                                mdeberta-v3-base               0.546349  \n",
              "                                mdeberta-v3-base               0.546349  \n",
              "                                roberta-large-openai-detector  0.406698  \n",
              "                                roberta-large-openai-detector  0.406698  \n",
              "                                xlm-roberta-large              0.464705  \n",
              "                                xlm-roberta-large              0.464705  \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased   0.471506  \n",
              "                                bert-base-multilingual-cased   0.471506  \n",
              "                                electra-large-discriminator    0.319260  \n",
              "                                electra-large-discriminator    0.319260  \n",
              "                                gpt2-medium                    0.292554  \n",
              "                                gpt2-medium                    0.292554  \n",
              "                                mGPT                           0.464688  \n",
              "                                mGPT                           0.464688  \n",
              "                                mdeberta-v3-base               0.494788  \n",
              "                                mdeberta-v3-base               0.494788  \n",
              "                                roberta-large-openai-detector  0.393647  \n",
              "                                roberta-large-openai-detector  0.393647  \n",
              "                                xlm-roberta-large              0.441229  \n",
              "                                xlm-roberta-large              0.441229  \n",
              "               gpt-4            bert-base-multilingual-cased   0.515462  \n",
              "                                bert-base-multilingual-cased   0.515462  \n",
              "                                electra-large-discriminator    0.325855  \n",
              "                                electra-large-discriminator    0.325855  \n",
              "                                gpt2-medium                    0.233255  \n",
              "                                gpt2-medium                    0.233255  \n",
              "                                mGPT                           0.430409  \n",
              "                                mGPT                           0.430409  \n",
              "                                mdeberta-v3-base               0.473011  \n",
              "                                mdeberta-v3-base               0.473011  \n",
              "                                roberta-large-openai-detector  0.365708  \n",
              "                                roberta-large-openai-detector  0.365708  \n",
              "                                xlm-roberta-large              0.365840  \n",
              "                                xlm-roberta-large              0.365840  \n",
              "               llama-65b        bert-base-multilingual-cased   0.409573  \n",
              "                                bert-base-multilingual-cased   0.409573  \n",
              "                                electra-large-discriminator    0.447883  \n",
              "                                electra-large-discriminator    0.447883  \n",
              "                                gpt2-medium                    0.420968  \n",
              "                                gpt2-medium                    0.420968  \n",
              "                                mGPT                           0.524760  \n",
              "                                mGPT                           0.524760  \n",
              "                                mdeberta-v3-base               0.484322  \n",
              "                                mdeberta-v3-base               0.484322  \n",
              "                                roberta-large-openai-detector  0.579094  \n",
              "                                roberta-large-openai-detector  0.579094  \n",
              "                                xlm-roberta-large              0.465634  \n",
              "                                xlm-roberta-large              0.465634  \n",
              "               opt-66b          bert-base-multilingual-cased   0.531365  \n",
              "                                bert-base-multilingual-cased   0.531365  \n",
              "                                electra-large-discriminator    0.478770  \n",
              "                                electra-large-discriminator    0.478770  \n",
              "                                gpt2-medium                    0.437147  \n",
              "                                gpt2-medium                    0.437147  \n",
              "                                mGPT                           0.549055  \n",
              "                                mGPT                           0.549055  \n",
              "                                mdeberta-v3-base               0.528281  \n",
              "                                mdeberta-v3-base               0.528281  \n",
              "                                roberta-large-openai-detector  0.542597  \n",
              "                                roberta-large-openai-detector  0.542597  \n",
              "                                xlm-roberta-large              0.577967  \n",
              "                                xlm-roberta-large              0.577967  \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased   0.348358  \n",
              "                                bert-base-multilingual-cased   0.348358  \n",
              "                                electra-large-discriminator    0.501950  \n",
              "                                electra-large-discriminator    0.501950  \n",
              "                                gpt2-medium                    0.335497  \n",
              "                                gpt2-medium                    0.335497  \n",
              "                                mGPT                           0.434361  \n",
              "                                mGPT                           0.434361  \n",
              "                                mdeberta-v3-base               0.369579  \n",
              "                                mdeberta-v3-base               0.369579  \n",
              "                                roberta-large-openai-detector  0.541318  \n",
              "                                roberta-large-openai-detector  0.541318  \n",
              "                                xlm-roberta-large              0.290052  \n",
              "                                xlm-roberta-large              0.290052  \n",
              "               text-davinci-003 bert-base-multilingual-cased   0.511852  \n",
              "                                bert-base-multilingual-cased   0.511852  \n",
              "                                electra-large-discriminator    0.344822  \n",
              "                                electra-large-discriminator    0.344822  \n",
              "                                gpt2-medium                    0.382733  \n",
              "                                gpt2-medium                    0.382733  \n",
              "                                mGPT                           0.469657  \n",
              "                                mGPT                           0.469657  \n",
              "                                mdeberta-v3-base               0.483200  \n",
              "                                mdeberta-v3-base               0.483200  \n",
              "                                roberta-large-openai-detector  0.376126  \n",
              "                                roberta-large-openai-detector  0.376126  \n",
              "                                xlm-roberta-large              0.506892  \n",
              "                                xlm-roberta-large              0.506892  \n",
              "               vicuna-13b       bert-base-multilingual-cased   0.533970  \n",
              "                                bert-base-multilingual-cased   0.533970  \n",
              "                                electra-large-discriminator    0.362544  \n",
              "                                electra-large-discriminator    0.362544  \n",
              "                                gpt2-medium                    0.445570  \n",
              "                                gpt2-medium                    0.445570  \n",
              "                                mGPT                           0.561170  \n",
              "                                mGPT                           0.561170  \n",
              "                                mdeberta-v3-base               0.533426  \n",
              "                                mdeberta-v3-base               0.533426  \n",
              "                                roberta-large-openai-detector  0.439047  \n",
              "                                roberta-large-openai-detector  0.439047  \n",
              "                                xlm-roberta-large              0.489621  \n",
              "                                xlm-roberta-large              0.489621  \n",
              "ru             all              bert-base-multilingual-cased   0.675434  \n",
              "                                electra-large-discriminator    0.470887  \n",
              "                                gpt2-medium                    0.594514  \n",
              "                                mGPT                           0.721899  \n",
              "                                mdeberta-v3-base               0.746595  \n",
              "                                roberta-large-openai-detector  0.564216  \n",
              "                                xlm-roberta-large              0.798669  \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased   0.567941  \n",
              "                                bert-base-multilingual-cased   0.567941  \n",
              "                                electra-large-discriminator    0.369966  \n",
              "                                electra-large-discriminator    0.369966  \n",
              "                                gpt2-medium                    0.249810  \n",
              "                                gpt2-medium                    0.249810  \n",
              "                                mGPT                           0.544351  \n",
              "                                mGPT                           0.544351  \n",
              "                                mdeberta-v3-base               0.659287  \n",
              "                                mdeberta-v3-base               0.659287  \n",
              "                                roberta-large-openai-detector  0.505909  \n",
              "                                roberta-large-openai-detector  0.505909  \n",
              "                                xlm-roberta-large              0.579426  \n",
              "                                xlm-roberta-large              0.579426  \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased   0.525860  \n",
              "                                bert-base-multilingual-cased   0.525860  \n",
              "                                electra-large-discriminator    0.235810  \n",
              "                                electra-large-discriminator    0.235810  \n",
              "                                gpt2-medium                    0.205929  \n",
              "                                gpt2-medium                    0.205929  \n",
              "                                mGPT                           0.457593  \n",
              "                                mGPT                           0.457593  \n",
              "                                mdeberta-v3-base               0.513331  \n",
              "                                mdeberta-v3-base               0.513331  \n",
              "                                roberta-large-openai-detector  0.448999  \n",
              "                                roberta-large-openai-detector  0.448999  \n",
              "                                xlm-roberta-large              0.543448  \n",
              "                                xlm-roberta-large              0.543448  \n",
              "               gpt-4            bert-base-multilingual-cased   0.521967  \n",
              "                                bert-base-multilingual-cased   0.521967  \n",
              "                                electra-large-discriminator    0.260882  \n",
              "                                electra-large-discriminator    0.260882  \n",
              "                                gpt2-medium                    0.252007  \n",
              "                                gpt2-medium                    0.252007  \n",
              "                                mGPT                           0.440709  \n",
              "                                mGPT                           0.440709  \n",
              "                                mdeberta-v3-base               0.456370  \n",
              "                                mdeberta-v3-base               0.456370  \n",
              "                                roberta-large-openai-detector  0.472522  \n",
              "                                roberta-large-openai-detector  0.472522  \n",
              "                                xlm-roberta-large              0.503719  \n",
              "                                xlm-roberta-large              0.503719  \n",
              "               llama-65b        bert-base-multilingual-cased   0.495636  \n",
              "                                bert-base-multilingual-cased   0.495636  \n",
              "                                electra-large-discriminator    0.489651  \n",
              "                                electra-large-discriminator    0.489651  \n",
              "                                gpt2-medium                    0.404211  \n",
              "                                gpt2-medium                    0.404211  \n",
              "                                mGPT                           0.535351  \n",
              "                                mGPT                           0.535351  \n",
              "                                mdeberta-v3-base               0.551419  \n",
              "                                mdeberta-v3-base               0.551419  \n",
              "                                roberta-large-openai-detector  0.481693  \n",
              "                                roberta-large-openai-detector  0.481693  \n",
              "                                xlm-roberta-large              0.517053  \n",
              "                                xlm-roberta-large              0.517053  \n",
              "               opt-66b          bert-base-multilingual-cased   0.309555  \n",
              "                                bert-base-multilingual-cased   0.309555  \n",
              "                                electra-large-discriminator    0.499178  \n",
              "                                electra-large-discriminator    0.499178  \n",
              "                                gpt2-medium                    0.288770  \n",
              "                                gpt2-medium                    0.288770  \n",
              "                                mGPT                           0.371776  \n",
              "                                mGPT                           0.371776  \n",
              "                                mdeberta-v3-base               0.308479  \n",
              "                                mdeberta-v3-base               0.308479  \n",
              "                                roberta-large-openai-detector  0.485909  \n",
              "                                roberta-large-openai-detector  0.485909  \n",
              "                                xlm-roberta-large              0.340420  \n",
              "                                xlm-roberta-large              0.340420  \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased   0.176563  \n",
              "                                bert-base-multilingual-cased   0.176563  \n",
              "                                electra-large-discriminator    0.433427  \n",
              "                                electra-large-discriminator    0.433427  \n",
              "                                gpt2-medium                    0.236609  \n",
              "                                gpt2-medium                    0.236609  \n",
              "                                mGPT                           0.203065  \n",
              "                                mGPT                           0.203065  \n",
              "                                mdeberta-v3-base               0.209753  \n",
              "                                mdeberta-v3-base               0.209753  \n",
              "                                roberta-large-openai-detector  0.521333  \n",
              "                                roberta-large-openai-detector  0.521333  \n",
              "                                xlm-roberta-large              0.223035  \n",
              "                                xlm-roberta-large              0.223035  \n",
              "               text-davinci-003 bert-base-multilingual-cased   0.397715  \n",
              "                                bert-base-multilingual-cased   0.397715  \n",
              "                                electra-large-discriminator    0.233410  \n",
              "                                electra-large-discriminator    0.233410  \n",
              "                                gpt2-medium                    0.223241  \n",
              "                                gpt2-medium                    0.223241  \n",
              "                                mGPT                           0.388129  \n",
              "                                mGPT                           0.388129  \n",
              "                                mdeberta-v3-base               0.354182  \n",
              "                                mdeberta-v3-base               0.354182  \n",
              "                                roberta-large-openai-detector  0.463071  \n",
              "                                roberta-large-openai-detector  0.463071  \n",
              "                                xlm-roberta-large              0.358429  \n",
              "                                xlm-roberta-large              0.358429  \n",
              "               vicuna-13b       bert-base-multilingual-cased   0.564186  \n",
              "                                bert-base-multilingual-cased   0.564186  \n",
              "                                electra-large-discriminator    0.513115  \n",
              "                                electra-large-discriminator    0.513115  \n",
              "                                gpt2-medium                    0.299265  \n",
              "                                gpt2-medium                    0.299265  \n",
              "                                mGPT                           0.621305  \n",
              "                                mGPT                           0.621305  \n",
              "                                mdeberta-v3-base               0.622840  \n",
              "                                mdeberta-v3-base               0.622840  \n",
              "                                roberta-large-openai-detector  0.502183  \n",
              "                                roberta-large-openai-detector  0.502183  \n",
              "                                xlm-roberta-large              0.594081  \n",
              "                                xlm-roberta-large              0.594081  \n",
              "all            all              bert-base-multilingual-cased   0.756344  \n",
              "                                electra-large-discriminator    0.505921  \n",
              "                                gpt2-medium                    0.664612  \n",
              "                                mGPT                           0.695916  \n",
              "                                mdeberta-v3-base               0.848011  \n",
              "                                roberta-large-openai-detector  0.736044  \n",
              "                                xlm-roberta-large              0.824012  \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased   0.552802  \n",
              "                                bert-base-multilingual-cased   0.552802  \n",
              "                                electra-large-discriminator    0.321353  \n",
              "                                electra-large-discriminator    0.321353  \n",
              "                                gpt2-medium                    0.380017  \n",
              "                                gpt2-medium                    0.380017  \n",
              "                                mGPT                           0.522982  \n",
              "                                mGPT                           0.522982  \n",
              "                                mdeberta-v3-base               0.551927  \n",
              "                                mdeberta-v3-base               0.551927  \n",
              "                                roberta-large-openai-detector  0.513234  \n",
              "                                roberta-large-openai-detector  0.513234  \n",
              "                                xlm-roberta-large              0.541894  \n",
              "                                xlm-roberta-large              0.541894  \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased   0.489238  \n",
              "                                bert-base-multilingual-cased   0.489238  \n",
              "                                electra-large-discriminator    0.259361  \n",
              "                                electra-large-discriminator    0.259361  \n",
              "                                gpt2-medium                    0.355082  \n",
              "                                gpt2-medium                    0.355082  \n",
              "                                mGPT                           0.417104  \n",
              "                                mGPT                           0.417104  \n",
              "                                mdeberta-v3-base               0.501057  \n",
              "                                mdeberta-v3-base               0.501057  \n",
              "                                roberta-large-openai-detector  0.338767  \n",
              "                                roberta-large-openai-detector  0.338767  \n",
              "                                xlm-roberta-large              0.494016  \n",
              "                                xlm-roberta-large              0.494016  \n",
              "               gpt-4            bert-base-multilingual-cased   0.497424  \n",
              "                                bert-base-multilingual-cased   0.497424  \n",
              "                                electra-large-discriminator    0.303492  \n",
              "                                electra-large-discriminator    0.303492  \n",
              "                                gpt2-medium                    0.366916  \n",
              "                                gpt2-medium                    0.366916  \n",
              "                                mGPT                           0.452888  \n",
              "                                mGPT                           0.452888  \n",
              "                                mdeberta-v3-base               0.486767  \n",
              "                                mdeberta-v3-base               0.486767  \n",
              "                                roberta-large-openai-detector  0.421871  \n",
              "                                roberta-large-openai-detector  0.421871  \n",
              "                                xlm-roberta-large              0.494915  \n",
              "                                xlm-roberta-large              0.494915  \n",
              "               llama-65b        bert-base-multilingual-cased   0.411371  \n",
              "                                bert-base-multilingual-cased   0.411371  \n",
              "                                electra-large-discriminator    0.331128  \n",
              "                                electra-large-discriminator    0.331128  \n",
              "                                gpt2-medium                    0.365997  \n",
              "                                gpt2-medium                    0.365997  \n",
              "                                mGPT                           0.441762  \n",
              "                                mGPT                           0.441762  \n",
              "                                mdeberta-v3-base               0.506674  \n",
              "                                mdeberta-v3-base               0.506674  \n",
              "                                roberta-large-openai-detector  0.481338  \n",
              "                                roberta-large-openai-detector  0.481338  \n",
              "                                xlm-roberta-large              0.462752  \n",
              "                                xlm-roberta-large              0.462752  \n",
              "               opt-66b          bert-base-multilingual-cased   0.451727  \n",
              "                                bert-base-multilingual-cased   0.451727  \n",
              "                                electra-large-discriminator    0.401549  \n",
              "                                electra-large-discriminator    0.401549  \n",
              "                                gpt2-medium                    0.431336  \n",
              "                                gpt2-medium                    0.431336  \n",
              "                                mGPT                           0.522330  \n",
              "                                mGPT                           0.522330  \n",
              "                                mdeberta-v3-base               0.555966  \n",
              "                                mdeberta-v3-base               0.555966  \n",
              "                                roberta-large-openai-detector  0.485099  \n",
              "                                roberta-large-openai-detector  0.485099  \n",
              "                                xlm-roberta-large              0.458762  \n",
              "                                xlm-roberta-large              0.458762  \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased   0.488158  \n",
              "                                bert-base-multilingual-cased   0.488158  \n",
              "                                electra-large-discriminator    0.324427  \n",
              "                                electra-large-discriminator    0.324427  \n",
              "                                gpt2-medium                    0.395710  \n",
              "                                gpt2-medium                    0.395710  \n",
              "                                mGPT                           0.427575  \n",
              "                                mGPT                           0.427575  \n",
              "                                mdeberta-v3-base               0.487085  \n",
              "                                mdeberta-v3-base               0.487085  \n",
              "                                roberta-large-openai-detector  0.445988  \n",
              "                                roberta-large-openai-detector  0.445988  \n",
              "                                xlm-roberta-large              0.345646  \n",
              "                                xlm-roberta-large              0.345646  \n",
              "               text-davinci-003 bert-base-multilingual-cased   0.516124  \n",
              "                                bert-base-multilingual-cased   0.516124  \n",
              "                                electra-large-discriminator    0.290891  \n",
              "                                electra-large-discriminator    0.290891  \n",
              "                                gpt2-medium                    0.414019  \n",
              "                                gpt2-medium                    0.414019  \n",
              "                                mGPT                           0.458002  \n",
              "                                mGPT                           0.458002  \n",
              "                                mdeberta-v3-base               0.487793  \n",
              "                                mdeberta-v3-base               0.487793  \n",
              "                                roberta-large-openai-detector  0.521483  \n",
              "                                roberta-large-openai-detector  0.521483  \n",
              "                                xlm-roberta-large              0.454262  \n",
              "                                xlm-roberta-large              0.454262  \n",
              "               vicuna-13b       bert-base-multilingual-cased   0.544604  \n",
              "                                bert-base-multilingual-cased   0.544604  \n",
              "                                electra-large-discriminator    0.329236  \n",
              "                                electra-large-discriminator    0.329236  \n",
              "                                gpt2-medium                    0.405648  \n",
              "                                gpt2-medium                    0.405648  \n",
              "                                mGPT                           0.522266  \n",
              "                                mGPT                           0.522266  \n",
              "                                mdeberta-v3-base               0.532327  \n",
              "                                mdeberta-v3-base               0.532327  \n",
              "                                roberta-large-openai-detector  0.529117  \n",
              "                                roberta-large-openai-detector  0.529117  \n",
              "                                xlm-roberta-large              0.579232  \n",
              "                                xlm-roberta-large              0.579232  \n",
              "en3            all              bert-base-multilingual-cased   0.526371  \n",
              "                                electra-large-discriminator    0.541786  \n",
              "                                gpt2-medium                    0.461908  \n",
              "                                mGPT                           0.560242  \n",
              "                                mdeberta-v3-base               0.573683  \n",
              "                                roberta-large-openai-detector  0.610576  \n",
              "                                xlm-roberta-large              0.606566  \n",
              "               alpaca-lora-30b  bert-base-multilingual-cased   0.539153  \n",
              "                                bert-base-multilingual-cased   0.539153  \n",
              "                                electra-large-discriminator    0.388707  \n",
              "                                electra-large-discriminator    0.388707  \n",
              "                                gpt2-medium                    0.211332  \n",
              "                                gpt2-medium                    0.211332  \n",
              "                                mGPT                           0.494811  \n",
              "                                mGPT                           0.494811  \n",
              "                                mdeberta-v3-base               0.496785  \n",
              "                                mdeberta-v3-base               0.496785  \n",
              "                                roberta-large-openai-detector  0.577122  \n",
              "                                roberta-large-openai-detector  0.577122  \n",
              "                                xlm-roberta-large              0.537613  \n",
              "                                xlm-roberta-large              0.537613  \n",
              "               gpt-3.5-turbo    bert-base-multilingual-cased   0.508449  \n",
              "                                bert-base-multilingual-cased   0.508449  \n",
              "                                electra-large-discriminator    0.266367  \n",
              "                                electra-large-discriminator    0.266367  \n",
              "                                gpt2-medium                    0.178861  \n",
              "                                gpt2-medium                    0.178861  \n",
              "                                mGPT                           0.417025  \n",
              "                                mGPT                           0.417025  \n",
              "                                mdeberta-v3-base               0.471563  \n",
              "                                mdeberta-v3-base               0.471563  \n",
              "                                roberta-large-openai-detector  0.521696  \n",
              "                                roberta-large-openai-detector  0.521696  \n",
              "                                xlm-roberta-large              0.489415  \n",
              "                                xlm-roberta-large              0.489415  \n",
              "               gpt-4            bert-base-multilingual-cased   0.474859  \n",
              "                                bert-base-multilingual-cased   0.474859  \n",
              "                                electra-large-discriminator    0.294223  \n",
              "                                electra-large-discriminator    0.294223  \n",
              "                                gpt2-medium                    0.184907  \n",
              "                                gpt2-medium                    0.184907  \n",
              "                                mGPT                           0.402445  \n",
              "                                mGPT                           0.402445  \n",
              "                                mdeberta-v3-base               0.454663  \n",
              "                                mdeberta-v3-base               0.454663  \n",
              "                                roberta-large-openai-detector  0.466843  \n",
              "                                roberta-large-openai-detector  0.466843  \n",
              "                                xlm-roberta-large              0.495228  \n",
              "                                xlm-roberta-large              0.495228  \n",
              "               llama-65b        bert-base-multilingual-cased   0.432415  \n",
              "                                bert-base-multilingual-cased   0.432415  \n",
              "                                electra-large-discriminator    0.544748  \n",
              "                                electra-large-discriminator    0.544748  \n",
              "                                gpt2-medium                    0.495248  \n",
              "                                gpt2-medium                    0.495248  \n",
              "                                mGPT                           0.541871  \n",
              "                                mGPT                           0.541871  \n",
              "                                mdeberta-v3-base               0.467960  \n",
              "                                mdeberta-v3-base               0.467960  \n",
              "                                roberta-large-openai-detector  0.551131  \n",
              "                                roberta-large-openai-detector  0.551131  \n",
              "                                xlm-roberta-large              0.560237  \n",
              "                                xlm-roberta-large              0.560237  \n",
              "               opt-66b          bert-base-multilingual-cased   0.516472  \n",
              "                                bert-base-multilingual-cased   0.516472  \n",
              "                                electra-large-discriminator    0.529356  \n",
              "                                electra-large-discriminator    0.529356  \n",
              "                                gpt2-medium                    0.409883  \n",
              "                                gpt2-medium                    0.409883  \n",
              "                                mGPT                           0.504160  \n",
              "                                mGPT                           0.504160  \n",
              "                                mdeberta-v3-base               0.530852  \n",
              "                                mdeberta-v3-base               0.530852  \n",
              "                                roberta-large-openai-detector  0.613234  \n",
              "                                roberta-large-openai-detector  0.613234  \n",
              "                                xlm-roberta-large              0.479673  \n",
              "                                xlm-roberta-large              0.479673  \n",
              "               opt-iml-max-1.3b bert-base-multilingual-cased   0.567327  \n",
              "                                bert-base-multilingual-cased   0.567327  \n",
              "                                electra-large-discriminator    0.499904  \n",
              "                                electra-large-discriminator    0.499904  \n",
              "                                gpt2-medium                    0.421614  \n",
              "                                gpt2-medium                    0.421614  \n",
              "                                mGPT                           0.564812  \n",
              "                                mGPT                           0.564812  \n",
              "                                mdeberta-v3-base               0.606918  \n",
              "                                mdeberta-v3-base               0.606918  \n",
              "                                roberta-large-openai-detector  0.642569  \n",
              "                                roberta-large-openai-detector  0.642569  \n",
              "                                xlm-roberta-large              0.519412  \n",
              "                                xlm-roberta-large              0.519412  \n",
              "               text-davinci-003 bert-base-multilingual-cased   0.487528  \n",
              "                                bert-base-multilingual-cased   0.487528  \n",
              "                                electra-large-discriminator    0.176044  \n",
              "                                electra-large-discriminator    0.176044  \n",
              "                                gpt2-medium                    0.187736  \n",
              "                                gpt2-medium                    0.187736  \n",
              "                                mGPT                           0.427558  \n",
              "                                mGPT                           0.427558  \n",
              "                                mdeberta-v3-base               0.476244  \n",
              "                                mdeberta-v3-base               0.476244  \n",
              "                                roberta-large-openai-detector  0.462450  \n",
              "                                roberta-large-openai-detector  0.462450  \n",
              "                                xlm-roberta-large              0.409586  \n",
              "                                xlm-roberta-large              0.409586  \n",
              "               vicuna-13b       bert-base-multilingual-cased   0.563109  \n",
              "                                bert-base-multilingual-cased   0.563109  \n",
              "                                electra-large-discriminator    0.443065  \n",
              "                                electra-large-discriminator    0.443065  \n",
              "                                gpt2-medium                    0.357012  \n",
              "                                gpt2-medium                    0.357012  \n",
              "                                mGPT                           0.513120  \n",
              "                                mGPT                           0.513120  \n",
              "                                mdeberta-v3-base               0.475553  \n",
              "                                mdeberta-v3-base               0.475553  \n",
              "                                roberta-large-openai-detector  0.572519  \n",
              "                                roberta-large-openai-detector  0.572519  \n",
              "                                xlm-roberta-large              0.518315  \n",
              "                                xlm-roberta-large              0.518315  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fc1a3e3e-5985-4bbf-ae52-11ca9dc62f37\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>gpt-4</th>\n",
              "      <th>gpt-3.5-turbo</th>\n",
              "      <th>text-davinci-003</th>\n",
              "      <th>vicuna-13b</th>\n",
              "      <th>alpaca-lora-30b</th>\n",
              "      <th>opt-iml-max-1.3b</th>\n",
              "      <th>llama-65b</th>\n",
              "      <th>opt-66b</th>\n",
              "      <th>all</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train Language</th>\n",
              "      <th>Train LLM</th>\n",
              "      <th>Model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"119\" valign=\"top\">en</th>\n",
              "      <th rowspan=\"7\" valign=\"top\">all</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.613070</td>\n",
              "      <td>0.613582</td>\n",
              "      <td>0.605591</td>\n",
              "      <td>0.604411</td>\n",
              "      <td>0.591141</td>\n",
              "      <td>0.521586</td>\n",
              "      <td>0.605250</td>\n",
              "      <td>0.545696</td>\n",
              "      <td>0.628257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.572043</td>\n",
              "      <td>0.524899</td>\n",
              "      <td>0.499486</td>\n",
              "      <td>0.569871</td>\n",
              "      <td>0.524444</td>\n",
              "      <td>0.597828</td>\n",
              "      <td>0.627450</td>\n",
              "      <td>0.560828</td>\n",
              "      <td>0.555896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.490022</td>\n",
              "      <td>0.487222</td>\n",
              "      <td>0.485969</td>\n",
              "      <td>0.535650</td>\n",
              "      <td>0.471993</td>\n",
              "      <td>0.567807</td>\n",
              "      <td>0.634057</td>\n",
              "      <td>0.555311</td>\n",
              "      <td>0.484911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.517999</td>\n",
              "      <td>0.519259</td>\n",
              "      <td>0.515067</td>\n",
              "      <td>0.521795</td>\n",
              "      <td>0.512592</td>\n",
              "      <td>0.455756</td>\n",
              "      <td>0.523695</td>\n",
              "      <td>0.496428</td>\n",
              "      <td>0.572694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.575883</td>\n",
              "      <td>0.575150</td>\n",
              "      <td>0.566313</td>\n",
              "      <td>0.568594</td>\n",
              "      <td>0.543716</td>\n",
              "      <td>0.509931</td>\n",
              "      <td>0.568337</td>\n",
              "      <td>0.520285</td>\n",
              "      <td>0.614824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.427121</td>\n",
              "      <td>0.427305</td>\n",
              "      <td>0.427240</td>\n",
              "      <td>0.426098</td>\n",
              "      <td>0.426897</td>\n",
              "      <td>0.424688</td>\n",
              "      <td>0.424856</td>\n",
              "      <td>0.425894</td>\n",
              "      <td>0.554132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.447902</td>\n",
              "      <td>0.447416</td>\n",
              "      <td>0.447934</td>\n",
              "      <td>0.447804</td>\n",
              "      <td>0.447578</td>\n",
              "      <td>0.440570</td>\n",
              "      <td>0.445515</td>\n",
              "      <td>0.437878</td>\n",
              "      <td>0.567889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">alpaca-lora-30b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.796355</td>\n",
              "      <td>0.823652</td>\n",
              "      <td>0.810178</td>\n",
              "      <td>0.762162</td>\n",
              "      <td>0.788933</td>\n",
              "      <td>0.486282</td>\n",
              "      <td>0.433616</td>\n",
              "      <td>0.431465</td>\n",
              "      <td>0.540083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.796355</td>\n",
              "      <td>0.823652</td>\n",
              "      <td>0.810178</td>\n",
              "      <td>0.762162</td>\n",
              "      <td>0.788933</td>\n",
              "      <td>0.486282</td>\n",
              "      <td>0.433616</td>\n",
              "      <td>0.431465</td>\n",
              "      <td>0.540083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.500101</td>\n",
              "      <td>0.524322</td>\n",
              "      <td>0.535580</td>\n",
              "      <td>0.545989</td>\n",
              "      <td>0.569812</td>\n",
              "      <td>0.516792</td>\n",
              "      <td>0.405988</td>\n",
              "      <td>0.459469</td>\n",
              "      <td>0.314809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.500101</td>\n",
              "      <td>0.524322</td>\n",
              "      <td>0.535580</td>\n",
              "      <td>0.545989</td>\n",
              "      <td>0.569812</td>\n",
              "      <td>0.516792</td>\n",
              "      <td>0.405988</td>\n",
              "      <td>0.459469</td>\n",
              "      <td>0.314809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.443838</td>\n",
              "      <td>0.458944</td>\n",
              "      <td>0.526691</td>\n",
              "      <td>0.476839</td>\n",
              "      <td>0.542632</td>\n",
              "      <td>0.493881</td>\n",
              "      <td>0.562619</td>\n",
              "      <td>0.440140</td>\n",
              "      <td>0.278989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.443838</td>\n",
              "      <td>0.458944</td>\n",
              "      <td>0.526691</td>\n",
              "      <td>0.476839</td>\n",
              "      <td>0.542632</td>\n",
              "      <td>0.493881</td>\n",
              "      <td>0.562619</td>\n",
              "      <td>0.440140</td>\n",
              "      <td>0.278989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.743686</td>\n",
              "      <td>0.802225</td>\n",
              "      <td>0.765636</td>\n",
              "      <td>0.739632</td>\n",
              "      <td>0.755631</td>\n",
              "      <td>0.362395</td>\n",
              "      <td>0.442688</td>\n",
              "      <td>0.438631</td>\n",
              "      <td>0.498148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.743686</td>\n",
              "      <td>0.802225</td>\n",
              "      <td>0.765636</td>\n",
              "      <td>0.739632</td>\n",
              "      <td>0.755631</td>\n",
              "      <td>0.362395</td>\n",
              "      <td>0.442688</td>\n",
              "      <td>0.438631</td>\n",
              "      <td>0.498148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.824227</td>\n",
              "      <td>0.811764</td>\n",
              "      <td>0.779460</td>\n",
              "      <td>0.740397</td>\n",
              "      <td>0.742818</td>\n",
              "      <td>0.317518</td>\n",
              "      <td>0.367561</td>\n",
              "      <td>0.328532</td>\n",
              "      <td>0.480343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.824227</td>\n",
              "      <td>0.811764</td>\n",
              "      <td>0.779460</td>\n",
              "      <td>0.740397</td>\n",
              "      <td>0.742818</td>\n",
              "      <td>0.317518</td>\n",
              "      <td>0.367561</td>\n",
              "      <td>0.328532</td>\n",
              "      <td>0.480343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.564559</td>\n",
              "      <td>0.566007</td>\n",
              "      <td>0.566308</td>\n",
              "      <td>0.555558</td>\n",
              "      <td>0.562747</td>\n",
              "      <td>0.506063</td>\n",
              "      <td>0.459477</td>\n",
              "      <td>0.527190</td>\n",
              "      <td>0.594013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.564559</td>\n",
              "      <td>0.566007</td>\n",
              "      <td>0.566308</td>\n",
              "      <td>0.555558</td>\n",
              "      <td>0.562747</td>\n",
              "      <td>0.506063</td>\n",
              "      <td>0.459477</td>\n",
              "      <td>0.527190</td>\n",
              "      <td>0.594013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.802856</td>\n",
              "      <td>0.826377</td>\n",
              "      <td>0.794029</td>\n",
              "      <td>0.754177</td>\n",
              "      <td>0.764315</td>\n",
              "      <td>0.510659</td>\n",
              "      <td>0.491207</td>\n",
              "      <td>0.465985</td>\n",
              "      <td>0.552626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.802856</td>\n",
              "      <td>0.826377</td>\n",
              "      <td>0.794029</td>\n",
              "      <td>0.754177</td>\n",
              "      <td>0.764315</td>\n",
              "      <td>0.510659</td>\n",
              "      <td>0.491207</td>\n",
              "      <td>0.465985</td>\n",
              "      <td>0.552626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">gpt-3.5-turbo</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.884076</td>\n",
              "      <td>0.910558</td>\n",
              "      <td>0.851200</td>\n",
              "      <td>0.828103</td>\n",
              "      <td>0.779677</td>\n",
              "      <td>0.395933</td>\n",
              "      <td>0.375808</td>\n",
              "      <td>0.402321</td>\n",
              "      <td>0.510298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.884076</td>\n",
              "      <td>0.910558</td>\n",
              "      <td>0.851200</td>\n",
              "      <td>0.828103</td>\n",
              "      <td>0.779677</td>\n",
              "      <td>0.395933</td>\n",
              "      <td>0.375808</td>\n",
              "      <td>0.402321</td>\n",
              "      <td>0.510298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.587872</td>\n",
              "      <td>0.605227</td>\n",
              "      <td>0.593694</td>\n",
              "      <td>0.608747</td>\n",
              "      <td>0.601589</td>\n",
              "      <td>0.486370</td>\n",
              "      <td>0.438110</td>\n",
              "      <td>0.451089</td>\n",
              "      <td>0.350254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.587872</td>\n",
              "      <td>0.605227</td>\n",
              "      <td>0.593694</td>\n",
              "      <td>0.608747</td>\n",
              "      <td>0.601589</td>\n",
              "      <td>0.486370</td>\n",
              "      <td>0.438110</td>\n",
              "      <td>0.451089</td>\n",
              "      <td>0.350254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.438164</td>\n",
              "      <td>0.430198</td>\n",
              "      <td>0.414659</td>\n",
              "      <td>0.453172</td>\n",
              "      <td>0.421494</td>\n",
              "      <td>0.416237</td>\n",
              "      <td>0.499467</td>\n",
              "      <td>0.367236</td>\n",
              "      <td>0.197480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.438164</td>\n",
              "      <td>0.430198</td>\n",
              "      <td>0.414659</td>\n",
              "      <td>0.453172</td>\n",
              "      <td>0.421494</td>\n",
              "      <td>0.416237</td>\n",
              "      <td>0.499467</td>\n",
              "      <td>0.367236</td>\n",
              "      <td>0.197480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.785794</td>\n",
              "      <td>0.828625</td>\n",
              "      <td>0.757231</td>\n",
              "      <td>0.750252</td>\n",
              "      <td>0.684079</td>\n",
              "      <td>0.345741</td>\n",
              "      <td>0.372614</td>\n",
              "      <td>0.387935</td>\n",
              "      <td>0.445926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.785794</td>\n",
              "      <td>0.828625</td>\n",
              "      <td>0.757231</td>\n",
              "      <td>0.750252</td>\n",
              "      <td>0.684079</td>\n",
              "      <td>0.345741</td>\n",
              "      <td>0.372614</td>\n",
              "      <td>0.387935</td>\n",
              "      <td>0.445926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.731336</td>\n",
              "      <td>0.713590</td>\n",
              "      <td>0.696568</td>\n",
              "      <td>0.670367</td>\n",
              "      <td>0.658615</td>\n",
              "      <td>0.272867</td>\n",
              "      <td>0.324795</td>\n",
              "      <td>0.297651</td>\n",
              "      <td>0.437736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.731336</td>\n",
              "      <td>0.713590</td>\n",
              "      <td>0.696568</td>\n",
              "      <td>0.670367</td>\n",
              "      <td>0.658615</td>\n",
              "      <td>0.272867</td>\n",
              "      <td>0.324795</td>\n",
              "      <td>0.297651</td>\n",
              "      <td>0.437736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.610473</td>\n",
              "      <td>0.614087</td>\n",
              "      <td>0.616739</td>\n",
              "      <td>0.594035</td>\n",
              "      <td>0.601326</td>\n",
              "      <td>0.471355</td>\n",
              "      <td>0.421149</td>\n",
              "      <td>0.546130</td>\n",
              "      <td>0.560357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.610473</td>\n",
              "      <td>0.614087</td>\n",
              "      <td>0.616739</td>\n",
              "      <td>0.594035</td>\n",
              "      <td>0.601326</td>\n",
              "      <td>0.471355</td>\n",
              "      <td>0.421149</td>\n",
              "      <td>0.546130</td>\n",
              "      <td>0.560357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.786173</td>\n",
              "      <td>0.830767</td>\n",
              "      <td>0.719763</td>\n",
              "      <td>0.721473</td>\n",
              "      <td>0.682943</td>\n",
              "      <td>0.388087</td>\n",
              "      <td>0.396658</td>\n",
              "      <td>0.405500</td>\n",
              "      <td>0.418850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.786173</td>\n",
              "      <td>0.830767</td>\n",
              "      <td>0.719763</td>\n",
              "      <td>0.721473</td>\n",
              "      <td>0.682943</td>\n",
              "      <td>0.388087</td>\n",
              "      <td>0.396658</td>\n",
              "      <td>0.405500</td>\n",
              "      <td>0.418850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">gpt-4</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.845805</td>\n",
              "      <td>0.795424</td>\n",
              "      <td>0.708960</td>\n",
              "      <td>0.663212</td>\n",
              "      <td>0.642841</td>\n",
              "      <td>0.358506</td>\n",
              "      <td>0.374432</td>\n",
              "      <td>0.374916</td>\n",
              "      <td>0.402692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.845805</td>\n",
              "      <td>0.795424</td>\n",
              "      <td>0.708960</td>\n",
              "      <td>0.663212</td>\n",
              "      <td>0.642841</td>\n",
              "      <td>0.358506</td>\n",
              "      <td>0.374432</td>\n",
              "      <td>0.374916</td>\n",
              "      <td>0.402692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.536176</td>\n",
              "      <td>0.522535</td>\n",
              "      <td>0.497165</td>\n",
              "      <td>0.515616</td>\n",
              "      <td>0.475945</td>\n",
              "      <td>0.375501</td>\n",
              "      <td>0.371973</td>\n",
              "      <td>0.375672</td>\n",
              "      <td>0.243161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.536176</td>\n",
              "      <td>0.522535</td>\n",
              "      <td>0.497165</td>\n",
              "      <td>0.515616</td>\n",
              "      <td>0.475945</td>\n",
              "      <td>0.375501</td>\n",
              "      <td>0.371973</td>\n",
              "      <td>0.375672</td>\n",
              "      <td>0.243161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.446006</td>\n",
              "      <td>0.433410</td>\n",
              "      <td>0.403710</td>\n",
              "      <td>0.438667</td>\n",
              "      <td>0.419926</td>\n",
              "      <td>0.403546</td>\n",
              "      <td>0.520717</td>\n",
              "      <td>0.362785</td>\n",
              "      <td>0.197699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.446006</td>\n",
              "      <td>0.433410</td>\n",
              "      <td>0.403710</td>\n",
              "      <td>0.438667</td>\n",
              "      <td>0.419926</td>\n",
              "      <td>0.403546</td>\n",
              "      <td>0.520717</td>\n",
              "      <td>0.362785</td>\n",
              "      <td>0.197699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.765694</td>\n",
              "      <td>0.761554</td>\n",
              "      <td>0.705776</td>\n",
              "      <td>0.698069</td>\n",
              "      <td>0.638721</td>\n",
              "      <td>0.355808</td>\n",
              "      <td>0.418632</td>\n",
              "      <td>0.425517</td>\n",
              "      <td>0.442825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.765694</td>\n",
              "      <td>0.761554</td>\n",
              "      <td>0.705776</td>\n",
              "      <td>0.698069</td>\n",
              "      <td>0.638721</td>\n",
              "      <td>0.355808</td>\n",
              "      <td>0.418632</td>\n",
              "      <td>0.425517</td>\n",
              "      <td>0.442825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.846920</td>\n",
              "      <td>0.807743</td>\n",
              "      <td>0.620146</td>\n",
              "      <td>0.614968</td>\n",
              "      <td>0.533108</td>\n",
              "      <td>0.335712</td>\n",
              "      <td>0.334625</td>\n",
              "      <td>0.338509</td>\n",
              "      <td>0.350923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.846920</td>\n",
              "      <td>0.807743</td>\n",
              "      <td>0.620146</td>\n",
              "      <td>0.614968</td>\n",
              "      <td>0.533108</td>\n",
              "      <td>0.335712</td>\n",
              "      <td>0.334625</td>\n",
              "      <td>0.338509</td>\n",
              "      <td>0.350923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.613606</td>\n",
              "      <td>0.613081</td>\n",
              "      <td>0.606976</td>\n",
              "      <td>0.588854</td>\n",
              "      <td>0.580427</td>\n",
              "      <td>0.377659</td>\n",
              "      <td>0.357731</td>\n",
              "      <td>0.508470</td>\n",
              "      <td>0.503498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.613606</td>\n",
              "      <td>0.613081</td>\n",
              "      <td>0.606976</td>\n",
              "      <td>0.588854</td>\n",
              "      <td>0.580427</td>\n",
              "      <td>0.377659</td>\n",
              "      <td>0.357731</td>\n",
              "      <td>0.508470</td>\n",
              "      <td>0.503498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.774210</td>\n",
              "      <td>0.715108</td>\n",
              "      <td>0.652566</td>\n",
              "      <td>0.675404</td>\n",
              "      <td>0.570681</td>\n",
              "      <td>0.374332</td>\n",
              "      <td>0.381520</td>\n",
              "      <td>0.382052</td>\n",
              "      <td>0.354579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.774210</td>\n",
              "      <td>0.715108</td>\n",
              "      <td>0.652566</td>\n",
              "      <td>0.675404</td>\n",
              "      <td>0.570681</td>\n",
              "      <td>0.374332</td>\n",
              "      <td>0.381520</td>\n",
              "      <td>0.382052</td>\n",
              "      <td>0.354579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">llama-65b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.432711</td>\n",
              "      <td>0.317376</td>\n",
              "      <td>0.296864</td>\n",
              "      <td>0.452214</td>\n",
              "      <td>0.382769</td>\n",
              "      <td>0.640336</td>\n",
              "      <td>0.684084</td>\n",
              "      <td>0.601528</td>\n",
              "      <td>0.411308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.432711</td>\n",
              "      <td>0.317376</td>\n",
              "      <td>0.296864</td>\n",
              "      <td>0.452214</td>\n",
              "      <td>0.382769</td>\n",
              "      <td>0.640336</td>\n",
              "      <td>0.684084</td>\n",
              "      <td>0.601528</td>\n",
              "      <td>0.411308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.480630</td>\n",
              "      <td>0.484301</td>\n",
              "      <td>0.509602</td>\n",
              "      <td>0.492094</td>\n",
              "      <td>0.493706</td>\n",
              "      <td>0.509512</td>\n",
              "      <td>0.511065</td>\n",
              "      <td>0.510785</td>\n",
              "      <td>0.531405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.480630</td>\n",
              "      <td>0.484301</td>\n",
              "      <td>0.509602</td>\n",
              "      <td>0.492094</td>\n",
              "      <td>0.493706</td>\n",
              "      <td>0.509512</td>\n",
              "      <td>0.511065</td>\n",
              "      <td>0.510785</td>\n",
              "      <td>0.531405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.409221</td>\n",
              "      <td>0.422094</td>\n",
              "      <td>0.453134</td>\n",
              "      <td>0.464423</td>\n",
              "      <td>0.419304</td>\n",
              "      <td>0.495358</td>\n",
              "      <td>0.533748</td>\n",
              "      <td>0.495689</td>\n",
              "      <td>0.466852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.409221</td>\n",
              "      <td>0.422094</td>\n",
              "      <td>0.453134</td>\n",
              "      <td>0.464423</td>\n",
              "      <td>0.419304</td>\n",
              "      <td>0.495358</td>\n",
              "      <td>0.533748</td>\n",
              "      <td>0.495689</td>\n",
              "      <td>0.466852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.508570</td>\n",
              "      <td>0.482354</td>\n",
              "      <td>0.484928</td>\n",
              "      <td>0.530832</td>\n",
              "      <td>0.478274</td>\n",
              "      <td>0.540967</td>\n",
              "      <td>0.560818</td>\n",
              "      <td>0.533706</td>\n",
              "      <td>0.541660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.508570</td>\n",
              "      <td>0.482354</td>\n",
              "      <td>0.484928</td>\n",
              "      <td>0.530832</td>\n",
              "      <td>0.478274</td>\n",
              "      <td>0.540967</td>\n",
              "      <td>0.560818</td>\n",
              "      <td>0.533706</td>\n",
              "      <td>0.541660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.562645</td>\n",
              "      <td>0.535639</td>\n",
              "      <td>0.543878</td>\n",
              "      <td>0.605376</td>\n",
              "      <td>0.547297</td>\n",
              "      <td>0.631759</td>\n",
              "      <td>0.634777</td>\n",
              "      <td>0.607457</td>\n",
              "      <td>0.588254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.562645</td>\n",
              "      <td>0.535639</td>\n",
              "      <td>0.543878</td>\n",
              "      <td>0.605376</td>\n",
              "      <td>0.547297</td>\n",
              "      <td>0.631759</td>\n",
              "      <td>0.634777</td>\n",
              "      <td>0.607457</td>\n",
              "      <td>0.588254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.415707</td>\n",
              "      <td>0.415258</td>\n",
              "      <td>0.415731</td>\n",
              "      <td>0.413273</td>\n",
              "      <td>0.413060</td>\n",
              "      <td>0.406047</td>\n",
              "      <td>0.412366</td>\n",
              "      <td>0.409882</td>\n",
              "      <td>0.537760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.415707</td>\n",
              "      <td>0.415258</td>\n",
              "      <td>0.415731</td>\n",
              "      <td>0.413273</td>\n",
              "      <td>0.413060</td>\n",
              "      <td>0.406047</td>\n",
              "      <td>0.412366</td>\n",
              "      <td>0.409882</td>\n",
              "      <td>0.537760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.518337</td>\n",
              "      <td>0.469120</td>\n",
              "      <td>0.481723</td>\n",
              "      <td>0.575652</td>\n",
              "      <td>0.513304</td>\n",
              "      <td>0.655350</td>\n",
              "      <td>0.662487</td>\n",
              "      <td>0.614469</td>\n",
              "      <td>0.527563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.518337</td>\n",
              "      <td>0.469120</td>\n",
              "      <td>0.481723</td>\n",
              "      <td>0.575652</td>\n",
              "      <td>0.513304</td>\n",
              "      <td>0.655350</td>\n",
              "      <td>0.662487</td>\n",
              "      <td>0.614469</td>\n",
              "      <td>0.527563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">opt-66b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.526402</td>\n",
              "      <td>0.444345</td>\n",
              "      <td>0.414439</td>\n",
              "      <td>0.503717</td>\n",
              "      <td>0.458338</td>\n",
              "      <td>0.576559</td>\n",
              "      <td>0.629073</td>\n",
              "      <td>0.562338</td>\n",
              "      <td>0.482495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.526402</td>\n",
              "      <td>0.444345</td>\n",
              "      <td>0.414439</td>\n",
              "      <td>0.503717</td>\n",
              "      <td>0.458338</td>\n",
              "      <td>0.576559</td>\n",
              "      <td>0.629073</td>\n",
              "      <td>0.562338</td>\n",
              "      <td>0.482495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.521714</td>\n",
              "      <td>0.470984</td>\n",
              "      <td>0.473351</td>\n",
              "      <td>0.551458</td>\n",
              "      <td>0.502842</td>\n",
              "      <td>0.628491</td>\n",
              "      <td>0.677941</td>\n",
              "      <td>0.597358</td>\n",
              "      <td>0.470752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.521714</td>\n",
              "      <td>0.470984</td>\n",
              "      <td>0.473351</td>\n",
              "      <td>0.551458</td>\n",
              "      <td>0.502842</td>\n",
              "      <td>0.628491</td>\n",
              "      <td>0.677941</td>\n",
              "      <td>0.597358</td>\n",
              "      <td>0.470752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.457371</td>\n",
              "      <td>0.455639</td>\n",
              "      <td>0.520984</td>\n",
              "      <td>0.497737</td>\n",
              "      <td>0.476860</td>\n",
              "      <td>0.567559</td>\n",
              "      <td>0.665022</td>\n",
              "      <td>0.537868</td>\n",
              "      <td>0.345167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.457371</td>\n",
              "      <td>0.455639</td>\n",
              "      <td>0.520984</td>\n",
              "      <td>0.497737</td>\n",
              "      <td>0.476860</td>\n",
              "      <td>0.567559</td>\n",
              "      <td>0.665022</td>\n",
              "      <td>0.537868</td>\n",
              "      <td>0.345167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.624758</td>\n",
              "      <td>0.618479</td>\n",
              "      <td>0.583475</td>\n",
              "      <td>0.616905</td>\n",
              "      <td>0.486970</td>\n",
              "      <td>0.438807</td>\n",
              "      <td>0.601623</td>\n",
              "      <td>0.534366</td>\n",
              "      <td>0.477264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.624758</td>\n",
              "      <td>0.618479</td>\n",
              "      <td>0.583475</td>\n",
              "      <td>0.616905</td>\n",
              "      <td>0.486970</td>\n",
              "      <td>0.438807</td>\n",
              "      <td>0.601623</td>\n",
              "      <td>0.534366</td>\n",
              "      <td>0.477264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.592428</td>\n",
              "      <td>0.520340</td>\n",
              "      <td>0.458583</td>\n",
              "      <td>0.602537</td>\n",
              "      <td>0.426629</td>\n",
              "      <td>0.574441</td>\n",
              "      <td>0.655563</td>\n",
              "      <td>0.596935</td>\n",
              "      <td>0.443604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.592428</td>\n",
              "      <td>0.520340</td>\n",
              "      <td>0.458583</td>\n",
              "      <td>0.602537</td>\n",
              "      <td>0.426629</td>\n",
              "      <td>0.574441</td>\n",
              "      <td>0.655563</td>\n",
              "      <td>0.596935</td>\n",
              "      <td>0.443604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.478735</td>\n",
              "      <td>0.477587</td>\n",
              "      <td>0.478462</td>\n",
              "      <td>0.472583</td>\n",
              "      <td>0.473071</td>\n",
              "      <td>0.471897</td>\n",
              "      <td>0.461880</td>\n",
              "      <td>0.470761</td>\n",
              "      <td>0.579981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.478735</td>\n",
              "      <td>0.477587</td>\n",
              "      <td>0.478462</td>\n",
              "      <td>0.472583</td>\n",
              "      <td>0.473071</td>\n",
              "      <td>0.471897</td>\n",
              "      <td>0.461880</td>\n",
              "      <td>0.470761</td>\n",
              "      <td>0.579981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.554454</td>\n",
              "      <td>0.504792</td>\n",
              "      <td>0.518040</td>\n",
              "      <td>0.592877</td>\n",
              "      <td>0.470675</td>\n",
              "      <td>0.676182</td>\n",
              "      <td>0.708483</td>\n",
              "      <td>0.611580</td>\n",
              "      <td>0.498294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.554454</td>\n",
              "      <td>0.504792</td>\n",
              "      <td>0.518040</td>\n",
              "      <td>0.592877</td>\n",
              "      <td>0.470675</td>\n",
              "      <td>0.676182</td>\n",
              "      <td>0.708483</td>\n",
              "      <td>0.611580</td>\n",
              "      <td>0.498294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">opt-iml-max-1.3b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.533043</td>\n",
              "      <td>0.528943</td>\n",
              "      <td>0.510768</td>\n",
              "      <td>0.544160</td>\n",
              "      <td>0.517325</td>\n",
              "      <td>0.562628</td>\n",
              "      <td>0.563494</td>\n",
              "      <td>0.545035</td>\n",
              "      <td>0.579916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.533043</td>\n",
              "      <td>0.528943</td>\n",
              "      <td>0.510768</td>\n",
              "      <td>0.544160</td>\n",
              "      <td>0.517325</td>\n",
              "      <td>0.562628</td>\n",
              "      <td>0.563494</td>\n",
              "      <td>0.545035</td>\n",
              "      <td>0.579916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.478760</td>\n",
              "      <td>0.461260</td>\n",
              "      <td>0.474519</td>\n",
              "      <td>0.500856</td>\n",
              "      <td>0.501064</td>\n",
              "      <td>0.624022</td>\n",
              "      <td>0.602123</td>\n",
              "      <td>0.565670</td>\n",
              "      <td>0.509822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.478760</td>\n",
              "      <td>0.461260</td>\n",
              "      <td>0.474519</td>\n",
              "      <td>0.500856</td>\n",
              "      <td>0.501064</td>\n",
              "      <td>0.624022</td>\n",
              "      <td>0.602123</td>\n",
              "      <td>0.565670</td>\n",
              "      <td>0.509822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.505258</td>\n",
              "      <td>0.493618</td>\n",
              "      <td>0.522037</td>\n",
              "      <td>0.543087</td>\n",
              "      <td>0.519183</td>\n",
              "      <td>0.614131</td>\n",
              "      <td>0.689406</td>\n",
              "      <td>0.549681</td>\n",
              "      <td>0.413160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.505258</td>\n",
              "      <td>0.493618</td>\n",
              "      <td>0.522037</td>\n",
              "      <td>0.543087</td>\n",
              "      <td>0.519183</td>\n",
              "      <td>0.614131</td>\n",
              "      <td>0.689406</td>\n",
              "      <td>0.549681</td>\n",
              "      <td>0.413160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.606138</td>\n",
              "      <td>0.620005</td>\n",
              "      <td>0.613530</td>\n",
              "      <td>0.633120</td>\n",
              "      <td>0.600379</td>\n",
              "      <td>0.617217</td>\n",
              "      <td>0.623846</td>\n",
              "      <td>0.590380</td>\n",
              "      <td>0.603522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.606138</td>\n",
              "      <td>0.620005</td>\n",
              "      <td>0.613530</td>\n",
              "      <td>0.633120</td>\n",
              "      <td>0.600379</td>\n",
              "      <td>0.617217</td>\n",
              "      <td>0.623846</td>\n",
              "      <td>0.590380</td>\n",
              "      <td>0.603522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.509315</td>\n",
              "      <td>0.537425</td>\n",
              "      <td>0.597235</td>\n",
              "      <td>0.795252</td>\n",
              "      <td>0.666393</td>\n",
              "      <td>0.913757</td>\n",
              "      <td>0.823460</td>\n",
              "      <td>0.813907</td>\n",
              "      <td>0.533466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.509315</td>\n",
              "      <td>0.537425</td>\n",
              "      <td>0.597235</td>\n",
              "      <td>0.795252</td>\n",
              "      <td>0.666393</td>\n",
              "      <td>0.913757</td>\n",
              "      <td>0.823460</td>\n",
              "      <td>0.813907</td>\n",
              "      <td>0.533466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.649095</td>\n",
              "      <td>0.659102</td>\n",
              "      <td>0.654303</td>\n",
              "      <td>0.653459</td>\n",
              "      <td>0.643833</td>\n",
              "      <td>0.677238</td>\n",
              "      <td>0.572192</td>\n",
              "      <td>0.648384</td>\n",
              "      <td>0.652380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.649095</td>\n",
              "      <td>0.659102</td>\n",
              "      <td>0.654303</td>\n",
              "      <td>0.653459</td>\n",
              "      <td>0.643833</td>\n",
              "      <td>0.677238</td>\n",
              "      <td>0.572192</td>\n",
              "      <td>0.648384</td>\n",
              "      <td>0.652380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.519586</td>\n",
              "      <td>0.521796</td>\n",
              "      <td>0.557576</td>\n",
              "      <td>0.667456</td>\n",
              "      <td>0.573627</td>\n",
              "      <td>0.773080</td>\n",
              "      <td>0.757210</td>\n",
              "      <td>0.682991</td>\n",
              "      <td>0.517943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.519586</td>\n",
              "      <td>0.521796</td>\n",
              "      <td>0.557576</td>\n",
              "      <td>0.667456</td>\n",
              "      <td>0.573627</td>\n",
              "      <td>0.773080</td>\n",
              "      <td>0.757210</td>\n",
              "      <td>0.682991</td>\n",
              "      <td>0.517943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">text-davinci-003</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.811605</td>\n",
              "      <td>0.869161</td>\n",
              "      <td>0.828877</td>\n",
              "      <td>0.763961</td>\n",
              "      <td>0.742691</td>\n",
              "      <td>0.370307</td>\n",
              "      <td>0.348985</td>\n",
              "      <td>0.387589</td>\n",
              "      <td>0.466336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.811605</td>\n",
              "      <td>0.869161</td>\n",
              "      <td>0.828877</td>\n",
              "      <td>0.763961</td>\n",
              "      <td>0.742691</td>\n",
              "      <td>0.370307</td>\n",
              "      <td>0.348985</td>\n",
              "      <td>0.387589</td>\n",
              "      <td>0.466336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.531004</td>\n",
              "      <td>0.592415</td>\n",
              "      <td>0.587801</td>\n",
              "      <td>0.594161</td>\n",
              "      <td>0.577547</td>\n",
              "      <td>0.530221</td>\n",
              "      <td>0.401311</td>\n",
              "      <td>0.461366</td>\n",
              "      <td>0.352606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.531004</td>\n",
              "      <td>0.592415</td>\n",
              "      <td>0.587801</td>\n",
              "      <td>0.594161</td>\n",
              "      <td>0.577547</td>\n",
              "      <td>0.530221</td>\n",
              "      <td>0.401311</td>\n",
              "      <td>0.461366</td>\n",
              "      <td>0.352606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.437797</td>\n",
              "      <td>0.428374</td>\n",
              "      <td>0.417170</td>\n",
              "      <td>0.441097</td>\n",
              "      <td>0.417578</td>\n",
              "      <td>0.408826</td>\n",
              "      <td>0.491612</td>\n",
              "      <td>0.365072</td>\n",
              "      <td>0.193042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.437797</td>\n",
              "      <td>0.428374</td>\n",
              "      <td>0.417170</td>\n",
              "      <td>0.441097</td>\n",
              "      <td>0.417578</td>\n",
              "      <td>0.408826</td>\n",
              "      <td>0.491612</td>\n",
              "      <td>0.365072</td>\n",
              "      <td>0.193042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.770905</td>\n",
              "      <td>0.804355</td>\n",
              "      <td>0.763631</td>\n",
              "      <td>0.730595</td>\n",
              "      <td>0.683438</td>\n",
              "      <td>0.351834</td>\n",
              "      <td>0.362415</td>\n",
              "      <td>0.410279</td>\n",
              "      <td>0.462483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.770905</td>\n",
              "      <td>0.804355</td>\n",
              "      <td>0.763631</td>\n",
              "      <td>0.730595</td>\n",
              "      <td>0.683438</td>\n",
              "      <td>0.351834</td>\n",
              "      <td>0.362415</td>\n",
              "      <td>0.410279</td>\n",
              "      <td>0.462483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.753282</td>\n",
              "      <td>0.739783</td>\n",
              "      <td>0.719272</td>\n",
              "      <td>0.687262</td>\n",
              "      <td>0.660011</td>\n",
              "      <td>0.264892</td>\n",
              "      <td>0.318991</td>\n",
              "      <td>0.324835</td>\n",
              "      <td>0.478074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.753282</td>\n",
              "      <td>0.739783</td>\n",
              "      <td>0.719272</td>\n",
              "      <td>0.687262</td>\n",
              "      <td>0.660011</td>\n",
              "      <td>0.264892</td>\n",
              "      <td>0.318991</td>\n",
              "      <td>0.324835</td>\n",
              "      <td>0.478074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.567119</td>\n",
              "      <td>0.569066</td>\n",
              "      <td>0.572172</td>\n",
              "      <td>0.553437</td>\n",
              "      <td>0.561767</td>\n",
              "      <td>0.481713</td>\n",
              "      <td>0.402408</td>\n",
              "      <td>0.521642</td>\n",
              "      <td>0.564681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.567119</td>\n",
              "      <td>0.569066</td>\n",
              "      <td>0.572172</td>\n",
              "      <td>0.553437</td>\n",
              "      <td>0.561767</td>\n",
              "      <td>0.481713</td>\n",
              "      <td>0.402408</td>\n",
              "      <td>0.521642</td>\n",
              "      <td>0.564681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.746851</td>\n",
              "      <td>0.799051</td>\n",
              "      <td>0.742989</td>\n",
              "      <td>0.706243</td>\n",
              "      <td>0.684713</td>\n",
              "      <td>0.521610</td>\n",
              "      <td>0.415832</td>\n",
              "      <td>0.465613</td>\n",
              "      <td>0.452250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.746851</td>\n",
              "      <td>0.799051</td>\n",
              "      <td>0.742989</td>\n",
              "      <td>0.706243</td>\n",
              "      <td>0.684713</td>\n",
              "      <td>0.521610</td>\n",
              "      <td>0.415832</td>\n",
              "      <td>0.465613</td>\n",
              "      <td>0.452250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">vicuna-13b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.867224</td>\n",
              "      <td>0.874497</td>\n",
              "      <td>0.821160</td>\n",
              "      <td>0.835301</td>\n",
              "      <td>0.755192</td>\n",
              "      <td>0.425580</td>\n",
              "      <td>0.441932</td>\n",
              "      <td>0.416745</td>\n",
              "      <td>0.529381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.867224</td>\n",
              "      <td>0.874497</td>\n",
              "      <td>0.821160</td>\n",
              "      <td>0.835301</td>\n",
              "      <td>0.755192</td>\n",
              "      <td>0.425580</td>\n",
              "      <td>0.441932</td>\n",
              "      <td>0.416745</td>\n",
              "      <td>0.529381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.566895</td>\n",
              "      <td>0.576884</td>\n",
              "      <td>0.565475</td>\n",
              "      <td>0.592047</td>\n",
              "      <td>0.582279</td>\n",
              "      <td>0.613638</td>\n",
              "      <td>0.519718</td>\n",
              "      <td>0.516087</td>\n",
              "      <td>0.445433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.566895</td>\n",
              "      <td>0.576884</td>\n",
              "      <td>0.565475</td>\n",
              "      <td>0.592047</td>\n",
              "      <td>0.582279</td>\n",
              "      <td>0.613638</td>\n",
              "      <td>0.519718</td>\n",
              "      <td>0.516087</td>\n",
              "      <td>0.445433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.442135</td>\n",
              "      <td>0.452078</td>\n",
              "      <td>0.555839</td>\n",
              "      <td>0.520420</td>\n",
              "      <td>0.511708</td>\n",
              "      <td>0.532865</td>\n",
              "      <td>0.535956</td>\n",
              "      <td>0.499351</td>\n",
              "      <td>0.313205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.442135</td>\n",
              "      <td>0.452078</td>\n",
              "      <td>0.555839</td>\n",
              "      <td>0.520420</td>\n",
              "      <td>0.511708</td>\n",
              "      <td>0.532865</td>\n",
              "      <td>0.535956</td>\n",
              "      <td>0.499351</td>\n",
              "      <td>0.313205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.736586</td>\n",
              "      <td>0.756777</td>\n",
              "      <td>0.724204</td>\n",
              "      <td>0.731212</td>\n",
              "      <td>0.674120</td>\n",
              "      <td>0.366510</td>\n",
              "      <td>0.427459</td>\n",
              "      <td>0.470568</td>\n",
              "      <td>0.500709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.736586</td>\n",
              "      <td>0.756777</td>\n",
              "      <td>0.724204</td>\n",
              "      <td>0.731212</td>\n",
              "      <td>0.674120</td>\n",
              "      <td>0.366510</td>\n",
              "      <td>0.427459</td>\n",
              "      <td>0.470568</td>\n",
              "      <td>0.500709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.592511</td>\n",
              "      <td>0.577340</td>\n",
              "      <td>0.553817</td>\n",
              "      <td>0.532847</td>\n",
              "      <td>0.514061</td>\n",
              "      <td>0.213053</td>\n",
              "      <td>0.401312</td>\n",
              "      <td>0.279484</td>\n",
              "      <td>0.453134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.592511</td>\n",
              "      <td>0.577340</td>\n",
              "      <td>0.553817</td>\n",
              "      <td>0.532847</td>\n",
              "      <td>0.514061</td>\n",
              "      <td>0.213053</td>\n",
              "      <td>0.401312</td>\n",
              "      <td>0.279484</td>\n",
              "      <td>0.453134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.609310</td>\n",
              "      <td>0.614995</td>\n",
              "      <td>0.617004</td>\n",
              "      <td>0.598000</td>\n",
              "      <td>0.602860</td>\n",
              "      <td>0.530956</td>\n",
              "      <td>0.442067</td>\n",
              "      <td>0.562276</td>\n",
              "      <td>0.579163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.609310</td>\n",
              "      <td>0.614995</td>\n",
              "      <td>0.617004</td>\n",
              "      <td>0.598000</td>\n",
              "      <td>0.602860</td>\n",
              "      <td>0.530956</td>\n",
              "      <td>0.442067</td>\n",
              "      <td>0.562276</td>\n",
              "      <td>0.579163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.747565</td>\n",
              "      <td>0.780721</td>\n",
              "      <td>0.703831</td>\n",
              "      <td>0.723371</td>\n",
              "      <td>0.659794</td>\n",
              "      <td>0.511644</td>\n",
              "      <td>0.487835</td>\n",
              "      <td>0.489583</td>\n",
              "      <td>0.465287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.747565</td>\n",
              "      <td>0.780721</td>\n",
              "      <td>0.703831</td>\n",
              "      <td>0.723371</td>\n",
              "      <td>0.659794</td>\n",
              "      <td>0.511644</td>\n",
              "      <td>0.487835</td>\n",
              "      <td>0.489583</td>\n",
              "      <td>0.465287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"119\" valign=\"top\">es</th>\n",
              "      <th rowspan=\"7\" valign=\"top\">all</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.756407</td>\n",
              "      <td>0.774542</td>\n",
              "      <td>0.762005</td>\n",
              "      <td>0.770255</td>\n",
              "      <td>0.747767</td>\n",
              "      <td>0.746052</td>\n",
              "      <td>0.762353</td>\n",
              "      <td>0.683472</td>\n",
              "      <td>0.740026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.408230</td>\n",
              "      <td>0.413019</td>\n",
              "      <td>0.435535</td>\n",
              "      <td>0.427387</td>\n",
              "      <td>0.427450</td>\n",
              "      <td>0.426456</td>\n",
              "      <td>0.441124</td>\n",
              "      <td>0.435071</td>\n",
              "      <td>0.491472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.569450</td>\n",
              "      <td>0.610274</td>\n",
              "      <td>0.629649</td>\n",
              "      <td>0.621749</td>\n",
              "      <td>0.639213</td>\n",
              "      <td>0.640059</td>\n",
              "      <td>0.645790</td>\n",
              "      <td>0.565513</td>\n",
              "      <td>0.567999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.717659</td>\n",
              "      <td>0.734052</td>\n",
              "      <td>0.719308</td>\n",
              "      <td>0.724551</td>\n",
              "      <td>0.713553</td>\n",
              "      <td>0.705262</td>\n",
              "      <td>0.728480</td>\n",
              "      <td>0.641124</td>\n",
              "      <td>0.684915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.769217</td>\n",
              "      <td>0.773139</td>\n",
              "      <td>0.772076</td>\n",
              "      <td>0.772198</td>\n",
              "      <td>0.767961</td>\n",
              "      <td>0.776808</td>\n",
              "      <td>0.761112</td>\n",
              "      <td>0.737539</td>\n",
              "      <td>0.796051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.655892</td>\n",
              "      <td>0.679865</td>\n",
              "      <td>0.668878</td>\n",
              "      <td>0.681316</td>\n",
              "      <td>0.670930</td>\n",
              "      <td>0.729118</td>\n",
              "      <td>0.713048</td>\n",
              "      <td>0.655543</td>\n",
              "      <td>0.634903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.822754</td>\n",
              "      <td>0.827960</td>\n",
              "      <td>0.824201</td>\n",
              "      <td>0.822695</td>\n",
              "      <td>0.811687</td>\n",
              "      <td>0.791989</td>\n",
              "      <td>0.813617</td>\n",
              "      <td>0.761340</td>\n",
              "      <td>0.811016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">alpaca-lora-30b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.807827</td>\n",
              "      <td>0.890533</td>\n",
              "      <td>0.858084</td>\n",
              "      <td>0.826751</td>\n",
              "      <td>0.829515</td>\n",
              "      <td>0.528624</td>\n",
              "      <td>0.515693</td>\n",
              "      <td>0.452312</td>\n",
              "      <td>0.544745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.807827</td>\n",
              "      <td>0.890533</td>\n",
              "      <td>0.858084</td>\n",
              "      <td>0.826751</td>\n",
              "      <td>0.829515</td>\n",
              "      <td>0.528624</td>\n",
              "      <td>0.515693</td>\n",
              "      <td>0.452312</td>\n",
              "      <td>0.544745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.510482</td>\n",
              "      <td>0.541388</td>\n",
              "      <td>0.547345</td>\n",
              "      <td>0.562389</td>\n",
              "      <td>0.556214</td>\n",
              "      <td>0.479522</td>\n",
              "      <td>0.536341</td>\n",
              "      <td>0.468060</td>\n",
              "      <td>0.335094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.510482</td>\n",
              "      <td>0.541388</td>\n",
              "      <td>0.547345</td>\n",
              "      <td>0.562389</td>\n",
              "      <td>0.556214</td>\n",
              "      <td>0.479522</td>\n",
              "      <td>0.536341</td>\n",
              "      <td>0.468060</td>\n",
              "      <td>0.335094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.449078</td>\n",
              "      <td>0.597503</td>\n",
              "      <td>0.663043</td>\n",
              "      <td>0.573020</td>\n",
              "      <td>0.725897</td>\n",
              "      <td>0.583718</td>\n",
              "      <td>0.571407</td>\n",
              "      <td>0.491556</td>\n",
              "      <td>0.396313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.449078</td>\n",
              "      <td>0.597503</td>\n",
              "      <td>0.663043</td>\n",
              "      <td>0.573020</td>\n",
              "      <td>0.725897</td>\n",
              "      <td>0.583718</td>\n",
              "      <td>0.571407</td>\n",
              "      <td>0.491556</td>\n",
              "      <td>0.396313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.696568</td>\n",
              "      <td>0.800509</td>\n",
              "      <td>0.774817</td>\n",
              "      <td>0.750793</td>\n",
              "      <td>0.802501</td>\n",
              "      <td>0.559165</td>\n",
              "      <td>0.611429</td>\n",
              "      <td>0.531391</td>\n",
              "      <td>0.555471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.696568</td>\n",
              "      <td>0.800509</td>\n",
              "      <td>0.774817</td>\n",
              "      <td>0.750793</td>\n",
              "      <td>0.802501</td>\n",
              "      <td>0.559165</td>\n",
              "      <td>0.611429</td>\n",
              "      <td>0.531391</td>\n",
              "      <td>0.555471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.823715</td>\n",
              "      <td>0.850513</td>\n",
              "      <td>0.863198</td>\n",
              "      <td>0.848412</td>\n",
              "      <td>0.846514</td>\n",
              "      <td>0.600382</td>\n",
              "      <td>0.476565</td>\n",
              "      <td>0.517264</td>\n",
              "      <td>0.546349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.823715</td>\n",
              "      <td>0.850513</td>\n",
              "      <td>0.863198</td>\n",
              "      <td>0.848412</td>\n",
              "      <td>0.846514</td>\n",
              "      <td>0.600382</td>\n",
              "      <td>0.476565</td>\n",
              "      <td>0.517264</td>\n",
              "      <td>0.546349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.645201</td>\n",
              "      <td>0.684684</td>\n",
              "      <td>0.666668</td>\n",
              "      <td>0.662942</td>\n",
              "      <td>0.680669</td>\n",
              "      <td>0.375803</td>\n",
              "      <td>0.525403</td>\n",
              "      <td>0.476398</td>\n",
              "      <td>0.406698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.645201</td>\n",
              "      <td>0.684684</td>\n",
              "      <td>0.666668</td>\n",
              "      <td>0.662942</td>\n",
              "      <td>0.680669</td>\n",
              "      <td>0.375803</td>\n",
              "      <td>0.525403</td>\n",
              "      <td>0.476398</td>\n",
              "      <td>0.406698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.645166</td>\n",
              "      <td>0.813569</td>\n",
              "      <td>0.816889</td>\n",
              "      <td>0.765276</td>\n",
              "      <td>0.859926</td>\n",
              "      <td>0.530708</td>\n",
              "      <td>0.427914</td>\n",
              "      <td>0.435287</td>\n",
              "      <td>0.464705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.645166</td>\n",
              "      <td>0.813569</td>\n",
              "      <td>0.816889</td>\n",
              "      <td>0.765276</td>\n",
              "      <td>0.859926</td>\n",
              "      <td>0.530708</td>\n",
              "      <td>0.427914</td>\n",
              "      <td>0.435287</td>\n",
              "      <td>0.464705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">gpt-3.5-turbo</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.828953</td>\n",
              "      <td>0.901102</td>\n",
              "      <td>0.836356</td>\n",
              "      <td>0.790922</td>\n",
              "      <td>0.752371</td>\n",
              "      <td>0.373587</td>\n",
              "      <td>0.347307</td>\n",
              "      <td>0.384162</td>\n",
              "      <td>0.471506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.828953</td>\n",
              "      <td>0.901102</td>\n",
              "      <td>0.836356</td>\n",
              "      <td>0.790922</td>\n",
              "      <td>0.752371</td>\n",
              "      <td>0.373587</td>\n",
              "      <td>0.347307</td>\n",
              "      <td>0.384162</td>\n",
              "      <td>0.471506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.561989</td>\n",
              "      <td>0.595763</td>\n",
              "      <td>0.545695</td>\n",
              "      <td>0.573226</td>\n",
              "      <td>0.524984</td>\n",
              "      <td>0.427711</td>\n",
              "      <td>0.454531</td>\n",
              "      <td>0.445054</td>\n",
              "      <td>0.319260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.561989</td>\n",
              "      <td>0.595763</td>\n",
              "      <td>0.545695</td>\n",
              "      <td>0.573226</td>\n",
              "      <td>0.524984</td>\n",
              "      <td>0.427711</td>\n",
              "      <td>0.454531</td>\n",
              "      <td>0.445054</td>\n",
              "      <td>0.319260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.502223</td>\n",
              "      <td>0.578518</td>\n",
              "      <td>0.560992</td>\n",
              "      <td>0.536628</td>\n",
              "      <td>0.518531</td>\n",
              "      <td>0.512145</td>\n",
              "      <td>0.529858</td>\n",
              "      <td>0.396593</td>\n",
              "      <td>0.292554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.502223</td>\n",
              "      <td>0.578518</td>\n",
              "      <td>0.560992</td>\n",
              "      <td>0.536628</td>\n",
              "      <td>0.518531</td>\n",
              "      <td>0.512145</td>\n",
              "      <td>0.529858</td>\n",
              "      <td>0.396593</td>\n",
              "      <td>0.292554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.803299</td>\n",
              "      <td>0.859639</td>\n",
              "      <td>0.797670</td>\n",
              "      <td>0.772096</td>\n",
              "      <td>0.717230</td>\n",
              "      <td>0.370160</td>\n",
              "      <td>0.393437</td>\n",
              "      <td>0.409314</td>\n",
              "      <td>0.464688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.803299</td>\n",
              "      <td>0.859639</td>\n",
              "      <td>0.797670</td>\n",
              "      <td>0.772096</td>\n",
              "      <td>0.717230</td>\n",
              "      <td>0.370160</td>\n",
              "      <td>0.393437</td>\n",
              "      <td>0.409314</td>\n",
              "      <td>0.464688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.903984</td>\n",
              "      <td>0.926604</td>\n",
              "      <td>0.845145</td>\n",
              "      <td>0.802990</td>\n",
              "      <td>0.763541</td>\n",
              "      <td>0.326371</td>\n",
              "      <td>0.347225</td>\n",
              "      <td>0.340518</td>\n",
              "      <td>0.494788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.903984</td>\n",
              "      <td>0.926604</td>\n",
              "      <td>0.845145</td>\n",
              "      <td>0.802990</td>\n",
              "      <td>0.763541</td>\n",
              "      <td>0.326371</td>\n",
              "      <td>0.347225</td>\n",
              "      <td>0.340518</td>\n",
              "      <td>0.494788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.687462</td>\n",
              "      <td>0.698075</td>\n",
              "      <td>0.666956</td>\n",
              "      <td>0.655411</td>\n",
              "      <td>0.620171</td>\n",
              "      <td>0.332869</td>\n",
              "      <td>0.446844</td>\n",
              "      <td>0.425731</td>\n",
              "      <td>0.393647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.687462</td>\n",
              "      <td>0.698075</td>\n",
              "      <td>0.666956</td>\n",
              "      <td>0.655411</td>\n",
              "      <td>0.620171</td>\n",
              "      <td>0.332869</td>\n",
              "      <td>0.446844</td>\n",
              "      <td>0.425731</td>\n",
              "      <td>0.393647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.801038</td>\n",
              "      <td>0.893940</td>\n",
              "      <td>0.812715</td>\n",
              "      <td>0.773826</td>\n",
              "      <td>0.712522</td>\n",
              "      <td>0.400503</td>\n",
              "      <td>0.381634</td>\n",
              "      <td>0.381594</td>\n",
              "      <td>0.441229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.801038</td>\n",
              "      <td>0.893940</td>\n",
              "      <td>0.812715</td>\n",
              "      <td>0.773826</td>\n",
              "      <td>0.712522</td>\n",
              "      <td>0.400503</td>\n",
              "      <td>0.381634</td>\n",
              "      <td>0.381594</td>\n",
              "      <td>0.441229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">gpt-4</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.880243</td>\n",
              "      <td>0.870233</td>\n",
              "      <td>0.823954</td>\n",
              "      <td>0.796894</td>\n",
              "      <td>0.741813</td>\n",
              "      <td>0.379837</td>\n",
              "      <td>0.428534</td>\n",
              "      <td>0.425133</td>\n",
              "      <td>0.515462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.880243</td>\n",
              "      <td>0.870233</td>\n",
              "      <td>0.823954</td>\n",
              "      <td>0.796894</td>\n",
              "      <td>0.741813</td>\n",
              "      <td>0.379837</td>\n",
              "      <td>0.428534</td>\n",
              "      <td>0.425133</td>\n",
              "      <td>0.515462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.578472</td>\n",
              "      <td>0.553701</td>\n",
              "      <td>0.545626</td>\n",
              "      <td>0.551024</td>\n",
              "      <td>0.506006</td>\n",
              "      <td>0.451181</td>\n",
              "      <td>0.455306</td>\n",
              "      <td>0.463157</td>\n",
              "      <td>0.325855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.578472</td>\n",
              "      <td>0.553701</td>\n",
              "      <td>0.545626</td>\n",
              "      <td>0.551024</td>\n",
              "      <td>0.506006</td>\n",
              "      <td>0.451181</td>\n",
              "      <td>0.455306</td>\n",
              "      <td>0.463157</td>\n",
              "      <td>0.325855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.555052</td>\n",
              "      <td>0.505868</td>\n",
              "      <td>0.410940</td>\n",
              "      <td>0.468006</td>\n",
              "      <td>0.423609</td>\n",
              "      <td>0.430259</td>\n",
              "      <td>0.517202</td>\n",
              "      <td>0.371317</td>\n",
              "      <td>0.233255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.555052</td>\n",
              "      <td>0.505868</td>\n",
              "      <td>0.410940</td>\n",
              "      <td>0.468006</td>\n",
              "      <td>0.423609</td>\n",
              "      <td>0.430259</td>\n",
              "      <td>0.517202</td>\n",
              "      <td>0.371317</td>\n",
              "      <td>0.233255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.852589</td>\n",
              "      <td>0.842028</td>\n",
              "      <td>0.742801</td>\n",
              "      <td>0.737230</td>\n",
              "      <td>0.630055</td>\n",
              "      <td>0.361372</td>\n",
              "      <td>0.419598</td>\n",
              "      <td>0.382275</td>\n",
              "      <td>0.430409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.852589</td>\n",
              "      <td>0.842028</td>\n",
              "      <td>0.742801</td>\n",
              "      <td>0.737230</td>\n",
              "      <td>0.630055</td>\n",
              "      <td>0.361372</td>\n",
              "      <td>0.419598</td>\n",
              "      <td>0.382275</td>\n",
              "      <td>0.430409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.911326</td>\n",
              "      <td>0.909492</td>\n",
              "      <td>0.797744</td>\n",
              "      <td>0.776773</td>\n",
              "      <td>0.689545</td>\n",
              "      <td>0.337111</td>\n",
              "      <td>0.357258</td>\n",
              "      <td>0.347109</td>\n",
              "      <td>0.473011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.911326</td>\n",
              "      <td>0.909492</td>\n",
              "      <td>0.797744</td>\n",
              "      <td>0.776773</td>\n",
              "      <td>0.689545</td>\n",
              "      <td>0.337111</td>\n",
              "      <td>0.357258</td>\n",
              "      <td>0.347109</td>\n",
              "      <td>0.473011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.706333</td>\n",
              "      <td>0.689199</td>\n",
              "      <td>0.642154</td>\n",
              "      <td>0.637546</td>\n",
              "      <td>0.584510</td>\n",
              "      <td>0.318732</td>\n",
              "      <td>0.423075</td>\n",
              "      <td>0.386238</td>\n",
              "      <td>0.365708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.706333</td>\n",
              "      <td>0.689199</td>\n",
              "      <td>0.642154</td>\n",
              "      <td>0.637546</td>\n",
              "      <td>0.584510</td>\n",
              "      <td>0.318732</td>\n",
              "      <td>0.423075</td>\n",
              "      <td>0.386238</td>\n",
              "      <td>0.365708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.767969</td>\n",
              "      <td>0.762675</td>\n",
              "      <td>0.678276</td>\n",
              "      <td>0.699953</td>\n",
              "      <td>0.623344</td>\n",
              "      <td>0.378265</td>\n",
              "      <td>0.369487</td>\n",
              "      <td>0.366985</td>\n",
              "      <td>0.365840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.767969</td>\n",
              "      <td>0.762675</td>\n",
              "      <td>0.678276</td>\n",
              "      <td>0.699953</td>\n",
              "      <td>0.623344</td>\n",
              "      <td>0.378265</td>\n",
              "      <td>0.369487</td>\n",
              "      <td>0.366985</td>\n",
              "      <td>0.365840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">llama-65b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.429948</td>\n",
              "      <td>0.381501</td>\n",
              "      <td>0.345809</td>\n",
              "      <td>0.508116</td>\n",
              "      <td>0.442373</td>\n",
              "      <td>0.735577</td>\n",
              "      <td>0.821421</td>\n",
              "      <td>0.668702</td>\n",
              "      <td>0.409573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.429948</td>\n",
              "      <td>0.381501</td>\n",
              "      <td>0.345809</td>\n",
              "      <td>0.508116</td>\n",
              "      <td>0.442373</td>\n",
              "      <td>0.735577</td>\n",
              "      <td>0.821421</td>\n",
              "      <td>0.668702</td>\n",
              "      <td>0.409573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.480871</td>\n",
              "      <td>0.463229</td>\n",
              "      <td>0.484928</td>\n",
              "      <td>0.523059</td>\n",
              "      <td>0.504539</td>\n",
              "      <td>0.615126</td>\n",
              "      <td>0.718008</td>\n",
              "      <td>0.569875</td>\n",
              "      <td>0.447883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.480871</td>\n",
              "      <td>0.463229</td>\n",
              "      <td>0.484928</td>\n",
              "      <td>0.523059</td>\n",
              "      <td>0.504539</td>\n",
              "      <td>0.615126</td>\n",
              "      <td>0.718008</td>\n",
              "      <td>0.569875</td>\n",
              "      <td>0.447883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.472059</td>\n",
              "      <td>0.453580</td>\n",
              "      <td>0.483164</td>\n",
              "      <td>0.502418</td>\n",
              "      <td>0.496316</td>\n",
              "      <td>0.592352</td>\n",
              "      <td>0.740466</td>\n",
              "      <td>0.532473</td>\n",
              "      <td>0.420968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.472059</td>\n",
              "      <td>0.453580</td>\n",
              "      <td>0.483164</td>\n",
              "      <td>0.502418</td>\n",
              "      <td>0.496316</td>\n",
              "      <td>0.592352</td>\n",
              "      <td>0.740466</td>\n",
              "      <td>0.532473</td>\n",
              "      <td>0.420968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.519882</td>\n",
              "      <td>0.544659</td>\n",
              "      <td>0.526209</td>\n",
              "      <td>0.574171</td>\n",
              "      <td>0.577591</td>\n",
              "      <td>0.641868</td>\n",
              "      <td>0.694319</td>\n",
              "      <td>0.554199</td>\n",
              "      <td>0.524760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.519882</td>\n",
              "      <td>0.544659</td>\n",
              "      <td>0.526209</td>\n",
              "      <td>0.574171</td>\n",
              "      <td>0.577591</td>\n",
              "      <td>0.641868</td>\n",
              "      <td>0.694319</td>\n",
              "      <td>0.554199</td>\n",
              "      <td>0.524760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.438188</td>\n",
              "      <td>0.422407</td>\n",
              "      <td>0.474644</td>\n",
              "      <td>0.644163</td>\n",
              "      <td>0.588732</td>\n",
              "      <td>0.857894</td>\n",
              "      <td>0.867849</td>\n",
              "      <td>0.771485</td>\n",
              "      <td>0.484322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.438188</td>\n",
              "      <td>0.422407</td>\n",
              "      <td>0.474644</td>\n",
              "      <td>0.644163</td>\n",
              "      <td>0.588732</td>\n",
              "      <td>0.857894</td>\n",
              "      <td>0.867849</td>\n",
              "      <td>0.771485</td>\n",
              "      <td>0.484322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.628179</td>\n",
              "      <td>0.633109</td>\n",
              "      <td>0.611076</td>\n",
              "      <td>0.622882</td>\n",
              "      <td>0.579316</td>\n",
              "      <td>0.493428</td>\n",
              "      <td>0.660175</td>\n",
              "      <td>0.542222</td>\n",
              "      <td>0.579094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.628179</td>\n",
              "      <td>0.633109</td>\n",
              "      <td>0.611076</td>\n",
              "      <td>0.622882</td>\n",
              "      <td>0.579316</td>\n",
              "      <td>0.493428</td>\n",
              "      <td>0.660175</td>\n",
              "      <td>0.542222</td>\n",
              "      <td>0.579094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.445081</td>\n",
              "      <td>0.428384</td>\n",
              "      <td>0.425906</td>\n",
              "      <td>0.616786</td>\n",
              "      <td>0.575689</td>\n",
              "      <td>0.833164</td>\n",
              "      <td>0.856514</td>\n",
              "      <td>0.723967</td>\n",
              "      <td>0.465634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.445081</td>\n",
              "      <td>0.428384</td>\n",
              "      <td>0.425906</td>\n",
              "      <td>0.616786</td>\n",
              "      <td>0.575689</td>\n",
              "      <td>0.833164</td>\n",
              "      <td>0.856514</td>\n",
              "      <td>0.723967</td>\n",
              "      <td>0.465634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">opt-66b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.536997</td>\n",
              "      <td>0.542004</td>\n",
              "      <td>0.518158</td>\n",
              "      <td>0.639956</td>\n",
              "      <td>0.537798</td>\n",
              "      <td>0.739716</td>\n",
              "      <td>0.747646</td>\n",
              "      <td>0.707425</td>\n",
              "      <td>0.531365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.536997</td>\n",
              "      <td>0.542004</td>\n",
              "      <td>0.518158</td>\n",
              "      <td>0.639956</td>\n",
              "      <td>0.537798</td>\n",
              "      <td>0.739716</td>\n",
              "      <td>0.747646</td>\n",
              "      <td>0.707425</td>\n",
              "      <td>0.531365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.478527</td>\n",
              "      <td>0.461158</td>\n",
              "      <td>0.460408</td>\n",
              "      <td>0.528197</td>\n",
              "      <td>0.473990</td>\n",
              "      <td>0.620457</td>\n",
              "      <td>0.650425</td>\n",
              "      <td>0.600901</td>\n",
              "      <td>0.478770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.478527</td>\n",
              "      <td>0.461158</td>\n",
              "      <td>0.460408</td>\n",
              "      <td>0.528197</td>\n",
              "      <td>0.473990</td>\n",
              "      <td>0.620457</td>\n",
              "      <td>0.650425</td>\n",
              "      <td>0.600901</td>\n",
              "      <td>0.478770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.457473</td>\n",
              "      <td>0.503340</td>\n",
              "      <td>0.574086</td>\n",
              "      <td>0.564522</td>\n",
              "      <td>0.537059</td>\n",
              "      <td>0.650606</td>\n",
              "      <td>0.610230</td>\n",
              "      <td>0.601931</td>\n",
              "      <td>0.437147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.457473</td>\n",
              "      <td>0.503340</td>\n",
              "      <td>0.574086</td>\n",
              "      <td>0.564522</td>\n",
              "      <td>0.537059</td>\n",
              "      <td>0.650606</td>\n",
              "      <td>0.610230</td>\n",
              "      <td>0.601931</td>\n",
              "      <td>0.437147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.539005</td>\n",
              "      <td>0.575161</td>\n",
              "      <td>0.568385</td>\n",
              "      <td>0.598629</td>\n",
              "      <td>0.569766</td>\n",
              "      <td>0.603779</td>\n",
              "      <td>0.569991</td>\n",
              "      <td>0.597176</td>\n",
              "      <td>0.549055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.539005</td>\n",
              "      <td>0.575161</td>\n",
              "      <td>0.568385</td>\n",
              "      <td>0.598629</td>\n",
              "      <td>0.569766</td>\n",
              "      <td>0.603779</td>\n",
              "      <td>0.569991</td>\n",
              "      <td>0.597176</td>\n",
              "      <td>0.549055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.400021</td>\n",
              "      <td>0.401939</td>\n",
              "      <td>0.607262</td>\n",
              "      <td>0.741862</td>\n",
              "      <td>0.747028</td>\n",
              "      <td>0.886105</td>\n",
              "      <td>0.802689</td>\n",
              "      <td>0.865094</td>\n",
              "      <td>0.528281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.400021</td>\n",
              "      <td>0.401939</td>\n",
              "      <td>0.607262</td>\n",
              "      <td>0.741862</td>\n",
              "      <td>0.747028</td>\n",
              "      <td>0.886105</td>\n",
              "      <td>0.802689</td>\n",
              "      <td>0.865094</td>\n",
              "      <td>0.528281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.634111</td>\n",
              "      <td>0.647780</td>\n",
              "      <td>0.650938</td>\n",
              "      <td>0.636044</td>\n",
              "      <td>0.631064</td>\n",
              "      <td>0.668285</td>\n",
              "      <td>0.607509</td>\n",
              "      <td>0.668286</td>\n",
              "      <td>0.542597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.634111</td>\n",
              "      <td>0.647780</td>\n",
              "      <td>0.650938</td>\n",
              "      <td>0.636044</td>\n",
              "      <td>0.631064</td>\n",
              "      <td>0.668285</td>\n",
              "      <td>0.607509</td>\n",
              "      <td>0.668286</td>\n",
              "      <td>0.542597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.465354</td>\n",
              "      <td>0.478076</td>\n",
              "      <td>0.588880</td>\n",
              "      <td>0.662631</td>\n",
              "      <td>0.634030</td>\n",
              "      <td>0.708624</td>\n",
              "      <td>0.706596</td>\n",
              "      <td>0.705301</td>\n",
              "      <td>0.577967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.465354</td>\n",
              "      <td>0.478076</td>\n",
              "      <td>0.588880</td>\n",
              "      <td>0.662631</td>\n",
              "      <td>0.634030</td>\n",
              "      <td>0.708624</td>\n",
              "      <td>0.706596</td>\n",
              "      <td>0.705301</td>\n",
              "      <td>0.577967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">opt-iml-max-1.3b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.345517</td>\n",
              "      <td>0.359474</td>\n",
              "      <td>0.350286</td>\n",
              "      <td>0.501182</td>\n",
              "      <td>0.434089</td>\n",
              "      <td>0.917835</td>\n",
              "      <td>0.765078</td>\n",
              "      <td>0.694672</td>\n",
              "      <td>0.348358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.345517</td>\n",
              "      <td>0.359474</td>\n",
              "      <td>0.350286</td>\n",
              "      <td>0.501182</td>\n",
              "      <td>0.434089</td>\n",
              "      <td>0.917835</td>\n",
              "      <td>0.765078</td>\n",
              "      <td>0.694672</td>\n",
              "      <td>0.348358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.490858</td>\n",
              "      <td>0.525103</td>\n",
              "      <td>0.528393</td>\n",
              "      <td>0.573783</td>\n",
              "      <td>0.544521</td>\n",
              "      <td>0.707548</td>\n",
              "      <td>0.655723</td>\n",
              "      <td>0.584892</td>\n",
              "      <td>0.501950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.490858</td>\n",
              "      <td>0.525103</td>\n",
              "      <td>0.528393</td>\n",
              "      <td>0.573783</td>\n",
              "      <td>0.544521</td>\n",
              "      <td>0.707548</td>\n",
              "      <td>0.655723</td>\n",
              "      <td>0.584892</td>\n",
              "      <td>0.501950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.439919</td>\n",
              "      <td>0.466809</td>\n",
              "      <td>0.521470</td>\n",
              "      <td>0.511101</td>\n",
              "      <td>0.476267</td>\n",
              "      <td>0.677026</td>\n",
              "      <td>0.571864</td>\n",
              "      <td>0.497204</td>\n",
              "      <td>0.335497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.439919</td>\n",
              "      <td>0.466809</td>\n",
              "      <td>0.521470</td>\n",
              "      <td>0.511101</td>\n",
              "      <td>0.476267</td>\n",
              "      <td>0.677026</td>\n",
              "      <td>0.571864</td>\n",
              "      <td>0.497204</td>\n",
              "      <td>0.335497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.364142</td>\n",
              "      <td>0.406000</td>\n",
              "      <td>0.480591</td>\n",
              "      <td>0.590208</td>\n",
              "      <td>0.600459</td>\n",
              "      <td>0.878735</td>\n",
              "      <td>0.738389</td>\n",
              "      <td>0.677734</td>\n",
              "      <td>0.434361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.364142</td>\n",
              "      <td>0.406000</td>\n",
              "      <td>0.480591</td>\n",
              "      <td>0.590208</td>\n",
              "      <td>0.600459</td>\n",
              "      <td>0.878735</td>\n",
              "      <td>0.738389</td>\n",
              "      <td>0.677734</td>\n",
              "      <td>0.434361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.335227</td>\n",
              "      <td>0.345571</td>\n",
              "      <td>0.469239</td>\n",
              "      <td>0.567421</td>\n",
              "      <td>0.578770</td>\n",
              "      <td>0.932284</td>\n",
              "      <td>0.569028</td>\n",
              "      <td>0.804597</td>\n",
              "      <td>0.369579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.335227</td>\n",
              "      <td>0.345571</td>\n",
              "      <td>0.469239</td>\n",
              "      <td>0.567421</td>\n",
              "      <td>0.578770</td>\n",
              "      <td>0.932284</td>\n",
              "      <td>0.569028</td>\n",
              "      <td>0.804597</td>\n",
              "      <td>0.369579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.582879</td>\n",
              "      <td>0.623194</td>\n",
              "      <td>0.630301</td>\n",
              "      <td>0.642404</td>\n",
              "      <td>0.626819</td>\n",
              "      <td>0.734720</td>\n",
              "      <td>0.571362</td>\n",
              "      <td>0.649810</td>\n",
              "      <td>0.541318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.582879</td>\n",
              "      <td>0.623194</td>\n",
              "      <td>0.630301</td>\n",
              "      <td>0.642404</td>\n",
              "      <td>0.626819</td>\n",
              "      <td>0.734720</td>\n",
              "      <td>0.571362</td>\n",
              "      <td>0.649810</td>\n",
              "      <td>0.541318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.332034</td>\n",
              "      <td>0.331025</td>\n",
              "      <td>0.343768</td>\n",
              "      <td>0.450039</td>\n",
              "      <td>0.473695</td>\n",
              "      <td>0.885644</td>\n",
              "      <td>0.521044</td>\n",
              "      <td>0.675749</td>\n",
              "      <td>0.290052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.332034</td>\n",
              "      <td>0.331025</td>\n",
              "      <td>0.343768</td>\n",
              "      <td>0.450039</td>\n",
              "      <td>0.473695</td>\n",
              "      <td>0.885644</td>\n",
              "      <td>0.521044</td>\n",
              "      <td>0.675749</td>\n",
              "      <td>0.290052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">text-davinci-003</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.784046</td>\n",
              "      <td>0.870012</td>\n",
              "      <td>0.839783</td>\n",
              "      <td>0.810548</td>\n",
              "      <td>0.758460</td>\n",
              "      <td>0.499226</td>\n",
              "      <td>0.383222</td>\n",
              "      <td>0.445866</td>\n",
              "      <td>0.511852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.784046</td>\n",
              "      <td>0.870012</td>\n",
              "      <td>0.839783</td>\n",
              "      <td>0.810548</td>\n",
              "      <td>0.758460</td>\n",
              "      <td>0.499226</td>\n",
              "      <td>0.383222</td>\n",
              "      <td>0.445866</td>\n",
              "      <td>0.511852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.542862</td>\n",
              "      <td>0.580711</td>\n",
              "      <td>0.590808</td>\n",
              "      <td>0.574487</td>\n",
              "      <td>0.544943</td>\n",
              "      <td>0.459035</td>\n",
              "      <td>0.434165</td>\n",
              "      <td>0.471284</td>\n",
              "      <td>0.344822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.542862</td>\n",
              "      <td>0.580711</td>\n",
              "      <td>0.590808</td>\n",
              "      <td>0.574487</td>\n",
              "      <td>0.544943</td>\n",
              "      <td>0.459035</td>\n",
              "      <td>0.434165</td>\n",
              "      <td>0.471284</td>\n",
              "      <td>0.344822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.474970</td>\n",
              "      <td>0.658208</td>\n",
              "      <td>0.727539</td>\n",
              "      <td>0.615896</td>\n",
              "      <td>0.654878</td>\n",
              "      <td>0.613219</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.482187</td>\n",
              "      <td>0.382733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.474970</td>\n",
              "      <td>0.658208</td>\n",
              "      <td>0.727539</td>\n",
              "      <td>0.615896</td>\n",
              "      <td>0.654878</td>\n",
              "      <td>0.613219</td>\n",
              "      <td>0.490427</td>\n",
              "      <td>0.482187</td>\n",
              "      <td>0.382733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.772954</td>\n",
              "      <td>0.860416</td>\n",
              "      <td>0.822961</td>\n",
              "      <td>0.782344</td>\n",
              "      <td>0.735110</td>\n",
              "      <td>0.377743</td>\n",
              "      <td>0.402494</td>\n",
              "      <td>0.402151</td>\n",
              "      <td>0.469657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.772954</td>\n",
              "      <td>0.860416</td>\n",
              "      <td>0.822961</td>\n",
              "      <td>0.782344</td>\n",
              "      <td>0.735110</td>\n",
              "      <td>0.377743</td>\n",
              "      <td>0.402494</td>\n",
              "      <td>0.402151</td>\n",
              "      <td>0.469657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.888102</td>\n",
              "      <td>0.889804</td>\n",
              "      <td>0.845859</td>\n",
              "      <td>0.813574</td>\n",
              "      <td>0.778314</td>\n",
              "      <td>0.351735</td>\n",
              "      <td>0.361963</td>\n",
              "      <td>0.355353</td>\n",
              "      <td>0.483200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.888102</td>\n",
              "      <td>0.889804</td>\n",
              "      <td>0.845859</td>\n",
              "      <td>0.813574</td>\n",
              "      <td>0.778314</td>\n",
              "      <td>0.351735</td>\n",
              "      <td>0.361963</td>\n",
              "      <td>0.355353</td>\n",
              "      <td>0.483200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.660211</td>\n",
              "      <td>0.692269</td>\n",
              "      <td>0.664114</td>\n",
              "      <td>0.645926</td>\n",
              "      <td>0.614743</td>\n",
              "      <td>0.337915</td>\n",
              "      <td>0.452305</td>\n",
              "      <td>0.415002</td>\n",
              "      <td>0.376126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.660211</td>\n",
              "      <td>0.692269</td>\n",
              "      <td>0.664114</td>\n",
              "      <td>0.645926</td>\n",
              "      <td>0.614743</td>\n",
              "      <td>0.337915</td>\n",
              "      <td>0.452305</td>\n",
              "      <td>0.415002</td>\n",
              "      <td>0.376126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.856881</td>\n",
              "      <td>0.934541</td>\n",
              "      <td>0.911300</td>\n",
              "      <td>0.862708</td>\n",
              "      <td>0.790945</td>\n",
              "      <td>0.435786</td>\n",
              "      <td>0.389762</td>\n",
              "      <td>0.400079</td>\n",
              "      <td>0.506892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.856881</td>\n",
              "      <td>0.934541</td>\n",
              "      <td>0.911300</td>\n",
              "      <td>0.862708</td>\n",
              "      <td>0.790945</td>\n",
              "      <td>0.435786</td>\n",
              "      <td>0.389762</td>\n",
              "      <td>0.400079</td>\n",
              "      <td>0.506892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">vicuna-13b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.844382</td>\n",
              "      <td>0.895358</td>\n",
              "      <td>0.852523</td>\n",
              "      <td>0.848329</td>\n",
              "      <td>0.781171</td>\n",
              "      <td>0.499232</td>\n",
              "      <td>0.481860</td>\n",
              "      <td>0.445165</td>\n",
              "      <td>0.533970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.844382</td>\n",
              "      <td>0.895358</td>\n",
              "      <td>0.852523</td>\n",
              "      <td>0.848329</td>\n",
              "      <td>0.781171</td>\n",
              "      <td>0.499232</td>\n",
              "      <td>0.481860</td>\n",
              "      <td>0.445165</td>\n",
              "      <td>0.533970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.570722</td>\n",
              "      <td>0.595000</td>\n",
              "      <td>0.541493</td>\n",
              "      <td>0.601691</td>\n",
              "      <td>0.542844</td>\n",
              "      <td>0.487593</td>\n",
              "      <td>0.558606</td>\n",
              "      <td>0.474528</td>\n",
              "      <td>0.362544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.570722</td>\n",
              "      <td>0.595000</td>\n",
              "      <td>0.541493</td>\n",
              "      <td>0.601691</td>\n",
              "      <td>0.542844</td>\n",
              "      <td>0.487593</td>\n",
              "      <td>0.558606</td>\n",
              "      <td>0.474528</td>\n",
              "      <td>0.362544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.581898</td>\n",
              "      <td>0.688590</td>\n",
              "      <td>0.708367</td>\n",
              "      <td>0.706606</td>\n",
              "      <td>0.667251</td>\n",
              "      <td>0.661265</td>\n",
              "      <td>0.596212</td>\n",
              "      <td>0.504030</td>\n",
              "      <td>0.445570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.581898</td>\n",
              "      <td>0.688590</td>\n",
              "      <td>0.708367</td>\n",
              "      <td>0.706606</td>\n",
              "      <td>0.667251</td>\n",
              "      <td>0.661265</td>\n",
              "      <td>0.596212</td>\n",
              "      <td>0.504030</td>\n",
              "      <td>0.445570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.766373</td>\n",
              "      <td>0.796709</td>\n",
              "      <td>0.782388</td>\n",
              "      <td>0.781468</td>\n",
              "      <td>0.749968</td>\n",
              "      <td>0.482835</td>\n",
              "      <td>0.600672</td>\n",
              "      <td>0.511069</td>\n",
              "      <td>0.561170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.766373</td>\n",
              "      <td>0.796709</td>\n",
              "      <td>0.782388</td>\n",
              "      <td>0.781468</td>\n",
              "      <td>0.749968</td>\n",
              "      <td>0.482835</td>\n",
              "      <td>0.600672</td>\n",
              "      <td>0.511069</td>\n",
              "      <td>0.561170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.863041</td>\n",
              "      <td>0.900852</td>\n",
              "      <td>0.851754</td>\n",
              "      <td>0.865591</td>\n",
              "      <td>0.772824</td>\n",
              "      <td>0.479366</td>\n",
              "      <td>0.479286</td>\n",
              "      <td>0.408273</td>\n",
              "      <td>0.533426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.863041</td>\n",
              "      <td>0.900852</td>\n",
              "      <td>0.851754</td>\n",
              "      <td>0.865591</td>\n",
              "      <td>0.772824</td>\n",
              "      <td>0.479366</td>\n",
              "      <td>0.479286</td>\n",
              "      <td>0.408273</td>\n",
              "      <td>0.533426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.687239</td>\n",
              "      <td>0.730351</td>\n",
              "      <td>0.696573</td>\n",
              "      <td>0.714052</td>\n",
              "      <td>0.648929</td>\n",
              "      <td>0.404100</td>\n",
              "      <td>0.536251</td>\n",
              "      <td>0.478893</td>\n",
              "      <td>0.439047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.687239</td>\n",
              "      <td>0.730351</td>\n",
              "      <td>0.696573</td>\n",
              "      <td>0.714052</td>\n",
              "      <td>0.648929</td>\n",
              "      <td>0.404100</td>\n",
              "      <td>0.536251</td>\n",
              "      <td>0.478893</td>\n",
              "      <td>0.439047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.699092</td>\n",
              "      <td>0.792443</td>\n",
              "      <td>0.774209</td>\n",
              "      <td>0.813266</td>\n",
              "      <td>0.708626</td>\n",
              "      <td>0.667884</td>\n",
              "      <td>0.509451</td>\n",
              "      <td>0.506038</td>\n",
              "      <td>0.489621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.699092</td>\n",
              "      <td>0.792443</td>\n",
              "      <td>0.774209</td>\n",
              "      <td>0.813266</td>\n",
              "      <td>0.708626</td>\n",
              "      <td>0.667884</td>\n",
              "      <td>0.509451</td>\n",
              "      <td>0.506038</td>\n",
              "      <td>0.489621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"119\" valign=\"top\">ru</th>\n",
              "      <th rowspan=\"7\" valign=\"top\">all</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.593363</td>\n",
              "      <td>0.595614</td>\n",
              "      <td>0.591678</td>\n",
              "      <td>0.593899</td>\n",
              "      <td>0.589619</td>\n",
              "      <td>0.579198</td>\n",
              "      <td>0.591222</td>\n",
              "      <td>0.570235</td>\n",
              "      <td>0.675434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.334938</td>\n",
              "      <td>0.334597</td>\n",
              "      <td>0.334734</td>\n",
              "      <td>0.335041</td>\n",
              "      <td>0.335007</td>\n",
              "      <td>0.331835</td>\n",
              "      <td>0.333641</td>\n",
              "      <td>0.334533</td>\n",
              "      <td>0.470887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.546518</td>\n",
              "      <td>0.562153</td>\n",
              "      <td>0.537755</td>\n",
              "      <td>0.562294</td>\n",
              "      <td>0.570469</td>\n",
              "      <td>0.570463</td>\n",
              "      <td>0.590907</td>\n",
              "      <td>0.548742</td>\n",
              "      <td>0.594514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.675996</td>\n",
              "      <td>0.680912</td>\n",
              "      <td>0.678807</td>\n",
              "      <td>0.677413</td>\n",
              "      <td>0.677112</td>\n",
              "      <td>0.677970</td>\n",
              "      <td>0.682123</td>\n",
              "      <td>0.619571</td>\n",
              "      <td>0.721899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.674903</td>\n",
              "      <td>0.673815</td>\n",
              "      <td>0.675664</td>\n",
              "      <td>0.674276</td>\n",
              "      <td>0.677108</td>\n",
              "      <td>0.671397</td>\n",
              "      <td>0.674720</td>\n",
              "      <td>0.666684</td>\n",
              "      <td>0.746595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.444655</td>\n",
              "      <td>0.446701</td>\n",
              "      <td>0.445661</td>\n",
              "      <td>0.445628</td>\n",
              "      <td>0.445594</td>\n",
              "      <td>0.441196</td>\n",
              "      <td>0.446650</td>\n",
              "      <td>0.443981</td>\n",
              "      <td>0.564216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.776748</td>\n",
              "      <td>0.779663</td>\n",
              "      <td>0.776265</td>\n",
              "      <td>0.774785</td>\n",
              "      <td>0.771267</td>\n",
              "      <td>0.783235</td>\n",
              "      <td>0.778175</td>\n",
              "      <td>0.733798</td>\n",
              "      <td>0.798669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">alpaca-lora-30b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.727572</td>\n",
              "      <td>0.742415</td>\n",
              "      <td>0.727944</td>\n",
              "      <td>0.707865</td>\n",
              "      <td>0.712663</td>\n",
              "      <td>0.507114</td>\n",
              "      <td>0.490572</td>\n",
              "      <td>0.489059</td>\n",
              "      <td>0.567941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.727572</td>\n",
              "      <td>0.742415</td>\n",
              "      <td>0.727944</td>\n",
              "      <td>0.707865</td>\n",
              "      <td>0.712663</td>\n",
              "      <td>0.507114</td>\n",
              "      <td>0.490572</td>\n",
              "      <td>0.489059</td>\n",
              "      <td>0.567941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.490623</td>\n",
              "      <td>0.473657</td>\n",
              "      <td>0.474564</td>\n",
              "      <td>0.487297</td>\n",
              "      <td>0.488869</td>\n",
              "      <td>0.572538</td>\n",
              "      <td>0.638808</td>\n",
              "      <td>0.526478</td>\n",
              "      <td>0.369966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.490623</td>\n",
              "      <td>0.473657</td>\n",
              "      <td>0.474564</td>\n",
              "      <td>0.487297</td>\n",
              "      <td>0.488869</td>\n",
              "      <td>0.572538</td>\n",
              "      <td>0.638808</td>\n",
              "      <td>0.526478</td>\n",
              "      <td>0.369966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.433799</td>\n",
              "      <td>0.497830</td>\n",
              "      <td>0.415352</td>\n",
              "      <td>0.415410</td>\n",
              "      <td>0.609512</td>\n",
              "      <td>0.466738</td>\n",
              "      <td>0.574986</td>\n",
              "      <td>0.385549</td>\n",
              "      <td>0.249810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.433799</td>\n",
              "      <td>0.497830</td>\n",
              "      <td>0.415352</td>\n",
              "      <td>0.415410</td>\n",
              "      <td>0.609512</td>\n",
              "      <td>0.466738</td>\n",
              "      <td>0.574986</td>\n",
              "      <td>0.385549</td>\n",
              "      <td>0.249810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.660963</td>\n",
              "      <td>0.708923</td>\n",
              "      <td>0.688471</td>\n",
              "      <td>0.678443</td>\n",
              "      <td>0.707655</td>\n",
              "      <td>0.498553</td>\n",
              "      <td>0.569842</td>\n",
              "      <td>0.500403</td>\n",
              "      <td>0.544351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.660963</td>\n",
              "      <td>0.708923</td>\n",
              "      <td>0.688471</td>\n",
              "      <td>0.678443</td>\n",
              "      <td>0.707655</td>\n",
              "      <td>0.498553</td>\n",
              "      <td>0.569842</td>\n",
              "      <td>0.500403</td>\n",
              "      <td>0.544351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.795880</td>\n",
              "      <td>0.836616</td>\n",
              "      <td>0.870000</td>\n",
              "      <td>0.863713</td>\n",
              "      <td>0.901845</td>\n",
              "      <td>0.861113</td>\n",
              "      <td>0.578751</td>\n",
              "      <td>0.743632</td>\n",
              "      <td>0.659287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.795880</td>\n",
              "      <td>0.836616</td>\n",
              "      <td>0.870000</td>\n",
              "      <td>0.863713</td>\n",
              "      <td>0.901845</td>\n",
              "      <td>0.861113</td>\n",
              "      <td>0.578751</td>\n",
              "      <td>0.743632</td>\n",
              "      <td>0.659287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.513894</td>\n",
              "      <td>0.523997</td>\n",
              "      <td>0.513443</td>\n",
              "      <td>0.492256</td>\n",
              "      <td>0.535886</td>\n",
              "      <td>0.425370</td>\n",
              "      <td>0.488967</td>\n",
              "      <td>0.472350</td>\n",
              "      <td>0.505909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.513894</td>\n",
              "      <td>0.523997</td>\n",
              "      <td>0.513443</td>\n",
              "      <td>0.492256</td>\n",
              "      <td>0.535886</td>\n",
              "      <td>0.425370</td>\n",
              "      <td>0.488967</td>\n",
              "      <td>0.472350</td>\n",
              "      <td>0.505909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.776239</td>\n",
              "      <td>0.850243</td>\n",
              "      <td>0.873881</td>\n",
              "      <td>0.847002</td>\n",
              "      <td>0.921184</td>\n",
              "      <td>0.800625</td>\n",
              "      <td>0.511200</td>\n",
              "      <td>0.574964</td>\n",
              "      <td>0.579426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.776239</td>\n",
              "      <td>0.850243</td>\n",
              "      <td>0.873881</td>\n",
              "      <td>0.847002</td>\n",
              "      <td>0.921184</td>\n",
              "      <td>0.800625</td>\n",
              "      <td>0.511200</td>\n",
              "      <td>0.574964</td>\n",
              "      <td>0.579426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">gpt-3.5-turbo</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.854759</td>\n",
              "      <td>0.883391</td>\n",
              "      <td>0.835752</td>\n",
              "      <td>0.817148</td>\n",
              "      <td>0.761681</td>\n",
              "      <td>0.425899</td>\n",
              "      <td>0.380810</td>\n",
              "      <td>0.436265</td>\n",
              "      <td>0.525860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.854759</td>\n",
              "      <td>0.883391</td>\n",
              "      <td>0.835752</td>\n",
              "      <td>0.817148</td>\n",
              "      <td>0.761681</td>\n",
              "      <td>0.425899</td>\n",
              "      <td>0.380810</td>\n",
              "      <td>0.436265</td>\n",
              "      <td>0.525860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.498302</td>\n",
              "      <td>0.488864</td>\n",
              "      <td>0.434711</td>\n",
              "      <td>0.438362</td>\n",
              "      <td>0.440807</td>\n",
              "      <td>0.390803</td>\n",
              "      <td>0.408386</td>\n",
              "      <td>0.385726</td>\n",
              "      <td>0.235810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.498302</td>\n",
              "      <td>0.488864</td>\n",
              "      <td>0.434711</td>\n",
              "      <td>0.438362</td>\n",
              "      <td>0.440807</td>\n",
              "      <td>0.390803</td>\n",
              "      <td>0.408386</td>\n",
              "      <td>0.385726</td>\n",
              "      <td>0.235810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.459909</td>\n",
              "      <td>0.487876</td>\n",
              "      <td>0.423387</td>\n",
              "      <td>0.432814</td>\n",
              "      <td>0.453942</td>\n",
              "      <td>0.432484</td>\n",
              "      <td>0.439461</td>\n",
              "      <td>0.362316</td>\n",
              "      <td>0.205929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.459909</td>\n",
              "      <td>0.487876</td>\n",
              "      <td>0.423387</td>\n",
              "      <td>0.432814</td>\n",
              "      <td>0.453942</td>\n",
              "      <td>0.432484</td>\n",
              "      <td>0.439461</td>\n",
              "      <td>0.362316</td>\n",
              "      <td>0.205929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.784028</td>\n",
              "      <td>0.838748</td>\n",
              "      <td>0.771182</td>\n",
              "      <td>0.759379</td>\n",
              "      <td>0.710733</td>\n",
              "      <td>0.366569</td>\n",
              "      <td>0.382755</td>\n",
              "      <td>0.428041</td>\n",
              "      <td>0.457593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.784028</td>\n",
              "      <td>0.838748</td>\n",
              "      <td>0.771182</td>\n",
              "      <td>0.759379</td>\n",
              "      <td>0.710733</td>\n",
              "      <td>0.366569</td>\n",
              "      <td>0.382755</td>\n",
              "      <td>0.428041</td>\n",
              "      <td>0.457593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.907412</td>\n",
              "      <td>0.918469</td>\n",
              "      <td>0.860763</td>\n",
              "      <td>0.851493</td>\n",
              "      <td>0.812685</td>\n",
              "      <td>0.364402</td>\n",
              "      <td>0.402492</td>\n",
              "      <td>0.379878</td>\n",
              "      <td>0.513331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.907412</td>\n",
              "      <td>0.918469</td>\n",
              "      <td>0.860763</td>\n",
              "      <td>0.851493</td>\n",
              "      <td>0.812685</td>\n",
              "      <td>0.364402</td>\n",
              "      <td>0.402492</td>\n",
              "      <td>0.379878</td>\n",
              "      <td>0.513331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.524653</td>\n",
              "      <td>0.525704</td>\n",
              "      <td>0.499794</td>\n",
              "      <td>0.499337</td>\n",
              "      <td>0.512572</td>\n",
              "      <td>0.362901</td>\n",
              "      <td>0.461201</td>\n",
              "      <td>0.478841</td>\n",
              "      <td>0.448999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.524653</td>\n",
              "      <td>0.525704</td>\n",
              "      <td>0.499794</td>\n",
              "      <td>0.499337</td>\n",
              "      <td>0.512572</td>\n",
              "      <td>0.362901</td>\n",
              "      <td>0.461201</td>\n",
              "      <td>0.478841</td>\n",
              "      <td>0.448999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.931978</td>\n",
              "      <td>0.947180</td>\n",
              "      <td>0.905740</td>\n",
              "      <td>0.883186</td>\n",
              "      <td>0.852313</td>\n",
              "      <td>0.412969</td>\n",
              "      <td>0.431128</td>\n",
              "      <td>0.419648</td>\n",
              "      <td>0.543448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.931978</td>\n",
              "      <td>0.947180</td>\n",
              "      <td>0.905740</td>\n",
              "      <td>0.883186</td>\n",
              "      <td>0.852313</td>\n",
              "      <td>0.412969</td>\n",
              "      <td>0.431128</td>\n",
              "      <td>0.419648</td>\n",
              "      <td>0.543448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">gpt-4</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.774697</td>\n",
              "      <td>0.767134</td>\n",
              "      <td>0.735353</td>\n",
              "      <td>0.713914</td>\n",
              "      <td>0.664545</td>\n",
              "      <td>0.405104</td>\n",
              "      <td>0.409385</td>\n",
              "      <td>0.464901</td>\n",
              "      <td>0.521967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.774697</td>\n",
              "      <td>0.767134</td>\n",
              "      <td>0.735353</td>\n",
              "      <td>0.713914</td>\n",
              "      <td>0.664545</td>\n",
              "      <td>0.405104</td>\n",
              "      <td>0.409385</td>\n",
              "      <td>0.464901</td>\n",
              "      <td>0.521967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.478839</td>\n",
              "      <td>0.478184</td>\n",
              "      <td>0.429674</td>\n",
              "      <td>0.452126</td>\n",
              "      <td>0.456044</td>\n",
              "      <td>0.432759</td>\n",
              "      <td>0.482879</td>\n",
              "      <td>0.424568</td>\n",
              "      <td>0.260882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.478839</td>\n",
              "      <td>0.478184</td>\n",
              "      <td>0.429674</td>\n",
              "      <td>0.452126</td>\n",
              "      <td>0.456044</td>\n",
              "      <td>0.432759</td>\n",
              "      <td>0.482879</td>\n",
              "      <td>0.424568</td>\n",
              "      <td>0.260882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.500350</td>\n",
              "      <td>0.493490</td>\n",
              "      <td>0.454937</td>\n",
              "      <td>0.458278</td>\n",
              "      <td>0.489256</td>\n",
              "      <td>0.464348</td>\n",
              "      <td>0.480221</td>\n",
              "      <td>0.410579</td>\n",
              "      <td>0.252007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.500350</td>\n",
              "      <td>0.493490</td>\n",
              "      <td>0.454937</td>\n",
              "      <td>0.458278</td>\n",
              "      <td>0.489256</td>\n",
              "      <td>0.464348</td>\n",
              "      <td>0.480221</td>\n",
              "      <td>0.410579</td>\n",
              "      <td>0.252007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.856636</td>\n",
              "      <td>0.845817</td>\n",
              "      <td>0.758187</td>\n",
              "      <td>0.749687</td>\n",
              "      <td>0.644358</td>\n",
              "      <td>0.365638</td>\n",
              "      <td>0.399595</td>\n",
              "      <td>0.408986</td>\n",
              "      <td>0.440709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.856636</td>\n",
              "      <td>0.845817</td>\n",
              "      <td>0.758187</td>\n",
              "      <td>0.749687</td>\n",
              "      <td>0.644358</td>\n",
              "      <td>0.365638</td>\n",
              "      <td>0.399595</td>\n",
              "      <td>0.408986</td>\n",
              "      <td>0.440709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.813415</td>\n",
              "      <td>0.812637</td>\n",
              "      <td>0.780542</td>\n",
              "      <td>0.759735</td>\n",
              "      <td>0.719917</td>\n",
              "      <td>0.342521</td>\n",
              "      <td>0.376977</td>\n",
              "      <td>0.383364</td>\n",
              "      <td>0.456370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.813415</td>\n",
              "      <td>0.812637</td>\n",
              "      <td>0.780542</td>\n",
              "      <td>0.759735</td>\n",
              "      <td>0.719917</td>\n",
              "      <td>0.342521</td>\n",
              "      <td>0.376977</td>\n",
              "      <td>0.383364</td>\n",
              "      <td>0.456370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.535498</td>\n",
              "      <td>0.498346</td>\n",
              "      <td>0.481389</td>\n",
              "      <td>0.479392</td>\n",
              "      <td>0.481780</td>\n",
              "      <td>0.353031</td>\n",
              "      <td>0.432247</td>\n",
              "      <td>0.460050</td>\n",
              "      <td>0.472522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.535498</td>\n",
              "      <td>0.498346</td>\n",
              "      <td>0.481389</td>\n",
              "      <td>0.479392</td>\n",
              "      <td>0.481780</td>\n",
              "      <td>0.353031</td>\n",
              "      <td>0.432247</td>\n",
              "      <td>0.460050</td>\n",
              "      <td>0.472522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.949946</td>\n",
              "      <td>0.937806</td>\n",
              "      <td>0.874254</td>\n",
              "      <td>0.834223</td>\n",
              "      <td>0.766843</td>\n",
              "      <td>0.365146</td>\n",
              "      <td>0.385470</td>\n",
              "      <td>0.377855</td>\n",
              "      <td>0.503719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.949946</td>\n",
              "      <td>0.937806</td>\n",
              "      <td>0.874254</td>\n",
              "      <td>0.834223</td>\n",
              "      <td>0.766843</td>\n",
              "      <td>0.365146</td>\n",
              "      <td>0.385470</td>\n",
              "      <td>0.377855</td>\n",
              "      <td>0.503719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">llama-65b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.555053</td>\n",
              "      <td>0.496416</td>\n",
              "      <td>0.444447</td>\n",
              "      <td>0.547635</td>\n",
              "      <td>0.445523</td>\n",
              "      <td>0.651701</td>\n",
              "      <td>0.707122</td>\n",
              "      <td>0.624746</td>\n",
              "      <td>0.495636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.555053</td>\n",
              "      <td>0.496416</td>\n",
              "      <td>0.444447</td>\n",
              "      <td>0.547635</td>\n",
              "      <td>0.445523</td>\n",
              "      <td>0.651701</td>\n",
              "      <td>0.707122</td>\n",
              "      <td>0.624746</td>\n",
              "      <td>0.495636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.454623</td>\n",
              "      <td>0.457380</td>\n",
              "      <td>0.466246</td>\n",
              "      <td>0.470846</td>\n",
              "      <td>0.469011</td>\n",
              "      <td>0.464780</td>\n",
              "      <td>0.500342</td>\n",
              "      <td>0.475367</td>\n",
              "      <td>0.489651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.454623</td>\n",
              "      <td>0.457380</td>\n",
              "      <td>0.466246</td>\n",
              "      <td>0.470846</td>\n",
              "      <td>0.469011</td>\n",
              "      <td>0.464780</td>\n",
              "      <td>0.500342</td>\n",
              "      <td>0.475367</td>\n",
              "      <td>0.489651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.409295</td>\n",
              "      <td>0.418240</td>\n",
              "      <td>0.399905</td>\n",
              "      <td>0.437714</td>\n",
              "      <td>0.442277</td>\n",
              "      <td>0.485469</td>\n",
              "      <td>0.650575</td>\n",
              "      <td>0.479126</td>\n",
              "      <td>0.404211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.409295</td>\n",
              "      <td>0.418240</td>\n",
              "      <td>0.399905</td>\n",
              "      <td>0.437714</td>\n",
              "      <td>0.442277</td>\n",
              "      <td>0.485469</td>\n",
              "      <td>0.650575</td>\n",
              "      <td>0.479126</td>\n",
              "      <td>0.404211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.489052</td>\n",
              "      <td>0.483550</td>\n",
              "      <td>0.484449</td>\n",
              "      <td>0.514634</td>\n",
              "      <td>0.528312</td>\n",
              "      <td>0.579402</td>\n",
              "      <td>0.592451</td>\n",
              "      <td>0.517087</td>\n",
              "      <td>0.535351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.489052</td>\n",
              "      <td>0.483550</td>\n",
              "      <td>0.484449</td>\n",
              "      <td>0.514634</td>\n",
              "      <td>0.528312</td>\n",
              "      <td>0.579402</td>\n",
              "      <td>0.592451</td>\n",
              "      <td>0.517087</td>\n",
              "      <td>0.535351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.481618</td>\n",
              "      <td>0.423566</td>\n",
              "      <td>0.554618</td>\n",
              "      <td>0.637247</td>\n",
              "      <td>0.627281</td>\n",
              "      <td>0.725998</td>\n",
              "      <td>0.726336</td>\n",
              "      <td>0.712269</td>\n",
              "      <td>0.551419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.481618</td>\n",
              "      <td>0.423566</td>\n",
              "      <td>0.554618</td>\n",
              "      <td>0.637247</td>\n",
              "      <td>0.627281</td>\n",
              "      <td>0.725998</td>\n",
              "      <td>0.726336</td>\n",
              "      <td>0.712269</td>\n",
              "      <td>0.551419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.448728</td>\n",
              "      <td>0.432759</td>\n",
              "      <td>0.437915</td>\n",
              "      <td>0.442989</td>\n",
              "      <td>0.453975</td>\n",
              "      <td>0.439600</td>\n",
              "      <td>0.535198</td>\n",
              "      <td>0.477820</td>\n",
              "      <td>0.481693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.448728</td>\n",
              "      <td>0.432759</td>\n",
              "      <td>0.437915</td>\n",
              "      <td>0.442989</td>\n",
              "      <td>0.453975</td>\n",
              "      <td>0.439600</td>\n",
              "      <td>0.535198</td>\n",
              "      <td>0.477820</td>\n",
              "      <td>0.481693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.472980</td>\n",
              "      <td>0.447753</td>\n",
              "      <td>0.476672</td>\n",
              "      <td>0.597346</td>\n",
              "      <td>0.580996</td>\n",
              "      <td>0.741714</td>\n",
              "      <td>0.744336</td>\n",
              "      <td>0.701213</td>\n",
              "      <td>0.517053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.472980</td>\n",
              "      <td>0.447753</td>\n",
              "      <td>0.476672</td>\n",
              "      <td>0.597346</td>\n",
              "      <td>0.580996</td>\n",
              "      <td>0.741714</td>\n",
              "      <td>0.744336</td>\n",
              "      <td>0.701213</td>\n",
              "      <td>0.517053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">opt-66b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.359153</td>\n",
              "      <td>0.334250</td>\n",
              "      <td>0.335930</td>\n",
              "      <td>0.402940</td>\n",
              "      <td>0.398935</td>\n",
              "      <td>0.757279</td>\n",
              "      <td>0.794267</td>\n",
              "      <td>0.709071</td>\n",
              "      <td>0.309555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.359153</td>\n",
              "      <td>0.334250</td>\n",
              "      <td>0.335930</td>\n",
              "      <td>0.402940</td>\n",
              "      <td>0.398935</td>\n",
              "      <td>0.757279</td>\n",
              "      <td>0.794267</td>\n",
              "      <td>0.709071</td>\n",
              "      <td>0.309555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.418851</td>\n",
              "      <td>0.420189</td>\n",
              "      <td>0.447306</td>\n",
              "      <td>0.440878</td>\n",
              "      <td>0.442376</td>\n",
              "      <td>0.467420</td>\n",
              "      <td>0.461874</td>\n",
              "      <td>0.475864</td>\n",
              "      <td>0.499178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.418851</td>\n",
              "      <td>0.420189</td>\n",
              "      <td>0.447306</td>\n",
              "      <td>0.440878</td>\n",
              "      <td>0.442376</td>\n",
              "      <td>0.467420</td>\n",
              "      <td>0.461874</td>\n",
              "      <td>0.475864</td>\n",
              "      <td>0.499178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.395000</td>\n",
              "      <td>0.422160</td>\n",
              "      <td>0.490206</td>\n",
              "      <td>0.463408</td>\n",
              "      <td>0.476458</td>\n",
              "      <td>0.582652</td>\n",
              "      <td>0.593218</td>\n",
              "      <td>0.552630</td>\n",
              "      <td>0.288770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.395000</td>\n",
              "      <td>0.422160</td>\n",
              "      <td>0.490206</td>\n",
              "      <td>0.463408</td>\n",
              "      <td>0.476458</td>\n",
              "      <td>0.582652</td>\n",
              "      <td>0.593218</td>\n",
              "      <td>0.552630</td>\n",
              "      <td>0.288770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.359644</td>\n",
              "      <td>0.360391</td>\n",
              "      <td>0.395866</td>\n",
              "      <td>0.486644</td>\n",
              "      <td>0.512730</td>\n",
              "      <td>0.831490</td>\n",
              "      <td>0.648147</td>\n",
              "      <td>0.679307</td>\n",
              "      <td>0.371776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.359644</td>\n",
              "      <td>0.360391</td>\n",
              "      <td>0.395866</td>\n",
              "      <td>0.486644</td>\n",
              "      <td>0.512730</td>\n",
              "      <td>0.831490</td>\n",
              "      <td>0.648147</td>\n",
              "      <td>0.679307</td>\n",
              "      <td>0.371776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.337545</td>\n",
              "      <td>0.339834</td>\n",
              "      <td>0.426238</td>\n",
              "      <td>0.480679</td>\n",
              "      <td>0.512682</td>\n",
              "      <td>0.866291</td>\n",
              "      <td>0.461495</td>\n",
              "      <td>0.728873</td>\n",
              "      <td>0.308479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.337545</td>\n",
              "      <td>0.339834</td>\n",
              "      <td>0.426238</td>\n",
              "      <td>0.480679</td>\n",
              "      <td>0.512682</td>\n",
              "      <td>0.866291</td>\n",
              "      <td>0.461495</td>\n",
              "      <td>0.728873</td>\n",
              "      <td>0.308479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.435946</td>\n",
              "      <td>0.420931</td>\n",
              "      <td>0.443540</td>\n",
              "      <td>0.429432</td>\n",
              "      <td>0.438215</td>\n",
              "      <td>0.414764</td>\n",
              "      <td>0.451066</td>\n",
              "      <td>0.488984</td>\n",
              "      <td>0.485909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.435946</td>\n",
              "      <td>0.420931</td>\n",
              "      <td>0.443540</td>\n",
              "      <td>0.429432</td>\n",
              "      <td>0.438215</td>\n",
              "      <td>0.414764</td>\n",
              "      <td>0.451066</td>\n",
              "      <td>0.488984</td>\n",
              "      <td>0.485909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.332034</td>\n",
              "      <td>0.330367</td>\n",
              "      <td>0.393379</td>\n",
              "      <td>0.455400</td>\n",
              "      <td>0.507385</td>\n",
              "      <td>0.936430</td>\n",
              "      <td>0.609225</td>\n",
              "      <td>0.794838</td>\n",
              "      <td>0.340420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.332034</td>\n",
              "      <td>0.330367</td>\n",
              "      <td>0.393379</td>\n",
              "      <td>0.455400</td>\n",
              "      <td>0.507385</td>\n",
              "      <td>0.936430</td>\n",
              "      <td>0.609225</td>\n",
              "      <td>0.794838</td>\n",
              "      <td>0.340420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">opt-iml-max-1.3b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.332727</td>\n",
              "      <td>0.332053</td>\n",
              "      <td>0.332121</td>\n",
              "      <td>0.337201</td>\n",
              "      <td>0.345562</td>\n",
              "      <td>0.707558</td>\n",
              "      <td>0.377582</td>\n",
              "      <td>0.430485</td>\n",
              "      <td>0.176563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.332727</td>\n",
              "      <td>0.332053</td>\n",
              "      <td>0.332121</td>\n",
              "      <td>0.337201</td>\n",
              "      <td>0.345562</td>\n",
              "      <td>0.707558</td>\n",
              "      <td>0.377582</td>\n",
              "      <td>0.430485</td>\n",
              "      <td>0.176563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.358508</td>\n",
              "      <td>0.435283</td>\n",
              "      <td>0.509972</td>\n",
              "      <td>0.455219</td>\n",
              "      <td>0.560732</td>\n",
              "      <td>0.590938</td>\n",
              "      <td>0.510998</td>\n",
              "      <td>0.550968</td>\n",
              "      <td>0.433427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.358508</td>\n",
              "      <td>0.435283</td>\n",
              "      <td>0.509972</td>\n",
              "      <td>0.455219</td>\n",
              "      <td>0.560732</td>\n",
              "      <td>0.590938</td>\n",
              "      <td>0.510998</td>\n",
              "      <td>0.550968</td>\n",
              "      <td>0.433427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.373721</td>\n",
              "      <td>0.392233</td>\n",
              "      <td>0.443796</td>\n",
              "      <td>0.427904</td>\n",
              "      <td>0.443109</td>\n",
              "      <td>0.598311</td>\n",
              "      <td>0.522873</td>\n",
              "      <td>0.493475</td>\n",
              "      <td>0.236609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.373721</td>\n",
              "      <td>0.392233</td>\n",
              "      <td>0.443796</td>\n",
              "      <td>0.427904</td>\n",
              "      <td>0.443109</td>\n",
              "      <td>0.598311</td>\n",
              "      <td>0.522873</td>\n",
              "      <td>0.493475</td>\n",
              "      <td>0.236609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.338130</td>\n",
              "      <td>0.325670</td>\n",
              "      <td>0.334078</td>\n",
              "      <td>0.365224</td>\n",
              "      <td>0.370714</td>\n",
              "      <td>0.705426</td>\n",
              "      <td>0.432819</td>\n",
              "      <td>0.476578</td>\n",
              "      <td>0.203065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.338130</td>\n",
              "      <td>0.325670</td>\n",
              "      <td>0.334078</td>\n",
              "      <td>0.365224</td>\n",
              "      <td>0.370714</td>\n",
              "      <td>0.705426</td>\n",
              "      <td>0.432819</td>\n",
              "      <td>0.476578</td>\n",
              "      <td>0.203065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.337146</td>\n",
              "      <td>0.340148</td>\n",
              "      <td>0.348160</td>\n",
              "      <td>0.384318</td>\n",
              "      <td>0.448456</td>\n",
              "      <td>0.706799</td>\n",
              "      <td>0.361107</td>\n",
              "      <td>0.540236</td>\n",
              "      <td>0.209753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.337146</td>\n",
              "      <td>0.340148</td>\n",
              "      <td>0.348160</td>\n",
              "      <td>0.384318</td>\n",
              "      <td>0.448456</td>\n",
              "      <td>0.706799</td>\n",
              "      <td>0.361107</td>\n",
              "      <td>0.540236</td>\n",
              "      <td>0.209753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.478941</td>\n",
              "      <td>0.483521</td>\n",
              "      <td>0.519878</td>\n",
              "      <td>0.492644</td>\n",
              "      <td>0.499802</td>\n",
              "      <td>0.519722</td>\n",
              "      <td>0.477338</td>\n",
              "      <td>0.519294</td>\n",
              "      <td>0.521333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.478941</td>\n",
              "      <td>0.483521</td>\n",
              "      <td>0.519878</td>\n",
              "      <td>0.492644</td>\n",
              "      <td>0.499802</td>\n",
              "      <td>0.519722</td>\n",
              "      <td>0.477338</td>\n",
              "      <td>0.519294</td>\n",
              "      <td>0.521333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.331246</td>\n",
              "      <td>0.332259</td>\n",
              "      <td>0.336034</td>\n",
              "      <td>0.362727</td>\n",
              "      <td>0.420958</td>\n",
              "      <td>0.799578</td>\n",
              "      <td>0.379103</td>\n",
              "      <td>0.559416</td>\n",
              "      <td>0.223035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.331246</td>\n",
              "      <td>0.332259</td>\n",
              "      <td>0.336034</td>\n",
              "      <td>0.362727</td>\n",
              "      <td>0.420958</td>\n",
              "      <td>0.799578</td>\n",
              "      <td>0.379103</td>\n",
              "      <td>0.559416</td>\n",
              "      <td>0.223035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">text-davinci-003</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.655340</td>\n",
              "      <td>0.794014</td>\n",
              "      <td>0.780767</td>\n",
              "      <td>0.684073</td>\n",
              "      <td>0.718904</td>\n",
              "      <td>0.375260</td>\n",
              "      <td>0.345585</td>\n",
              "      <td>0.379790</td>\n",
              "      <td>0.397715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.655340</td>\n",
              "      <td>0.794014</td>\n",
              "      <td>0.780767</td>\n",
              "      <td>0.684073</td>\n",
              "      <td>0.718904</td>\n",
              "      <td>0.375260</td>\n",
              "      <td>0.345585</td>\n",
              "      <td>0.379790</td>\n",
              "      <td>0.397715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.364191</td>\n",
              "      <td>0.388732</td>\n",
              "      <td>0.437249</td>\n",
              "      <td>0.413060</td>\n",
              "      <td>0.428917</td>\n",
              "      <td>0.455254</td>\n",
              "      <td>0.495071</td>\n",
              "      <td>0.451824</td>\n",
              "      <td>0.233410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.364191</td>\n",
              "      <td>0.388732</td>\n",
              "      <td>0.437249</td>\n",
              "      <td>0.413060</td>\n",
              "      <td>0.428917</td>\n",
              "      <td>0.455254</td>\n",
              "      <td>0.495071</td>\n",
              "      <td>0.451824</td>\n",
              "      <td>0.233410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.375325</td>\n",
              "      <td>0.387423</td>\n",
              "      <td>0.514701</td>\n",
              "      <td>0.414780</td>\n",
              "      <td>0.448965</td>\n",
              "      <td>0.507149</td>\n",
              "      <td>0.485981</td>\n",
              "      <td>0.445593</td>\n",
              "      <td>0.223241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.375325</td>\n",
              "      <td>0.387423</td>\n",
              "      <td>0.514701</td>\n",
              "      <td>0.414780</td>\n",
              "      <td>0.448965</td>\n",
              "      <td>0.507149</td>\n",
              "      <td>0.485981</td>\n",
              "      <td>0.445593</td>\n",
              "      <td>0.223241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.657068</td>\n",
              "      <td>0.732203</td>\n",
              "      <td>0.768223</td>\n",
              "      <td>0.699299</td>\n",
              "      <td>0.727483</td>\n",
              "      <td>0.391856</td>\n",
              "      <td>0.372177</td>\n",
              "      <td>0.415079</td>\n",
              "      <td>0.388129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.657068</td>\n",
              "      <td>0.732203</td>\n",
              "      <td>0.768223</td>\n",
              "      <td>0.699299</td>\n",
              "      <td>0.727483</td>\n",
              "      <td>0.391856</td>\n",
              "      <td>0.372177</td>\n",
              "      <td>0.415079</td>\n",
              "      <td>0.388129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.434200</td>\n",
              "      <td>0.505115</td>\n",
              "      <td>0.672618</td>\n",
              "      <td>0.623038</td>\n",
              "      <td>0.769534</td>\n",
              "      <td>0.664845</td>\n",
              "      <td>0.361779</td>\n",
              "      <td>0.603316</td>\n",
              "      <td>0.354182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.434200</td>\n",
              "      <td>0.505115</td>\n",
              "      <td>0.672618</td>\n",
              "      <td>0.623038</td>\n",
              "      <td>0.769534</td>\n",
              "      <td>0.664845</td>\n",
              "      <td>0.361779</td>\n",
              "      <td>0.603316</td>\n",
              "      <td>0.354182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.490727</td>\n",
              "      <td>0.484581</td>\n",
              "      <td>0.543917</td>\n",
              "      <td>0.483043</td>\n",
              "      <td>0.502528</td>\n",
              "      <td>0.351229</td>\n",
              "      <td>0.415617</td>\n",
              "      <td>0.484497</td>\n",
              "      <td>0.463071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.490727</td>\n",
              "      <td>0.484581</td>\n",
              "      <td>0.543917</td>\n",
              "      <td>0.483043</td>\n",
              "      <td>0.502528</td>\n",
              "      <td>0.351229</td>\n",
              "      <td>0.415617</td>\n",
              "      <td>0.484497</td>\n",
              "      <td>0.463071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.462627</td>\n",
              "      <td>0.592280</td>\n",
              "      <td>0.709935</td>\n",
              "      <td>0.621001</td>\n",
              "      <td>0.723169</td>\n",
              "      <td>0.635119</td>\n",
              "      <td>0.362306</td>\n",
              "      <td>0.479156</td>\n",
              "      <td>0.358429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.462627</td>\n",
              "      <td>0.592280</td>\n",
              "      <td>0.709935</td>\n",
              "      <td>0.621001</td>\n",
              "      <td>0.723169</td>\n",
              "      <td>0.635119</td>\n",
              "      <td>0.362306</td>\n",
              "      <td>0.479156</td>\n",
              "      <td>0.358429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">vicuna-13b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.541668</td>\n",
              "      <td>0.544359</td>\n",
              "      <td>0.532151</td>\n",
              "      <td>0.542860</td>\n",
              "      <td>0.532085</td>\n",
              "      <td>0.433768</td>\n",
              "      <td>0.495575</td>\n",
              "      <td>0.456960</td>\n",
              "      <td>0.564186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.541668</td>\n",
              "      <td>0.544359</td>\n",
              "      <td>0.532151</td>\n",
              "      <td>0.542860</td>\n",
              "      <td>0.532085</td>\n",
              "      <td>0.433768</td>\n",
              "      <td>0.495575</td>\n",
              "      <td>0.456960</td>\n",
              "      <td>0.564186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.460503</td>\n",
              "      <td>0.459935</td>\n",
              "      <td>0.461325</td>\n",
              "      <td>0.467982</td>\n",
              "      <td>0.470776</td>\n",
              "      <td>0.470456</td>\n",
              "      <td>0.485209</td>\n",
              "      <td>0.477359</td>\n",
              "      <td>0.513115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.460503</td>\n",
              "      <td>0.459935</td>\n",
              "      <td>0.461325</td>\n",
              "      <td>0.467982</td>\n",
              "      <td>0.470776</td>\n",
              "      <td>0.470456</td>\n",
              "      <td>0.485209</td>\n",
              "      <td>0.477359</td>\n",
              "      <td>0.513115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.480937</td>\n",
              "      <td>0.496502</td>\n",
              "      <td>0.439953</td>\n",
              "      <td>0.547681</td>\n",
              "      <td>0.503075</td>\n",
              "      <td>0.513854</td>\n",
              "      <td>0.645866</td>\n",
              "      <td>0.451901</td>\n",
              "      <td>0.299265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.480937</td>\n",
              "      <td>0.496502</td>\n",
              "      <td>0.439953</td>\n",
              "      <td>0.547681</td>\n",
              "      <td>0.503075</td>\n",
              "      <td>0.513854</td>\n",
              "      <td>0.645866</td>\n",
              "      <td>0.451901</td>\n",
              "      <td>0.299265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.661594</td>\n",
              "      <td>0.676031</td>\n",
              "      <td>0.659223</td>\n",
              "      <td>0.676492</td>\n",
              "      <td>0.667321</td>\n",
              "      <td>0.584928</td>\n",
              "      <td>0.587543</td>\n",
              "      <td>0.546205</td>\n",
              "      <td>0.621305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.661594</td>\n",
              "      <td>0.676031</td>\n",
              "      <td>0.659223</td>\n",
              "      <td>0.676492</td>\n",
              "      <td>0.667321</td>\n",
              "      <td>0.584928</td>\n",
              "      <td>0.587543</td>\n",
              "      <td>0.546205</td>\n",
              "      <td>0.621305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.758154</td>\n",
              "      <td>0.800096</td>\n",
              "      <td>0.834161</td>\n",
              "      <td>0.895699</td>\n",
              "      <td>0.851321</td>\n",
              "      <td>0.908030</td>\n",
              "      <td>0.607534</td>\n",
              "      <td>0.728884</td>\n",
              "      <td>0.622840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.758154</td>\n",
              "      <td>0.800096</td>\n",
              "      <td>0.834161</td>\n",
              "      <td>0.895699</td>\n",
              "      <td>0.851321</td>\n",
              "      <td>0.908030</td>\n",
              "      <td>0.607534</td>\n",
              "      <td>0.728884</td>\n",
              "      <td>0.622840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.514584</td>\n",
              "      <td>0.513713</td>\n",
              "      <td>0.499775</td>\n",
              "      <td>0.526876</td>\n",
              "      <td>0.508797</td>\n",
              "      <td>0.446061</td>\n",
              "      <td>0.520642</td>\n",
              "      <td>0.487261</td>\n",
              "      <td>0.502183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.514584</td>\n",
              "      <td>0.513713</td>\n",
              "      <td>0.499775</td>\n",
              "      <td>0.526876</td>\n",
              "      <td>0.508797</td>\n",
              "      <td>0.446061</td>\n",
              "      <td>0.520642</td>\n",
              "      <td>0.487261</td>\n",
              "      <td>0.502183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.799182</td>\n",
              "      <td>0.831837</td>\n",
              "      <td>0.842878</td>\n",
              "      <td>0.885241</td>\n",
              "      <td>0.821978</td>\n",
              "      <td>0.761966</td>\n",
              "      <td>0.533176</td>\n",
              "      <td>0.593499</td>\n",
              "      <td>0.594081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.799182</td>\n",
              "      <td>0.831837</td>\n",
              "      <td>0.842878</td>\n",
              "      <td>0.885241</td>\n",
              "      <td>0.821978</td>\n",
              "      <td>0.761966</td>\n",
              "      <td>0.533176</td>\n",
              "      <td>0.593499</td>\n",
              "      <td>0.594081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"119\" valign=\"top\">all</th>\n",
              "      <th rowspan=\"7\" valign=\"top\">all</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.742030</td>\n",
              "      <td>0.743030</td>\n",
              "      <td>0.739269</td>\n",
              "      <td>0.739102</td>\n",
              "      <td>0.725558</td>\n",
              "      <td>0.713254</td>\n",
              "      <td>0.733861</td>\n",
              "      <td>0.685665</td>\n",
              "      <td>0.756344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.420135</td>\n",
              "      <td>0.417417</td>\n",
              "      <td>0.415117</td>\n",
              "      <td>0.422630</td>\n",
              "      <td>0.420880</td>\n",
              "      <td>0.421625</td>\n",
              "      <td>0.439717</td>\n",
              "      <td>0.427767</td>\n",
              "      <td>0.505921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.639329</td>\n",
              "      <td>0.662633</td>\n",
              "      <td>0.650695</td>\n",
              "      <td>0.650933</td>\n",
              "      <td>0.659945</td>\n",
              "      <td>0.650515</td>\n",
              "      <td>0.662629</td>\n",
              "      <td>0.590778</td>\n",
              "      <td>0.664612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.718145</td>\n",
              "      <td>0.731644</td>\n",
              "      <td>0.725047</td>\n",
              "      <td>0.725872</td>\n",
              "      <td>0.722575</td>\n",
              "      <td>0.713906</td>\n",
              "      <td>0.726538</td>\n",
              "      <td>0.614153</td>\n",
              "      <td>0.695916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.855155</td>\n",
              "      <td>0.844191</td>\n",
              "      <td>0.850010</td>\n",
              "      <td>0.843834</td>\n",
              "      <td>0.854785</td>\n",
              "      <td>0.862336</td>\n",
              "      <td>0.859953</td>\n",
              "      <td>0.829534</td>\n",
              "      <td>0.848011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.732333</td>\n",
              "      <td>0.740233</td>\n",
              "      <td>0.724094</td>\n",
              "      <td>0.741445</td>\n",
              "      <td>0.727622</td>\n",
              "      <td>0.765409</td>\n",
              "      <td>0.766711</td>\n",
              "      <td>0.736193</td>\n",
              "      <td>0.736044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.780130</td>\n",
              "      <td>0.780568</td>\n",
              "      <td>0.778774</td>\n",
              "      <td>0.779190</td>\n",
              "      <td>0.775379</td>\n",
              "      <td>0.777838</td>\n",
              "      <td>0.776027</td>\n",
              "      <td>0.748398</td>\n",
              "      <td>0.824012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">alpaca-lora-30b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.883190</td>\n",
              "      <td>0.928132</td>\n",
              "      <td>0.900453</td>\n",
              "      <td>0.864471</td>\n",
              "      <td>0.869751</td>\n",
              "      <td>0.481000</td>\n",
              "      <td>0.503212</td>\n",
              "      <td>0.421225</td>\n",
              "      <td>0.552802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.883190</td>\n",
              "      <td>0.928132</td>\n",
              "      <td>0.900453</td>\n",
              "      <td>0.864471</td>\n",
              "      <td>0.869751</td>\n",
              "      <td>0.481000</td>\n",
              "      <td>0.503212</td>\n",
              "      <td>0.421225</td>\n",
              "      <td>0.552802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.582197</td>\n",
              "      <td>0.627844</td>\n",
              "      <td>0.600716</td>\n",
              "      <td>0.622260</td>\n",
              "      <td>0.650700</td>\n",
              "      <td>0.447202</td>\n",
              "      <td>0.448890</td>\n",
              "      <td>0.403770</td>\n",
              "      <td>0.321353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.582197</td>\n",
              "      <td>0.627844</td>\n",
              "      <td>0.600716</td>\n",
              "      <td>0.622260</td>\n",
              "      <td>0.650700</td>\n",
              "      <td>0.447202</td>\n",
              "      <td>0.448890</td>\n",
              "      <td>0.403770</td>\n",
              "      <td>0.321353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.521494</td>\n",
              "      <td>0.672544</td>\n",
              "      <td>0.621441</td>\n",
              "      <td>0.598261</td>\n",
              "      <td>0.796096</td>\n",
              "      <td>0.572778</td>\n",
              "      <td>0.565056</td>\n",
              "      <td>0.450887</td>\n",
              "      <td>0.380017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.521494</td>\n",
              "      <td>0.672544</td>\n",
              "      <td>0.621441</td>\n",
              "      <td>0.598261</td>\n",
              "      <td>0.796096</td>\n",
              "      <td>0.572778</td>\n",
              "      <td>0.565056</td>\n",
              "      <td>0.450887</td>\n",
              "      <td>0.380017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.747279</td>\n",
              "      <td>0.836006</td>\n",
              "      <td>0.809960</td>\n",
              "      <td>0.780015</td>\n",
              "      <td>0.841418</td>\n",
              "      <td>0.463764</td>\n",
              "      <td>0.573566</td>\n",
              "      <td>0.462548</td>\n",
              "      <td>0.522982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.747279</td>\n",
              "      <td>0.836006</td>\n",
              "      <td>0.809960</td>\n",
              "      <td>0.780015</td>\n",
              "      <td>0.841418</td>\n",
              "      <td>0.463764</td>\n",
              "      <td>0.573566</td>\n",
              "      <td>0.462548</td>\n",
              "      <td>0.522982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.860280</td>\n",
              "      <td>0.868372</td>\n",
              "      <td>0.870658</td>\n",
              "      <td>0.826217</td>\n",
              "      <td>0.854249</td>\n",
              "      <td>0.578029</td>\n",
              "      <td>0.528942</td>\n",
              "      <td>0.520888</td>\n",
              "      <td>0.551927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.860280</td>\n",
              "      <td>0.868372</td>\n",
              "      <td>0.870658</td>\n",
              "      <td>0.826217</td>\n",
              "      <td>0.854249</td>\n",
              "      <td>0.578029</td>\n",
              "      <td>0.528942</td>\n",
              "      <td>0.520888</td>\n",
              "      <td>0.551927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.740567</td>\n",
              "      <td>0.825424</td>\n",
              "      <td>0.790835</td>\n",
              "      <td>0.769592</td>\n",
              "      <td>0.847530</td>\n",
              "      <td>0.538735</td>\n",
              "      <td>0.513108</td>\n",
              "      <td>0.545008</td>\n",
              "      <td>0.513234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.740567</td>\n",
              "      <td>0.825424</td>\n",
              "      <td>0.790835</td>\n",
              "      <td>0.769592</td>\n",
              "      <td>0.847530</td>\n",
              "      <td>0.538735</td>\n",
              "      <td>0.513108</td>\n",
              "      <td>0.545008</td>\n",
              "      <td>0.513234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.833294</td>\n",
              "      <td>0.914097</td>\n",
              "      <td>0.897182</td>\n",
              "      <td>0.873733</td>\n",
              "      <td>0.922378</td>\n",
              "      <td>0.501793</td>\n",
              "      <td>0.464140</td>\n",
              "      <td>0.431615</td>\n",
              "      <td>0.541894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.833294</td>\n",
              "      <td>0.914097</td>\n",
              "      <td>0.897182</td>\n",
              "      <td>0.873733</td>\n",
              "      <td>0.922378</td>\n",
              "      <td>0.501793</td>\n",
              "      <td>0.464140</td>\n",
              "      <td>0.431615</td>\n",
              "      <td>0.541894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">gpt-3.5-turbo</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.895417</td>\n",
              "      <td>0.947168</td>\n",
              "      <td>0.856150</td>\n",
              "      <td>0.853558</td>\n",
              "      <td>0.759746</td>\n",
              "      <td>0.372440</td>\n",
              "      <td>0.365711</td>\n",
              "      <td>0.360731</td>\n",
              "      <td>0.489238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.895417</td>\n",
              "      <td>0.947168</td>\n",
              "      <td>0.856150</td>\n",
              "      <td>0.853558</td>\n",
              "      <td>0.759746</td>\n",
              "      <td>0.372440</td>\n",
              "      <td>0.365711</td>\n",
              "      <td>0.360731</td>\n",
              "      <td>0.489238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.548083</td>\n",
              "      <td>0.602245</td>\n",
              "      <td>0.550862</td>\n",
              "      <td>0.578723</td>\n",
              "      <td>0.514294</td>\n",
              "      <td>0.370216</td>\n",
              "      <td>0.359195</td>\n",
              "      <td>0.360602</td>\n",
              "      <td>0.259361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.548083</td>\n",
              "      <td>0.602245</td>\n",
              "      <td>0.550862</td>\n",
              "      <td>0.578723</td>\n",
              "      <td>0.514294</td>\n",
              "      <td>0.370216</td>\n",
              "      <td>0.359195</td>\n",
              "      <td>0.360602</td>\n",
              "      <td>0.259361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.670842</td>\n",
              "      <td>0.765369</td>\n",
              "      <td>0.650399</td>\n",
              "      <td>0.669185</td>\n",
              "      <td>0.610807</td>\n",
              "      <td>0.491351</td>\n",
              "      <td>0.367576</td>\n",
              "      <td>0.375207</td>\n",
              "      <td>0.355082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.670842</td>\n",
              "      <td>0.765369</td>\n",
              "      <td>0.650399</td>\n",
              "      <td>0.669185</td>\n",
              "      <td>0.610807</td>\n",
              "      <td>0.491351</td>\n",
              "      <td>0.367576</td>\n",
              "      <td>0.375207</td>\n",
              "      <td>0.355082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.736664</td>\n",
              "      <td>0.856212</td>\n",
              "      <td>0.766684</td>\n",
              "      <td>0.737011</td>\n",
              "      <td>0.702000</td>\n",
              "      <td>0.341799</td>\n",
              "      <td>0.359979</td>\n",
              "      <td>0.362091</td>\n",
              "      <td>0.417104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.736664</td>\n",
              "      <td>0.856212</td>\n",
              "      <td>0.766684</td>\n",
              "      <td>0.737011</td>\n",
              "      <td>0.702000</td>\n",
              "      <td>0.341799</td>\n",
              "      <td>0.359979</td>\n",
              "      <td>0.362091</td>\n",
              "      <td>0.417104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.926275</td>\n",
              "      <td>0.924743</td>\n",
              "      <td>0.851794</td>\n",
              "      <td>0.826534</td>\n",
              "      <td>0.785708</td>\n",
              "      <td>0.337009</td>\n",
              "      <td>0.361438</td>\n",
              "      <td>0.345353</td>\n",
              "      <td>0.501057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.926275</td>\n",
              "      <td>0.924743</td>\n",
              "      <td>0.851794</td>\n",
              "      <td>0.826534</td>\n",
              "      <td>0.785708</td>\n",
              "      <td>0.337009</td>\n",
              "      <td>0.361438</td>\n",
              "      <td>0.345353</td>\n",
              "      <td>0.501057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.660923</td>\n",
              "      <td>0.737495</td>\n",
              "      <td>0.651513</td>\n",
              "      <td>0.658047</td>\n",
              "      <td>0.613129</td>\n",
              "      <td>0.379544</td>\n",
              "      <td>0.365184</td>\n",
              "      <td>0.376208</td>\n",
              "      <td>0.338767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.660923</td>\n",
              "      <td>0.737495</td>\n",
              "      <td>0.651513</td>\n",
              "      <td>0.658047</td>\n",
              "      <td>0.613129</td>\n",
              "      <td>0.379544</td>\n",
              "      <td>0.365184</td>\n",
              "      <td>0.376208</td>\n",
              "      <td>0.338767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.967289</td>\n",
              "      <td>0.875907</td>\n",
              "      <td>0.838572</td>\n",
              "      <td>0.762452</td>\n",
              "      <td>0.358216</td>\n",
              "      <td>0.359891</td>\n",
              "      <td>0.358718</td>\n",
              "      <td>0.494016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.939394</td>\n",
              "      <td>0.967289</td>\n",
              "      <td>0.875907</td>\n",
              "      <td>0.838572</td>\n",
              "      <td>0.762452</td>\n",
              "      <td>0.358216</td>\n",
              "      <td>0.359891</td>\n",
              "      <td>0.358718</td>\n",
              "      <td>0.494016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">gpt-4</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.946400</td>\n",
              "      <td>0.927683</td>\n",
              "      <td>0.847399</td>\n",
              "      <td>0.836558</td>\n",
              "      <td>0.740100</td>\n",
              "      <td>0.355474</td>\n",
              "      <td>0.377686</td>\n",
              "      <td>0.368102</td>\n",
              "      <td>0.497424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.946400</td>\n",
              "      <td>0.927683</td>\n",
              "      <td>0.847399</td>\n",
              "      <td>0.836558</td>\n",
              "      <td>0.740100</td>\n",
              "      <td>0.355474</td>\n",
              "      <td>0.377686</td>\n",
              "      <td>0.368102</td>\n",
              "      <td>0.497424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.666535</td>\n",
              "      <td>0.645391</td>\n",
              "      <td>0.594819</td>\n",
              "      <td>0.631515</td>\n",
              "      <td>0.556494</td>\n",
              "      <td>0.379866</td>\n",
              "      <td>0.373655</td>\n",
              "      <td>0.373050</td>\n",
              "      <td>0.303492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.666535</td>\n",
              "      <td>0.645391</td>\n",
              "      <td>0.594819</td>\n",
              "      <td>0.631515</td>\n",
              "      <td>0.556494</td>\n",
              "      <td>0.379866</td>\n",
              "      <td>0.373655</td>\n",
              "      <td>0.373050</td>\n",
              "      <td>0.303492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.786658</td>\n",
              "      <td>0.722624</td>\n",
              "      <td>0.581261</td>\n",
              "      <td>0.655184</td>\n",
              "      <td>0.579252</td>\n",
              "      <td>0.500542</td>\n",
              "      <td>0.410286</td>\n",
              "      <td>0.417728</td>\n",
              "      <td>0.366916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.786658</td>\n",
              "      <td>0.722624</td>\n",
              "      <td>0.581261</td>\n",
              "      <td>0.655184</td>\n",
              "      <td>0.579252</td>\n",
              "      <td>0.500542</td>\n",
              "      <td>0.410286</td>\n",
              "      <td>0.417728</td>\n",
              "      <td>0.366916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.878417</td>\n",
              "      <td>0.871056</td>\n",
              "      <td>0.793422</td>\n",
              "      <td>0.780189</td>\n",
              "      <td>0.697109</td>\n",
              "      <td>0.352665</td>\n",
              "      <td>0.385132</td>\n",
              "      <td>0.382936</td>\n",
              "      <td>0.452888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.878417</td>\n",
              "      <td>0.871056</td>\n",
              "      <td>0.793422</td>\n",
              "      <td>0.780189</td>\n",
              "      <td>0.697109</td>\n",
              "      <td>0.352665</td>\n",
              "      <td>0.385132</td>\n",
              "      <td>0.382936</td>\n",
              "      <td>0.452888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.944254</td>\n",
              "      <td>0.929526</td>\n",
              "      <td>0.838802</td>\n",
              "      <td>0.810602</td>\n",
              "      <td>0.734456</td>\n",
              "      <td>0.341549</td>\n",
              "      <td>0.356173</td>\n",
              "      <td>0.352552</td>\n",
              "      <td>0.486767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.944254</td>\n",
              "      <td>0.929526</td>\n",
              "      <td>0.838802</td>\n",
              "      <td>0.810602</td>\n",
              "      <td>0.734456</td>\n",
              "      <td>0.341549</td>\n",
              "      <td>0.356173</td>\n",
              "      <td>0.352552</td>\n",
              "      <td>0.486767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.843479</td>\n",
              "      <td>0.811925</td>\n",
              "      <td>0.734350</td>\n",
              "      <td>0.738498</td>\n",
              "      <td>0.651862</td>\n",
              "      <td>0.389554</td>\n",
              "      <td>0.379720</td>\n",
              "      <td>0.403679</td>\n",
              "      <td>0.421871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.843479</td>\n",
              "      <td>0.811925</td>\n",
              "      <td>0.734350</td>\n",
              "      <td>0.738498</td>\n",
              "      <td>0.651862</td>\n",
              "      <td>0.389554</td>\n",
              "      <td>0.379720</td>\n",
              "      <td>0.403679</td>\n",
              "      <td>0.421871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.973437</td>\n",
              "      <td>0.953157</td>\n",
              "      <td>0.861394</td>\n",
              "      <td>0.838126</td>\n",
              "      <td>0.742782</td>\n",
              "      <td>0.350308</td>\n",
              "      <td>0.363593</td>\n",
              "      <td>0.356027</td>\n",
              "      <td>0.494915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.973437</td>\n",
              "      <td>0.953157</td>\n",
              "      <td>0.861394</td>\n",
              "      <td>0.838126</td>\n",
              "      <td>0.742782</td>\n",
              "      <td>0.350308</td>\n",
              "      <td>0.363593</td>\n",
              "      <td>0.356027</td>\n",
              "      <td>0.494915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">llama-65b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.522707</td>\n",
              "      <td>0.397352</td>\n",
              "      <td>0.333450</td>\n",
              "      <td>0.526138</td>\n",
              "      <td>0.453402</td>\n",
              "      <td>0.765505</td>\n",
              "      <td>0.875430</td>\n",
              "      <td>0.709120</td>\n",
              "      <td>0.411371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.522707</td>\n",
              "      <td>0.397352</td>\n",
              "      <td>0.333450</td>\n",
              "      <td>0.526138</td>\n",
              "      <td>0.453402</td>\n",
              "      <td>0.765505</td>\n",
              "      <td>0.875430</td>\n",
              "      <td>0.709120</td>\n",
              "      <td>0.411371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.403537</td>\n",
              "      <td>0.376820</td>\n",
              "      <td>0.374356</td>\n",
              "      <td>0.477141</td>\n",
              "      <td>0.473302</td>\n",
              "      <td>0.633728</td>\n",
              "      <td>0.894659</td>\n",
              "      <td>0.594474</td>\n",
              "      <td>0.331128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.403537</td>\n",
              "      <td>0.376820</td>\n",
              "      <td>0.374356</td>\n",
              "      <td>0.477141</td>\n",
              "      <td>0.473302</td>\n",
              "      <td>0.633728</td>\n",
              "      <td>0.894659</td>\n",
              "      <td>0.594474</td>\n",
              "      <td>0.331128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.434776</td>\n",
              "      <td>0.427964</td>\n",
              "      <td>0.406685</td>\n",
              "      <td>0.487092</td>\n",
              "      <td>0.481104</td>\n",
              "      <td>0.682165</td>\n",
              "      <td>0.904397</td>\n",
              "      <td>0.629567</td>\n",
              "      <td>0.365997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.434776</td>\n",
              "      <td>0.427964</td>\n",
              "      <td>0.406685</td>\n",
              "      <td>0.487092</td>\n",
              "      <td>0.481104</td>\n",
              "      <td>0.682165</td>\n",
              "      <td>0.904397</td>\n",
              "      <td>0.629567</td>\n",
              "      <td>0.365997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.557785</td>\n",
              "      <td>0.482048</td>\n",
              "      <td>0.414811</td>\n",
              "      <td>0.593470</td>\n",
              "      <td>0.515372</td>\n",
              "      <td>0.796851</td>\n",
              "      <td>0.890213</td>\n",
              "      <td>0.657316</td>\n",
              "      <td>0.441762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.557785</td>\n",
              "      <td>0.482048</td>\n",
              "      <td>0.414811</td>\n",
              "      <td>0.593470</td>\n",
              "      <td>0.515372</td>\n",
              "      <td>0.796851</td>\n",
              "      <td>0.890213</td>\n",
              "      <td>0.657316</td>\n",
              "      <td>0.441762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.533878</td>\n",
              "      <td>0.454116</td>\n",
              "      <td>0.482325</td>\n",
              "      <td>0.677106</td>\n",
              "      <td>0.591946</td>\n",
              "      <td>0.859332</td>\n",
              "      <td>0.870969</td>\n",
              "      <td>0.792183</td>\n",
              "      <td>0.506674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.533878</td>\n",
              "      <td>0.454116</td>\n",
              "      <td>0.482325</td>\n",
              "      <td>0.677106</td>\n",
              "      <td>0.591946</td>\n",
              "      <td>0.859332</td>\n",
              "      <td>0.870969</td>\n",
              "      <td>0.792183</td>\n",
              "      <td>0.506674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.590498</td>\n",
              "      <td>0.565367</td>\n",
              "      <td>0.546769</td>\n",
              "      <td>0.644447</td>\n",
              "      <td>0.556488</td>\n",
              "      <td>0.752134</td>\n",
              "      <td>0.892680</td>\n",
              "      <td>0.718293</td>\n",
              "      <td>0.481338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.590498</td>\n",
              "      <td>0.565367</td>\n",
              "      <td>0.546769</td>\n",
              "      <td>0.644447</td>\n",
              "      <td>0.556488</td>\n",
              "      <td>0.752134</td>\n",
              "      <td>0.892680</td>\n",
              "      <td>0.718293</td>\n",
              "      <td>0.481338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.504345</td>\n",
              "      <td>0.413110</td>\n",
              "      <td>0.397285</td>\n",
              "      <td>0.598527</td>\n",
              "      <td>0.520847</td>\n",
              "      <td>0.813623</td>\n",
              "      <td>0.850675</td>\n",
              "      <td>0.733263</td>\n",
              "      <td>0.462752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.504345</td>\n",
              "      <td>0.413110</td>\n",
              "      <td>0.397285</td>\n",
              "      <td>0.598527</td>\n",
              "      <td>0.520847</td>\n",
              "      <td>0.813623</td>\n",
              "      <td>0.850675</td>\n",
              "      <td>0.733263</td>\n",
              "      <td>0.462752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">opt-66b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.523638</td>\n",
              "      <td>0.494671</td>\n",
              "      <td>0.461256</td>\n",
              "      <td>0.608878</td>\n",
              "      <td>0.495871</td>\n",
              "      <td>0.834210</td>\n",
              "      <td>0.858509</td>\n",
              "      <td>0.800106</td>\n",
              "      <td>0.451727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.523638</td>\n",
              "      <td>0.494671</td>\n",
              "      <td>0.461256</td>\n",
              "      <td>0.608878</td>\n",
              "      <td>0.495871</td>\n",
              "      <td>0.834210</td>\n",
              "      <td>0.858509</td>\n",
              "      <td>0.800106</td>\n",
              "      <td>0.451727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.539051</td>\n",
              "      <td>0.476240</td>\n",
              "      <td>0.472782</td>\n",
              "      <td>0.568054</td>\n",
              "      <td>0.514539</td>\n",
              "      <td>0.712291</td>\n",
              "      <td>0.757972</td>\n",
              "      <td>0.666038</td>\n",
              "      <td>0.401549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.539051</td>\n",
              "      <td>0.476240</td>\n",
              "      <td>0.472782</td>\n",
              "      <td>0.568054</td>\n",
              "      <td>0.514539</td>\n",
              "      <td>0.712291</td>\n",
              "      <td>0.757972</td>\n",
              "      <td>0.666038</td>\n",
              "      <td>0.401549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.461834</td>\n",
              "      <td>0.506001</td>\n",
              "      <td>0.577805</td>\n",
              "      <td>0.592408</td>\n",
              "      <td>0.536317</td>\n",
              "      <td>0.794005</td>\n",
              "      <td>0.647234</td>\n",
              "      <td>0.759248</td>\n",
              "      <td>0.431336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.461834</td>\n",
              "      <td>0.506001</td>\n",
              "      <td>0.577805</td>\n",
              "      <td>0.592408</td>\n",
              "      <td>0.536317</td>\n",
              "      <td>0.794005</td>\n",
              "      <td>0.647234</td>\n",
              "      <td>0.759248</td>\n",
              "      <td>0.431336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.498187</td>\n",
              "      <td>0.529334</td>\n",
              "      <td>0.590694</td>\n",
              "      <td>0.700501</td>\n",
              "      <td>0.659507</td>\n",
              "      <td>0.902213</td>\n",
              "      <td>0.795716</td>\n",
              "      <td>0.851116</td>\n",
              "      <td>0.522330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.498187</td>\n",
              "      <td>0.529334</td>\n",
              "      <td>0.590694</td>\n",
              "      <td>0.700501</td>\n",
              "      <td>0.659507</td>\n",
              "      <td>0.902213</td>\n",
              "      <td>0.795716</td>\n",
              "      <td>0.851116</td>\n",
              "      <td>0.522330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.486604</td>\n",
              "      <td>0.441337</td>\n",
              "      <td>0.619535</td>\n",
              "      <td>0.770342</td>\n",
              "      <td>0.736051</td>\n",
              "      <td>0.914443</td>\n",
              "      <td>0.875216</td>\n",
              "      <td>0.908434</td>\n",
              "      <td>0.555966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.486604</td>\n",
              "      <td>0.441337</td>\n",
              "      <td>0.619535</td>\n",
              "      <td>0.770342</td>\n",
              "      <td>0.736051</td>\n",
              "      <td>0.914443</td>\n",
              "      <td>0.875216</td>\n",
              "      <td>0.908434</td>\n",
              "      <td>0.555966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.513527</td>\n",
              "      <td>0.553044</td>\n",
              "      <td>0.619316</td>\n",
              "      <td>0.642645</td>\n",
              "      <td>0.619146</td>\n",
              "      <td>0.847524</td>\n",
              "      <td>0.672685</td>\n",
              "      <td>0.814549</td>\n",
              "      <td>0.485099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.513527</td>\n",
              "      <td>0.553044</td>\n",
              "      <td>0.619316</td>\n",
              "      <td>0.642645</td>\n",
              "      <td>0.619146</td>\n",
              "      <td>0.847524</td>\n",
              "      <td>0.672685</td>\n",
              "      <td>0.814549</td>\n",
              "      <td>0.485099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.434749</td>\n",
              "      <td>0.378145</td>\n",
              "      <td>0.440987</td>\n",
              "      <td>0.576043</td>\n",
              "      <td>0.587883</td>\n",
              "      <td>0.962542</td>\n",
              "      <td>0.886844</td>\n",
              "      <td>0.914877</td>\n",
              "      <td>0.458762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.434749</td>\n",
              "      <td>0.378145</td>\n",
              "      <td>0.440987</td>\n",
              "      <td>0.576043</td>\n",
              "      <td>0.587883</td>\n",
              "      <td>0.962542</td>\n",
              "      <td>0.886844</td>\n",
              "      <td>0.914877</td>\n",
              "      <td>0.458762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">opt-iml-max-1.3b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.527543</td>\n",
              "      <td>0.521568</td>\n",
              "      <td>0.482112</td>\n",
              "      <td>0.689920</td>\n",
              "      <td>0.586596</td>\n",
              "      <td>0.933727</td>\n",
              "      <td>0.856475</td>\n",
              "      <td>0.814461</td>\n",
              "      <td>0.488158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.527543</td>\n",
              "      <td>0.521568</td>\n",
              "      <td>0.482112</td>\n",
              "      <td>0.689920</td>\n",
              "      <td>0.586596</td>\n",
              "      <td>0.933727</td>\n",
              "      <td>0.856475</td>\n",
              "      <td>0.814461</td>\n",
              "      <td>0.488158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.368141</td>\n",
              "      <td>0.400034</td>\n",
              "      <td>0.398083</td>\n",
              "      <td>0.516882</td>\n",
              "      <td>0.468703</td>\n",
              "      <td>0.769950</td>\n",
              "      <td>0.772747</td>\n",
              "      <td>0.612871</td>\n",
              "      <td>0.324427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.368141</td>\n",
              "      <td>0.400034</td>\n",
              "      <td>0.398083</td>\n",
              "      <td>0.516882</td>\n",
              "      <td>0.468703</td>\n",
              "      <td>0.769950</td>\n",
              "      <td>0.772747</td>\n",
              "      <td>0.612871</td>\n",
              "      <td>0.324427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.461409</td>\n",
              "      <td>0.500207</td>\n",
              "      <td>0.516679</td>\n",
              "      <td>0.604245</td>\n",
              "      <td>0.531220</td>\n",
              "      <td>0.876682</td>\n",
              "      <td>0.634695</td>\n",
              "      <td>0.658790</td>\n",
              "      <td>0.395710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.461409</td>\n",
              "      <td>0.500207</td>\n",
              "      <td>0.516679</td>\n",
              "      <td>0.604245</td>\n",
              "      <td>0.531220</td>\n",
              "      <td>0.876682</td>\n",
              "      <td>0.634695</td>\n",
              "      <td>0.658790</td>\n",
              "      <td>0.395710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.426201</td>\n",
              "      <td>0.469781</td>\n",
              "      <td>0.491033</td>\n",
              "      <td>0.676543</td>\n",
              "      <td>0.592099</td>\n",
              "      <td>0.949505</td>\n",
              "      <td>0.774886</td>\n",
              "      <td>0.664608</td>\n",
              "      <td>0.427575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.426201</td>\n",
              "      <td>0.469781</td>\n",
              "      <td>0.491033</td>\n",
              "      <td>0.676543</td>\n",
              "      <td>0.592099</td>\n",
              "      <td>0.949505</td>\n",
              "      <td>0.774886</td>\n",
              "      <td>0.664608</td>\n",
              "      <td>0.427575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.433971</td>\n",
              "      <td>0.464366</td>\n",
              "      <td>0.546656</td>\n",
              "      <td>0.741362</td>\n",
              "      <td>0.658911</td>\n",
              "      <td>0.960685</td>\n",
              "      <td>0.818042</td>\n",
              "      <td>0.859511</td>\n",
              "      <td>0.487085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.433971</td>\n",
              "      <td>0.464366</td>\n",
              "      <td>0.546656</td>\n",
              "      <td>0.741362</td>\n",
              "      <td>0.658911</td>\n",
              "      <td>0.960685</td>\n",
              "      <td>0.818042</td>\n",
              "      <td>0.859511</td>\n",
              "      <td>0.487085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.524179</td>\n",
              "      <td>0.568078</td>\n",
              "      <td>0.638827</td>\n",
              "      <td>0.622713</td>\n",
              "      <td>0.589335</td>\n",
              "      <td>0.903642</td>\n",
              "      <td>0.540688</td>\n",
              "      <td>0.684239</td>\n",
              "      <td>0.445988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.524179</td>\n",
              "      <td>0.568078</td>\n",
              "      <td>0.638827</td>\n",
              "      <td>0.622713</td>\n",
              "      <td>0.589335</td>\n",
              "      <td>0.903642</td>\n",
              "      <td>0.540688</td>\n",
              "      <td>0.684239</td>\n",
              "      <td>0.445988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.380097</td>\n",
              "      <td>0.364368</td>\n",
              "      <td>0.382583</td>\n",
              "      <td>0.527326</td>\n",
              "      <td>0.503676</td>\n",
              "      <td>0.966133</td>\n",
              "      <td>0.572232</td>\n",
              "      <td>0.696015</td>\n",
              "      <td>0.345646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.380097</td>\n",
              "      <td>0.364368</td>\n",
              "      <td>0.382583</td>\n",
              "      <td>0.527326</td>\n",
              "      <td>0.503676</td>\n",
              "      <td>0.966133</td>\n",
              "      <td>0.572232</td>\n",
              "      <td>0.696015</td>\n",
              "      <td>0.345646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">text-davinci-003</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.878401</td>\n",
              "      <td>0.942565</td>\n",
              "      <td>0.927200</td>\n",
              "      <td>0.837861</td>\n",
              "      <td>0.814043</td>\n",
              "      <td>0.391896</td>\n",
              "      <td>0.356377</td>\n",
              "      <td>0.386660</td>\n",
              "      <td>0.516124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.878401</td>\n",
              "      <td>0.942565</td>\n",
              "      <td>0.927200</td>\n",
              "      <td>0.837861</td>\n",
              "      <td>0.814043</td>\n",
              "      <td>0.391896</td>\n",
              "      <td>0.356377</td>\n",
              "      <td>0.386660</td>\n",
              "      <td>0.516124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.566864</td>\n",
              "      <td>0.634362</td>\n",
              "      <td>0.607457</td>\n",
              "      <td>0.611247</td>\n",
              "      <td>0.553411</td>\n",
              "      <td>0.401165</td>\n",
              "      <td>0.385329</td>\n",
              "      <td>0.381847</td>\n",
              "      <td>0.290891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.566864</td>\n",
              "      <td>0.634362</td>\n",
              "      <td>0.607457</td>\n",
              "      <td>0.611247</td>\n",
              "      <td>0.553411</td>\n",
              "      <td>0.401165</td>\n",
              "      <td>0.385329</td>\n",
              "      <td>0.381847</td>\n",
              "      <td>0.290891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.524432</td>\n",
              "      <td>0.694682</td>\n",
              "      <td>0.801445</td>\n",
              "      <td>0.682702</td>\n",
              "      <td>0.703286</td>\n",
              "      <td>0.639227</td>\n",
              "      <td>0.443965</td>\n",
              "      <td>0.493947</td>\n",
              "      <td>0.414019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.524432</td>\n",
              "      <td>0.694682</td>\n",
              "      <td>0.801445</td>\n",
              "      <td>0.682702</td>\n",
              "      <td>0.703286</td>\n",
              "      <td>0.639227</td>\n",
              "      <td>0.443965</td>\n",
              "      <td>0.493947</td>\n",
              "      <td>0.414019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.792974</td>\n",
              "      <td>0.880861</td>\n",
              "      <td>0.863465</td>\n",
              "      <td>0.796022</td>\n",
              "      <td>0.740844</td>\n",
              "      <td>0.376911</td>\n",
              "      <td>0.364982</td>\n",
              "      <td>0.394864</td>\n",
              "      <td>0.458002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.792974</td>\n",
              "      <td>0.880861</td>\n",
              "      <td>0.863465</td>\n",
              "      <td>0.796022</td>\n",
              "      <td>0.740844</td>\n",
              "      <td>0.376911</td>\n",
              "      <td>0.364982</td>\n",
              "      <td>0.394864</td>\n",
              "      <td>0.458002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.890076</td>\n",
              "      <td>0.885835</td>\n",
              "      <td>0.889408</td>\n",
              "      <td>0.809454</td>\n",
              "      <td>0.819020</td>\n",
              "      <td>0.380513</td>\n",
              "      <td>0.364060</td>\n",
              "      <td>0.383281</td>\n",
              "      <td>0.487793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.890076</td>\n",
              "      <td>0.885835</td>\n",
              "      <td>0.889408</td>\n",
              "      <td>0.809454</td>\n",
              "      <td>0.819020</td>\n",
              "      <td>0.380513</td>\n",
              "      <td>0.364060</td>\n",
              "      <td>0.383281</td>\n",
              "      <td>0.487793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.721004</td>\n",
              "      <td>0.762168</td>\n",
              "      <td>0.827556</td>\n",
              "      <td>0.749570</td>\n",
              "      <td>0.741933</td>\n",
              "      <td>0.549049</td>\n",
              "      <td>0.447690</td>\n",
              "      <td>0.587390</td>\n",
              "      <td>0.521483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.721004</td>\n",
              "      <td>0.762168</td>\n",
              "      <td>0.827556</td>\n",
              "      <td>0.749570</td>\n",
              "      <td>0.741933</td>\n",
              "      <td>0.549049</td>\n",
              "      <td>0.447690</td>\n",
              "      <td>0.587390</td>\n",
              "      <td>0.521483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.765965</td>\n",
              "      <td>0.866400</td>\n",
              "      <td>0.862075</td>\n",
              "      <td>0.774199</td>\n",
              "      <td>0.756600</td>\n",
              "      <td>0.424504</td>\n",
              "      <td>0.369830</td>\n",
              "      <td>0.382837</td>\n",
              "      <td>0.454262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.765965</td>\n",
              "      <td>0.866400</td>\n",
              "      <td>0.862075</td>\n",
              "      <td>0.774199</td>\n",
              "      <td>0.756600</td>\n",
              "      <td>0.424504</td>\n",
              "      <td>0.369830</td>\n",
              "      <td>0.382837</td>\n",
              "      <td>0.454262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">vicuna-13b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.890515</td>\n",
              "      <td>0.922273</td>\n",
              "      <td>0.859693</td>\n",
              "      <td>0.903072</td>\n",
              "      <td>0.807397</td>\n",
              "      <td>0.481618</td>\n",
              "      <td>0.486941</td>\n",
              "      <td>0.409251</td>\n",
              "      <td>0.544604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.890515</td>\n",
              "      <td>0.922273</td>\n",
              "      <td>0.859693</td>\n",
              "      <td>0.903072</td>\n",
              "      <td>0.807397</td>\n",
              "      <td>0.481618</td>\n",
              "      <td>0.486941</td>\n",
              "      <td>0.409251</td>\n",
              "      <td>0.544604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.602345</td>\n",
              "      <td>0.638156</td>\n",
              "      <td>0.591192</td>\n",
              "      <td>0.658108</td>\n",
              "      <td>0.589176</td>\n",
              "      <td>0.456001</td>\n",
              "      <td>0.502353</td>\n",
              "      <td>0.410590</td>\n",
              "      <td>0.329236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.602345</td>\n",
              "      <td>0.638156</td>\n",
              "      <td>0.591192</td>\n",
              "      <td>0.658108</td>\n",
              "      <td>0.589176</td>\n",
              "      <td>0.456001</td>\n",
              "      <td>0.502353</td>\n",
              "      <td>0.410590</td>\n",
              "      <td>0.329236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.624563</td>\n",
              "      <td>0.711251</td>\n",
              "      <td>0.649574</td>\n",
              "      <td>0.775774</td>\n",
              "      <td>0.669699</td>\n",
              "      <td>0.639953</td>\n",
              "      <td>0.475326</td>\n",
              "      <td>0.468093</td>\n",
              "      <td>0.405648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.624563</td>\n",
              "      <td>0.711251</td>\n",
              "      <td>0.649574</td>\n",
              "      <td>0.775774</td>\n",
              "      <td>0.669699</td>\n",
              "      <td>0.639953</td>\n",
              "      <td>0.475326</td>\n",
              "      <td>0.468093</td>\n",
              "      <td>0.405648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.823762</td>\n",
              "      <td>0.865952</td>\n",
              "      <td>0.817581</td>\n",
              "      <td>0.852928</td>\n",
              "      <td>0.794869</td>\n",
              "      <td>0.440699</td>\n",
              "      <td>0.552195</td>\n",
              "      <td>0.439702</td>\n",
              "      <td>0.522266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.823762</td>\n",
              "      <td>0.865952</td>\n",
              "      <td>0.817581</td>\n",
              "      <td>0.852928</td>\n",
              "      <td>0.794869</td>\n",
              "      <td>0.440699</td>\n",
              "      <td>0.552195</td>\n",
              "      <td>0.439702</td>\n",
              "      <td>0.522266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.903552</td>\n",
              "      <td>0.883311</td>\n",
              "      <td>0.859736</td>\n",
              "      <td>0.865503</td>\n",
              "      <td>0.821485</td>\n",
              "      <td>0.491275</td>\n",
              "      <td>0.485060</td>\n",
              "      <td>0.423140</td>\n",
              "      <td>0.532327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.903552</td>\n",
              "      <td>0.883311</td>\n",
              "      <td>0.859736</td>\n",
              "      <td>0.865503</td>\n",
              "      <td>0.821485</td>\n",
              "      <td>0.491275</td>\n",
              "      <td>0.485060</td>\n",
              "      <td>0.423140</td>\n",
              "      <td>0.532327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.817151</td>\n",
              "      <td>0.862368</td>\n",
              "      <td>0.817243</td>\n",
              "      <td>0.871341</td>\n",
              "      <td>0.796307</td>\n",
              "      <td>0.526786</td>\n",
              "      <td>0.566316</td>\n",
              "      <td>0.518273</td>\n",
              "      <td>0.529117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.817151</td>\n",
              "      <td>0.862368</td>\n",
              "      <td>0.817243</td>\n",
              "      <td>0.871341</td>\n",
              "      <td>0.796307</td>\n",
              "      <td>0.526786</td>\n",
              "      <td>0.566316</td>\n",
              "      <td>0.518273</td>\n",
              "      <td>0.529117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.920155</td>\n",
              "      <td>0.933794</td>\n",
              "      <td>0.893835</td>\n",
              "      <td>0.926867</td>\n",
              "      <td>0.834466</td>\n",
              "      <td>0.545461</td>\n",
              "      <td>0.532225</td>\n",
              "      <td>0.464019</td>\n",
              "      <td>0.579232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.920155</td>\n",
              "      <td>0.933794</td>\n",
              "      <td>0.893835</td>\n",
              "      <td>0.926867</td>\n",
              "      <td>0.834466</td>\n",
              "      <td>0.545461</td>\n",
              "      <td>0.532225</td>\n",
              "      <td>0.464019</td>\n",
              "      <td>0.579232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"119\" valign=\"top\">en3</th>\n",
              "      <th rowspan=\"7\" valign=\"top\">all</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.396812</td>\n",
              "      <td>0.396643</td>\n",
              "      <td>0.396746</td>\n",
              "      <td>0.396119</td>\n",
              "      <td>0.396425</td>\n",
              "      <td>0.393226</td>\n",
              "      <td>0.394164</td>\n",
              "      <td>0.394157</td>\n",
              "      <td>0.526371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.414982</td>\n",
              "      <td>0.413729</td>\n",
              "      <td>0.415095</td>\n",
              "      <td>0.414973</td>\n",
              "      <td>0.415208</td>\n",
              "      <td>0.411713</td>\n",
              "      <td>0.412369</td>\n",
              "      <td>0.411422</td>\n",
              "      <td>0.541786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.438186</td>\n",
              "      <td>0.423047</td>\n",
              "      <td>0.413818</td>\n",
              "      <td>0.472503</td>\n",
              "      <td>0.427483</td>\n",
              "      <td>0.531722</td>\n",
              "      <td>0.590560</td>\n",
              "      <td>0.521381</td>\n",
              "      <td>0.461908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.569036</td>\n",
              "      <td>0.564114</td>\n",
              "      <td>0.551642</td>\n",
              "      <td>0.568690</td>\n",
              "      <td>0.535509</td>\n",
              "      <td>0.414920</td>\n",
              "      <td>0.570724</td>\n",
              "      <td>0.504929</td>\n",
              "      <td>0.560242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.456115</td>\n",
              "      <td>0.455917</td>\n",
              "      <td>0.456844</td>\n",
              "      <td>0.456315</td>\n",
              "      <td>0.455687</td>\n",
              "      <td>0.451096</td>\n",
              "      <td>0.452209</td>\n",
              "      <td>0.445304</td>\n",
              "      <td>0.573683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.506289</td>\n",
              "      <td>0.504309</td>\n",
              "      <td>0.500837</td>\n",
              "      <td>0.503992</td>\n",
              "      <td>0.501980</td>\n",
              "      <td>0.505160</td>\n",
              "      <td>0.503919</td>\n",
              "      <td>0.503466</td>\n",
              "      <td>0.610576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.509404</td>\n",
              "      <td>0.509294</td>\n",
              "      <td>0.509011</td>\n",
              "      <td>0.509199</td>\n",
              "      <td>0.505425</td>\n",
              "      <td>0.482085</td>\n",
              "      <td>0.504289</td>\n",
              "      <td>0.487426</td>\n",
              "      <td>0.606566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">alpaca-lora-30b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.815090</td>\n",
              "      <td>0.854275</td>\n",
              "      <td>0.835100</td>\n",
              "      <td>0.787890</td>\n",
              "      <td>0.807579</td>\n",
              "      <td>0.426781</td>\n",
              "      <td>0.496862</td>\n",
              "      <td>0.410618</td>\n",
              "      <td>0.539153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.815090</td>\n",
              "      <td>0.854275</td>\n",
              "      <td>0.835100</td>\n",
              "      <td>0.787890</td>\n",
              "      <td>0.807579</td>\n",
              "      <td>0.426781</td>\n",
              "      <td>0.496862</td>\n",
              "      <td>0.410618</td>\n",
              "      <td>0.539153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.539969</td>\n",
              "      <td>0.547081</td>\n",
              "      <td>0.539625</td>\n",
              "      <td>0.554727</td>\n",
              "      <td>0.565463</td>\n",
              "      <td>0.520691</td>\n",
              "      <td>0.465029</td>\n",
              "      <td>0.488219</td>\n",
              "      <td>0.388707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.539969</td>\n",
              "      <td>0.547081</td>\n",
              "      <td>0.539625</td>\n",
              "      <td>0.554727</td>\n",
              "      <td>0.565463</td>\n",
              "      <td>0.520691</td>\n",
              "      <td>0.465029</td>\n",
              "      <td>0.488219</td>\n",
              "      <td>0.388707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.434758</td>\n",
              "      <td>0.429913</td>\n",
              "      <td>0.424284</td>\n",
              "      <td>0.453424</td>\n",
              "      <td>0.481300</td>\n",
              "      <td>0.428447</td>\n",
              "      <td>0.519389</td>\n",
              "      <td>0.378684</td>\n",
              "      <td>0.211332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.434758</td>\n",
              "      <td>0.429913</td>\n",
              "      <td>0.424284</td>\n",
              "      <td>0.453424</td>\n",
              "      <td>0.481300</td>\n",
              "      <td>0.428447</td>\n",
              "      <td>0.519389</td>\n",
              "      <td>0.378684</td>\n",
              "      <td>0.211332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.740861</td>\n",
              "      <td>0.774562</td>\n",
              "      <td>0.743497</td>\n",
              "      <td>0.727912</td>\n",
              "      <td>0.734528</td>\n",
              "      <td>0.337669</td>\n",
              "      <td>0.512682</td>\n",
              "      <td>0.433243</td>\n",
              "      <td>0.494811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.740861</td>\n",
              "      <td>0.774562</td>\n",
              "      <td>0.743497</td>\n",
              "      <td>0.727912</td>\n",
              "      <td>0.734528</td>\n",
              "      <td>0.337669</td>\n",
              "      <td>0.512682</td>\n",
              "      <td>0.433243</td>\n",
              "      <td>0.494811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.732471</td>\n",
              "      <td>0.724337</td>\n",
              "      <td>0.720857</td>\n",
              "      <td>0.693065</td>\n",
              "      <td>0.681353</td>\n",
              "      <td>0.301433</td>\n",
              "      <td>0.417425</td>\n",
              "      <td>0.341440</td>\n",
              "      <td>0.496785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.732471</td>\n",
              "      <td>0.724337</td>\n",
              "      <td>0.720857</td>\n",
              "      <td>0.693065</td>\n",
              "      <td>0.681353</td>\n",
              "      <td>0.301433</td>\n",
              "      <td>0.417425</td>\n",
              "      <td>0.341440</td>\n",
              "      <td>0.496785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.507571</td>\n",
              "      <td>0.508672</td>\n",
              "      <td>0.508389</td>\n",
              "      <td>0.501537</td>\n",
              "      <td>0.507334</td>\n",
              "      <td>0.480115</td>\n",
              "      <td>0.434277</td>\n",
              "      <td>0.479982</td>\n",
              "      <td>0.577122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.507571</td>\n",
              "      <td>0.508672</td>\n",
              "      <td>0.508389</td>\n",
              "      <td>0.501537</td>\n",
              "      <td>0.507334</td>\n",
              "      <td>0.480115</td>\n",
              "      <td>0.434277</td>\n",
              "      <td>0.479982</td>\n",
              "      <td>0.577122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.842186</td>\n",
              "      <td>0.848953</td>\n",
              "      <td>0.834026</td>\n",
              "      <td>0.797257</td>\n",
              "      <td>0.793299</td>\n",
              "      <td>0.394107</td>\n",
              "      <td>0.453519</td>\n",
              "      <td>0.413505</td>\n",
              "      <td>0.537613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.842186</td>\n",
              "      <td>0.848953</td>\n",
              "      <td>0.834026</td>\n",
              "      <td>0.797257</td>\n",
              "      <td>0.793299</td>\n",
              "      <td>0.394107</td>\n",
              "      <td>0.453519</td>\n",
              "      <td>0.413505</td>\n",
              "      <td>0.537613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">gpt-3.5-turbo</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.897896</td>\n",
              "      <td>0.924718</td>\n",
              "      <td>0.858929</td>\n",
              "      <td>0.826905</td>\n",
              "      <td>0.783661</td>\n",
              "      <td>0.381844</td>\n",
              "      <td>0.374024</td>\n",
              "      <td>0.391338</td>\n",
              "      <td>0.508449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.897896</td>\n",
              "      <td>0.924718</td>\n",
              "      <td>0.858929</td>\n",
              "      <td>0.826905</td>\n",
              "      <td>0.783661</td>\n",
              "      <td>0.381844</td>\n",
              "      <td>0.374024</td>\n",
              "      <td>0.391338</td>\n",
              "      <td>0.508449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.477612</td>\n",
              "      <td>0.500790</td>\n",
              "      <td>0.498706</td>\n",
              "      <td>0.527972</td>\n",
              "      <td>0.508214</td>\n",
              "      <td>0.440049</td>\n",
              "      <td>0.397803</td>\n",
              "      <td>0.415385</td>\n",
              "      <td>0.266367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.477612</td>\n",
              "      <td>0.500790</td>\n",
              "      <td>0.498706</td>\n",
              "      <td>0.527972</td>\n",
              "      <td>0.508214</td>\n",
              "      <td>0.440049</td>\n",
              "      <td>0.397803</td>\n",
              "      <td>0.415385</td>\n",
              "      <td>0.266367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.430465</td>\n",
              "      <td>0.428445</td>\n",
              "      <td>0.411862</td>\n",
              "      <td>0.443775</td>\n",
              "      <td>0.411083</td>\n",
              "      <td>0.382515</td>\n",
              "      <td>0.440530</td>\n",
              "      <td>0.349465</td>\n",
              "      <td>0.178861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.430465</td>\n",
              "      <td>0.428445</td>\n",
              "      <td>0.411862</td>\n",
              "      <td>0.443775</td>\n",
              "      <td>0.411083</td>\n",
              "      <td>0.382515</td>\n",
              "      <td>0.440530</td>\n",
              "      <td>0.349465</td>\n",
              "      <td>0.178861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.765838</td>\n",
              "      <td>0.830221</td>\n",
              "      <td>0.737524</td>\n",
              "      <td>0.714768</td>\n",
              "      <td>0.639292</td>\n",
              "      <td>0.322327</td>\n",
              "      <td>0.351787</td>\n",
              "      <td>0.356470</td>\n",
              "      <td>0.417025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.765838</td>\n",
              "      <td>0.830221</td>\n",
              "      <td>0.737524</td>\n",
              "      <td>0.714768</td>\n",
              "      <td>0.639292</td>\n",
              "      <td>0.322327</td>\n",
              "      <td>0.351787</td>\n",
              "      <td>0.356470</td>\n",
              "      <td>0.417025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.795732</td>\n",
              "      <td>0.786177</td>\n",
              "      <td>0.723222</td>\n",
              "      <td>0.705498</td>\n",
              "      <td>0.674072</td>\n",
              "      <td>0.271748</td>\n",
              "      <td>0.344195</td>\n",
              "      <td>0.302841</td>\n",
              "      <td>0.471563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.795732</td>\n",
              "      <td>0.786177</td>\n",
              "      <td>0.723222</td>\n",
              "      <td>0.705498</td>\n",
              "      <td>0.674072</td>\n",
              "      <td>0.271748</td>\n",
              "      <td>0.344195</td>\n",
              "      <td>0.302841</td>\n",
              "      <td>0.471563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.743814</td>\n",
              "      <td>0.768797</td>\n",
              "      <td>0.763752</td>\n",
              "      <td>0.744766</td>\n",
              "      <td>0.721918</td>\n",
              "      <td>0.515958</td>\n",
              "      <td>0.464334</td>\n",
              "      <td>0.579071</td>\n",
              "      <td>0.521696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.743814</td>\n",
              "      <td>0.768797</td>\n",
              "      <td>0.763752</td>\n",
              "      <td>0.744766</td>\n",
              "      <td>0.721918</td>\n",
              "      <td>0.515958</td>\n",
              "      <td>0.464334</td>\n",
              "      <td>0.579071</td>\n",
              "      <td>0.521696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.915388</td>\n",
              "      <td>0.936736</td>\n",
              "      <td>0.828342</td>\n",
              "      <td>0.811050</td>\n",
              "      <td>0.743945</td>\n",
              "      <td>0.377967</td>\n",
              "      <td>0.372926</td>\n",
              "      <td>0.367510</td>\n",
              "      <td>0.489415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.915388</td>\n",
              "      <td>0.936736</td>\n",
              "      <td>0.828342</td>\n",
              "      <td>0.811050</td>\n",
              "      <td>0.743945</td>\n",
              "      <td>0.377967</td>\n",
              "      <td>0.372926</td>\n",
              "      <td>0.367510</td>\n",
              "      <td>0.489415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">gpt-4</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.877989</td>\n",
              "      <td>0.839875</td>\n",
              "      <td>0.767030</td>\n",
              "      <td>0.748190</td>\n",
              "      <td>0.694034</td>\n",
              "      <td>0.349331</td>\n",
              "      <td>0.372454</td>\n",
              "      <td>0.395701</td>\n",
              "      <td>0.474859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.877989</td>\n",
              "      <td>0.839875</td>\n",
              "      <td>0.767030</td>\n",
              "      <td>0.748190</td>\n",
              "      <td>0.694034</td>\n",
              "      <td>0.349331</td>\n",
              "      <td>0.372454</td>\n",
              "      <td>0.395701</td>\n",
              "      <td>0.474859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.580817</td>\n",
              "      <td>0.544298</td>\n",
              "      <td>0.530090</td>\n",
              "      <td>0.561800</td>\n",
              "      <td>0.540311</td>\n",
              "      <td>0.443509</td>\n",
              "      <td>0.399115</td>\n",
              "      <td>0.418281</td>\n",
              "      <td>0.294223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.580817</td>\n",
              "      <td>0.544298</td>\n",
              "      <td>0.530090</td>\n",
              "      <td>0.561800</td>\n",
              "      <td>0.540311</td>\n",
              "      <td>0.443509</td>\n",
              "      <td>0.399115</td>\n",
              "      <td>0.418281</td>\n",
              "      <td>0.294223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.436012</td>\n",
              "      <td>0.427382</td>\n",
              "      <td>0.413139</td>\n",
              "      <td>0.440473</td>\n",
              "      <td>0.416507</td>\n",
              "      <td>0.389897</td>\n",
              "      <td>0.464101</td>\n",
              "      <td>0.361000</td>\n",
              "      <td>0.184907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.436012</td>\n",
              "      <td>0.427382</td>\n",
              "      <td>0.413139</td>\n",
              "      <td>0.440473</td>\n",
              "      <td>0.416507</td>\n",
              "      <td>0.389897</td>\n",
              "      <td>0.464101</td>\n",
              "      <td>0.361000</td>\n",
              "      <td>0.184907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.835305</td>\n",
              "      <td>0.797218</td>\n",
              "      <td>0.678715</td>\n",
              "      <td>0.684853</td>\n",
              "      <td>0.600872</td>\n",
              "      <td>0.331883</td>\n",
              "      <td>0.358096</td>\n",
              "      <td>0.373165</td>\n",
              "      <td>0.402445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.835305</td>\n",
              "      <td>0.797218</td>\n",
              "      <td>0.678715</td>\n",
              "      <td>0.684853</td>\n",
              "      <td>0.600872</td>\n",
              "      <td>0.331883</td>\n",
              "      <td>0.358096</td>\n",
              "      <td>0.373165</td>\n",
              "      <td>0.402445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.771539</td>\n",
              "      <td>0.793915</td>\n",
              "      <td>0.717014</td>\n",
              "      <td>0.698062</td>\n",
              "      <td>0.660488</td>\n",
              "      <td>0.291053</td>\n",
              "      <td>0.349302</td>\n",
              "      <td>0.322611</td>\n",
              "      <td>0.454663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.771539</td>\n",
              "      <td>0.793915</td>\n",
              "      <td>0.717014</td>\n",
              "      <td>0.698062</td>\n",
              "      <td>0.660488</td>\n",
              "      <td>0.291053</td>\n",
              "      <td>0.349302</td>\n",
              "      <td>0.322611</td>\n",
              "      <td>0.454663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.650100</td>\n",
              "      <td>0.647093</td>\n",
              "      <td>0.639401</td>\n",
              "      <td>0.629417</td>\n",
              "      <td>0.612845</td>\n",
              "      <td>0.354317</td>\n",
              "      <td>0.396930</td>\n",
              "      <td>0.527413</td>\n",
              "      <td>0.466843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.650100</td>\n",
              "      <td>0.647093</td>\n",
              "      <td>0.639401</td>\n",
              "      <td>0.629417</td>\n",
              "      <td>0.612845</td>\n",
              "      <td>0.354317</td>\n",
              "      <td>0.396930</td>\n",
              "      <td>0.527413</td>\n",
              "      <td>0.466843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.912705</td>\n",
              "      <td>0.887759</td>\n",
              "      <td>0.806010</td>\n",
              "      <td>0.793684</td>\n",
              "      <td>0.758701</td>\n",
              "      <td>0.372316</td>\n",
              "      <td>0.399539</td>\n",
              "      <td>0.402976</td>\n",
              "      <td>0.495228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.912705</td>\n",
              "      <td>0.887759</td>\n",
              "      <td>0.806010</td>\n",
              "      <td>0.793684</td>\n",
              "      <td>0.758701</td>\n",
              "      <td>0.372316</td>\n",
              "      <td>0.399539</td>\n",
              "      <td>0.402976</td>\n",
              "      <td>0.495228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">llama-65b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.503145</td>\n",
              "      <td>0.367020</td>\n",
              "      <td>0.318726</td>\n",
              "      <td>0.491762</td>\n",
              "      <td>0.414377</td>\n",
              "      <td>0.665641</td>\n",
              "      <td>0.724555</td>\n",
              "      <td>0.632193</td>\n",
              "      <td>0.432415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.503145</td>\n",
              "      <td>0.367020</td>\n",
              "      <td>0.318726</td>\n",
              "      <td>0.491762</td>\n",
              "      <td>0.414377</td>\n",
              "      <td>0.665641</td>\n",
              "      <td>0.724555</td>\n",
              "      <td>0.632193</td>\n",
              "      <td>0.432415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.422044</td>\n",
              "      <td>0.419384</td>\n",
              "      <td>0.423078</td>\n",
              "      <td>0.420476</td>\n",
              "      <td>0.422278</td>\n",
              "      <td>0.419668</td>\n",
              "      <td>0.420318</td>\n",
              "      <td>0.418780</td>\n",
              "      <td>0.544748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.422044</td>\n",
              "      <td>0.419384</td>\n",
              "      <td>0.423078</td>\n",
              "      <td>0.420476</td>\n",
              "      <td>0.422278</td>\n",
              "      <td>0.419668</td>\n",
              "      <td>0.420318</td>\n",
              "      <td>0.418780</td>\n",
              "      <td>0.544748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.416851</td>\n",
              "      <td>0.416172</td>\n",
              "      <td>0.435301</td>\n",
              "      <td>0.450519</td>\n",
              "      <td>0.410406</td>\n",
              "      <td>0.476261</td>\n",
              "      <td>0.498253</td>\n",
              "      <td>0.477082</td>\n",
              "      <td>0.495248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.416851</td>\n",
              "      <td>0.416172</td>\n",
              "      <td>0.435301</td>\n",
              "      <td>0.450519</td>\n",
              "      <td>0.410406</td>\n",
              "      <td>0.476261</td>\n",
              "      <td>0.498253</td>\n",
              "      <td>0.477082</td>\n",
              "      <td>0.495248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.549666</td>\n",
              "      <td>0.517200</td>\n",
              "      <td>0.501149</td>\n",
              "      <td>0.550172</td>\n",
              "      <td>0.492332</td>\n",
              "      <td>0.524732</td>\n",
              "      <td>0.589714</td>\n",
              "      <td>0.544761</td>\n",
              "      <td>0.541871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.549666</td>\n",
              "      <td>0.517200</td>\n",
              "      <td>0.501149</td>\n",
              "      <td>0.550172</td>\n",
              "      <td>0.492332</td>\n",
              "      <td>0.524732</td>\n",
              "      <td>0.589714</td>\n",
              "      <td>0.544761</td>\n",
              "      <td>0.541871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.508899</td>\n",
              "      <td>0.428285</td>\n",
              "      <td>0.388016</td>\n",
              "      <td>0.611489</td>\n",
              "      <td>0.468837</td>\n",
              "      <td>0.753374</td>\n",
              "      <td>0.775922</td>\n",
              "      <td>0.683452</td>\n",
              "      <td>0.467960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.508899</td>\n",
              "      <td>0.428285</td>\n",
              "      <td>0.388016</td>\n",
              "      <td>0.611489</td>\n",
              "      <td>0.468837</td>\n",
              "      <td>0.753374</td>\n",
              "      <td>0.775922</td>\n",
              "      <td>0.683452</td>\n",
              "      <td>0.467960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.429331</td>\n",
              "      <td>0.427935</td>\n",
              "      <td>0.428986</td>\n",
              "      <td>0.426997</td>\n",
              "      <td>0.426591</td>\n",
              "      <td>0.425474</td>\n",
              "      <td>0.426586</td>\n",
              "      <td>0.425292</td>\n",
              "      <td>0.551131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.429331</td>\n",
              "      <td>0.427935</td>\n",
              "      <td>0.428986</td>\n",
              "      <td>0.426997</td>\n",
              "      <td>0.426591</td>\n",
              "      <td>0.425474</td>\n",
              "      <td>0.426586</td>\n",
              "      <td>0.425292</td>\n",
              "      <td>0.551131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.543955</td>\n",
              "      <td>0.512116</td>\n",
              "      <td>0.509665</td>\n",
              "      <td>0.562724</td>\n",
              "      <td>0.538878</td>\n",
              "      <td>0.616834</td>\n",
              "      <td>0.628387</td>\n",
              "      <td>0.583703</td>\n",
              "      <td>0.560237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.543955</td>\n",
              "      <td>0.512116</td>\n",
              "      <td>0.509665</td>\n",
              "      <td>0.562724</td>\n",
              "      <td>0.538878</td>\n",
              "      <td>0.616834</td>\n",
              "      <td>0.628387</td>\n",
              "      <td>0.583703</td>\n",
              "      <td>0.560237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">opt-66b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.591382</td>\n",
              "      <td>0.554463</td>\n",
              "      <td>0.506736</td>\n",
              "      <td>0.605541</td>\n",
              "      <td>0.532646</td>\n",
              "      <td>0.598676</td>\n",
              "      <td>0.697701</td>\n",
              "      <td>0.612799</td>\n",
              "      <td>0.516472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.591382</td>\n",
              "      <td>0.554463</td>\n",
              "      <td>0.506736</td>\n",
              "      <td>0.605541</td>\n",
              "      <td>0.532646</td>\n",
              "      <td>0.598676</td>\n",
              "      <td>0.697701</td>\n",
              "      <td>0.612799</td>\n",
              "      <td>0.516472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.593698</td>\n",
              "      <td>0.532817</td>\n",
              "      <td>0.506487</td>\n",
              "      <td>0.591001</td>\n",
              "      <td>0.528278</td>\n",
              "      <td>0.673953</td>\n",
              "      <td>0.707458</td>\n",
              "      <td>0.628242</td>\n",
              "      <td>0.529356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.593698</td>\n",
              "      <td>0.532817</td>\n",
              "      <td>0.506487</td>\n",
              "      <td>0.591001</td>\n",
              "      <td>0.528278</td>\n",
              "      <td>0.673953</td>\n",
              "      <td>0.707458</td>\n",
              "      <td>0.628242</td>\n",
              "      <td>0.529356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.476345</td>\n",
              "      <td>0.469929</td>\n",
              "      <td>0.500059</td>\n",
              "      <td>0.526138</td>\n",
              "      <td>0.485899</td>\n",
              "      <td>0.597020</td>\n",
              "      <td>0.705766</td>\n",
              "      <td>0.583681</td>\n",
              "      <td>0.409883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.476345</td>\n",
              "      <td>0.469929</td>\n",
              "      <td>0.500059</td>\n",
              "      <td>0.526138</td>\n",
              "      <td>0.485899</td>\n",
              "      <td>0.597020</td>\n",
              "      <td>0.705766</td>\n",
              "      <td>0.583681</td>\n",
              "      <td>0.409883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.647876</td>\n",
              "      <td>0.634054</td>\n",
              "      <td>0.593213</td>\n",
              "      <td>0.625004</td>\n",
              "      <td>0.500024</td>\n",
              "      <td>0.460110</td>\n",
              "      <td>0.637743</td>\n",
              "      <td>0.563462</td>\n",
              "      <td>0.504160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.647876</td>\n",
              "      <td>0.634054</td>\n",
              "      <td>0.593213</td>\n",
              "      <td>0.625004</td>\n",
              "      <td>0.500024</td>\n",
              "      <td>0.460110</td>\n",
              "      <td>0.637743</td>\n",
              "      <td>0.563462</td>\n",
              "      <td>0.504160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.613564</td>\n",
              "      <td>0.610537</td>\n",
              "      <td>0.573370</td>\n",
              "      <td>0.586491</td>\n",
              "      <td>0.508379</td>\n",
              "      <td>0.474023</td>\n",
              "      <td>0.582629</td>\n",
              "      <td>0.502744</td>\n",
              "      <td>0.530852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.613564</td>\n",
              "      <td>0.610537</td>\n",
              "      <td>0.573370</td>\n",
              "      <td>0.586491</td>\n",
              "      <td>0.508379</td>\n",
              "      <td>0.474023</td>\n",
              "      <td>0.582629</td>\n",
              "      <td>0.502744</td>\n",
              "      <td>0.530852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.523213</td>\n",
              "      <td>0.520497</td>\n",
              "      <td>0.519302</td>\n",
              "      <td>0.515992</td>\n",
              "      <td>0.511218</td>\n",
              "      <td>0.517334</td>\n",
              "      <td>0.509336</td>\n",
              "      <td>0.517843</td>\n",
              "      <td>0.613234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.523213</td>\n",
              "      <td>0.520497</td>\n",
              "      <td>0.519302</td>\n",
              "      <td>0.515992</td>\n",
              "      <td>0.511218</td>\n",
              "      <td>0.517334</td>\n",
              "      <td>0.509336</td>\n",
              "      <td>0.517843</td>\n",
              "      <td>0.613234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.573028</td>\n",
              "      <td>0.513684</td>\n",
              "      <td>0.461860</td>\n",
              "      <td>0.596003</td>\n",
              "      <td>0.446879</td>\n",
              "      <td>0.548935</td>\n",
              "      <td>0.688343</td>\n",
              "      <td>0.582297</td>\n",
              "      <td>0.479673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.573028</td>\n",
              "      <td>0.513684</td>\n",
              "      <td>0.461860</td>\n",
              "      <td>0.596003</td>\n",
              "      <td>0.446879</td>\n",
              "      <td>0.548935</td>\n",
              "      <td>0.688343</td>\n",
              "      <td>0.582297</td>\n",
              "      <td>0.479673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">opt-iml-max-1.3b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.608342</td>\n",
              "      <td>0.603939</td>\n",
              "      <td>0.574315</td>\n",
              "      <td>0.656640</td>\n",
              "      <td>0.578536</td>\n",
              "      <td>0.660214</td>\n",
              "      <td>0.701585</td>\n",
              "      <td>0.645344</td>\n",
              "      <td>0.567327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.608342</td>\n",
              "      <td>0.603939</td>\n",
              "      <td>0.574315</td>\n",
              "      <td>0.656640</td>\n",
              "      <td>0.578536</td>\n",
              "      <td>0.660214</td>\n",
              "      <td>0.701585</td>\n",
              "      <td>0.645344</td>\n",
              "      <td>0.567327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.496345</td>\n",
              "      <td>0.461284</td>\n",
              "      <td>0.471376</td>\n",
              "      <td>0.510125</td>\n",
              "      <td>0.502396</td>\n",
              "      <td>0.664909</td>\n",
              "      <td>0.641222</td>\n",
              "      <td>0.600095</td>\n",
              "      <td>0.499904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.496345</td>\n",
              "      <td>0.461284</td>\n",
              "      <td>0.471376</td>\n",
              "      <td>0.510125</td>\n",
              "      <td>0.502396</td>\n",
              "      <td>0.664909</td>\n",
              "      <td>0.641222</td>\n",
              "      <td>0.600095</td>\n",
              "      <td>0.499904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.513750</td>\n",
              "      <td>0.513391</td>\n",
              "      <td>0.509593</td>\n",
              "      <td>0.554412</td>\n",
              "      <td>0.505228</td>\n",
              "      <td>0.655718</td>\n",
              "      <td>0.678519</td>\n",
              "      <td>0.574215</td>\n",
              "      <td>0.421614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.513750</td>\n",
              "      <td>0.513391</td>\n",
              "      <td>0.509593</td>\n",
              "      <td>0.554412</td>\n",
              "      <td>0.505228</td>\n",
              "      <td>0.655718</td>\n",
              "      <td>0.678519</td>\n",
              "      <td>0.574215</td>\n",
              "      <td>0.421614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.620189</td>\n",
              "      <td>0.623164</td>\n",
              "      <td>0.602370</td>\n",
              "      <td>0.637804</td>\n",
              "      <td>0.568213</td>\n",
              "      <td>0.530247</td>\n",
              "      <td>0.634501</td>\n",
              "      <td>0.582643</td>\n",
              "      <td>0.564812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.620189</td>\n",
              "      <td>0.623164</td>\n",
              "      <td>0.602370</td>\n",
              "      <td>0.637804</td>\n",
              "      <td>0.568213</td>\n",
              "      <td>0.530247</td>\n",
              "      <td>0.634501</td>\n",
              "      <td>0.582643</td>\n",
              "      <td>0.564812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.609107</td>\n",
              "      <td>0.643656</td>\n",
              "      <td>0.686362</td>\n",
              "      <td>0.781939</td>\n",
              "      <td>0.682897</td>\n",
              "      <td>0.821387</td>\n",
              "      <td>0.746017</td>\n",
              "      <td>0.770143</td>\n",
              "      <td>0.606918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.609107</td>\n",
              "      <td>0.643656</td>\n",
              "      <td>0.686362</td>\n",
              "      <td>0.781939</td>\n",
              "      <td>0.682897</td>\n",
              "      <td>0.821387</td>\n",
              "      <td>0.746017</td>\n",
              "      <td>0.770143</td>\n",
              "      <td>0.606918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.642522</td>\n",
              "      <td>0.657189</td>\n",
              "      <td>0.647068</td>\n",
              "      <td>0.655868</td>\n",
              "      <td>0.624747</td>\n",
              "      <td>0.686816</td>\n",
              "      <td>0.592852</td>\n",
              "      <td>0.661270</td>\n",
              "      <td>0.642569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.642522</td>\n",
              "      <td>0.657189</td>\n",
              "      <td>0.647068</td>\n",
              "      <td>0.655868</td>\n",
              "      <td>0.624747</td>\n",
              "      <td>0.686816</td>\n",
              "      <td>0.592852</td>\n",
              "      <td>0.661270</td>\n",
              "      <td>0.642569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.585287</td>\n",
              "      <td>0.568583</td>\n",
              "      <td>0.551929</td>\n",
              "      <td>0.670401</td>\n",
              "      <td>0.564548</td>\n",
              "      <td>0.714663</td>\n",
              "      <td>0.746239</td>\n",
              "      <td>0.679977</td>\n",
              "      <td>0.519412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.585287</td>\n",
              "      <td>0.568583</td>\n",
              "      <td>0.551929</td>\n",
              "      <td>0.670401</td>\n",
              "      <td>0.564548</td>\n",
              "      <td>0.714663</td>\n",
              "      <td>0.746239</td>\n",
              "      <td>0.679977</td>\n",
              "      <td>0.519412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">text-davinci-003</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.823819</td>\n",
              "      <td>0.877743</td>\n",
              "      <td>0.856395</td>\n",
              "      <td>0.781326</td>\n",
              "      <td>0.765857</td>\n",
              "      <td>0.372565</td>\n",
              "      <td>0.352382</td>\n",
              "      <td>0.390528</td>\n",
              "      <td>0.487528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.823819</td>\n",
              "      <td>0.877743</td>\n",
              "      <td>0.856395</td>\n",
              "      <td>0.781326</td>\n",
              "      <td>0.765857</td>\n",
              "      <td>0.372565</td>\n",
              "      <td>0.352382</td>\n",
              "      <td>0.390528</td>\n",
              "      <td>0.487528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.428300</td>\n",
              "      <td>0.431995</td>\n",
              "      <td>0.432967</td>\n",
              "      <td>0.439509</td>\n",
              "      <td>0.429113</td>\n",
              "      <td>0.370776</td>\n",
              "      <td>0.360770</td>\n",
              "      <td>0.361828</td>\n",
              "      <td>0.176044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.428300</td>\n",
              "      <td>0.431995</td>\n",
              "      <td>0.432967</td>\n",
              "      <td>0.439509</td>\n",
              "      <td>0.429113</td>\n",
              "      <td>0.370776</td>\n",
              "      <td>0.360770</td>\n",
              "      <td>0.361828</td>\n",
              "      <td>0.176044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.435057</td>\n",
              "      <td>0.426725</td>\n",
              "      <td>0.417224</td>\n",
              "      <td>0.436099</td>\n",
              "      <td>0.416163</td>\n",
              "      <td>0.400234</td>\n",
              "      <td>0.475738</td>\n",
              "      <td>0.361108</td>\n",
              "      <td>0.187736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.435057</td>\n",
              "      <td>0.426725</td>\n",
              "      <td>0.417224</td>\n",
              "      <td>0.436099</td>\n",
              "      <td>0.416163</td>\n",
              "      <td>0.400234</td>\n",
              "      <td>0.475738</td>\n",
              "      <td>0.361108</td>\n",
              "      <td>0.187736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.791793</td>\n",
              "      <td>0.841265</td>\n",
              "      <td>0.768366</td>\n",
              "      <td>0.726614</td>\n",
              "      <td>0.665491</td>\n",
              "      <td>0.333927</td>\n",
              "      <td>0.353695</td>\n",
              "      <td>0.366245</td>\n",
              "      <td>0.427558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.791793</td>\n",
              "      <td>0.841265</td>\n",
              "      <td>0.768366</td>\n",
              "      <td>0.726614</td>\n",
              "      <td>0.665491</td>\n",
              "      <td>0.333927</td>\n",
              "      <td>0.353695</td>\n",
              "      <td>0.366245</td>\n",
              "      <td>0.427558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.810319</td>\n",
              "      <td>0.789113</td>\n",
              "      <td>0.765471</td>\n",
              "      <td>0.723634</td>\n",
              "      <td>0.705935</td>\n",
              "      <td>0.291870</td>\n",
              "      <td>0.320987</td>\n",
              "      <td>0.334326</td>\n",
              "      <td>0.476244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.810319</td>\n",
              "      <td>0.789113</td>\n",
              "      <td>0.765471</td>\n",
              "      <td>0.723634</td>\n",
              "      <td>0.705935</td>\n",
              "      <td>0.291870</td>\n",
              "      <td>0.320987</td>\n",
              "      <td>0.334326</td>\n",
              "      <td>0.476244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.713313</td>\n",
              "      <td>0.753398</td>\n",
              "      <td>0.745457</td>\n",
              "      <td>0.736050</td>\n",
              "      <td>0.714581</td>\n",
              "      <td>0.496221</td>\n",
              "      <td>0.401836</td>\n",
              "      <td>0.543846</td>\n",
              "      <td>0.462450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.713313</td>\n",
              "      <td>0.753398</td>\n",
              "      <td>0.745457</td>\n",
              "      <td>0.736050</td>\n",
              "      <td>0.714581</td>\n",
              "      <td>0.496221</td>\n",
              "      <td>0.401836</td>\n",
              "      <td>0.543846</td>\n",
              "      <td>0.462450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.749457</td>\n",
              "      <td>0.806275</td>\n",
              "      <td>0.737452</td>\n",
              "      <td>0.704876</td>\n",
              "      <td>0.680123</td>\n",
              "      <td>0.409468</td>\n",
              "      <td>0.367216</td>\n",
              "      <td>0.404657</td>\n",
              "      <td>0.409586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.749457</td>\n",
              "      <td>0.806275</td>\n",
              "      <td>0.737452</td>\n",
              "      <td>0.704876</td>\n",
              "      <td>0.680123</td>\n",
              "      <td>0.409468</td>\n",
              "      <td>0.367216</td>\n",
              "      <td>0.404657</td>\n",
              "      <td>0.409586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"14\" valign=\"top\">vicuna-13b</th>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.859931</td>\n",
              "      <td>0.861890</td>\n",
              "      <td>0.821508</td>\n",
              "      <td>0.834733</td>\n",
              "      <td>0.768132</td>\n",
              "      <td>0.459754</td>\n",
              "      <td>0.538549</td>\n",
              "      <td>0.449614</td>\n",
              "      <td>0.563109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bert-base-multilingual-cased</th>\n",
              "      <td>0.859931</td>\n",
              "      <td>0.861890</td>\n",
              "      <td>0.821508</td>\n",
              "      <td>0.834733</td>\n",
              "      <td>0.768132</td>\n",
              "      <td>0.459754</td>\n",
              "      <td>0.538549</td>\n",
              "      <td>0.449614</td>\n",
              "      <td>0.563109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.603294</td>\n",
              "      <td>0.608145</td>\n",
              "      <td>0.578532</td>\n",
              "      <td>0.632847</td>\n",
              "      <td>0.607712</td>\n",
              "      <td>0.593568</td>\n",
              "      <td>0.529225</td>\n",
              "      <td>0.509856</td>\n",
              "      <td>0.443065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electra-large-discriminator</th>\n",
              "      <td>0.603294</td>\n",
              "      <td>0.608145</td>\n",
              "      <td>0.578532</td>\n",
              "      <td>0.632847</td>\n",
              "      <td>0.607712</td>\n",
              "      <td>0.593568</td>\n",
              "      <td>0.529225</td>\n",
              "      <td>0.509856</td>\n",
              "      <td>0.443065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.481501</td>\n",
              "      <td>0.502959</td>\n",
              "      <td>0.551711</td>\n",
              "      <td>0.549010</td>\n",
              "      <td>0.544429</td>\n",
              "      <td>0.543328</td>\n",
              "      <td>0.545911</td>\n",
              "      <td>0.511891</td>\n",
              "      <td>0.357012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpt2-medium</th>\n",
              "      <td>0.481501</td>\n",
              "      <td>0.502959</td>\n",
              "      <td>0.551711</td>\n",
              "      <td>0.549010</td>\n",
              "      <td>0.544429</td>\n",
              "      <td>0.543328</td>\n",
              "      <td>0.545911</td>\n",
              "      <td>0.511891</td>\n",
              "      <td>0.357012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.767501</td>\n",
              "      <td>0.769260</td>\n",
              "      <td>0.740019</td>\n",
              "      <td>0.746739</td>\n",
              "      <td>0.680461</td>\n",
              "      <td>0.350788</td>\n",
              "      <td>0.487444</td>\n",
              "      <td>0.441330</td>\n",
              "      <td>0.513120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mGPT</th>\n",
              "      <td>0.767501</td>\n",
              "      <td>0.769260</td>\n",
              "      <td>0.740019</td>\n",
              "      <td>0.746739</td>\n",
              "      <td>0.680461</td>\n",
              "      <td>0.350788</td>\n",
              "      <td>0.487444</td>\n",
              "      <td>0.441330</td>\n",
              "      <td>0.513120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.759918</td>\n",
              "      <td>0.745103</td>\n",
              "      <td>0.690637</td>\n",
              "      <td>0.682103</td>\n",
              "      <td>0.636707</td>\n",
              "      <td>0.319401</td>\n",
              "      <td>0.448960</td>\n",
              "      <td>0.339154</td>\n",
              "      <td>0.475553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mdeberta-v3-base</th>\n",
              "      <td>0.759918</td>\n",
              "      <td>0.745103</td>\n",
              "      <td>0.690637</td>\n",
              "      <td>0.682103</td>\n",
              "      <td>0.636707</td>\n",
              "      <td>0.319401</td>\n",
              "      <td>0.448960</td>\n",
              "      <td>0.339154</td>\n",
              "      <td>0.475553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.498172</td>\n",
              "      <td>0.498064</td>\n",
              "      <td>0.497785</td>\n",
              "      <td>0.496890</td>\n",
              "      <td>0.496210</td>\n",
              "      <td>0.454638</td>\n",
              "      <td>0.432668</td>\n",
              "      <td>0.477027</td>\n",
              "      <td>0.572519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.498172</td>\n",
              "      <td>0.498064</td>\n",
              "      <td>0.497785</td>\n",
              "      <td>0.496890</td>\n",
              "      <td>0.496210</td>\n",
              "      <td>0.454638</td>\n",
              "      <td>0.432668</td>\n",
              "      <td>0.477027</td>\n",
              "      <td>0.572519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.867251</td>\n",
              "      <td>0.867704</td>\n",
              "      <td>0.789901</td>\n",
              "      <td>0.790022</td>\n",
              "      <td>0.708935</td>\n",
              "      <td>0.393022</td>\n",
              "      <td>0.437325</td>\n",
              "      <td>0.426190</td>\n",
              "      <td>0.518315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xlm-roberta-large</th>\n",
              "      <td>0.867251</td>\n",
              "      <td>0.867704</td>\n",
              "      <td>0.789901</td>\n",
              "      <td>0.790022</td>\n",
              "      <td>0.708935</td>\n",
              "      <td>0.393022</td>\n",
              "      <td>0.437325</td>\n",
              "      <td>0.426190</td>\n",
              "      <td>0.518315</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc1a3e3e-5985-4bbf-ae52-11ca9dc62f37')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fc1a3e3e-5985-4bbf-ae52-11ca9dc62f37 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fc1a3e3e-5985-4bbf-ae52-11ca9dc62f37');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cb9fc9c0-31ac-4ae6-b6bf-aef83e3ff605\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cb9fc9c0-31ac-4ae6-b6bf-aef83e3ff605')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cb9fc9c0-31ac-4ae6-b6bf-aef83e3ff605 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_all = results_all.reset_index()"
      ],
      "metadata": {
        "id": "M458dakszeYG"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_all.style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4)"
      ],
      "metadata": {
        "id": "w-x0c5pZiIUT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c88bf5f2-7016-4513-e6b2-1db53fbe8ae2"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001af67d0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_02857_row0_col3, #T_02857_row0_col4, #T_02857_row4_col11, #T_02857_row31_col3, #T_02857_row31_col4, #T_02857_row31_col5, #T_02857_row32_col3, #T_02857_row32_col4, #T_02857_row32_col5, #T_02857_row43_col6, #T_02857_row44_col6, #T_02857_row45_col3, #T_02857_row45_col4, #T_02857_row46_col3, #T_02857_row46_col4, #T_02857_row61_col10, #T_02857_row62_col10, #T_02857_row69_col6, #T_02857_row70_col6, #T_02857_row75_col10, #T_02857_row76_col10, #T_02857_row81_col8, #T_02857_row82_col8, #T_02857_row83_col5, #T_02857_row84_col5, #T_02857_row107_col8, #T_02857_row108_col8, #T_02857_row115_col4, #T_02857_row115_col5, #T_02857_row116_col4, #T_02857_row116_col5, #T_02857_row121_col4, #T_02857_row132_col9, #T_02857_row133_col9, #T_02857_row170_col8, #T_02857_row171_col8, #T_02857_row178_col5, #T_02857_row179_col5, #T_02857_row180_col6, #T_02857_row181_col6, #T_02857_row186_col9, #T_02857_row187_col9, #T_02857_row214_col6, #T_02857_row214_col8, #T_02857_row215_col6, #T_02857_row215_col8, #T_02857_row220_col7, #T_02857_row221_col7, #T_02857_row249_col7, #T_02857_row250_col7, #T_02857_row360_col10, #T_02857_row382_col7, #T_02857_row383_col7, #T_02857_row388_col7, #T_02857_row389_col7, #T_02857_row436_col10, #T_02857_row437_col10, #T_02857_row450_col6, #T_02857_row451_col6, #T_02857_row481_col11, #T_02857_row521_col7, #T_02857_row522_col7, #T_02857_row533_col6, #T_02857_row534_col6, #T_02857_row537_col8, #T_02857_row538_col8, #T_02857_row539_col10, #T_02857_row540_col10, #T_02857_row547_col3, #T_02857_row547_col4, #T_02857_row548_col3, #T_02857_row548_col4, #T_02857_row549_col11, #T_02857_row550_col11 {\n",
              "  background-color: #bdc8e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row0_col5, #T_02857_row0_col6, #T_02857_row0_col9, #T_02857_row23_col4, #T_02857_row23_col6, #T_02857_row23_col7, #T_02857_row24_col4, #T_02857_row24_col6, #T_02857_row24_col7, #T_02857_row45_col5, #T_02857_row46_col5, #T_02857_row57_col6, #T_02857_row57_col10, #T_02857_row58_col6, #T_02857_row58_col10, #T_02857_row69_col9, #T_02857_row70_col9, #T_02857_row71_col6, #T_02857_row72_col6, #T_02857_row79_col9, #T_02857_row80_col9, #T_02857_row83_col3, #T_02857_row83_col11, #T_02857_row84_col3, #T_02857_row84_col11, #T_02857_row115_col3, #T_02857_row115_col7, #T_02857_row116_col3, #T_02857_row116_col7, #T_02857_row186_col10, #T_02857_row187_col10, #T_02857_row188_col8, #T_02857_row189_col8, #T_02857_row190_col5, #T_02857_row191_col5, #T_02857_row192_col9, #T_02857_row193_col9, #T_02857_row226_col6, #T_02857_row227_col6, #T_02857_row313_col9, #T_02857_row314_col9, #T_02857_row337_col10, #T_02857_row338_col10, #T_02857_row351_col9, #T_02857_row352_col9, #T_02857_row380_col4, #T_02857_row381_col4, #T_02857_row420_col6, #T_02857_row421_col6, #T_02857_row438_col6, #T_02857_row439_col6, #T_02857_row450_col5, #T_02857_row451_col5, #T_02857_row464_col3, #T_02857_row465_col3, #T_02857_row482_col11, #T_02857_row539_col6, #T_02857_row540_col6, #T_02857_row553_col3, #T_02857_row553_col4, #T_02857_row554_col3, #T_02857_row554_col4, #T_02857_row559_col5, #T_02857_row560_col5, #T_02857_row561_col3, #T_02857_row561_col11, #T_02857_row562_col3, #T_02857_row562_col11, #T_02857_row583_col3, #T_02857_row583_col4, #T_02857_row583_col7, #T_02857_row584_col3, #T_02857_row584_col4, #T_02857_row584_col7 {\n",
              "  background-color: #bfc9e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row0_col7, #T_02857_row23_col3, #T_02857_row23_col5, #T_02857_row24_col3, #T_02857_row24_col5, #T_02857_row45_col6, #T_02857_row46_col6, #T_02857_row57_col11, #T_02857_row58_col11, #T_02857_row71_col3, #T_02857_row72_col3, #T_02857_row75_col6, #T_02857_row76_col6, #T_02857_row83_col10, #T_02857_row84_col10, #T_02857_row93_col4, #T_02857_row93_col5, #T_02857_row94_col4, #T_02857_row94_col5, #T_02857_row107_col6, #T_02857_row108_col6, #T_02857_row113_col3, #T_02857_row114_col3, #T_02857_row172_col8, #T_02857_row173_col8, #T_02857_row176_col7, #T_02857_row177_col7, #T_02857_row194_col5, #T_02857_row195_col5, #T_02857_row202_col6, #T_02857_row203_col6, #T_02857_row212_col5, #T_02857_row213_col5, #T_02857_row238_col3, #T_02857_row238_col5, #T_02857_row238_col7, #T_02857_row238_col9, #T_02857_row240_col9, #T_02857_row293_col9, #T_02857_row294_col9, #T_02857_row305_col9, #T_02857_row306_col9, #T_02857_row317_col8, #T_02857_row318_col8, #T_02857_row341_col4, #T_02857_row342_col4, #T_02857_row349_col9, #T_02857_row350_col9, #T_02857_row355_col10, #T_02857_row356_col10, #T_02857_row359_col10, #T_02857_row412_col6, #T_02857_row413_col6, #T_02857_row414_col7, #T_02857_row415_col7, #T_02857_row416_col3, #T_02857_row417_col3, #T_02857_row424_col6, #T_02857_row425_col6, #T_02857_row426_col5, #T_02857_row427_col5, #T_02857_row432_col7, #T_02857_row433_col7, #T_02857_row434_col7, #T_02857_row435_col7, #T_02857_row440_col7, #T_02857_row441_col7, #T_02857_row444_col7, #T_02857_row445_col7, #T_02857_row458_col10, #T_02857_row459_col10, #T_02857_row464_col5, #T_02857_row464_col7, #T_02857_row465_col5, #T_02857_row465_col7, #T_02857_row478_col9, #T_02857_row531_col9, #T_02857_row532_col9, #T_02857_row539_col3, #T_02857_row540_col3, #T_02857_row541_col3, #T_02857_row541_col6, #T_02857_row542_col3, #T_02857_row542_col6, #T_02857_row545_col5, #T_02857_row546_col5, #T_02857_row547_col6, #T_02857_row548_col6, #T_02857_row563_col9, #T_02857_row564_col9, #T_02857_row583_col8, #T_02857_row584_col8 {\n",
              "  background-color: #c1cae2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row0_col8, #T_02857_row3_col3, #T_02857_row3_col4, #T_02857_row3_col6, #T_02857_row4_col10, #T_02857_row9_col8, #T_02857_row10_col8, #T_02857_row37_col4, #T_02857_row38_col4, #T_02857_row39_col9, #T_02857_row40_col9, #T_02857_row61_col3, #T_02857_row62_col3, #T_02857_row65_col3, #T_02857_row66_col3, #T_02857_row67_col5, #T_02857_row68_col5, #T_02857_row71_col4, #T_02857_row72_col4, #T_02857_row75_col5, #T_02857_row76_col5, #T_02857_row77_col7, #T_02857_row78_col7, #T_02857_row81_col5, #T_02857_row81_col7, #T_02857_row82_col5, #T_02857_row82_col7, #T_02857_row89_col3, #T_02857_row89_col4, #T_02857_row89_col11, #T_02857_row90_col3, #T_02857_row90_col4, #T_02857_row90_col11, #T_02857_row101_col10, #T_02857_row102_col10, #T_02857_row103_col8, #T_02857_row104_col8, #T_02857_row107_col9, #T_02857_row107_col10, #T_02857_row108_col9, #T_02857_row108_col10, #T_02857_row109_col6, #T_02857_row110_col6, #T_02857_row126_col9, #T_02857_row127_col9, #T_02857_row134_col10, #T_02857_row135_col10, #T_02857_row144_col7, #T_02857_row145_col7, #T_02857_row158_col9, #T_02857_row159_col9, #T_02857_row170_col6, #T_02857_row171_col6, #T_02857_row174_col3, #T_02857_row175_col3, #T_02857_row182_col5, #T_02857_row183_col5, #T_02857_row200_col5, #T_02857_row201_col5, #T_02857_row208_col9, #T_02857_row209_col9, #T_02857_row273_col11, #T_02857_row274_col11, #T_02857_row293_col10, #T_02857_row294_col10, #T_02857_row299_col11, #T_02857_row300_col11, #T_02857_row319_col9, #T_02857_row320_col9, #T_02857_row325_col5, #T_02857_row325_col8, #T_02857_row325_col10, #T_02857_row325_col11, #T_02857_row326_col5, #T_02857_row326_col8, #T_02857_row326_col10, #T_02857_row326_col11, #T_02857_row353_col9, #T_02857_row354_col9, #T_02857_row368_col3, #T_02857_row369_col3, #T_02857_row370_col11, #T_02857_row371_col11, #T_02857_row372_col10, #T_02857_row373_col10, #T_02857_row406_col3, #T_02857_row407_col3, #T_02857_row418_col7, #T_02857_row419_col7, #T_02857_row426_col11, #T_02857_row427_col11, #T_02857_row434_col4, #T_02857_row435_col4, #T_02857_row436_col6, #T_02857_row437_col6, #T_02857_row438_col5, #T_02857_row439_col5, #T_02857_row448_col11, #T_02857_row449_col11, #T_02857_row458_col11, #T_02857_row459_col11, #T_02857_row468_col11, #T_02857_row469_col11, #T_02857_row472_col10, #T_02857_row473_col10, #T_02857_row478_col10, #T_02857_row485_col8, #T_02857_row486_col8, #T_02857_row487_col9, #T_02857_row488_col9, #T_02857_row507_col8, #T_02857_row507_col11, #T_02857_row508_col8, #T_02857_row508_col11, #T_02857_row531_col4, #T_02857_row532_col4, #T_02857_row539_col11, #T_02857_row540_col11, #T_02857_row549_col3, #T_02857_row549_col4, #T_02857_row549_col5, #T_02857_row549_col6, #T_02857_row549_col8, #T_02857_row549_col10, #T_02857_row550_col3, #T_02857_row550_col4, #T_02857_row550_col5, #T_02857_row550_col6, #T_02857_row550_col8, #T_02857_row550_col10, #T_02857_row565_col11, #T_02857_row566_col11, #T_02857_row593_col11, #T_02857_row594_col11 {\n",
              "  background-color: #cdd0e5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row0_col10, #T_02857_row4_col7, #T_02857_row7_col11, #T_02857_row8_col11, #T_02857_row9_col6, #T_02857_row10_col6, #T_02857_row11_col7, #T_02857_row12_col7, #T_02857_row31_col10, #T_02857_row32_col10, #T_02857_row55_col8, #T_02857_row55_col11, #T_02857_row56_col8, #T_02857_row56_col11, #T_02857_row57_col5, #T_02857_row58_col5, #T_02857_row77_col6, #T_02857_row77_col10, #T_02857_row78_col6, #T_02857_row78_col10, #T_02857_row81_col6, #T_02857_row82_col6, #T_02857_row126_col11, #T_02857_row127_col11, #T_02857_row128_col4, #T_02857_row129_col4, #T_02857_row134_col11, #T_02857_row135_col11, #T_02857_row142_col5, #T_02857_row143_col5, #T_02857_row156_col5, #T_02857_row157_col5, #T_02857_row174_col4, #T_02857_row175_col4, #T_02857_row178_col10, #T_02857_row179_col10, #T_02857_row182_col4, #T_02857_row183_col4, #T_02857_row192_col11, #T_02857_row193_col11, #T_02857_row198_col7, #T_02857_row199_col7, #T_02857_row206_col11, #T_02857_row207_col11, #T_02857_row212_col3, #T_02857_row212_col7, #T_02857_row213_col3, #T_02857_row213_col7, #T_02857_row226_col5, #T_02857_row226_col7, #T_02857_row227_col5, #T_02857_row227_col7, #T_02857_row240_col3, #T_02857_row251_col11, #T_02857_row252_col11, #T_02857_row271_col11, #T_02857_row272_col11, #T_02857_row323_col10, #T_02857_row324_col10, #T_02857_row339_col5, #T_02857_row340_col5, #T_02857_row343_col3, #T_02857_row343_col4, #T_02857_row343_col6, #T_02857_row344_col3, #T_02857_row344_col4, #T_02857_row344_col6, #T_02857_row349_col10, #T_02857_row350_col10, #T_02857_row374_col10, #T_02857_row375_col10, #T_02857_row376_col11, #T_02857_row377_col11, #T_02857_row416_col5, #T_02857_row417_col5, #T_02857_row442_col5, #T_02857_row443_col5, #T_02857_row444_col9, #T_02857_row445_col9, #T_02857_row462_col11, #T_02857_row463_col11, #T_02857_row474_col8, #T_02857_row475_col8, #T_02857_row477_col11, #T_02857_row483_col11, #T_02857_row484_col11, #T_02857_row485_col3, #T_02857_row485_col5, #T_02857_row486_col3, #T_02857_row486_col5, #T_02857_row513_col4, #T_02857_row513_col7, #T_02857_row514_col4, #T_02857_row514_col7, #T_02857_row527_col11, #T_02857_row528_col11, #T_02857_row531_col10, #T_02857_row531_col11, #T_02857_row532_col10, #T_02857_row532_col11, #T_02857_row537_col3, #T_02857_row538_col3, #T_02857_row577_col10, #T_02857_row578_col10, #T_02857_row585_col7, #T_02857_row585_col8, #T_02857_row585_col9, #T_02857_row586_col7, #T_02857_row586_col8, #T_02857_row586_col9 {\n",
              "  background-color: #c9cee4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row0_col11, #T_02857_row1_col9, #T_02857_row57_col8, #T_02857_row58_col8, #T_02857_row63_col9, #T_02857_row64_col9, #T_02857_row65_col8, #T_02857_row66_col8, #T_02857_row121_col5, #T_02857_row160_col7, #T_02857_row161_col7, #T_02857_row178_col3, #T_02857_row179_col3, #T_02857_row192_col7, #T_02857_row193_col7, #T_02857_row206_col5, #T_02857_row206_col7, #T_02857_row207_col5, #T_02857_row207_col7, #T_02857_row295_col7, #T_02857_row296_col7, #T_02857_row366_col4, #T_02857_row367_col4, #T_02857_row394_col6, #T_02857_row395_col6, #T_02857_row410_col10, #T_02857_row411_col10, #T_02857_row521_col6, #T_02857_row522_col6, #T_02857_row525_col10, #T_02857_row526_col10, #T_02857_row537_col9, #T_02857_row538_col9, #T_02857_row541_col10, #T_02857_row542_col10, #T_02857_row545_col6, #T_02857_row546_col6 {\n",
              "  background-color: #bbc7e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row1_col3, #T_02857_row3_col11, #T_02857_row4_col3, #T_02857_row4_col4, #T_02857_row47_col7, #T_02857_row48_col7, #T_02857_row61_col6, #T_02857_row62_col6, #T_02857_row63_col8, #T_02857_row64_col8, #T_02857_row71_col8, #T_02857_row72_col8, #T_02857_row87_col9, #T_02857_row88_col9, #T_02857_row89_col7, #T_02857_row90_col7, #T_02857_row93_col7, #T_02857_row94_col7, #T_02857_row101_col5, #T_02857_row102_col5, #T_02857_row107_col4, #T_02857_row108_col4, #T_02857_row113_col4, #T_02857_row114_col4, #T_02857_row130_col6, #T_02857_row130_col9, #T_02857_row131_col6, #T_02857_row131_col9, #T_02857_row142_col6, #T_02857_row143_col6, #T_02857_row174_col6, #T_02857_row174_col7, #T_02857_row175_col6, #T_02857_row175_col7, #T_02857_row180_col7, #T_02857_row181_col7, #T_02857_row186_col5, #T_02857_row187_col5, #T_02857_row188_col4, #T_02857_row189_col4, #T_02857_row194_col11, #T_02857_row195_col11, #T_02857_row198_col6, #T_02857_row199_col6, #T_02857_row200_col9, #T_02857_row201_col9, #T_02857_row206_col9, #T_02857_row207_col9, #T_02857_row212_col6, #T_02857_row213_col6, #T_02857_row226_col3, #T_02857_row227_col3, #T_02857_row240_col7, #T_02857_row240_col8, #T_02857_row247_col8, #T_02857_row248_col8, #T_02857_row249_col9, #T_02857_row250_col9, #T_02857_row257_col10, #T_02857_row258_col10, #T_02857_row368_col8, #T_02857_row369_col8, #T_02857_row370_col9, #T_02857_row371_col9, #T_02857_row372_col8, #T_02857_row373_col8, #T_02857_row424_col5, #T_02857_row425_col5, #T_02857_row432_col6, #T_02857_row433_col6, #T_02857_row446_col9, #T_02857_row447_col9, #T_02857_row479_col9, #T_02857_row480_col11, #T_02857_row493_col11, #T_02857_row494_col11, #T_02857_row547_col5, #T_02857_row548_col5, #T_02857_row551_col3, #T_02857_row552_col3, #T_02857_row553_col5, #T_02857_row554_col5, #T_02857_row557_col10, #T_02857_row558_col10, #T_02857_row591_col11, #T_02857_row592_col11 {\n",
              "  background-color: #c4cbe3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row1_col4, #T_02857_row1_col7, #T_02857_row3_col9, #T_02857_row9_col4, #T_02857_row10_col4, #T_02857_row11_col5, #T_02857_row12_col5, #T_02857_row17_col10, #T_02857_row18_col10, #T_02857_row55_col6, #T_02857_row56_col6, #T_02857_row61_col11, #T_02857_row62_col11, #T_02857_row63_col3, #T_02857_row64_col3, #T_02857_row77_col4, #T_02857_row78_col4, #T_02857_row93_col3, #T_02857_row93_col8, #T_02857_row94_col3, #T_02857_row94_col8, #T_02857_row105_col11, #T_02857_row106_col11, #T_02857_row115_col8, #T_02857_row116_col8, #T_02857_row126_col8, #T_02857_row127_col8, #T_02857_row136_col9, #T_02857_row137_col9, #T_02857_row138_col8, #T_02857_row139_col8, #T_02857_row142_col7, #T_02857_row143_col7, #T_02857_row144_col9, #T_02857_row145_col9, #T_02857_row174_col5, #T_02857_row174_col11, #T_02857_row175_col5, #T_02857_row175_col11, #T_02857_row184_col6, #T_02857_row185_col6, #T_02857_row190_col11, #T_02857_row191_col11, #T_02857_row198_col4, #T_02857_row198_col5, #T_02857_row199_col4, #T_02857_row199_col5, #T_02857_row247_col10, #T_02857_row248_col10, #T_02857_row255_col4, #T_02857_row256_col4, #T_02857_row259_col11, #T_02857_row260_col11, #T_02857_row269_col3, #T_02857_row269_col4, #T_02857_row270_col3, #T_02857_row270_col4, #T_02857_row293_col7, #T_02857_row294_col7, #T_02857_row353_col6, #T_02857_row354_col6, #T_02857_row372_col9, #T_02857_row373_col9, #T_02857_row406_col6, #T_02857_row407_col6, #T_02857_row420_col3, #T_02857_row421_col3, #T_02857_row426_col4, #T_02857_row427_col4, #T_02857_row434_col3, #T_02857_row435_col3, #T_02857_row438_col7, #T_02857_row439_col7, #T_02857_row444_col3, #T_02857_row445_col3, #T_02857_row446_col6, #T_02857_row447_col6, #T_02857_row452_col3, #T_02857_row453_col3, #T_02857_row472_col8, #T_02857_row472_col11, #T_02857_row473_col8, #T_02857_row473_col11, #T_02857_row476_col11, #T_02857_row499_col6, #T_02857_row500_col6, #T_02857_row513_col5, #T_02857_row514_col5, #T_02857_row521_col10, #T_02857_row522_col10, #T_02857_row531_col8, #T_02857_row532_col8, #T_02857_row541_col7, #T_02857_row541_col11, #T_02857_row542_col7, #T_02857_row542_col11, #T_02857_row543_col6, #T_02857_row544_col6, #T_02857_row547_col11, #T_02857_row548_col11, #T_02857_row559_col8, #T_02857_row560_col8, #T_02857_row583_col9, #T_02857_row584_col9 {\n",
              "  background-color: #cccfe5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row1_col5, #T_02857_row3_col10, #T_02857_row11_col8, #T_02857_row12_col8, #T_02857_row13_col11, #T_02857_row14_col11, #T_02857_row25_col9, #T_02857_row26_col9, #T_02857_row37_col5, #T_02857_row38_col5, #T_02857_row51_col7, #T_02857_row52_col7, #T_02857_row53_col8, #T_02857_row53_col10, #T_02857_row54_col8, #T_02857_row54_col10, #T_02857_row67_col6, #T_02857_row68_col6, #T_02857_row75_col11, #T_02857_row76_col11, #T_02857_row81_col4, #T_02857_row82_col4, #T_02857_row109_col10, #T_02857_row110_col10, #T_02857_row148_col11, #T_02857_row149_col11, #T_02857_row172_col7, #T_02857_row173_col7, #T_02857_row178_col8, #T_02857_row179_col8, #T_02857_row200_col10, #T_02857_row201_col10, #T_02857_row210_col8, #T_02857_row211_col8, #T_02857_row224_col8, #T_02857_row225_col8, #T_02857_row249_col4, #T_02857_row250_col4, #T_02857_row251_col8, #T_02857_row252_col8, #T_02857_row255_col6, #T_02857_row256_col6, #T_02857_row261_col3, #T_02857_row262_col3, #T_02857_row269_col5, #T_02857_row269_col6, #T_02857_row270_col5, #T_02857_row270_col6, #T_02857_row277_col4, #T_02857_row278_col4, #T_02857_row283_col4, #T_02857_row284_col4, #T_02857_row287_col4, #T_02857_row287_col11, #T_02857_row288_col4, #T_02857_row288_col11, #T_02857_row303_col11, #T_02857_row304_col11, #T_02857_row319_col10, #T_02857_row320_col10, #T_02857_row325_col6, #T_02857_row325_col7, #T_02857_row326_col6, #T_02857_row326_col7, #T_02857_row331_col9, #T_02857_row332_col9, #T_02857_row343_col9, #T_02857_row344_col9, #T_02857_row347_col4, #T_02857_row348_col4, #T_02857_row353_col5, #T_02857_row354_col5, #T_02857_row390_col11, #T_02857_row391_col11, #T_02857_row392_col11, #T_02857_row393_col11, #T_02857_row404_col11, #T_02857_row405_col11, #T_02857_row420_col4, #T_02857_row420_col7, #T_02857_row421_col4, #T_02857_row421_col7, #T_02857_row426_col3, #T_02857_row427_col3, #T_02857_row452_col10, #T_02857_row453_col10, #T_02857_row483_col9, #T_02857_row484_col9, #T_02857_row489_col11, #T_02857_row490_col11, #T_02857_row491_col11, #T_02857_row492_col11, #T_02857_row499_col5, #T_02857_row500_col5, #T_02857_row523_col11, #T_02857_row524_col11, #T_02857_row529_col9, #T_02857_row529_col11, #T_02857_row530_col9, #T_02857_row530_col11, #T_02857_row531_col7, #T_02857_row532_col7, #T_02857_row555_col3, #T_02857_row555_col11, #T_02857_row556_col3, #T_02857_row556_col11, #T_02857_row577_col8, #T_02857_row578_col8, #T_02857_row591_col3, #T_02857_row591_col4, #T_02857_row591_col5, #T_02857_row591_col6, #T_02857_row591_col7, #T_02857_row592_col3, #T_02857_row592_col4, #T_02857_row592_col5, #T_02857_row592_col6, #T_02857_row592_col7 {\n",
              "  background-color: #d1d2e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row1_col6, #T_02857_row2_col8, #T_02857_row4_col5, #T_02857_row4_col6, #T_02857_row4_col9, #T_02857_row6_col11, #T_02857_row9_col7, #T_02857_row10_col7, #T_02857_row11_col9, #T_02857_row12_col9, #T_02857_row17_col3, #T_02857_row17_col4, #T_02857_row17_col5, #T_02857_row17_col7, #T_02857_row18_col3, #T_02857_row18_col4, #T_02857_row18_col5, #T_02857_row18_col7, #T_02857_row57_col3, #T_02857_row58_col3, #T_02857_row67_col8, #T_02857_row68_col8, #T_02857_row77_col8, #T_02857_row77_col9, #T_02857_row78_col8, #T_02857_row78_col9, #T_02857_row79_col10, #T_02857_row80_col10, #T_02857_row101_col3, #T_02857_row101_col4, #T_02857_row101_col11, #T_02857_row102_col3, #T_02857_row102_col4, #T_02857_row102_col11, #T_02857_row107_col3, #T_02857_row107_col5, #T_02857_row108_col3, #T_02857_row108_col5, #T_02857_row121_col3, #T_02857_row121_col10, #T_02857_row121_col11, #T_02857_row170_col10, #T_02857_row171_col10, #T_02857_row186_col6, #T_02857_row187_col6, #T_02857_row188_col5, #T_02857_row188_col7, #T_02857_row188_col9, #T_02857_row189_col5, #T_02857_row189_col7, #T_02857_row189_col9, #T_02857_row204_col6, #T_02857_row204_col9, #T_02857_row205_col6, #T_02857_row205_col9, #T_02857_row238_col10, #T_02857_row243_col11, #T_02857_row245_col11, #T_02857_row246_col11, #T_02857_row251_col9, #T_02857_row252_col9, #T_02857_row343_col11, #T_02857_row344_col11, #T_02857_row368_col9, #T_02857_row369_col9, #T_02857_row416_col4, #T_02857_row417_col4, #T_02857_row422_col6, #T_02857_row423_col6, #T_02857_row444_col4, #T_02857_row445_col4, #T_02857_row450_col3, #T_02857_row451_col3, #T_02857_row472_col9, #T_02857_row473_col9, #T_02857_row479_col3, #T_02857_row479_col4, #T_02857_row479_col6, #T_02857_row485_col7, #T_02857_row486_col7, #T_02857_row537_col6, #T_02857_row538_col6, #T_02857_row545_col10, #T_02857_row546_col10, #T_02857_row553_col11, #T_02857_row554_col11, #T_02857_row559_col7, #T_02857_row559_col11, #T_02857_row560_col7, #T_02857_row560_col11, #T_02857_row565_col4, #T_02857_row565_col7, #T_02857_row566_col4, #T_02857_row566_col7, #T_02857_row581_col11, #T_02857_row582_col11 {\n",
              "  background-color: #c5cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row1_col8, #T_02857_row17_col11, #T_02857_row18_col11, #T_02857_row31_col6, #T_02857_row31_col7, #T_02857_row32_col6, #T_02857_row32_col7, #T_02857_row49_col10, #T_02857_row50_col10, #T_02857_row65_col10, #T_02857_row66_col10, #T_02857_row71_col10, #T_02857_row72_col10, #T_02857_row83_col7, #T_02857_row84_col7, #T_02857_row85_col5, #T_02857_row86_col5, #T_02857_row93_col6, #T_02857_row94_col6, #T_02857_row115_col6, #T_02857_row116_col6, #T_02857_row130_col4, #T_02857_row131_col4, #T_02857_row134_col8, #T_02857_row135_col8, #T_02857_row142_col4, #T_02857_row143_col4, #T_02857_row184_col10, #T_02857_row185_col10, #T_02857_row188_col6, #T_02857_row188_col10, #T_02857_row189_col6, #T_02857_row189_col10, #T_02857_row202_col7, #T_02857_row203_col7, #T_02857_row226_col4, #T_02857_row227_col4, #T_02857_row228_col9, #T_02857_row229_col9, #T_02857_row230_col9, #T_02857_row231_col9, #T_02857_row238_col4, #T_02857_row238_col6, #T_02857_row240_col11, #T_02857_row299_col6, #T_02857_row300_col6, #T_02857_row319_col8, #T_02857_row320_col8, #T_02857_row355_col11, #T_02857_row356_col11, #T_02857_row366_col5, #T_02857_row367_col5, #T_02857_row368_col6, #T_02857_row369_col6, #T_02857_row394_col5, #T_02857_row395_col5, #T_02857_row408_col10, #T_02857_row409_col10, #T_02857_row418_col6, #T_02857_row419_col6, #T_02857_row517_col7, #T_02857_row518_col7, #T_02857_row539_col8, #T_02857_row540_col8, #T_02857_row543_col8, #T_02857_row544_col8, #T_02857_row551_col6, #T_02857_row552_col6, #T_02857_row555_col10, #T_02857_row556_col10 {\n",
              "  background-color: #c0c9e2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row1_col10, #T_02857_row1_col11, #T_02857_row2_col10, #T_02857_row17_col6, #T_02857_row18_col6, #T_02857_row31_col11, #T_02857_row32_col11, #T_02857_row55_col9, #T_02857_row56_col9, #T_02857_row63_col10, #T_02857_row64_col10, #T_02857_row89_col5, #T_02857_row90_col5, #T_02857_row101_col7, #T_02857_row102_col7, #T_02857_row109_col5, #T_02857_row110_col5, #T_02857_row115_col10, #T_02857_row116_col10, #T_02857_row128_col6, #T_02857_row128_col7, #T_02857_row129_col6, #T_02857_row129_col7, #T_02857_row132_col8, #T_02857_row132_col11, #T_02857_row133_col8, #T_02857_row133_col11, #T_02857_row142_col3, #T_02857_row143_col3, #T_02857_row144_col5, #T_02857_row145_col5, #T_02857_row158_col3, #T_02857_row159_col3, #T_02857_row226_col9, #T_02857_row227_col9, #T_02857_row230_col11, #T_02857_row231_col11, #T_02857_row240_col4, #T_02857_row240_col6, #T_02857_row287_col3, #T_02857_row288_col3, #T_02857_row317_col7, #T_02857_row318_col7, #T_02857_row327_col10, #T_02857_row328_col10, #T_02857_row394_col7, #T_02857_row395_col7, #T_02857_row412_col3, #T_02857_row413_col3, #T_02857_row416_col7, #T_02857_row417_col7, #T_02857_row428_col11, #T_02857_row429_col11, #T_02857_row479_col11, #T_02857_row485_col6, #T_02857_row486_col6, #T_02857_row513_col6, #T_02857_row514_col6, #T_02857_row537_col11, #T_02857_row538_col11 {\n",
              "  background-color: #c6cce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row2_col3, #T_02857_row2_col4, #T_02857_row2_col5, #T_02857_row2_col11, #T_02857_row7_col8, #T_02857_row8_col8, #T_02857_row19_col9, #T_02857_row20_col9, #T_02857_row23_col8, #T_02857_row24_col8, #T_02857_row51_col6, #T_02857_row52_col6, #T_02857_row55_col5, #T_02857_row56_col5, #T_02857_row69_col7, #T_02857_row70_col7, #T_02857_row95_col9, #T_02857_row96_col9, #T_02857_row117_col9, #T_02857_row117_col10, #T_02857_row118_col9, #T_02857_row118_col10, #T_02857_row120_col11, #T_02857_row130_col10, #T_02857_row131_col10, #T_02857_row170_col5, #T_02857_row171_col5, #T_02857_row198_col3, #T_02857_row199_col3, #T_02857_row214_col9, #T_02857_row215_col9, #T_02857_row226_col8, #T_02857_row227_col8, #T_02857_row236_col11, #T_02857_row237_col11, #T_02857_row245_col9, #T_02857_row245_col10, #T_02857_row246_col9, #T_02857_row246_col10, #T_02857_row247_col3, #T_02857_row247_col6, #T_02857_row247_col7, #T_02857_row248_col3, #T_02857_row248_col6, #T_02857_row248_col7, #T_02857_row255_col9, #T_02857_row256_col9, #T_02857_row261_col4, #T_02857_row262_col4, #T_02857_row263_col4, #T_02857_row264_col4, #T_02857_row277_col7, #T_02857_row278_col7, #T_02857_row289_col11, #T_02857_row290_col11, #T_02857_row291_col8, #T_02857_row292_col8, #T_02857_row293_col3, #T_02857_row293_col5, #T_02857_row294_col3, #T_02857_row294_col5, #T_02857_row305_col5, #T_02857_row306_col5, #T_02857_row307_col6, #T_02857_row308_col6, #T_02857_row311_col10, #T_02857_row311_col11, #T_02857_row312_col10, #T_02857_row312_col11, #T_02857_row333_col9, #T_02857_row334_col9, #T_02857_row339_col3, #T_02857_row339_col4, #T_02857_row339_col10, #T_02857_row340_col3, #T_02857_row340_col4, #T_02857_row340_col10, #T_02857_row345_col9, #T_02857_row346_col9, #T_02857_row353_col10, #T_02857_row354_col10, #T_02857_row378_col11, #T_02857_row379_col11, #T_02857_row382_col8, #T_02857_row383_col8, #T_02857_row400_col11, #T_02857_row401_col11, #T_02857_row410_col6, #T_02857_row411_col6, #T_02857_row428_col3, #T_02857_row429_col3, #T_02857_row430_col11, #T_02857_row431_col11, #T_02857_row434_col11, #T_02857_row435_col11, #T_02857_row440_col5, #T_02857_row441_col5, #T_02857_row442_col11, #T_02857_row443_col11, #T_02857_row456_col11, #T_02857_row457_col11, #T_02857_row462_col9, #T_02857_row463_col9, #T_02857_row470_col8, #T_02857_row470_col9, #T_02857_row471_col8, #T_02857_row471_col9, #T_02857_row482_col10, #T_02857_row485_col10, #T_02857_row486_col10, #T_02857_row509_col11, #T_02857_row510_col11, #T_02857_row525_col6, #T_02857_row526_col6, #T_02857_row543_col7, #T_02857_row544_col7, #T_02857_row567_col11, #T_02857_row568_col11, #T_02857_row587_col9, #T_02857_row588_col9 {\n",
              "  background-color: #d2d2e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row2_col6, #T_02857_row9_col5, #T_02857_row10_col5, #T_02857_row37_col3, #T_02857_row38_col3, #T_02857_row43_col7, #T_02857_row44_col7, #T_02857_row51_col11, #T_02857_row52_col11, #T_02857_row53_col9, #T_02857_row54_col9, #T_02857_row55_col10, #T_02857_row56_col10, #T_02857_row57_col4, #T_02857_row58_col4, #T_02857_row59_col11, #T_02857_row60_col11, #T_02857_row67_col10, #T_02857_row68_col10, #T_02857_row69_col10, #T_02857_row70_col10, #T_02857_row77_col3, #T_02857_row78_col3, #T_02857_row85_col4, #T_02857_row85_col11, #T_02857_row86_col4, #T_02857_row86_col11, #T_02857_row109_col8, #T_02857_row109_col9, #T_02857_row110_col8, #T_02857_row110_col9, #T_02857_row113_col6, #T_02857_row114_col6, #T_02857_row128_col9, #T_02857_row129_col9, #T_02857_row132_col10, #T_02857_row133_col10, #T_02857_row144_col6, #T_02857_row145_col6, #T_02857_row172_col10, #T_02857_row173_col10, #T_02857_row182_col3, #T_02857_row182_col7, #T_02857_row182_col11, #T_02857_row183_col3, #T_02857_row183_col7, #T_02857_row183_col11, #T_02857_row186_col7, #T_02857_row187_col7, #T_02857_row188_col3, #T_02857_row189_col3, #T_02857_row224_col11, #T_02857_row225_col11, #T_02857_row232_col11, #T_02857_row233_col11, #T_02857_row234_col9, #T_02857_row235_col9, #T_02857_row240_col5, #T_02857_row255_col7, #T_02857_row256_col7, #T_02857_row283_col3, #T_02857_row284_col3, #T_02857_row293_col11, #T_02857_row294_col11, #T_02857_row297_col9, #T_02857_row298_col9, #T_02857_row343_col5, #T_02857_row343_col7, #T_02857_row344_col5, #T_02857_row344_col7, #T_02857_row355_col9, #T_02857_row356_col9, #T_02857_row374_col8, #T_02857_row375_col8, #T_02857_row414_col3, #T_02857_row415_col3, #T_02857_row422_col3, #T_02857_row423_col3, #T_02857_row424_col7, #T_02857_row425_col7, #T_02857_row470_col11, #T_02857_row471_col11, #T_02857_row474_col9, #T_02857_row475_col9, #T_02857_row478_col8, #T_02857_row479_col7, #T_02857_row495_col11, #T_02857_row496_col11, #T_02857_row537_col7, #T_02857_row538_col7, #T_02857_row539_col7, #T_02857_row540_col7, #T_02857_row541_col4, #T_02857_row542_col4, #T_02857_row581_col9, #T_02857_row582_col9 {\n",
              "  background-color: #cacee5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row2_col7, #T_02857_row31_col8, #T_02857_row32_col8, #T_02857_row37_col7, #T_02857_row38_col7, #T_02857_row61_col4, #T_02857_row62_col4, #T_02857_row65_col4, #T_02857_row65_col5, #T_02857_row65_col11, #T_02857_row66_col4, #T_02857_row66_col5, #T_02857_row66_col11, #T_02857_row73_col6, #T_02857_row73_col7, #T_02857_row73_col8, #T_02857_row73_col10, #T_02857_row74_col6, #T_02857_row74_col7, #T_02857_row74_col8, #T_02857_row74_col10, #T_02857_row75_col7, #T_02857_row76_col7, #T_02857_row79_col5, #T_02857_row80_col5, #T_02857_row111_col10, #T_02857_row112_col10, #T_02857_row136_col10, #T_02857_row137_col10, #T_02857_row140_col11, #T_02857_row141_col11, #T_02857_row162_col11, #T_02857_row163_col11, #T_02857_row172_col3, #T_02857_row173_col3, #T_02857_row176_col5, #T_02857_row177_col5, #T_02857_row184_col7, #T_02857_row185_col7, #T_02857_row200_col7, #T_02857_row201_col7, #T_02857_row204_col5, #T_02857_row205_col5, #T_02857_row208_col7, #T_02857_row209_col7, #T_02857_row212_col10, #T_02857_row213_col10, #T_02857_row214_col3, #T_02857_row215_col3, #T_02857_row216_col11, #T_02857_row217_col11, #T_02857_row226_col10, #T_02857_row227_col10, #T_02857_row239_col11, #T_02857_row247_col4, #T_02857_row247_col5, #T_02857_row248_col4, #T_02857_row248_col5, #T_02857_row255_col10, #T_02857_row256_col10, #T_02857_row283_col11, #T_02857_row284_col11, #T_02857_row289_col6, #T_02857_row289_col7, #T_02857_row289_col10, #T_02857_row290_col6, #T_02857_row290_col7, #T_02857_row290_col10, #T_02857_row299_col3, #T_02857_row300_col3, #T_02857_row303_col10, #T_02857_row304_col10, #T_02857_row305_col7, #T_02857_row306_col7, #T_02857_row345_col7, #T_02857_row345_col8, #T_02857_row346_col7, #T_02857_row346_col8, #T_02857_row408_col7, #T_02857_row409_col7, #T_02857_row422_col4, #T_02857_row422_col5, #T_02857_row423_col4, #T_02857_row423_col5, #T_02857_row440_col4, #T_02857_row441_col4, #T_02857_row466_col9, #T_02857_row467_col9, #T_02857_row478_col6, #T_02857_row505_col11, #T_02857_row506_col11, #T_02857_row511_col11, #T_02857_row512_col11, #T_02857_row529_col8, #T_02857_row530_col8, #T_02857_row533_col7, #T_02857_row534_col7, #T_02857_row543_col3, #T_02857_row543_col4, #T_02857_row544_col3, #T_02857_row544_col4, #T_02857_row547_col8, #T_02857_row548_col8, #T_02857_row555_col5, #T_02857_row556_col5, #T_02857_row571_col9, #T_02857_row572_col9, #T_02857_row575_col11, #T_02857_row576_col11, #T_02857_row589_col11, #T_02857_row590_col11 {\n",
              "  background-color: #d3d4e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row2_col9, #T_02857_row41_col7, #T_02857_row42_col7, #T_02857_row49_col8, #T_02857_row50_col8, #T_02857_row57_col9, #T_02857_row58_col9, #T_02857_row83_col6, #T_02857_row84_col6, #T_02857_row121_col7, #T_02857_row121_col8, #T_02857_row124_col11, #T_02857_row164_col6, #T_02857_row165_col6, #T_02857_row178_col4, #T_02857_row179_col4, #T_02857_row182_col6, #T_02857_row183_col6, #T_02857_row192_col3, #T_02857_row192_col6, #T_02857_row193_col3, #T_02857_row193_col6, #T_02857_row194_col7, #T_02857_row195_col7, #T_02857_row247_col9, #T_02857_row248_col9, #T_02857_row295_col6, #T_02857_row296_col6, #T_02857_row341_col8, #T_02857_row342_col8, #T_02857_row359_col3, #T_02857_row408_col8, #T_02857_row409_col8, #T_02857_row438_col9, #T_02857_row439_col9, #T_02857_row444_col5, #T_02857_row445_col5, #T_02857_row450_col4, #T_02857_row451_col4, #T_02857_row452_col8, #T_02857_row453_col8, #T_02857_row464_col4, #T_02857_row465_col4, #T_02857_row466_col8, #T_02857_row467_col8, #T_02857_row503_col7, #T_02857_row504_col7, #T_02857_row521_col5, #T_02857_row522_col5, #T_02857_row545_col4, #T_02857_row545_col9, #T_02857_row546_col4, #T_02857_row546_col9, #T_02857_row559_col6, #T_02857_row559_col9, #T_02857_row560_col6, #T_02857_row560_col9, #T_02857_row583_col6, #T_02857_row584_col6, #T_02857_row589_col7, #T_02857_row590_col7 {\n",
              "  background-color: #b9c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row3_col5, #T_02857_row3_col7, #T_02857_row4_col8, #T_02857_row19_col8, #T_02857_row20_col8, #T_02857_row21_col11, #T_02857_row22_col11, #T_02857_row37_col6, #T_02857_row38_col6, #T_02857_row45_col10, #T_02857_row46_col10, #T_02857_row51_col5, #T_02857_row51_col8, #T_02857_row51_col9, #T_02857_row51_col10, #T_02857_row52_col5, #T_02857_row52_col8, #T_02857_row52_col9, #T_02857_row52_col10, #T_02857_row55_col3, #T_02857_row56_col3, #T_02857_row61_col7, #T_02857_row62_col7, #T_02857_row77_col5, #T_02857_row78_col5, #T_02857_row79_col11, #T_02857_row80_col11, #T_02857_row85_col3, #T_02857_row86_col3, #T_02857_row109_col7, #T_02857_row110_col7, #T_02857_row113_col7, #T_02857_row114_col7, #T_02857_row117_col8, #T_02857_row118_col8, #T_02857_row128_col3, #T_02857_row129_col3, #T_02857_row144_col8, #T_02857_row145_col8, #T_02857_row154_col11, #T_02857_row155_col11, #T_02857_row168_col6, #T_02857_row169_col6, #T_02857_row200_col6, #T_02857_row201_col6, #T_02857_row210_col11, #T_02857_row211_col11, #T_02857_row230_col10, #T_02857_row231_col10, #T_02857_row236_col9, #T_02857_row237_col9, #T_02857_row255_col3, #T_02857_row255_col5, #T_02857_row256_col3, #T_02857_row256_col5, #T_02857_row257_col9, #T_02857_row258_col9, #T_02857_row267_col11, #T_02857_row268_col11, #T_02857_row269_col7, #T_02857_row270_col7, #T_02857_row293_col6, #T_02857_row294_col6, #T_02857_row307_col7, #T_02857_row308_col7, #T_02857_row309_col7, #T_02857_row310_col7, #T_02857_row317_col5, #T_02857_row317_col9, #T_02857_row318_col5, #T_02857_row318_col9, #T_02857_row333_col5, #T_02857_row334_col5, #T_02857_row345_col11, #T_02857_row346_col11, #T_02857_row347_col8, #T_02857_row348_col8, #T_02857_row353_col3, #T_02857_row353_col4, #T_02857_row353_col7, #T_02857_row354_col3, #T_02857_row354_col4, #T_02857_row354_col7, #T_02857_row374_col9, #T_02857_row374_col11, #T_02857_row375_col9, #T_02857_row375_col11, #T_02857_row380_col7, #T_02857_row381_col7, #T_02857_row412_col7, #T_02857_row413_col7, #T_02857_row422_col7, #T_02857_row423_col7, #T_02857_row430_col3, #T_02857_row431_col3, #T_02857_row482_col3, #T_02857_row482_col4, #T_02857_row482_col5, #T_02857_row482_col6, #T_02857_row489_col9, #T_02857_row490_col9, #T_02857_row493_col4, #T_02857_row493_col5, #T_02857_row494_col4, #T_02857_row494_col5, #T_02857_row497_col11, #T_02857_row498_col11, #T_02857_row499_col7, #T_02857_row500_col7, #T_02857_row533_col3, #T_02857_row534_col3, #T_02857_row537_col4, #T_02857_row537_col5, #T_02857_row538_col4, #T_02857_row538_col5, #T_02857_row547_col7, #T_02857_row548_col7, #T_02857_row549_col7, #T_02857_row549_col9, #T_02857_row550_col7, #T_02857_row550_col9, #T_02857_row551_col4, #T_02857_row552_col4, #T_02857_row555_col6, #T_02857_row556_col6, #T_02857_row557_col3, #T_02857_row557_col4, #T_02857_row557_col5, #T_02857_row558_col3, #T_02857_row558_col4, #T_02857_row558_col5, #T_02857_row583_col10, #T_02857_row584_col10, #T_02857_row585_col10, #T_02857_row586_col10, #T_02857_row587_col11, #T_02857_row588_col11 {\n",
              "  background-color: #ced0e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row3_col8, #T_02857_row9_col10, #T_02857_row10_col10, #T_02857_row11_col4, #T_02857_row12_col4, #T_02857_row17_col9, #T_02857_row18_col9, #T_02857_row25_col6, #T_02857_row26_col6, #T_02857_row53_col5, #T_02857_row54_col5, #T_02857_row63_col7, #T_02857_row64_col7, #T_02857_row67_col3, #T_02857_row67_col4, #T_02857_row68_col3, #T_02857_row68_col4, #T_02857_row71_col5, #T_02857_row72_col5, #T_02857_row113_col11, #T_02857_row114_col11, #T_02857_row142_col9, #T_02857_row143_col9, #T_02857_row156_col9, #T_02857_row157_col9, #T_02857_row172_col4, #T_02857_row173_col4, #T_02857_row184_col5, #T_02857_row185_col5, #T_02857_row186_col3, #T_02857_row187_col3, #T_02857_row212_col8, #T_02857_row213_col8, #T_02857_row263_col3, #T_02857_row263_col7, #T_02857_row264_col3, #T_02857_row264_col7, #T_02857_row265_col11, #T_02857_row266_col11, #T_02857_row275_col7, #T_02857_row276_col7, #T_02857_row277_col5, #T_02857_row277_col6, #T_02857_row278_col5, #T_02857_row278_col6, #T_02857_row281_col11, #T_02857_row282_col11, #T_02857_row283_col10, #T_02857_row284_col10, #T_02857_row289_col3, #T_02857_row289_col4, #T_02857_row290_col3, #T_02857_row290_col4, #T_02857_row297_col7, #T_02857_row298_col7, #T_02857_row313_col6, #T_02857_row314_col6, #T_02857_row317_col6, #T_02857_row318_col6, #T_02857_row331_col8, #T_02857_row332_col8, #T_02857_row343_col10, #T_02857_row344_col10, #T_02857_row345_col3, #T_02857_row345_col4, #T_02857_row346_col3, #T_02857_row346_col4, #T_02857_row406_col7, #T_02857_row407_col7, #T_02857_row414_col4, #T_02857_row415_col4, #T_02857_row432_col11, #T_02857_row433_col11, #T_02857_row454_col11, #T_02857_row455_col11, #T_02857_row460_col11, #T_02857_row461_col11, #T_02857_row464_col8, #T_02857_row465_col8, #T_02857_row480_col3, #T_02857_row480_col4, #T_02857_row480_col5, #T_02857_row480_col6, #T_02857_row480_col7, #T_02857_row487_col6, #T_02857_row488_col6, #T_02857_row495_col9, #T_02857_row496_col9, #T_02857_row519_col11, #T_02857_row520_col11, #T_02857_row545_col8, #T_02857_row546_col8, #T_02857_row581_col8, #T_02857_row582_col8, #T_02857_row591_col8, #T_02857_row592_col8 {\n",
              "  background-color: #d5d5e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row5_col3, #T_02857_row5_col4, #T_02857_row5_col5, #T_02857_row5_col6, #T_02857_row5_col7, #T_02857_row5_col8, #T_02857_row5_col9, #T_02857_row5_col10, #T_02857_row25_col5, #T_02857_row25_col7, #T_02857_row25_col8, #T_02857_row26_col5, #T_02857_row26_col7, #T_02857_row26_col8, #T_02857_row31_col9, #T_02857_row32_col9, #T_02857_row33_col11, #T_02857_row34_col11, #T_02857_row39_col7, #T_02857_row40_col7, #T_02857_row41_col9, #T_02857_row41_col10, #T_02857_row42_col9, #T_02857_row42_col10, #T_02857_row53_col4, #T_02857_row53_col7, #T_02857_row54_col4, #T_02857_row54_col7, #T_02857_row59_col3, #T_02857_row59_col4, #T_02857_row59_col5, #T_02857_row60_col3, #T_02857_row60_col4, #T_02857_row60_col5, #T_02857_row63_col5, #T_02857_row64_col5, #T_02857_row71_col7, #T_02857_row72_col7, #T_02857_row95_col4, #T_02857_row95_col5, #T_02857_row95_col7, #T_02857_row96_col4, #T_02857_row96_col5, #T_02857_row96_col7, #T_02857_row103_col9, #T_02857_row104_col9, #T_02857_row105_col8, #T_02857_row105_col10, #T_02857_row106_col8, #T_02857_row106_col10, #T_02857_row111_col9, #T_02857_row112_col9, #T_02857_row120_col6, #T_02857_row120_col7, #T_02857_row120_col8, #T_02857_row138_col9, #T_02857_row139_col9, #T_02857_row142_col8, #T_02857_row143_col8, #T_02857_row150_col10, #T_02857_row151_col10, #T_02857_row154_col9, #T_02857_row154_col10, #T_02857_row155_col9, #T_02857_row155_col10, #T_02857_row158_col7, #T_02857_row159_col7, #T_02857_row160_col9, #T_02857_row161_col9, #T_02857_row164_col9, #T_02857_row165_col9, #T_02857_row172_col11, #T_02857_row173_col11, #T_02857_row176_col4, #T_02857_row177_col4, #T_02857_row180_col4, #T_02857_row180_col5, #T_02857_row181_col4, #T_02857_row181_col5, #T_02857_row220_col10, #T_02857_row221_col10, #T_02857_row249_col5, #T_02857_row249_col6, #T_02857_row250_col5, #T_02857_row250_col6, #T_02857_row255_col8, #T_02857_row256_col8, #T_02857_row259_col8, #T_02857_row260_col8, #T_02857_row263_col5, #T_02857_row264_col5, #T_02857_row265_col10, #T_02857_row266_col10, #T_02857_row271_col10, #T_02857_row272_col10, #T_02857_row275_col5, #T_02857_row275_col10, #T_02857_row276_col5, #T_02857_row276_col10, #T_02857_row291_col4, #T_02857_row292_col4, #T_02857_row295_col4, #T_02857_row296_col4, #T_02857_row303_col3, #T_02857_row303_col4, #T_02857_row304_col3, #T_02857_row304_col4, #T_02857_row305_col4, #T_02857_row306_col4, #T_02857_row309_col5, #T_02857_row310_col5, #T_02857_row311_col4, #T_02857_row311_col6, #T_02857_row311_col8, #T_02857_row312_col4, #T_02857_row312_col6, #T_02857_row312_col8, #T_02857_row319_col6, #T_02857_row320_col6, #T_02857_row327_col7, #T_02857_row328_col7, #T_02857_row331_col7, #T_02857_row332_col7, #T_02857_row333_col6, #T_02857_row334_col6, #T_02857_row335_col10, #T_02857_row336_col10, #T_02857_row339_col9, #T_02857_row340_col9, #T_02857_row358_col3, #T_02857_row358_col4, #T_02857_row358_col5, #T_02857_row358_col6, #T_02857_row358_col7, #T_02857_row358_col8, #T_02857_row358_col10, #T_02857_row364_col10, #T_02857_row365_col10, #T_02857_row384_col11, #T_02857_row385_col11, #T_02857_row396_col10, #T_02857_row397_col10, #T_02857_row402_col11, #T_02857_row403_col11, #T_02857_row410_col4, #T_02857_row411_col4, #T_02857_row412_col5, #T_02857_row413_col5, #T_02857_row440_col3, #T_02857_row440_col11, #T_02857_row441_col3, #T_02857_row441_col11, #T_02857_row460_col8, #T_02857_row461_col8, #T_02857_row470_col10, #T_02857_row471_col10, #T_02857_row477_col3, #T_02857_row477_col5, #T_02857_row477_col6, #T_02857_row477_col7, #T_02857_row478_col4, #T_02857_row478_col7, #T_02857_row479_col8, #T_02857_row483_col8, #T_02857_row484_col8, #T_02857_row487_col5, #T_02857_row487_col8, #T_02857_row488_col5, #T_02857_row488_col8, #T_02857_row491_col9, #T_02857_row492_col9, #T_02857_row499_col10, #T_02857_row500_col10, #T_02857_row501_col4, #T_02857_row502_col4, #T_02857_row503_col11, #T_02857_row504_col11, #T_02857_row513_col10, #T_02857_row514_col10, #T_02857_row515_col4, #T_02857_row515_col7, #T_02857_row516_col4, #T_02857_row516_col7, #T_02857_row525_col7, #T_02857_row526_col7, #T_02857_row527_col3, #T_02857_row527_col4, #T_02857_row527_col5, #T_02857_row527_col6, #T_02857_row527_col7, #T_02857_row527_col8, #T_02857_row527_col9, #T_02857_row527_col10, #T_02857_row528_col3, #T_02857_row528_col4, #T_02857_row528_col5, #T_02857_row528_col6, #T_02857_row528_col7, #T_02857_row528_col8, #T_02857_row528_col9, #T_02857_row528_col10, #T_02857_row529_col3, #T_02857_row529_col4, #T_02857_row530_col3, #T_02857_row530_col4, #T_02857_row533_col4, #T_02857_row534_col4, #T_02857_row535_col3, #T_02857_row535_col4, #T_02857_row535_col5, #T_02857_row535_col6, #T_02857_row535_col7, #T_02857_row535_col8, #T_02857_row535_col9, #T_02857_row535_col10, #T_02857_row536_col3, #T_02857_row536_col4, #T_02857_row536_col5, #T_02857_row536_col6, #T_02857_row536_col7, #T_02857_row536_col8, #T_02857_row536_col9, #T_02857_row536_col10, #T_02857_row557_col11, #T_02857_row558_col11, #T_02857_row569_col3, #T_02857_row569_col7, #T_02857_row570_col3, #T_02857_row570_col7, #T_02857_row571_col4, #T_02857_row571_col5, #T_02857_row571_col7, #T_02857_row572_col4, #T_02857_row572_col5, #T_02857_row572_col7, #T_02857_row573_col11, #T_02857_row574_col11, #T_02857_row593_col10, #T_02857_row594_col10 {\n",
              "  background-color: #d9d8ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row5_col11, #T_02857_row19_col11, #T_02857_row20_col11, #T_02857_row57_col7, #T_02857_row58_col7, #T_02857_row65_col6, #T_02857_row66_col6, #T_02857_row75_col3, #T_02857_row76_col3, #T_02857_row81_col10, #T_02857_row82_col10, #T_02857_row101_col6, #T_02857_row102_col6, #T_02857_row113_col5, #T_02857_row114_col5, #T_02857_row128_col5, #T_02857_row129_col5, #T_02857_row156_col4, #T_02857_row156_col6, #T_02857_row157_col4, #T_02857_row157_col6, #T_02857_row174_col10, #T_02857_row175_col10, #T_02857_row188_col11, #T_02857_row189_col11, #T_02857_row240_col10, #T_02857_row287_col6, #T_02857_row288_col6, #T_02857_row295_col5, #T_02857_row295_col11, #T_02857_row296_col5, #T_02857_row296_col11, #T_02857_row305_col10, #T_02857_row306_col10, #T_02857_row317_col10, #T_02857_row318_col10, #T_02857_row347_col6, #T_02857_row348_col6, #T_02857_row364_col11, #T_02857_row365_col11, #T_02857_row372_col11, #T_02857_row373_col11, #T_02857_row380_col3, #T_02857_row380_col5, #T_02857_row381_col3, #T_02857_row381_col5, #T_02857_row430_col4, #T_02857_row431_col4, #T_02857_row450_col7, #T_02857_row451_col7, #T_02857_row458_col8, #T_02857_row459_col8, #T_02857_row468_col9, #T_02857_row469_col9, #T_02857_row479_col5, #T_02857_row485_col4, #T_02857_row486_col4, #T_02857_row531_col3, #T_02857_row531_col6, #T_02857_row532_col3, #T_02857_row532_col6, #T_02857_row535_col11, #T_02857_row536_col11, #T_02857_row539_col4, #T_02857_row540_col4, #T_02857_row551_col8, #T_02857_row552_col8, #T_02857_row557_col6, #T_02857_row558_col6, #T_02857_row565_col5, #T_02857_row566_col5, #T_02857_row585_col5, #T_02857_row585_col6, #T_02857_row586_col5, #T_02857_row586_col6 {\n",
              "  background-color: #c8cde4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row6_col3, #T_02857_row6_col4, #T_02857_row6_col5, #T_02857_row6_col6, #T_02857_row6_col7, #T_02857_row6_col9, #T_02857_row23_col10, #T_02857_row24_col10, #T_02857_row27_col11, #T_02857_row28_col11, #T_02857_row39_col3, #T_02857_row40_col3, #T_02857_row49_col6, #T_02857_row50_col6, #T_02857_row103_col11, #T_02857_row104_col11, #T_02857_row107_col11, #T_02857_row108_col11, #T_02857_row109_col4, #T_02857_row110_col4, #T_02857_row126_col10, #T_02857_row127_col10, #T_02857_row130_col3, #T_02857_row131_col3, #T_02857_row150_col9, #T_02857_row151_col9, #T_02857_row156_col8, #T_02857_row157_col8, #T_02857_row170_col11, #T_02857_row171_col11, #T_02857_row208_col6, #T_02857_row209_col6, #T_02857_row210_col10, #T_02857_row211_col10, #T_02857_row220_col9, #T_02857_row221_col9, #T_02857_row228_col11, #T_02857_row229_col11, #T_02857_row243_col4, #T_02857_row243_col5, #T_02857_row243_col6, #T_02857_row243_col7, #T_02857_row243_col9, #T_02857_row269_col11, #T_02857_row270_col11, #T_02857_row275_col6, #T_02857_row276_col6, #T_02857_row287_col7, #T_02857_row288_col7, #T_02857_row297_col3, #T_02857_row298_col3, #T_02857_row299_col4, #T_02857_row300_col4, #T_02857_row303_col5, #T_02857_row304_col5, #T_02857_row311_col9, #T_02857_row312_col9, #T_02857_row323_col7, #T_02857_row324_col7, #T_02857_row331_col10, #T_02857_row332_col10, #T_02857_row333_col7, #T_02857_row333_col10, #T_02857_row334_col7, #T_02857_row334_col10, #T_02857_row347_col10, #T_02857_row348_col10, #T_02857_row353_col8, #T_02857_row354_col8, #T_02857_row366_col8, #T_02857_row366_col9, #T_02857_row367_col8, #T_02857_row367_col9, #T_02857_row368_col10, #T_02857_row369_col10, #T_02857_row398_col11, #T_02857_row399_col11, #T_02857_row420_col11, #T_02857_row421_col11, #T_02857_row444_col11, #T_02857_row445_col11, #T_02857_row458_col9, #T_02857_row459_col9, #T_02857_row480_col8, #T_02857_row480_col9, #T_02857_row529_col6, #T_02857_row530_col6, #T_02857_row551_col7, #T_02857_row552_col7, #T_02857_row581_col10, #T_02857_row582_col10, #T_02857_row589_col9, #T_02857_row590_col9 {\n",
              "  background-color: #d6d6e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row6_col8, #T_02857_row6_col10, #T_02857_row11_col3, #T_02857_row11_col10, #T_02857_row12_col3, #T_02857_row12_col10, #T_02857_row13_col9, #T_02857_row13_col10, #T_02857_row14_col9, #T_02857_row14_col10, #T_02857_row23_col9, #T_02857_row24_col9, #T_02857_row25_col3, #T_02857_row26_col3, #T_02857_row29_col11, #T_02857_row30_col11, #T_02857_row39_col6, #T_02857_row40_col6, #T_02857_row41_col11, #T_02857_row42_col11, #T_02857_row63_col4, #T_02857_row64_col4, #T_02857_row69_col8, #T_02857_row70_col8, #T_02857_row71_col11, #T_02857_row72_col11, #T_02857_row95_col3, #T_02857_row95_col6, #T_02857_row96_col3, #T_02857_row96_col6, #T_02857_row105_col9, #T_02857_row106_col9, #T_02857_row109_col3, #T_02857_row110_col3, #T_02857_row115_col9, #T_02857_row116_col9, #T_02857_row120_col9, #T_02857_row142_col10, #T_02857_row143_col10, #T_02857_row152_col11, #T_02857_row153_col11, #T_02857_row168_col7, #T_02857_row169_col7, #T_02857_row176_col3, #T_02857_row177_col3, #T_02857_row180_col3, #T_02857_row181_col3, #T_02857_row200_col3, #T_02857_row201_col3, #T_02857_row224_col10, #T_02857_row225_col10, #T_02857_row234_col11, #T_02857_row235_col11, #T_02857_row243_col3, #T_02857_row243_col8, #T_02857_row243_col10, #T_02857_row261_col6, #T_02857_row261_col7, #T_02857_row262_col6, #T_02857_row262_col7, #T_02857_row263_col9, #T_02857_row264_col9, #T_02857_row279_col11, #T_02857_row280_col11, #T_02857_row287_col5, #T_02857_row288_col5, #T_02857_row291_col6, #T_02857_row291_col7, #T_02857_row292_col6, #T_02857_row292_col7, #T_02857_row297_col5, #T_02857_row297_col6, #T_02857_row297_col8, #T_02857_row298_col5, #T_02857_row298_col6, #T_02857_row298_col8, #T_02857_row303_col6, #T_02857_row303_col7, #T_02857_row304_col6, #T_02857_row304_col7, #T_02857_row311_col5, #T_02857_row311_col7, #T_02857_row312_col5, #T_02857_row312_col7, #T_02857_row319_col5, #T_02857_row319_col7, #T_02857_row320_col5, #T_02857_row320_col7, #T_02857_row347_col5, #T_02857_row348_col5, #T_02857_row358_col9, #T_02857_row412_col11, #T_02857_row413_col11, #T_02857_row428_col4, #T_02857_row429_col4, #T_02857_row432_col5, #T_02857_row433_col5, #T_02857_row452_col9, #T_02857_row453_col9, #T_02857_row468_col8, #T_02857_row468_col10, #T_02857_row469_col8, #T_02857_row469_col10, #T_02857_row478_col3, #T_02857_row480_col10, #T_02857_row499_col8, #T_02857_row500_col8, #T_02857_row501_col6, #T_02857_row501_col9, #T_02857_row502_col6, #T_02857_row502_col9, #T_02857_row513_col8, #T_02857_row514_col8, #T_02857_row515_col6, #T_02857_row516_col6, #T_02857_row569_col6, #T_02857_row570_col6, #T_02857_row583_col11, #T_02857_row584_col11, #T_02857_row587_col10, #T_02857_row588_col10 {\n",
              "  background-color: #d7d6e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row7_col3, #T_02857_row8_col3, #T_02857_row19_col5, #T_02857_row20_col5, #T_02857_row35_col4, #T_02857_row36_col4, #T_02857_row85_col6, #T_02857_row86_col6, #T_02857_row123_col11, #T_02857_row125_col8, #T_02857_row140_col6, #T_02857_row141_col6, #T_02857_row222_col7, #T_02857_row223_col7, #T_02857_row230_col4, #T_02857_row231_col4, #T_02857_row236_col4, #T_02857_row237_col4, #T_02857_row253_col3, #T_02857_row254_col3, #T_02857_row301_col9, #T_02857_row302_col9, #T_02857_row313_col10, #T_02857_row314_col10, #T_02857_row329_col4, #T_02857_row330_col4, #T_02857_row368_col7, #T_02857_row369_col7, #T_02857_row374_col5, #T_02857_row375_col5, #T_02857_row398_col5, #T_02857_row399_col5, #T_02857_row412_col8, #T_02857_row413_col8, #T_02857_row414_col10, #T_02857_row415_col10, #T_02857_row424_col8, #T_02857_row425_col8, #T_02857_row426_col9, #T_02857_row427_col9, #T_02857_row454_col3, #T_02857_row454_col6, #T_02857_row455_col3, #T_02857_row455_col6, #T_02857_row468_col7, #T_02857_row469_col7, #T_02857_row472_col7, #T_02857_row473_col7, #T_02857_row495_col7, #T_02857_row496_col7, #T_02857_row505_col3, #T_02857_row506_col3, #T_02857_row519_col4, #T_02857_row520_col4, #T_02857_row523_col6, #T_02857_row524_col6, #T_02857_row573_col3, #T_02857_row574_col3, #T_02857_row575_col4, #T_02857_row576_col4, #T_02857_row593_col5, #T_02857_row593_col6, #T_02857_row594_col5, #T_02857_row594_col6 {\n",
              "  background-color: #9ebad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row7_col4, #T_02857_row8_col4, #T_02857_row15_col3, #T_02857_row16_col3, #T_02857_row19_col4, #T_02857_row20_col4, #T_02857_row21_col6, #T_02857_row22_col6, #T_02857_row85_col9, #T_02857_row86_col9, #T_02857_row105_col5, #T_02857_row106_col5, #T_02857_row125_col3, #T_02857_row125_col4, #T_02857_row125_col5, #T_02857_row125_col6, #T_02857_row126_col6, #T_02857_row127_col6, #T_02857_row134_col3, #T_02857_row135_col3, #T_02857_row154_col5, #T_02857_row155_col5, #T_02857_row168_col9, #T_02857_row169_col9, #T_02857_row216_col5, #T_02857_row217_col5, #T_02857_row355_col7, #T_02857_row356_col7, #T_02857_row363_col11, #T_02857_row372_col6, #T_02857_row373_col6, #T_02857_row374_col4, #T_02857_row375_col4, #T_02857_row386_col6, #T_02857_row387_col6, #T_02857_row458_col5, #T_02857_row459_col5, #T_02857_row468_col3, #T_02857_row469_col3, #T_02857_row470_col7, #T_02857_row471_col7, #T_02857_row497_col6, #T_02857_row498_col6, #T_02857_row561_col8, #T_02857_row562_col8, #T_02857_row567_col3, #T_02857_row568_col3, #T_02857_row581_col5, #T_02857_row582_col5 {\n",
              "  background-color: #97b7d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row7_col5, #T_02857_row8_col5, #T_02857_row15_col4, #T_02857_row16_col4, #T_02857_row43_col4, #T_02857_row44_col4, #T_02857_row91_col3, #T_02857_row92_col3, #T_02857_row125_col7, #T_02857_row125_col11, #T_02857_row126_col3, #T_02857_row127_col3, #T_02857_row210_col6, #T_02857_row211_col6, #T_02857_row370_col5, #T_02857_row371_col5, #T_02857_row400_col6, #T_02857_row401_col6, #T_02857_row402_col4, #T_02857_row403_col4, #T_02857_row456_col6, #T_02857_row457_col6, #T_02857_row462_col7, #T_02857_row463_col7, #T_02857_row483_col7, #T_02857_row484_col7, #T_02857_row509_col6, #T_02857_row510_col6, #T_02857_row523_col5, #T_02857_row524_col5, #T_02857_row575_col3, #T_02857_row576_col3, #T_02857_row579_col4, #T_02857_row580_col4 {\n",
              "  background-color: #9ab8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row7_col6, #T_02857_row8_col6, #T_02857_row19_col7, #T_02857_row20_col7, #T_02857_row41_col4, #T_02857_row42_col4, #T_02857_row91_col6, #T_02857_row92_col6, #T_02857_row97_col5, #T_02857_row98_col5, #T_02857_row119_col5, #T_02857_row119_col9, #T_02857_row123_col9, #T_02857_row125_col10, #T_02857_row138_col6, #T_02857_row139_col6, #T_02857_row148_col7, #T_02857_row149_col7, #T_02857_row166_col4, #T_02857_row167_col4, #T_02857_row196_col9, #T_02857_row197_col9, #T_02857_row210_col7, #T_02857_row211_col7, #T_02857_row259_col7, #T_02857_row260_col7, #T_02857_row265_col6, #T_02857_row266_col6, #T_02857_row279_col5, #T_02857_row280_col5, #T_02857_row281_col6, #T_02857_row282_col6, #T_02857_row351_col3, #T_02857_row352_col3, #T_02857_row355_col8, #T_02857_row356_col8, #T_02857_row362_col8, #T_02857_row378_col7, #T_02857_row379_col7, #T_02857_row382_col4, #T_02857_row383_col4, #T_02857_row390_col7, #T_02857_row391_col7, #T_02857_row406_col8, #T_02857_row407_col8, #T_02857_row422_col9, #T_02857_row423_col9, #T_02857_row424_col10, #T_02857_row425_col10, #T_02857_row458_col4, #T_02857_row459_col4, #T_02857_row507_col5, #T_02857_row508_col5, #T_02857_row523_col7, #T_02857_row524_col7, #T_02857_row575_col5, #T_02857_row576_col5, #T_02857_row589_col3, #T_02857_row590_col3 {\n",
              "  background-color: #a4bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row7_col7, #T_02857_row8_col7, #T_02857_row27_col3, #T_02857_row28_col3, #T_02857_row33_col3, #T_02857_row34_col3, #T_02857_row210_col3, #T_02857_row211_col3, #T_02857_row216_col6, #T_02857_row217_col6, #T_02857_row230_col5, #T_02857_row230_col6, #T_02857_row231_col5, #T_02857_row231_col6, #T_02857_row244_col8, #T_02857_row265_col3, #T_02857_row266_col3, #T_02857_row386_col7, #T_02857_row387_col7, #T_02857_row396_col3, #T_02857_row397_col3, #T_02857_row483_col6, #T_02857_row484_col6, #T_02857_row497_col7, #T_02857_row498_col7, #T_02857_row505_col4, #T_02857_row506_col4, #T_02857_row561_col6, #T_02857_row562_col6, #T_02857_row567_col6, #T_02857_row568_col6 {\n",
              "  background-color: #9fbad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row7_col9, #T_02857_row7_col10, #T_02857_row8_col9, #T_02857_row8_col10, #T_02857_row25_col4, #T_02857_row26_col4, #T_02857_row39_col4, #T_02857_row40_col4, #T_02857_row49_col3, #T_02857_row50_col3, #T_02857_row120_col5, #T_02857_row120_col10, #T_02857_row138_col10, #T_02857_row139_col10, #T_02857_row158_col8, #T_02857_row159_col8, #T_02857_row160_col11, #T_02857_row161_col11, #T_02857_row168_col3, #T_02857_row169_col3, #T_02857_row186_col11, #T_02857_row187_col11, #T_02857_row196_col7, #T_02857_row197_col7, #T_02857_row202_col11, #T_02857_row203_col11, #T_02857_row212_col9, #T_02857_row213_col9, #T_02857_row222_col8, #T_02857_row223_col8, #T_02857_row249_col3, #T_02857_row250_col3, #T_02857_row259_col10, #T_02857_row260_col10, #T_02857_row261_col5, #T_02857_row262_col5, #T_02857_row263_col6, #T_02857_row263_col8, #T_02857_row264_col6, #T_02857_row264_col8, #T_02857_row271_col9, #T_02857_row272_col9, #T_02857_row275_col8, #T_02857_row276_col8, #T_02857_row283_col9, #T_02857_row284_col9, #T_02857_row297_col4, #T_02857_row298_col4, #T_02857_row311_col3, #T_02857_row312_col3, #T_02857_row315_col10, #T_02857_row316_col10, #T_02857_row317_col4, #T_02857_row317_col11, #T_02857_row318_col4, #T_02857_row318_col11, #T_02857_row321_col9, #T_02857_row322_col9, #T_02857_row331_col5, #T_02857_row332_col5, #T_02857_row337_col3, #T_02857_row338_col3, #T_02857_row343_col8, #T_02857_row344_col8, #T_02857_row376_col10, #T_02857_row377_col10, #T_02857_row410_col3, #T_02857_row411_col3, #T_02857_row424_col11, #T_02857_row425_col11, #T_02857_row432_col3, #T_02857_row433_col3, #T_02857_row442_col3, #T_02857_row443_col3, #T_02857_row487_col3, #T_02857_row487_col4, #T_02857_row488_col3, #T_02857_row488_col4, #T_02857_row489_col10, #T_02857_row490_col10, #T_02857_row493_col9, #T_02857_row494_col9, #T_02857_row501_col3, #T_02857_row502_col3, #T_02857_row515_col3, #T_02857_row516_col3, #T_02857_row525_col11, #T_02857_row526_col11, #T_02857_row529_col5, #T_02857_row530_col5, #T_02857_row569_col4, #T_02857_row569_col5, #T_02857_row570_col4, #T_02857_row570_col5, #T_02857_row571_col3, #T_02857_row571_col6, #T_02857_row572_col3, #T_02857_row572_col6, #T_02857_row591_col9, #T_02857_row592_col9, #T_02857_row593_col9, #T_02857_row594_col9 {\n",
              "  background-color: #d8d7e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row9_col3, #T_02857_row10_col3, #T_02857_row17_col8, #T_02857_row18_col8, #T_02857_row45_col11, #T_02857_row46_col11, #T_02857_row63_col6, #T_02857_row64_col6, #T_02857_row65_col7, #T_02857_row66_col7, #T_02857_row75_col4, #T_02857_row76_col4, #T_02857_row79_col6, #T_02857_row79_col7, #T_02857_row80_col6, #T_02857_row80_col7, #T_02857_row81_col3, #T_02857_row82_col3, #T_02857_row111_col11, #T_02857_row112_col11, #T_02857_row144_col3, #T_02857_row145_col3, #T_02857_row156_col7, #T_02857_row157_col7, #T_02857_row158_col4, #T_02857_row159_col4, #T_02857_row170_col7, #T_02857_row171_col7, #T_02857_row172_col6, #T_02857_row173_col6, #T_02857_row186_col4, #T_02857_row187_col4, #T_02857_row196_col6, #T_02857_row197_col6, #T_02857_row198_col11, #T_02857_row199_col11, #T_02857_row222_col11, #T_02857_row223_col11, #T_02857_row228_col10, #T_02857_row229_col10, #T_02857_row236_col10, #T_02857_row237_col10, #T_02857_row245_col8, #T_02857_row246_col8, #T_02857_row251_col10, #T_02857_row252_col10, #T_02857_row255_col11, #T_02857_row256_col11, #T_02857_row277_col3, #T_02857_row278_col3, #T_02857_row285_col11, #T_02857_row286_col11, #T_02857_row289_col9, #T_02857_row290_col9, #T_02857_row313_col7, #T_02857_row314_col7, #T_02857_row333_col8, #T_02857_row334_col8, #T_02857_row337_col4, #T_02857_row338_col4, #T_02857_row339_col7, #T_02857_row340_col7, #T_02857_row347_col7, #T_02857_row348_col7, #T_02857_row353_col11, #T_02857_row354_col11, #T_02857_row358_col11, #T_02857_row364_col9, #T_02857_row365_col9, #T_02857_row376_col8, #T_02857_row377_col8, #T_02857_row386_col11, #T_02857_row387_col11, #T_02857_row396_col8, #T_02857_row397_col8, #T_02857_row414_col11, #T_02857_row415_col11, #T_02857_row418_col3, #T_02857_row419_col3, #T_02857_row424_col4, #T_02857_row425_col4, #T_02857_row438_col4, #T_02857_row439_col4, #T_02857_row446_col7, #T_02857_row447_col7, #T_02857_row464_col9, #T_02857_row465_col9, #T_02857_row479_col10, #T_02857_row481_col3, #T_02857_row481_col4, #T_02857_row481_col5, #T_02857_row481_col6, #T_02857_row481_col7, #T_02857_row481_col8, #T_02857_row481_col9, #T_02857_row481_col10, #T_02857_row482_col7, #T_02857_row482_col9, #T_02857_row493_col3, #T_02857_row493_col6, #T_02857_row493_col7, #T_02857_row494_col3, #T_02857_row494_col6, #T_02857_row494_col7, #T_02857_row499_col4, #T_02857_row500_col4, #T_02857_row525_col3, #T_02857_row526_col3, #T_02857_row531_col5, #T_02857_row532_col5, #T_02857_row539_col5, #T_02857_row540_col5, #T_02857_row541_col5, #T_02857_row542_col5, #T_02857_row543_col5, #T_02857_row544_col5, #T_02857_row545_col7, #T_02857_row545_col11, #T_02857_row546_col7, #T_02857_row546_col11, #T_02857_row547_col10, #T_02857_row548_col10, #T_02857_row555_col7, #T_02857_row556_col7, #T_02857_row557_col7, #T_02857_row558_col7, #T_02857_row585_col4, #T_02857_row586_col4 {\n",
              "  background-color: #d0d1e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row9_col9, #T_02857_row10_col9, #T_02857_row21_col10, #T_02857_row22_col10, #T_02857_row33_col10, #T_02857_row34_col10, #T_02857_row35_col11, #T_02857_row36_col11, #T_02857_row39_col5, #T_02857_row39_col8, #T_02857_row40_col5, #T_02857_row40_col8, #T_02857_row59_col8, #T_02857_row60_col8, #T_02857_row93_col9, #T_02857_row94_col9, #T_02857_row101_col9, #T_02857_row102_col9, #T_02857_row113_col9, #T_02857_row114_col9, #T_02857_row152_col8, #T_02857_row153_col8, #T_02857_row190_col3, #T_02857_row190_col4, #T_02857_row191_col3, #T_02857_row191_col4, #T_02857_row202_col4, #T_02857_row203_col4, #T_02857_row216_col9, #T_02857_row216_col10, #T_02857_row217_col9, #T_02857_row217_col10, #T_02857_row222_col10, #T_02857_row223_col10, #T_02857_row234_col8, #T_02857_row235_col8, #T_02857_row267_col9, #T_02857_row268_col9, #T_02857_row273_col8, #T_02857_row274_col8, #T_02857_row279_col9, #T_02857_row280_col9, #T_02857_row291_col5, #T_02857_row291_col11, #T_02857_row292_col5, #T_02857_row292_col11, #T_02857_row301_col6, #T_02857_row301_col7, #T_02857_row302_col6, #T_02857_row302_col7, #T_02857_row366_col10, #T_02857_row367_col10, #T_02857_row402_col10, #T_02857_row403_col10, #T_02857_row408_col3, #T_02857_row409_col3, #T_02857_row422_col11, #T_02857_row423_col11, #T_02857_row436_col4, #T_02857_row437_col4, #T_02857_row450_col8, #T_02857_row451_col8, #T_02857_row466_col11, #T_02857_row467_col11, #T_02857_row513_col9, #T_02857_row514_col9, #T_02857_row517_col11, #T_02857_row518_col11, #T_02857_row523_col9, #T_02857_row523_col10, #T_02857_row524_col9, #T_02857_row524_col10, #T_02857_row571_col8, #T_02857_row572_col8, #T_02857_row577_col9, #T_02857_row578_col9, #T_02857_row579_col10, #T_02857_row580_col10 {\n",
              "  background-color: #dbdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row9_col11, #T_02857_row10_col11, #T_02857_row15_col8, #T_02857_row16_col8, #T_02857_row49_col4, #T_02857_row50_col4, #T_02857_row99_col9, #T_02857_row100_col9, #T_02857_row109_col11, #T_02857_row110_col11, #T_02857_row142_col11, #T_02857_row143_col11, #T_02857_row164_col8, #T_02857_row165_col8, #T_02857_row525_col5, #T_02857_row526_col5, #T_02857_row589_col8, #T_02857_row590_col8 {\n",
              "  background-color: #e5e1ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row11_col6, #T_02857_row12_col6, #T_02857_row15_col11, #T_02857_row16_col11, #T_02857_row51_col3, #T_02857_row51_col4, #T_02857_row52_col3, #T_02857_row52_col4, #T_02857_row55_col4, #T_02857_row55_col7, #T_02857_row56_col4, #T_02857_row56_col7, #T_02857_row61_col5, #T_02857_row62_col5, #T_02857_row63_col11, #T_02857_row64_col11, #T_02857_row67_col7, #T_02857_row68_col7, #T_02857_row69_col11, #T_02857_row70_col11, #T_02857_row73_col3, #T_02857_row73_col4, #T_02857_row73_col5, #T_02857_row74_col3, #T_02857_row74_col4, #T_02857_row74_col5, #T_02857_row79_col3, #T_02857_row80_col3, #T_02857_row99_col11, #T_02857_row100_col11, #T_02857_row101_col8, #T_02857_row102_col8, #T_02857_row128_col8, #T_02857_row129_col8, #T_02857_row134_col9, #T_02857_row135_col9, #T_02857_row170_col3, #T_02857_row171_col3, #T_02857_row172_col5, #T_02857_row173_col5, #T_02857_row176_col11, #T_02857_row177_col11, #T_02857_row184_col3, #T_02857_row184_col11, #T_02857_row185_col3, #T_02857_row185_col11, #T_02857_row194_col4, #T_02857_row195_col4, #T_02857_row202_col5, #T_02857_row203_col5, #T_02857_row214_col10, #T_02857_row215_col10, #T_02857_row218_col11, #T_02857_row219_col11, #T_02857_row224_col9, #T_02857_row225_col9, #T_02857_row230_col8, #T_02857_row231_col8, #T_02857_row232_col8, #T_02857_row232_col9, #T_02857_row233_col8, #T_02857_row233_col9, #T_02857_row234_col10, #T_02857_row235_col10, #T_02857_row269_col10, #T_02857_row270_col10, #T_02857_row275_col3, #T_02857_row275_col4, #T_02857_row275_col9, #T_02857_row276_col3, #T_02857_row276_col4, #T_02857_row276_col9, #T_02857_row277_col9, #T_02857_row278_col9, #T_02857_row283_col5, #T_02857_row283_col6, #T_02857_row283_col7, #T_02857_row284_col5, #T_02857_row284_col6, #T_02857_row284_col7, #T_02857_row291_col10, #T_02857_row292_col10, #T_02857_row293_col4, #T_02857_row294_col4, #T_02857_row295_col3, #T_02857_row296_col3, #T_02857_row297_col10, #T_02857_row297_col11, #T_02857_row298_col10, #T_02857_row298_col11, #T_02857_row299_col5, #T_02857_row300_col5, #T_02857_row309_col6, #T_02857_row310_col6, #T_02857_row321_col10, #T_02857_row322_col10, #T_02857_row325_col3, #T_02857_row325_col4, #T_02857_row325_col9, #T_02857_row326_col3, #T_02857_row326_col4, #T_02857_row326_col9, #T_02857_row339_col6, #T_02857_row340_col6, #T_02857_row341_col10, #T_02857_row342_col10, #T_02857_row345_col10, #T_02857_row346_col10, #T_02857_row347_col3, #T_02857_row348_col3, #T_02857_row364_col8, #T_02857_row365_col8, #T_02857_row408_col6, #T_02857_row409_col6, #T_02857_row410_col7, #T_02857_row411_col7, #T_02857_row412_col4, #T_02857_row413_col4, #T_02857_row414_col5, #T_02857_row415_col5, #T_02857_row416_col11, #T_02857_row417_col11, #T_02857_row434_col5, #T_02857_row435_col5, #T_02857_row462_col8, #T_02857_row463_col8, #T_02857_row482_col8, #T_02857_row487_col7, #T_02857_row488_col7, #T_02857_row493_col8, #T_02857_row493_col10, #T_02857_row494_col8, #T_02857_row494_col10, #T_02857_row499_col3, #T_02857_row500_col3, #T_02857_row529_col10, #T_02857_row530_col10, #T_02857_row551_col11, #T_02857_row552_col11, #T_02857_row585_col3, #T_02857_row586_col3, #T_02857_row591_col10, #T_02857_row592_col10 {\n",
              "  background-color: #d2d3e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row11_col11, #T_02857_row12_col11, #T_02857_row113_col10, #T_02857_row114_col10 {\n",
              "  background-color: #e9e5f1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row13_col3, #T_02857_row14_col3, #T_02857_row15_col7, #T_02857_row16_col7, #T_02857_row91_col7, #T_02857_row92_col7, #T_02857_row103_col3, #T_02857_row103_col5, #T_02857_row104_col3, #T_02857_row104_col5, #T_02857_row117_col3, #T_02857_row118_col3, #T_02857_row119_col7, #T_02857_row119_col8, #T_02857_row160_col5, #T_02857_row161_col5, #T_02857_row182_col9, #T_02857_row183_col9, #T_02857_row190_col7, #T_02857_row191_col7, #T_02857_row230_col7, #T_02857_row231_col7, #T_02857_row242_col11, #T_02857_row245_col4, #T_02857_row246_col4, #T_02857_row253_col10, #T_02857_row254_col10, #T_02857_row279_col6, #T_02857_row280_col6, #T_02857_row299_col9, #T_02857_row300_col9, #T_02857_row357_col4, #T_02857_row363_col10, #T_02857_row370_col3, #T_02857_row371_col3, #T_02857_row404_col7, #T_02857_row405_col7, #T_02857_row458_col6, #T_02857_row459_col6, #T_02857_row489_col5, #T_02857_row490_col5, #T_02857_row507_col3, #T_02857_row507_col6, #T_02857_row508_col3, #T_02857_row508_col6, #T_02857_row509_col7, #T_02857_row510_col7, #T_02857_row511_col6, #T_02857_row512_col6, #T_02857_row561_col9, #T_02857_row562_col9, #T_02857_row565_col9, #T_02857_row566_col9, #T_02857_row577_col5, #T_02857_row578_col5, #T_02857_row579_col3, #T_02857_row580_col3, #T_02857_row587_col6, #T_02857_row588_col6, #T_02857_row589_col4, #T_02857_row590_col4 {\n",
              "  background-color: #a7bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row13_col4, #T_02857_row14_col4, #T_02857_row19_col3, #T_02857_row20_col3, #T_02857_row97_col4, #T_02857_row98_col4, #T_02857_row103_col4, #T_02857_row104_col4, #T_02857_row132_col4, #T_02857_row132_col7, #T_02857_row133_col4, #T_02857_row133_col7, #T_02857_row146_col3, #T_02857_row146_col5, #T_02857_row147_col3, #T_02857_row147_col5, #T_02857_row148_col6, #T_02857_row149_col6, #T_02857_row152_col3, #T_02857_row153_col3, #T_02857_row154_col6, #T_02857_row155_col6, #T_02857_row162_col5, #T_02857_row163_col5, #T_02857_row190_col9, #T_02857_row191_col9, #T_02857_row204_col10, #T_02857_row205_col10, #T_02857_row244_col11, #T_02857_row257_col8, #T_02857_row258_col8, #T_02857_row327_col8, #T_02857_row328_col8, #T_02857_row351_col4, #T_02857_row352_col4, #T_02857_row355_col3, #T_02857_row356_col3, #T_02857_row420_col10, #T_02857_row421_col10, #T_02857_row452_col5, #T_02857_row453_col5, #T_02857_row495_col6, #T_02857_row496_col6, #T_02857_row517_col4, #T_02857_row518_col4 {\n",
              "  background-color: #9cb9d9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row13_col5, #T_02857_row14_col5, #T_02857_row41_col3, #T_02857_row42_col3, #T_02857_row89_col8, #T_02857_row90_col8, #T_02857_row97_col3, #T_02857_row98_col3, #T_02857_row119_col6, #T_02857_row123_col3, #T_02857_row123_col4, #T_02857_row123_col5, #T_02857_row123_col6, #T_02857_row123_col7, #T_02857_row146_col6, #T_02857_row147_col6, #T_02857_row166_col3, #T_02857_row167_col3, #T_02857_row176_col10, #T_02857_row177_col10, #T_02857_row216_col3, #T_02857_row217_col3, #T_02857_row230_col3, #T_02857_row231_col3, #T_02857_row232_col7, #T_02857_row233_col7, #T_02857_row244_col7, #T_02857_row265_col5, #T_02857_row266_col5, #T_02857_row273_col4, #T_02857_row274_col4, #T_02857_row285_col7, #T_02857_row286_col7, #T_02857_row335_col5, #T_02857_row336_col5, #T_02857_row337_col7, #T_02857_row338_col7, #T_02857_row362_col9, #T_02857_row374_col6, #T_02857_row375_col6, #T_02857_row384_col5, #T_02857_row385_col5, #T_02857_row428_col6, #T_02857_row429_col6, #T_02857_row436_col8, #T_02857_row436_col9, #T_02857_row437_col8, #T_02857_row437_col9, #T_02857_row460_col3, #T_02857_row461_col3, #T_02857_row503_col3, #T_02857_row504_col3, #T_02857_row507_col4, #T_02857_row508_col4, #T_02857_row511_col5, #T_02857_row512_col5, #T_02857_row519_col3, #T_02857_row520_col3, #T_02857_row561_col10, #T_02857_row562_col10, #T_02857_row567_col7, #T_02857_row568_col7, #T_02857_row573_col5, #T_02857_row574_col5, #T_02857_row581_col7, #T_02857_row582_col7, #T_02857_row587_col3, #T_02857_row587_col4, #T_02857_row588_col3, #T_02857_row588_col4 {\n",
              "  background-color: #a2bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row13_col6, #T_02857_row14_col6, #T_02857_row15_col6, #T_02857_row16_col6, #T_02857_row99_col4, #T_02857_row100_col4, #T_02857_row111_col3, #T_02857_row112_col3, #T_02857_row119_col11, #T_02857_row123_col10, #T_02857_row154_col7, #T_02857_row155_col7, #T_02857_row160_col6, #T_02857_row161_col6, #T_02857_row168_col8, #T_02857_row169_col8, #T_02857_row172_col9, #T_02857_row173_col9, #T_02857_row182_col8, #T_02857_row183_col8, #T_02857_row190_col6, #T_02857_row191_col6, #T_02857_row202_col9, #T_02857_row203_col9, #T_02857_row206_col8, #T_02857_row207_col8, #T_02857_row216_col7, #T_02857_row217_col7, #T_02857_row273_col5, #T_02857_row274_col5, #T_02857_row299_col8, #T_02857_row300_col8, #T_02857_row357_col3, #T_02857_row357_col5, #T_02857_row357_col6, #T_02857_row362_col4, #T_02857_row362_col6, #T_02857_row362_col10, #T_02857_row362_col11, #T_02857_row374_col3, #T_02857_row375_col3, #T_02857_row384_col3, #T_02857_row384_col6, #T_02857_row385_col3, #T_02857_row385_col6, #T_02857_row388_col4, #T_02857_row389_col4, #T_02857_row392_col7, #T_02857_row393_col7, #T_02857_row400_col7, #T_02857_row401_col7, #T_02857_row402_col6, #T_02857_row403_col6, #T_02857_row428_col7, #T_02857_row429_col7, #T_02857_row442_col6, #T_02857_row443_col6, #T_02857_row454_col7, #T_02857_row455_col7, #T_02857_row458_col7, #T_02857_row459_col7, #T_02857_row489_col3, #T_02857_row489_col7, #T_02857_row490_col3, #T_02857_row490_col7, #T_02857_row503_col5, #T_02857_row504_col5, #T_02857_row577_col6, #T_02857_row578_col6, #T_02857_row579_col5, #T_02857_row580_col5, #T_02857_row587_col5, #T_02857_row588_col5 {\n",
              "  background-color: #a8bedc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row13_col7, #T_02857_row14_col7, #T_02857_row19_col6, #T_02857_row20_col6, #T_02857_row27_col5, #T_02857_row27_col6, #T_02857_row28_col5, #T_02857_row28_col6, #T_02857_row89_col9, #T_02857_row90_col9, #T_02857_row99_col3, #T_02857_row100_col3, #T_02857_row105_col7, #T_02857_row106_col7, #T_02857_row111_col4, #T_02857_row112_col4, #T_02857_row119_col3, #T_02857_row132_col6, #T_02857_row133_col6, #T_02857_row140_col7, #T_02857_row141_col7, #T_02857_row301_col8, #T_02857_row302_col8, #T_02857_row357_col11, #T_02857_row416_col8, #T_02857_row417_col8, #T_02857_row460_col7, #T_02857_row461_col7, #T_02857_row533_col8, #T_02857_row534_col8, #T_02857_row577_col4, #T_02857_row578_col4 {\n",
              "  background-color: #a5bddb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row13_col8, #T_02857_row14_col8, #T_02857_row39_col10, #T_02857_row40_col10, #T_02857_row95_col10, #T_02857_row96_col10, #T_02857_row97_col9, #T_02857_row98_col9, #T_02857_row111_col8, #T_02857_row112_col8, #T_02857_row160_col8, #T_02857_row161_col8, #T_02857_row164_col11, #T_02857_row165_col11, #T_02857_row166_col10, #T_02857_row166_col11, #T_02857_row167_col10, #T_02857_row167_col11, #T_02857_row196_col4, #T_02857_row197_col4, #T_02857_row202_col3, #T_02857_row203_col3, #T_02857_row218_col9, #T_02857_row219_col9, #T_02857_row226_col11, #T_02857_row227_col11, #T_02857_row263_col10, #T_02857_row264_col10, #T_02857_row265_col8, #T_02857_row266_col8, #T_02857_row267_col8, #T_02857_row268_col8, #T_02857_row269_col8, #T_02857_row270_col8, #T_02857_row279_col8, #T_02857_row280_col8, #T_02857_row285_col8, #T_02857_row286_col8, #T_02857_row307_col3, #T_02857_row307_col4, #T_02857_row308_col3, #T_02857_row308_col4, #T_02857_row321_col6, #T_02857_row322_col6, #T_02857_row323_col9, #T_02857_row324_col9, #T_02857_row327_col6, #T_02857_row328_col6, #T_02857_row331_col3, #T_02857_row332_col3, #T_02857_row337_col9, #T_02857_row338_col9, #T_02857_row341_col9, #T_02857_row342_col9, #T_02857_row378_col9, #T_02857_row378_col10, #T_02857_row379_col9, #T_02857_row379_col10, #T_02857_row380_col10, #T_02857_row381_col10, #T_02857_row384_col9, #T_02857_row384_col10, #T_02857_row385_col9, #T_02857_row385_col10, #T_02857_row386_col9, #T_02857_row387_col9, #T_02857_row388_col9, #T_02857_row389_col9, #T_02857_row390_col9, #T_02857_row391_col9, #T_02857_row396_col11, #T_02857_row397_col11, #T_02857_row404_col9, #T_02857_row405_col9, #T_02857_row410_col11, #T_02857_row411_col11, #T_02857_row446_col4, #T_02857_row447_col4, #T_02857_row454_col9, #T_02857_row455_col9, #T_02857_row456_col9, #T_02857_row457_col9, #T_02857_row515_col10, #T_02857_row516_col10, #T_02857_row525_col4, #T_02857_row526_col4, #T_02857_row569_col9, #T_02857_row569_col10, #T_02857_row570_col9, #T_02857_row570_col10, #T_02857_row571_col10, #T_02857_row572_col10, #T_02857_row573_col10, #T_02857_row574_col10 {\n",
              "  background-color: #e0dded;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row15_col5, #T_02857_row16_col5, #T_02857_row21_col7, #T_02857_row22_col7, #T_02857_row47_col3, #T_02857_row48_col3, #T_02857_row117_col4, #T_02857_row118_col4, #T_02857_row119_col4, #T_02857_row123_col8, #T_02857_row132_col5, #T_02857_row133_col5, #T_02857_row152_col6, #T_02857_row153_col6, #T_02857_row162_col6, #T_02857_row163_col6, #T_02857_row218_col7, #T_02857_row219_col7, #T_02857_row224_col7, #T_02857_row225_col7, #T_02857_row236_col5, #T_02857_row237_col5, #T_02857_row244_col3, #T_02857_row244_col4, #T_02857_row244_col5, #T_02857_row244_col6, #T_02857_row244_col9, #T_02857_row257_col3, #T_02857_row258_col3, #T_02857_row273_col3, #T_02857_row274_col3, #T_02857_row281_col5, #T_02857_row282_col5, #T_02857_row329_col5, #T_02857_row330_col5, #T_02857_row363_col3, #T_02857_row363_col4, #T_02857_row363_col5, #T_02857_row363_col6, #T_02857_row363_col7, #T_02857_row363_col8, #T_02857_row363_col9, #T_02857_row370_col6, #T_02857_row371_col6, #T_02857_row398_col6, #T_02857_row399_col6, #T_02857_row440_col9, #T_02857_row441_col9, #T_02857_row460_col6, #T_02857_row461_col6, #T_02857_row466_col6, #T_02857_row467_col6, #T_02857_row489_col4, #T_02857_row490_col4, #T_02857_row533_col9, #T_02857_row534_col9 {\n",
              "  background-color: #a1bbda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row15_col9, #T_02857_row16_col9, #T_02857_row25_col10, #T_02857_row26_col10, #T_02857_row27_col9, #T_02857_row28_col9, #T_02857_row35_col9, #T_02857_row35_col10, #T_02857_row36_col9, #T_02857_row36_col10, #T_02857_row37_col9, #T_02857_row38_col9, #T_02857_row47_col8, #T_02857_row48_col8, #T_02857_row91_col8, #T_02857_row92_col8, #T_02857_row140_col8, #T_02857_row141_col8, #T_02857_row146_col8, #T_02857_row147_col8, #T_02857_row158_col10, #T_02857_row159_col10, #T_02857_row166_col9, #T_02857_row167_col9, #T_02857_row204_col11, #T_02857_row205_col11, #T_02857_row247_col11, #T_02857_row248_col11, #T_02857_row307_col11, #T_02857_row308_col11, #T_02857_row319_col3, #T_02857_row320_col3, #T_02857_row321_col7, #T_02857_row322_col7, #T_02857_row335_col9, #T_02857_row336_col9, #T_02857_row378_col8, #T_02857_row379_col8, #T_02857_row380_col8, #T_02857_row381_col8, #T_02857_row382_col9, #T_02857_row383_col9, #T_02857_row392_col10, #T_02857_row393_col10, #T_02857_row394_col9, #T_02857_row394_col10, #T_02857_row395_col9, #T_02857_row395_col10, #T_02857_row408_col5, #T_02857_row409_col5, #T_02857_row436_col3, #T_02857_row437_col3, #T_02857_row460_col9, #T_02857_row461_col9, #T_02857_row497_col9, #T_02857_row498_col9, #T_02857_row509_col9, #T_02857_row509_col10, #T_02857_row510_col9, #T_02857_row510_col10, #T_02857_row511_col9, #T_02857_row512_col9, #T_02857_row517_col10, #T_02857_row518_col10, #T_02857_row523_col8, #T_02857_row524_col8, #T_02857_row567_col8, #T_02857_row568_col8, #T_02857_row569_col8, #T_02857_row570_col8, #T_02857_row579_col9, #T_02857_row580_col9 {\n",
              "  background-color: #dfddec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row15_col10, #T_02857_row16_col10, #T_02857_row43_col8, #T_02857_row43_col9, #T_02857_row44_col8, #T_02857_row44_col9, #T_02857_row128_col11, #T_02857_row129_col11, #T_02857_row150_col8, #T_02857_row151_col8, #T_02857_row200_col11, #T_02857_row201_col11, #T_02857_row204_col3, #T_02857_row205_col3, #T_02857_row208_col3, #T_02857_row208_col4, #T_02857_row209_col3, #T_02857_row209_col4, #T_02857_row239_col3, #T_02857_row239_col4, #T_02857_row239_col5, #T_02857_row239_col6, #T_02857_row239_col7, #T_02857_row239_col8, #T_02857_row239_col9, #T_02857_row239_col10, #T_02857_row301_col4, #T_02857_row301_col5, #T_02857_row302_col4, #T_02857_row302_col5, #T_02857_row313_col3, #T_02857_row313_col4, #T_02857_row314_col3, #T_02857_row314_col4, #T_02857_row315_col3, #T_02857_row315_col4, #T_02857_row315_col5, #T_02857_row316_col3, #T_02857_row316_col4, #T_02857_row316_col5, #T_02857_row321_col5, #T_02857_row322_col5, #T_02857_row327_col3, #T_02857_row327_col4, #T_02857_row328_col3, #T_02857_row328_col4, #T_02857_row406_col5, #T_02857_row407_col5, #T_02857_row408_col11, #T_02857_row409_col11, #T_02857_row464_col11, #T_02857_row465_col11, #T_02857_row517_col8, #T_02857_row518_col8, #T_02857_row573_col8, #T_02857_row574_col8, #T_02857_row575_col10, #T_02857_row576_col10 {\n",
              "  background-color: #e3e0ee;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row19_col10, #T_02857_row20_col10, #T_02857_row53_col6, #T_02857_row53_col11, #T_02857_row54_col6, #T_02857_row54_col11, #T_02857_row73_col9, #T_02857_row74_col9, #T_02857_row79_col4, #T_02857_row80_col4, #T_02857_row91_col11, #T_02857_row92_col11, #T_02857_row93_col10, #T_02857_row94_col10, #T_02857_row97_col11, #T_02857_row98_col11, #T_02857_row103_col10, #T_02857_row104_col10, #T_02857_row117_col11, #T_02857_row118_col11, #T_02857_row128_col10, #T_02857_row129_col10, #T_02857_row138_col11, #T_02857_row139_col11, #T_02857_row146_col11, #T_02857_row147_col11, #T_02857_row156_col10, #T_02857_row157_col10, #T_02857_row158_col6, #T_02857_row159_col6, #T_02857_row170_col4, #T_02857_row171_col4, #T_02857_row180_col11, #T_02857_row181_col11, #T_02857_row184_col4, #T_02857_row185_col4, #T_02857_row194_col3, #T_02857_row195_col3, #T_02857_row200_col4, #T_02857_row201_col4, #T_02857_row249_col8, #T_02857_row250_col8, #T_02857_row269_col9, #T_02857_row270_col9, #T_02857_row273_col10, #T_02857_row274_col10, #T_02857_row277_col8, #T_02857_row278_col8, #T_02857_row289_col5, #T_02857_row289_col8, #T_02857_row290_col5, #T_02857_row290_col8, #T_02857_row303_col8, #T_02857_row303_col9, #T_02857_row304_col8, #T_02857_row304_col9, #T_02857_row305_col6, #T_02857_row306_col6, #T_02857_row309_col9, #T_02857_row310_col9, #T_02857_row339_col11, #T_02857_row340_col11, #T_02857_row341_col3, #T_02857_row342_col3, #T_02857_row345_col5, #T_02857_row345_col6, #T_02857_row346_col5, #T_02857_row346_col6, #T_02857_row370_col8, #T_02857_row370_col10, #T_02857_row371_col8, #T_02857_row371_col10, #T_02857_row376_col9, #T_02857_row377_col9, #T_02857_row418_col11, #T_02857_row419_col11, #T_02857_row420_col5, #T_02857_row421_col5, #T_02857_row424_col3, #T_02857_row425_col3, #T_02857_row436_col7, #T_02857_row437_col7, #T_02857_row438_col3, #T_02857_row439_col3, #T_02857_row442_col4, #T_02857_row443_col4, #T_02857_row466_col10, #T_02857_row467_col10, #T_02857_row474_col10, #T_02857_row475_col10, #T_02857_row478_col11, #T_02857_row485_col9, #T_02857_row486_col9, #T_02857_row507_col9, #T_02857_row508_col9, #T_02857_row515_col9, #T_02857_row516_col9, #T_02857_row521_col11, #T_02857_row522_col11, #T_02857_row533_col11, #T_02857_row534_col11, #T_02857_row551_col5, #T_02857_row552_col5, #T_02857_row555_col4, #T_02857_row556_col4, #T_02857_row577_col11, #T_02857_row578_col11 {\n",
              "  background-color: #d4d4e8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row21_col3, #T_02857_row22_col3, #T_02857_row126_col4, #T_02857_row127_col4, #T_02857_row190_col8, #T_02857_row191_col8, #T_02857_row208_col8, #T_02857_row209_col8, #T_02857_row218_col3, #T_02857_row218_col4, #T_02857_row219_col3, #T_02857_row219_col4, #T_02857_row259_col4, #T_02857_row260_col4, #T_02857_row271_col6, #T_02857_row272_col6, #T_02857_row355_col6, #T_02857_row356_col6, #T_02857_row364_col3, #T_02857_row365_col3, #T_02857_row412_col9, #T_02857_row413_col9, #T_02857_row432_col9, #T_02857_row433_col9, #T_02857_row456_col3, #T_02857_row456_col4, #T_02857_row456_col5, #T_02857_row457_col3, #T_02857_row457_col4, #T_02857_row457_col5, #T_02857_row462_col3, #T_02857_row463_col3, #T_02857_row470_col4, #T_02857_row471_col4, #T_02857_row523_col4, #T_02857_row524_col4 {\n",
              "  background-color: #8bb2d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row21_col4, #T_02857_row22_col4, #T_02857_row85_col8, #T_02857_row86_col8, #T_02857_row162_col3, #T_02857_row162_col4, #T_02857_row163_col3, #T_02857_row163_col4, #T_02857_row222_col5, #T_02857_row223_col5, #T_02857_row267_col3, #T_02857_row268_col3, #T_02857_row351_col8, #T_02857_row352_col8, #T_02857_row428_col10, #T_02857_row429_col10, #T_02857_row523_col3, #T_02857_row524_col3 {\n",
              "  background-color: #86b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row21_col5, #T_02857_row22_col5, #T_02857_row35_col3, #T_02857_row36_col3, #T_02857_row43_col3, #T_02857_row44_col3, #T_02857_row134_col4, #T_02857_row134_col6, #T_02857_row134_col7, #T_02857_row135_col4, #T_02857_row135_col6, #T_02857_row135_col7, #T_02857_row148_col5, #T_02857_row149_col5, #T_02857_row218_col5, #T_02857_row219_col5, #T_02857_row224_col3, #T_02857_row224_col6, #T_02857_row225_col3, #T_02857_row225_col6, #T_02857_row257_col4, #T_02857_row257_col6, #T_02857_row258_col4, #T_02857_row258_col6, #T_02857_row267_col6, #T_02857_row268_col6, #T_02857_row279_col4, #T_02857_row280_col4, #T_02857_row351_col7, #T_02857_row352_col7, #T_02857_row361_col4, #T_02857_row361_col5, #T_02857_row361_col6, #T_02857_row361_col11, #T_02857_row374_col7, #T_02857_row375_col7, #T_02857_row392_col5, #T_02857_row393_col5, #T_02857_row418_col9, #T_02857_row419_col9, #T_02857_row426_col10, #T_02857_row427_col10, #T_02857_row430_col8, #T_02857_row431_col8, #T_02857_row495_col4, #T_02857_row496_col4 {\n",
              "  background-color: #93b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row21_col8, #T_02857_row22_col8, #T_02857_row33_col9, #T_02857_row34_col9, #T_02857_row130_col11, #T_02857_row131_col11, #T_02857_row144_col10, #T_02857_row145_col10, #T_02857_row146_col9, #T_02857_row147_col9, #T_02857_row150_col11, #T_02857_row151_col11, #T_02857_row261_col8, #T_02857_row262_col8, #T_02857_row305_col3, #T_02857_row306_col3, #T_02857_row307_col5, #T_02857_row308_col5, #T_02857_row313_col5, #T_02857_row314_col5, #T_02857_row319_col4, #T_02857_row320_col4, #T_02857_row329_col11, #T_02857_row330_col11, #T_02857_row335_col8, #T_02857_row336_col8, #T_02857_row406_col4, #T_02857_row407_col4, #T_02857_row418_col5, #T_02857_row419_col5, #T_02857_row436_col5, #T_02857_row437_col5, #T_02857_row438_col11, #T_02857_row439_col11, #T_02857_row448_col8, #T_02857_row449_col8, #T_02857_row454_col10, #T_02857_row455_col10, #T_02857_row476_col3, #T_02857_row476_col4, #T_02857_row476_col5, #T_02857_row476_col6, #T_02857_row476_col7, #T_02857_row476_col8, #T_02857_row476_col9, #T_02857_row476_col10, #T_02857_row495_col8, #T_02857_row496_col8, #T_02857_row497_col10, #T_02857_row498_col10, #T_02857_row499_col9, #T_02857_row500_col9, #T_02857_row511_col10, #T_02857_row512_col10, #T_02857_row521_col9, #T_02857_row522_col9, #T_02857_row593_col8, #T_02857_row594_col8 {\n",
              "  background-color: #dcdaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row21_col9, #T_02857_row22_col9, #T_02857_row37_col8, #T_02857_row37_col10, #T_02857_row38_col8, #T_02857_row38_col10, #T_02857_row45_col8, #T_02857_row46_col8, #T_02857_row47_col9, #T_02857_row47_col10, #T_02857_row48_col9, #T_02857_row48_col10, #T_02857_row49_col7, #T_02857_row50_col7, #T_02857_row136_col8, #T_02857_row137_col8, #T_02857_row152_col9, #T_02857_row152_col10, #T_02857_row153_col9, #T_02857_row153_col10, #T_02857_row154_col8, #T_02857_row155_col8, #T_02857_row160_col10, #T_02857_row161_col10, #T_02857_row166_col8, #T_02857_row167_col8, #T_02857_row168_col4, #T_02857_row169_col4, #T_02857_row214_col11, #T_02857_row215_col11, #T_02857_row216_col8, #T_02857_row217_col8, #T_02857_row220_col11, #T_02857_row221_col11, #T_02857_row259_col9, #T_02857_row260_col9, #T_02857_row265_col9, #T_02857_row266_col9, #T_02857_row267_col10, #T_02857_row268_col10, #T_02857_row281_col9, #T_02857_row282_col9, #T_02857_row285_col10, #T_02857_row286_col10, #T_02857_row315_col9, #T_02857_row316_col9, #T_02857_row327_col9, #T_02857_row328_col9, #T_02857_row329_col8, #T_02857_row329_col10, #T_02857_row330_col8, #T_02857_row330_col10, #T_02857_row333_col3, #T_02857_row334_col3, #T_02857_row368_col11, #T_02857_row369_col11, #T_02857_row382_col10, #T_02857_row383_col10, #T_02857_row388_col8, #T_02857_row388_col10, #T_02857_row389_col8, #T_02857_row389_col10, #T_02857_row392_col9, #T_02857_row393_col9, #T_02857_row394_col8, #T_02857_row395_col8, #T_02857_row402_col9, #T_02857_row403_col9, #T_02857_row408_col4, #T_02857_row409_col4, #T_02857_row432_col4, #T_02857_row433_col4, #T_02857_row446_col3, #T_02857_row446_col5, #T_02857_row447_col3, #T_02857_row447_col5, #T_02857_row450_col10, #T_02857_row451_col10, #T_02857_row454_col8, #T_02857_row455_col8, #T_02857_row456_col8, #T_02857_row457_col8, #T_02857_row487_col10, #T_02857_row488_col10, #T_02857_row497_col8, #T_02857_row498_col8, #T_02857_row501_col8, #T_02857_row502_col8, #T_02857_row509_col8, #T_02857_row510_col8 {\n",
              "  background-color: #dedcec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row23_col11, #T_02857_row24_col11, #T_02857_row27_col8, #T_02857_row28_col8, #T_02857_row43_col11, #T_02857_row44_col11, #T_02857_row67_col11, #T_02857_row68_col11, #T_02857_row91_col9, #T_02857_row92_col9, #T_02857_row140_col9, #T_02857_row141_col9, #T_02857_row148_col9, #T_02857_row149_col9, #T_02857_row162_col10, #T_02857_row163_col10, #T_02857_row168_col5, #T_02857_row169_col5, #T_02857_row196_col3, #T_02857_row196_col5, #T_02857_row196_col11, #T_02857_row197_col3, #T_02857_row197_col5, #T_02857_row197_col11, #T_02857_row204_col4, #T_02857_row205_col4, #T_02857_row208_col5, #T_02857_row209_col5, #T_02857_row212_col11, #T_02857_row213_col11, #T_02857_row315_col7, #T_02857_row316_col7, #T_02857_row323_col5, #T_02857_row324_col5, #T_02857_row329_col9, #T_02857_row330_col9, #T_02857_row339_col8, #T_02857_row340_col8, #T_02857_row386_col10, #T_02857_row387_col10, #T_02857_row404_col8, #T_02857_row405_col8, #T_02857_row446_col11, #T_02857_row447_col11, #T_02857_row501_col10, #T_02857_row502_col10, #T_02857_row505_col9, #T_02857_row506_col9, #T_02857_row511_col8, #T_02857_row512_col8, #T_02857_row519_col9, #T_02857_row520_col9, #T_02857_row587_col8, #T_02857_row588_col8 {\n",
              "  background-color: #e1dfed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row25_col11, #T_02857_row26_col11, #T_02857_row39_col11, #T_02857_row40_col11, #T_02857_row263_col11, #T_02857_row264_col11, #T_02857_row321_col11, #T_02857_row322_col11, #T_02857_row323_col11, #T_02857_row324_col11 {\n",
              "  background-color: #f0eaf4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row27_col4, #T_02857_row28_col4, #T_02857_row33_col4, #T_02857_row34_col4, #T_02857_row91_col5, #T_02857_row92_col5, #T_02857_row105_col6, #T_02857_row106_col6, #T_02857_row126_col7, #T_02857_row127_col7, #T_02857_row140_col3, #T_02857_row141_col3, #T_02857_row180_col8, #T_02857_row181_col8, #T_02857_row259_col5, #T_02857_row260_col5, #T_02857_row285_col6, #T_02857_row286_col6, #T_02857_row307_col8, #T_02857_row308_col8, #T_02857_row351_col5, #T_02857_row352_col5, #T_02857_row355_col4, #T_02857_row356_col4, #T_02857_row361_col10, #T_02857_row376_col3, #T_02857_row377_col3, #T_02857_row420_col8, #T_02857_row421_col8, #T_02857_row474_col7, #T_02857_row475_col7, #T_02857_row483_col5, #T_02857_row484_col5, #T_02857_row495_col5, #T_02857_row496_col5, #T_02857_row503_col4, #T_02857_row504_col4, #T_02857_row509_col5, #T_02857_row510_col5, #T_02857_row517_col3, #T_02857_row518_col3, #T_02857_row581_col6, #T_02857_row582_col6 {\n",
              "  background-color: #96b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row27_col7, #T_02857_row28_col7, #T_02857_row33_col7, #T_02857_row34_col7, #T_02857_row49_col9, #T_02857_row50_col9, #T_02857_row89_col10, #T_02857_row90_col10, #T_02857_row97_col7, #T_02857_row98_col7, #T_02857_row99_col6, #T_02857_row100_col6, #T_02857_row103_col7, #T_02857_row104_col7, #T_02857_row119_col10, #T_02857_row122_col11, #T_02857_row124_col4, #T_02857_row124_col6, #T_02857_row136_col4, #T_02857_row136_col7, #T_02857_row137_col4, #T_02857_row137_col7, #T_02857_row150_col3, #T_02857_row151_col3, #T_02857_row234_col3, #T_02857_row235_col3, #T_02857_row241_col4, #T_02857_row241_col9, #T_02857_row329_col6, #T_02857_row330_col6, #T_02857_row357_col10, #T_02857_row410_col8, #T_02857_row411_col8, #T_02857_row444_col10, #T_02857_row445_col10, #T_02857_row452_col6, #T_02857_row453_col6, #T_02857_row491_col7, #T_02857_row492_col7, #T_02857_row517_col6, #T_02857_row518_col6, #T_02857_row533_col10, #T_02857_row534_col10, #T_02857_row561_col5, #T_02857_row561_col7, #T_02857_row562_col5, #T_02857_row562_col7, #T_02857_row563_col8, #T_02857_row564_col8, #T_02857_row565_col10, #T_02857_row566_col10, #T_02857_row579_col7, #T_02857_row580_col7, #T_02857_row587_col7, #T_02857_row588_col7, #T_02857_row589_col6, #T_02857_row590_col6 {\n",
              "  background-color: #b1c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row27_col10, #T_02857_row28_col10, #T_02857_row33_col8, #T_02857_row34_col8, #T_02857_row91_col10, #T_02857_row92_col10, #T_02857_row140_col10, #T_02857_row141_col10, #T_02857_row164_col10, #T_02857_row165_col10, #T_02857_row210_col9, #T_02857_row211_col9, #T_02857_row222_col9, #T_02857_row223_col9, #T_02857_row249_col10, #T_02857_row250_col10, #T_02857_row261_col10, #T_02857_row262_col10, #T_02857_row281_col10, #T_02857_row282_col10, #T_02857_row285_col9, #T_02857_row286_col9, #T_02857_row323_col6, #T_02857_row324_col6, #T_02857_row331_col4, #T_02857_row332_col4, #T_02857_row333_col4, #T_02857_row334_col4, #T_02857_row335_col11, #T_02857_row336_col11, #T_02857_row398_col9, #T_02857_row398_col10, #T_02857_row399_col9, #T_02857_row399_col10, #T_02857_row402_col8, #T_02857_row403_col8, #T_02857_row448_col10, #T_02857_row449_col10, #T_02857_row450_col9, #T_02857_row451_col9, #T_02857_row456_col10, #T_02857_row457_col10, #T_02857_row460_col10, #T_02857_row461_col10, #T_02857_row485_col11, #T_02857_row486_col11, #T_02857_row515_col8, #T_02857_row516_col8, #T_02857_row533_col5, #T_02857_row534_col5, #T_02857_row567_col10, #T_02857_row568_col10 {\n",
              "  background-color: #dddbec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row29_col3, #T_02857_row30_col3, #T_02857_row97_col6, #T_02857_row98_col6, #T_02857_row111_col6, #T_02857_row112_col6, #T_02857_row122_col4, #T_02857_row122_col9, #T_02857_row124_col8, #T_02857_row214_col5, #T_02857_row215_col5, #T_02857_row234_col4, #T_02857_row235_col4, #T_02857_row244_col10, #T_02857_row245_col3, #T_02857_row245_col5, #T_02857_row246_col3, #T_02857_row246_col5, #T_02857_row309_col10, #T_02857_row310_col10, #T_02857_row335_col4, #T_02857_row335_col7, #T_02857_row336_col4, #T_02857_row336_col7, #T_02857_row351_col10, #T_02857_row352_col10, #T_02857_row357_col9, #T_02857_row360_col4, #T_02857_row362_col3, #T_02857_row362_col7, #T_02857_row402_col5, #T_02857_row403_col5, #T_02857_row418_col10, #T_02857_row419_col10, #T_02857_row489_col6, #T_02857_row490_col6, #T_02857_row491_col3, #T_02857_row492_col3, #T_02857_row573_col6, #T_02857_row574_col6 {\n",
              "  background-color: #a9bfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row29_col4, #T_02857_row30_col4, #T_02857_row47_col4, #T_02857_row48_col4, #T_02857_row122_col3, #T_02857_row122_col7, #T_02857_row124_col9, #T_02857_row146_col7, #T_02857_row147_col7, #T_02857_row152_col7, #T_02857_row153_col7, #T_02857_row170_col9, #T_02857_row171_col9, #T_02857_row234_col6, #T_02857_row235_col6, #T_02857_row245_col7, #T_02857_row246_col7, #T_02857_row273_col6, #T_02857_row274_col6, #T_02857_row295_col10, #T_02857_row296_col10, #T_02857_row357_col8, #T_02857_row360_col3, #T_02857_row360_col8, #T_02857_row416_col10, #T_02857_row417_col10, #T_02857_row422_col8, #T_02857_row423_col8, #T_02857_row466_col4, #T_02857_row467_col4, #T_02857_row503_col6, #T_02857_row504_col6, #T_02857_row519_col5, #T_02857_row520_col5, #T_02857_row565_col8, #T_02857_row566_col8, #T_02857_row577_col3, #T_02857_row577_col7, #T_02857_row578_col3, #T_02857_row578_col7 {\n",
              "  background-color: #acc0dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row29_col5, #T_02857_row30_col5, #T_02857_row41_col6, #T_02857_row42_col6, #T_02857_row132_col3, #T_02857_row133_col3, #T_02857_row150_col4, #T_02857_row151_col4, #T_02857_row166_col6, #T_02857_row167_col6, #T_02857_row234_col5, #T_02857_row235_col5, #T_02857_row236_col3, #T_02857_row237_col3, #T_02857_row299_col10, #T_02857_row300_col10, #T_02857_row335_col6, #T_02857_row336_col6, #T_02857_row360_col11, #T_02857_row384_col7, #T_02857_row385_col7, #T_02857_row398_col7, #T_02857_row399_col7, #T_02857_row426_col6, #T_02857_row427_col6, #T_02857_row446_col10, #T_02857_row447_col10, #T_02857_row519_col6, #T_02857_row520_col6, #T_02857_row539_col9, #T_02857_row540_col9, #T_02857_row553_col9, #T_02857_row554_col9 {\n",
              "  background-color: #afc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row29_col6, #T_02857_row30_col6, #T_02857_row67_col9, #T_02857_row68_col9, #T_02857_row85_col7, #T_02857_row86_col7, #T_02857_row89_col6, #T_02857_row90_col6, #T_02857_row124_col5, #T_02857_row124_col7, #T_02857_row136_col5, #T_02857_row137_col5, #T_02857_row150_col5, #T_02857_row151_col5, #T_02857_row168_col10, #T_02857_row169_col10, #T_02857_row192_col8, #T_02857_row192_col10, #T_02857_row193_col8, #T_02857_row193_col10, #T_02857_row220_col5, #T_02857_row221_col5, #T_02857_row228_col7, #T_02857_row229_col7, #T_02857_row236_col8, #T_02857_row237_col8, #T_02857_row242_col8, #T_02857_row242_col10, #T_02857_row273_col7, #T_02857_row274_col7, #T_02857_row337_col8, #T_02857_row338_col8, #T_02857_row349_col7, #T_02857_row350_col7, #T_02857_row359_col11, #T_02857_row382_col3, #T_02857_row382_col6, #T_02857_row383_col3, #T_02857_row383_col6, #T_02857_row394_col3, #T_02857_row395_col3, #T_02857_row422_col10, #T_02857_row423_col10, #T_02857_row440_col10, #T_02857_row441_col10, #T_02857_row466_col7, #T_02857_row467_col7, #T_02857_row525_col8, #T_02857_row526_col8, #T_02857_row555_col8, #T_02857_row556_col8, #T_02857_row565_col6, #T_02857_row566_col6, #T_02857_row573_col7, #T_02857_row574_col7 {\n",
              "  background-color: #b4c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row29_col7, #T_02857_row30_col7, #T_02857_row35_col6, #T_02857_row36_col6, #T_02857_row61_col9, #T_02857_row62_col9, #T_02857_row87_col4, #T_02857_row88_col4, #T_02857_row99_col7, #T_02857_row100_col7, #T_02857_row117_col7, #T_02857_row118_col7, #T_02857_row130_col5, #T_02857_row131_col5, #T_02857_row136_col6, #T_02857_row137_col6, #T_02857_row178_col9, #T_02857_row179_col9, #T_02857_row194_col6, #T_02857_row195_col6, #T_02857_row214_col4, #T_02857_row215_col4, #T_02857_row220_col3, #T_02857_row221_col3, #T_02857_row228_col8, #T_02857_row229_col8, #T_02857_row251_col3, #T_02857_row252_col3, #T_02857_row253_col11, #T_02857_row254_col11, #T_02857_row335_col3, #T_02857_row336_col3, #T_02857_row349_col3, #T_02857_row349_col5, #T_02857_row350_col3, #T_02857_row350_col5, #T_02857_row359_col4, #T_02857_row359_col7, #T_02857_row359_col9, #T_02857_row388_col3, #T_02857_row388_col6, #T_02857_row389_col3, #T_02857_row389_col6, #T_02857_row412_col10, #T_02857_row413_col10, #T_02857_row426_col7, #T_02857_row427_col7, #T_02857_row438_col10, #T_02857_row439_col10, #T_02857_row442_col7, #T_02857_row443_col7, #T_02857_row464_col6, #T_02857_row465_col6, #T_02857_row519_col7, #T_02857_row520_col7, #T_02857_row553_col6, #T_02857_row553_col8, #T_02857_row554_col6, #T_02857_row554_col8, #T_02857_row563_col4, #T_02857_row563_col10, #T_02857_row564_col4, #T_02857_row564_col10 {\n",
              "  background-color: #b5c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row29_col8, #T_02857_row30_col8, #T_02857_row499_col11, #T_02857_row500_col11, #T_02857_row505_col8, #T_02857_row506_col8 {\n",
              "  background-color: #eae6f1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row29_col9, #T_02857_row30_col9, #T_02857_row99_col10, #T_02857_row100_col10, #T_02857_row148_col8, #T_02857_row149_col8, #T_02857_row156_col11, #T_02857_row157_col11, #T_02857_row321_col4, #T_02857_row322_col4, #T_02857_row366_col11, #T_02857_row367_col11, #T_02857_row436_col11, #T_02857_row437_col11, #T_02857_row503_col8, #T_02857_row504_col8, #T_02857_row519_col10, #T_02857_row520_col10, #T_02857_row575_col9, #T_02857_row576_col9 {\n",
              "  background-color: #e4e1ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row29_col10, #T_02857_row30_col10, #T_02857_row49_col5, #T_02857_row50_col5, #T_02857_row144_col11, #T_02857_row145_col11, #T_02857_row208_col11, #T_02857_row209_col11, #T_02857_row347_col11, #T_02857_row348_col11, #T_02857_row394_col11, #T_02857_row395_col11, #T_02857_row450_col11, #T_02857_row451_col11, #T_02857_row491_col8, #T_02857_row492_col8, #T_02857_row505_col10, #T_02857_row506_col10, #T_02857_row513_col11, #T_02857_row514_col11, #T_02857_row519_col8, #T_02857_row520_col8, #T_02857_row575_col8, #T_02857_row576_col8 {\n",
              "  background-color: #e7e3f0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row33_col5, #T_02857_row33_col6, #T_02857_row34_col5, #T_02857_row34_col6, #T_02857_row99_col5, #T_02857_row100_col5, #T_02857_row111_col5, #T_02857_row112_col5, #T_02857_row117_col6, #T_02857_row118_col6, #T_02857_row122_col5, #T_02857_row122_col6, #T_02857_row130_col7, #T_02857_row131_col7, #T_02857_row180_col10, #T_02857_row181_col10, #T_02857_row241_col11, #T_02857_row281_col7, #T_02857_row282_col7, #T_02857_row295_col8, #T_02857_row295_col9, #T_02857_row296_col8, #T_02857_row296_col9, #T_02857_row329_col7, #T_02857_row330_col7, #T_02857_row341_col7, #T_02857_row342_col7, #T_02857_row357_col7, #T_02857_row360_col5, #T_02857_row360_col6, #T_02857_row360_col7, #T_02857_row360_col9, #T_02857_row362_col5, #T_02857_row396_col4, #T_02857_row397_col4, #T_02857_row458_col3, #T_02857_row459_col3, #T_02857_row491_col4, #T_02857_row491_col5, #T_02857_row492_col4, #T_02857_row492_col5, #T_02857_row505_col5, #T_02857_row506_col5, #T_02857_row507_col7, #T_02857_row508_col7, #T_02857_row525_col9, #T_02857_row526_col9, #T_02857_row575_col6, #T_02857_row576_col6 {\n",
              "  background-color: #abbfdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row35_col5, #T_02857_row36_col5, #T_02857_row41_col5, #T_02857_row42_col5, #T_02857_row75_col9, #T_02857_row76_col9, #T_02857_row103_col6, #T_02857_row104_col6, #T_02857_row117_col5, #T_02857_row118_col5, #T_02857_row122_col8, #T_02857_row164_col3, #T_02857_row165_col3, #T_02857_row182_col10, #T_02857_row183_col10, #T_02857_row194_col8, #T_02857_row194_col9, #T_02857_row194_col10, #T_02857_row195_col8, #T_02857_row195_col9, #T_02857_row195_col10, #T_02857_row198_col8, #T_02857_row199_col8, #T_02857_row228_col5, #T_02857_row228_col6, #T_02857_row229_col5, #T_02857_row229_col6, #T_02857_row236_col7, #T_02857_row237_col7, #T_02857_row245_col6, #T_02857_row246_col6, #T_02857_row251_col4, #T_02857_row251_col7, #T_02857_row252_col4, #T_02857_row252_col7, #T_02857_row265_col7, #T_02857_row266_col7, #T_02857_row287_col9, #T_02857_row288_col9, #T_02857_row301_col10, #T_02857_row302_col10, #T_02857_row315_col8, #T_02857_row316_col8, #T_02857_row321_col8, #T_02857_row322_col8, #T_02857_row323_col8, #T_02857_row324_col8, #T_02857_row341_col5, #T_02857_row342_col5, #T_02857_row406_col10, #T_02857_row407_col10, #T_02857_row452_col7, #T_02857_row453_col7, #T_02857_row505_col6, #T_02857_row506_col6, #T_02857_row541_col9, #T_02857_row542_col9, #T_02857_row543_col9, #T_02857_row544_col9, #T_02857_row575_col7, #T_02857_row576_col7, #T_02857_row579_col6, #T_02857_row580_col6, #T_02857_row593_col7, #T_02857_row594_col7 {\n",
              "  background-color: #adc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row35_col7, #T_02857_row36_col7, #T_02857_row87_col7, #T_02857_row87_col10, #T_02857_row88_col7, #T_02857_row88_col10, #T_02857_row121_col9, #T_02857_row122_col10, #T_02857_row136_col3, #T_02857_row137_col3, #T_02857_row138_col3, #T_02857_row139_col3, #T_02857_row164_col5, #T_02857_row165_col5, #T_02857_row174_col8, #T_02857_row175_col8, #T_02857_row176_col6, #T_02857_row177_col6, #T_02857_row192_col4, #T_02857_row193_col4, #T_02857_row206_col6, #T_02857_row207_col6, #T_02857_row220_col6, #T_02857_row221_col6, #T_02857_row279_col7, #T_02857_row280_col7, #T_02857_row307_col9, #T_02857_row308_col9, #T_02857_row347_col9, #T_02857_row348_col9, #T_02857_row394_col4, #T_02857_row395_col4, #T_02857_row416_col6, #T_02857_row417_col6, #T_02857_row424_col9, #T_02857_row425_col9, #T_02857_row430_col6, #T_02857_row431_col6, #T_02857_row521_col4, #T_02857_row522_col4, #T_02857_row545_col3, #T_02857_row546_col3, #T_02857_row553_col10, #T_02857_row554_col10, #T_02857_row555_col9, #T_02857_row556_col9, #T_02857_row561_col4, #T_02857_row562_col4, #T_02857_row563_col3, #T_02857_row563_col5, #T_02857_row563_col11, #T_02857_row564_col3, #T_02857_row564_col5, #T_02857_row564_col11 {\n",
              "  background-color: #b8c6e0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row35_col8, #T_02857_row36_col8, #T_02857_row41_col8, #T_02857_row42_col8, #T_02857_row45_col9, #T_02857_row46_col9, #T_02857_row47_col11, #T_02857_row48_col11, #T_02857_row93_col11, #T_02857_row94_col11, #T_02857_row97_col8, #T_02857_row98_col8, #T_02857_row162_col9, #T_02857_row163_col9, #T_02857_row218_col8, #T_02857_row218_col10, #T_02857_row219_col8, #T_02857_row219_col10, #T_02857_row283_col8, #T_02857_row284_col8, #T_02857_row301_col3, #T_02857_row302_col3, #T_02857_row317_col3, #T_02857_row318_col3, #T_02857_row337_col11, #T_02857_row338_col11, #T_02857_row341_col11, #T_02857_row342_col11, #T_02857_row380_col9, #T_02857_row381_col9, #T_02857_row382_col11, #T_02857_row383_col11, #T_02857_row390_col8, #T_02857_row390_col10, #T_02857_row391_col8, #T_02857_row391_col10, #T_02857_row392_col8, #T_02857_row393_col8, #T_02857_row398_col8, #T_02857_row399_col8, #T_02857_row400_col9, #T_02857_row400_col10, #T_02857_row401_col9, #T_02857_row401_col10, #T_02857_row404_col10, #T_02857_row405_col10, #T_02857_row448_col9, #T_02857_row449_col9, #T_02857_row503_col9, #T_02857_row503_col10, #T_02857_row504_col9, #T_02857_row504_col10, #T_02857_row517_col9, #T_02857_row518_col9, #T_02857_row521_col8, #T_02857_row522_col8, #T_02857_row567_col9, #T_02857_row568_col9, #T_02857_row573_col9, #T_02857_row574_col9, #T_02857_row585_col11, #T_02857_row586_col11 {\n",
              "  background-color: #e0deed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row37_col11, #T_02857_row38_col11, #T_02857_row249_col11, #T_02857_row250_col11 {\n",
              "  background-color: #ede7f2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row43_col5, #T_02857_row44_col5, #T_02857_row69_col3, #T_02857_row69_col4, #T_02857_row70_col3, #T_02857_row70_col4, #T_02857_row79_col8, #T_02857_row80_col8, #T_02857_row83_col4, #T_02857_row83_col8, #T_02857_row83_col9, #T_02857_row84_col4, #T_02857_row84_col8, #T_02857_row84_col9, #T_02857_row121_col6, #T_02857_row150_col7, #T_02857_row151_col7, #T_02857_row166_col7, #T_02857_row167_col7, #T_02857_row178_col6, #T_02857_row179_col6, #T_02857_row184_col8, #T_02857_row185_col8, #T_02857_row206_col4, #T_02857_row207_col4, #T_02857_row241_col10, #T_02857_row287_col10, #T_02857_row288_col10, #T_02857_row337_col6, #T_02857_row338_col6, #T_02857_row341_col6, #T_02857_row342_col6, #T_02857_row349_col11, #T_02857_row350_col11, #T_02857_row351_col11, #T_02857_row352_col11, #T_02857_row366_col6, #T_02857_row367_col6, #T_02857_row368_col5, #T_02857_row369_col5, #T_02857_row428_col5, #T_02857_row429_col5, #T_02857_row430_col5, #T_02857_row430_col7, #T_02857_row431_col5, #T_02857_row431_col7, #T_02857_row444_col6, #T_02857_row445_col6, #T_02857_row466_col3, #T_02857_row467_col3, #T_02857_row559_col3, #T_02857_row559_col4, #T_02857_row560_col3, #T_02857_row560_col4, #T_02857_row563_col7, #T_02857_row564_col7 {\n",
              "  background-color: #bcc7e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row43_col10, #T_02857_row44_col10, #T_02857_row148_col10, #T_02857_row149_col10, #T_02857_row162_col8, #T_02857_row163_col8, #T_02857_row220_col8, #T_02857_row221_col8, #T_02857_row281_col8, #T_02857_row282_col8, #T_02857_row309_col3, #T_02857_row309_col4, #T_02857_row310_col3, #T_02857_row310_col4, #T_02857_row313_col11, #T_02857_row314_col11, #T_02857_row315_col6, #T_02857_row316_col6, #T_02857_row321_col3, #T_02857_row322_col3, #T_02857_row323_col3, #T_02857_row323_col4, #T_02857_row324_col3, #T_02857_row324_col4, #T_02857_row327_col5, #T_02857_row328_col5, #T_02857_row384_col8, #T_02857_row385_col8, #T_02857_row386_col8, #T_02857_row387_col8, #T_02857_row388_col11, #T_02857_row389_col11, #T_02857_row400_col8, #T_02857_row401_col8, #T_02857_row489_col8, #T_02857_row490_col8, #T_02857_row491_col10, #T_02857_row492_col10, #T_02857_row589_col10, #T_02857_row590_col10 {\n",
              "  background-color: #e2dfee;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row45_col7, #T_02857_row46_col7, #T_02857_row69_col5, #T_02857_row70_col5, #T_02857_row73_col11, #T_02857_row74_col11, #T_02857_row77_col11, #T_02857_row78_col11, #T_02857_row107_col7, #T_02857_row108_col7, #T_02857_row115_col11, #T_02857_row116_col11, #T_02857_row130_col8, #T_02857_row131_col8, #T_02857_row144_col4, #T_02857_row145_col4, #T_02857_row156_col3, #T_02857_row157_col3, #T_02857_row164_col7, #T_02857_row165_col7, #T_02857_row178_col7, #T_02857_row178_col11, #T_02857_row179_col7, #T_02857_row179_col11, #T_02857_row198_col10, #T_02857_row199_col10, #T_02857_row204_col7, #T_02857_row205_col7, #T_02857_row206_col3, #T_02857_row207_col3, #T_02857_row212_col4, #T_02857_row213_col4, #T_02857_row228_col3, #T_02857_row229_col3, #T_02857_row238_col8, #T_02857_row253_col9, #T_02857_row254_col9, #T_02857_row257_col11, #T_02857_row258_col11, #T_02857_row293_col8, #T_02857_row294_col8, #T_02857_row299_col7, #T_02857_row300_col7, #T_02857_row305_col8, #T_02857_row306_col8, #T_02857_row349_col8, #T_02857_row350_col8, #T_02857_row366_col3, #T_02857_row367_col3, #T_02857_row380_col6, #T_02857_row381_col6, #T_02857_row396_col5, #T_02857_row396_col7, #T_02857_row397_col5, #T_02857_row397_col7, #T_02857_row474_col11, #T_02857_row475_col11, #T_02857_row507_col10, #T_02857_row508_col10, #T_02857_row513_col3, #T_02857_row514_col3, #T_02857_row537_col10, #T_02857_row538_col10, #T_02857_row543_col10, #T_02857_row544_col10, #T_02857_row547_col9, #T_02857_row548_col9, #T_02857_row551_col10, #T_02857_row552_col10, #T_02857_row553_col7, #T_02857_row554_col7, #T_02857_row559_col10, #T_02857_row560_col10, #T_02857_row565_col3, #T_02857_row566_col3, #T_02857_row583_col5, #T_02857_row584_col5 {\n",
              "  background-color: #c2cbe2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row47_col5, #T_02857_row48_col5, #T_02857_row61_col8, #T_02857_row62_col8, #T_02857_row71_col9, #T_02857_row72_col9, #T_02857_row87_col3, #T_02857_row87_col5, #T_02857_row87_col6, #T_02857_row87_col11, #T_02857_row88_col3, #T_02857_row88_col5, #T_02857_row88_col6, #T_02857_row88_col11, #T_02857_row124_col3, #T_02857_row124_col10, #T_02857_row150_col6, #T_02857_row151_col6, #T_02857_row184_col9, #T_02857_row185_col9, #T_02857_row186_col8, #T_02857_row187_col8, #T_02857_row192_col5, #T_02857_row193_col5, #T_02857_row198_col9, #T_02857_row199_col9, #T_02857_row206_col10, #T_02857_row207_col10, #T_02857_row214_col7, #T_02857_row215_col7, #T_02857_row234_col7, #T_02857_row235_col7, #T_02857_row287_col8, #T_02857_row288_col8, #T_02857_row291_col9, #T_02857_row292_col9, #T_02857_row329_col3, #T_02857_row330_col3, #T_02857_row359_col5, #T_02857_row359_col6, #T_02857_row359_col8, #T_02857_row366_col7, #T_02857_row367_col7, #T_02857_row382_col5, #T_02857_row383_col5, #T_02857_row388_col5, #T_02857_row389_col5, #T_02857_row396_col6, #T_02857_row397_col6, #T_02857_row402_col7, #T_02857_row403_col7, #T_02857_row466_col5, #T_02857_row467_col5, #T_02857_row521_col3, #T_02857_row522_col3, #T_02857_row557_col8, #T_02857_row558_col8, #T_02857_row563_col6, #T_02857_row564_col6 {\n",
              "  background-color: #b7c5df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row47_col6, #T_02857_row48_col6, #T_02857_row65_col9, #T_02857_row66_col9, #T_02857_row75_col8, #T_02857_row76_col8, #T_02857_row87_col8, #T_02857_row88_col8, #T_02857_row111_col7, #T_02857_row112_col7, #T_02857_row166_col5, #T_02857_row167_col5, #T_02857_row200_col8, #T_02857_row201_col8, #T_02857_row202_col10, #T_02857_row203_col10, #T_02857_row208_col10, #T_02857_row209_col10, #T_02857_row238_col11, #T_02857_row241_col3, #T_02857_row241_col5, #T_02857_row241_col6, #T_02857_row241_col7, #T_02857_row241_col8, #T_02857_row242_col3, #T_02857_row242_col4, #T_02857_row242_col5, #T_02857_row242_col6, #T_02857_row242_col7, #T_02857_row242_col9, #T_02857_row251_col6, #T_02857_row252_col6, #T_02857_row307_col10, #T_02857_row308_col10, #T_02857_row337_col5, #T_02857_row338_col5, #T_02857_row349_col4, #T_02857_row349_col6, #T_02857_row350_col4, #T_02857_row350_col6, #T_02857_row368_col4, #T_02857_row369_col4, #T_02857_row414_col6, #T_02857_row415_col6, #T_02857_row430_col9, #T_02857_row431_col9, #T_02857_row440_col6, #T_02857_row441_col6, #T_02857_row505_col7, #T_02857_row506_col7, #T_02857_row517_col5, #T_02857_row518_col5, #T_02857_row541_col8, #T_02857_row542_col8, #T_02857_row557_col9, #T_02857_row558_col9 {\n",
              "  background-color: #b3c3de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row49_col11, #T_02857_row50_col11, #T_02857_row53_col3, #T_02857_row54_col3, #T_02857_row59_col6, #T_02857_row59_col7, #T_02857_row59_col9, #T_02857_row59_col10, #T_02857_row60_col6, #T_02857_row60_col7, #T_02857_row60_col9, #T_02857_row60_col10, #T_02857_row81_col11, #T_02857_row82_col11, #T_02857_row95_col8, #T_02857_row96_col8, #T_02857_row97_col10, #T_02857_row98_col10, #T_02857_row120_col3, #T_02857_row120_col4, #T_02857_row136_col11, #T_02857_row137_col11, #T_02857_row146_col10, #T_02857_row147_col10, #T_02857_row158_col5, #T_02857_row159_col5, #T_02857_row168_col11, #T_02857_row169_col11, #T_02857_row232_col10, #T_02857_row233_col10, #T_02857_row261_col9, #T_02857_row262_col9, #T_02857_row271_col8, #T_02857_row272_col8, #T_02857_row273_col9, #T_02857_row274_col9, #T_02857_row277_col10, #T_02857_row278_col10, #T_02857_row279_col10, #T_02857_row280_col10, #T_02857_row291_col3, #T_02857_row292_col3, #T_02857_row331_col6, #T_02857_row332_col6, #T_02857_row396_col9, #T_02857_row397_col9, #T_02857_row406_col11, #T_02857_row407_col11, #T_02857_row410_col5, #T_02857_row411_col5, #T_02857_row418_col4, #T_02857_row419_col4, #T_02857_row452_col11, #T_02857_row453_col11, #T_02857_row462_col10, #T_02857_row463_col10, #T_02857_row464_col10, #T_02857_row465_col10, #T_02857_row477_col4, #T_02857_row477_col8, #T_02857_row477_col9, #T_02857_row477_col10, #T_02857_row478_col5, #T_02857_row483_col10, #T_02857_row484_col10, #T_02857_row495_col10, #T_02857_row496_col10, #T_02857_row501_col5, #T_02857_row501_col7, #T_02857_row502_col5, #T_02857_row502_col7, #T_02857_row515_col5, #T_02857_row516_col5, #T_02857_row529_col7, #T_02857_row530_col7, #T_02857_row543_col11, #T_02857_row544_col11, #T_02857_row579_col8, #T_02857_row579_col11, #T_02857_row580_col8, #T_02857_row580_col11 {\n",
              "  background-color: #dad9ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row81_col9, #T_02857_row82_col9, #T_02857_row162_col7, #T_02857_row163_col7, #T_02857_row164_col4, #T_02857_row165_col4, #T_02857_row174_col9, #T_02857_row175_col9, #T_02857_row196_col10, #T_02857_row197_col10, #T_02857_row220_col4, #T_02857_row221_col4, #T_02857_row228_col4, #T_02857_row229_col4, #T_02857_row251_col5, #T_02857_row252_col5, #T_02857_row434_col6, #T_02857_row435_col6, #T_02857_row452_col4, #T_02857_row453_col4, #T_02857_row491_col6, #T_02857_row492_col6, #T_02857_row511_col7, #T_02857_row512_col7, #T_02857_row551_col9, #T_02857_row552_col9, #T_02857_row589_col5, #T_02857_row590_col5 {\n",
              "  background-color: #b0c2de;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row85_col10, #T_02857_row86_col10, #T_02857_row125_col9, #T_02857_row138_col4, #T_02857_row138_col5, #T_02857_row139_col4, #T_02857_row139_col5, #T_02857_row152_col5, #T_02857_row153_col5, #T_02857_row218_col6, #T_02857_row219_col6, #T_02857_row236_col6, #T_02857_row237_col6, #T_02857_row259_col6, #T_02857_row260_col6, #T_02857_row267_col7, #T_02857_row268_col7, #T_02857_row281_col3, #T_02857_row281_col4, #T_02857_row282_col3, #T_02857_row282_col4, #T_02857_row418_col8, #T_02857_row419_col8, #T_02857_row430_col10, #T_02857_row431_col10, #T_02857_row434_col10, #T_02857_row435_col10, #T_02857_row442_col9, #T_02857_row443_col9, #T_02857_row448_col7, #T_02857_row449_col7, #T_02857_row456_col7, #T_02857_row457_col7, #T_02857_row468_col5, #T_02857_row469_col5, #T_02857_row472_col3, #T_02857_row472_col5, #T_02857_row473_col3, #T_02857_row473_col5, #T_02857_row483_col3, #T_02857_row484_col3 {\n",
              "  background-color: #99b8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row91_col4, #T_02857_row92_col4, #T_02857_row105_col3, #T_02857_row105_col4, #T_02857_row106_col3, #T_02857_row106_col4, #T_02857_row154_col4, #T_02857_row155_col4, #T_02857_row176_col9, #T_02857_row177_col9, #T_02857_row210_col4, #T_02857_row211_col4, #T_02857_row253_col5, #T_02857_row254_col5, #T_02857_row257_col5, #T_02857_row258_col5, #T_02857_row285_col5, #T_02857_row286_col5, #T_02857_row364_col7, #T_02857_row365_col7, #T_02857_row372_col4, #T_02857_row372_col5, #T_02857_row373_col4, #T_02857_row373_col5, #T_02857_row376_col6, #T_02857_row377_col6, #T_02857_row398_col4, #T_02857_row399_col4, #T_02857_row414_col9, #T_02857_row415_col9, #T_02857_row472_col6, #T_02857_row473_col6, #T_02857_row593_col3, #T_02857_row593_col4, #T_02857_row594_col3, #T_02857_row594_col4 {\n",
              "  background-color: #8eb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row95_col11, #T_02857_row96_col11, #T_02857_row571_col11, #T_02857_row572_col11 {\n",
              "  background-color: #f1ebf4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row99_col8, #T_02857_row100_col8, #T_02857_row275_col11, #T_02857_row276_col11, #T_02857_row380_col11, #T_02857_row381_col11 {\n",
              "  background-color: #ebe6f2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row113_col8, #T_02857_row114_col8, #T_02857_row487_col11, #T_02857_row488_col11 {\n",
              "  background-color: #efe9f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row126_col5, #T_02857_row127_col5, #T_02857_row160_col3, #T_02857_row161_col3, #T_02857_row176_col8, #T_02857_row177_col8, #T_02857_row180_col9, #T_02857_row181_col9, #T_02857_row222_col3, #T_02857_row223_col3, #T_02857_row224_col5, #T_02857_row225_col5, #T_02857_row232_col5, #T_02857_row233_col5, #T_02857_row259_col3, #T_02857_row260_col3, #T_02857_row271_col7, #T_02857_row272_col7, #T_02857_row279_col3, #T_02857_row280_col3, #T_02857_row361_col3, #T_02857_row361_col7, #T_02857_row372_col7, #T_02857_row373_col7, #T_02857_row378_col5, #T_02857_row378_col6, #T_02857_row379_col5, #T_02857_row379_col6, #T_02857_row384_col4, #T_02857_row385_col4, #T_02857_row386_col5, #T_02857_row387_col5, #T_02857_row414_col8, #T_02857_row415_col8, #T_02857_row420_col9, #T_02857_row421_col9, #T_02857_row434_col9, #T_02857_row435_col9, #T_02857_row468_col6, #T_02857_row469_col6, #T_02857_row483_col4, #T_02857_row484_col4, #T_02857_row497_col5, #T_02857_row498_col5, #T_02857_row567_col5, #T_02857_row568_col5 {\n",
              "  background-color: #91b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row134_col5, #T_02857_row135_col5, #T_02857_row138_col7, #T_02857_row139_col7, #T_02857_row146_col4, #T_02857_row147_col4, #T_02857_row190_col10, #T_02857_row191_col10, #T_02857_row216_col4, #T_02857_row217_col4, #T_02857_row222_col6, #T_02857_row223_col6, #T_02857_row232_col3, #T_02857_row232_col6, #T_02857_row233_col3, #T_02857_row233_col6, #T_02857_row253_col6, #T_02857_row253_col8, #T_02857_row254_col6, #T_02857_row254_col8, #T_02857_row267_col5, #T_02857_row268_col5, #T_02857_row309_col8, #T_02857_row310_col8, #T_02857_row361_col8, #T_02857_row361_col9, #T_02857_row364_col6, #T_02857_row365_col6, #T_02857_row372_col3, #T_02857_row373_col3, #T_02857_row404_col5, #T_02857_row405_col5, #T_02857_row442_col10, #T_02857_row443_col10, #T_02857_row454_col5, #T_02857_row455_col5, #T_02857_row460_col4, #T_02857_row460_col5, #T_02857_row461_col4, #T_02857_row461_col5, #T_02857_row462_col5, #T_02857_row463_col5, #T_02857_row468_col4, #T_02857_row469_col4, #T_02857_row470_col5, #T_02857_row470_col6, #T_02857_row471_col5, #T_02857_row471_col6, #T_02857_row472_col4, #T_02857_row473_col4, #T_02857_row581_col3, #T_02857_row581_col4, #T_02857_row582_col3, #T_02857_row582_col4 {\n",
              "  background-color: #8fb4d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row140_col4, #T_02857_row141_col4, #T_02857_row148_col3, #T_02857_row149_col3, #T_02857_row232_col4, #T_02857_row233_col4, #T_02857_row253_col7, #T_02857_row254_col7, #T_02857_row271_col5, #T_02857_row272_col5, #T_02857_row364_col5, #T_02857_row365_col5, #T_02857_row410_col9, #T_02857_row411_col9, #T_02857_row426_col8, #T_02857_row427_col8, #T_02857_row444_col8, #T_02857_row445_col8, #T_02857_row462_col6, #T_02857_row463_col6, #T_02857_row470_col3, #T_02857_row471_col3 {\n",
              "  background-color: #88b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row140_col5, #T_02857_row141_col5, #T_02857_row160_col4, #T_02857_row161_col4, #T_02857_row210_col5, #T_02857_row211_col5, #T_02857_row253_col4, #T_02857_row254_col4, #T_02857_row265_col4, #T_02857_row266_col4, #T_02857_row355_col5, #T_02857_row356_col5, #T_02857_row370_col4, #T_02857_row370_col7, #T_02857_row371_col4, #T_02857_row371_col7, #T_02857_row390_col6, #T_02857_row391_col6, #T_02857_row392_col6, #T_02857_row393_col6, #T_02857_row400_col5, #T_02857_row401_col5, #T_02857_row402_col3, #T_02857_row403_col3, #T_02857_row404_col6, #T_02857_row405_col6, #T_02857_row448_col6, #T_02857_row449_col6, #T_02857_row495_col3, #T_02857_row496_col3, #T_02857_row511_col4, #T_02857_row512_col4, #T_02857_row573_col4, #T_02857_row574_col4 {\n",
              "  background-color: #94b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row148_col4, #T_02857_row149_col4, #T_02857_row364_col4, #T_02857_row365_col4, #T_02857_row376_col7, #T_02857_row377_col7, #T_02857_row386_col3, #T_02857_row386_col4, #T_02857_row387_col3, #T_02857_row387_col4, #T_02857_row392_col4, #T_02857_row393_col4, #T_02857_row400_col4, #T_02857_row401_col4, #T_02857_row448_col5, #T_02857_row449_col5, #T_02857_row462_col4, #T_02857_row463_col4, #T_02857_row474_col6, #T_02857_row475_col6, #T_02857_row497_col4, #T_02857_row498_col4 {\n",
              "  background-color: #83afd3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row152_col4, #T_02857_row153_col4, #T_02857_row224_col4, #T_02857_row225_col4, #T_02857_row351_col6, #T_02857_row352_col6, #T_02857_row376_col5, #T_02857_row377_col5, #T_02857_row378_col3, #T_02857_row379_col3, #T_02857_row408_col9, #T_02857_row409_col9, #T_02857_row416_col9, #T_02857_row417_col9, #T_02857_row474_col5, #T_02857_row475_col5, #T_02857_row497_col3, #T_02857_row498_col3 {\n",
              "  background-color: #89b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row154_col3, #T_02857_row155_col3, #T_02857_row202_col8, #T_02857_row203_col8, #T_02857_row390_col5, #T_02857_row391_col5, #T_02857_row398_col3, #T_02857_row399_col3, #T_02857_row406_col9, #T_02857_row407_col9, #T_02857_row428_col9, #T_02857_row429_col9, #T_02857_row438_col8, #T_02857_row439_col8, #T_02857_row448_col3, #T_02857_row449_col3, #T_02857_row454_col4, #T_02857_row455_col4, #T_02857_row511_col3, #T_02857_row512_col3, #T_02857_row567_col4, #T_02857_row568_col4 {\n",
              "  background-color: #8cb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row158_col11, #T_02857_row159_col11, #T_02857_row331_col11, #T_02857_row332_col11 {\n",
              "  background-color: #eee8f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row196_col8, #T_02857_row197_col8, #T_02857_row257_col7, #T_02857_row258_col7, #T_02857_row267_col4, #T_02857_row268_col4, #T_02857_row376_col4, #T_02857_row377_col4, #T_02857_row428_col8, #T_02857_row429_col8, #T_02857_row432_col10, #T_02857_row433_col10, #T_02857_row474_col3, #T_02857_row475_col3, #T_02857_row509_col3, #T_02857_row510_col3 {\n",
              "  background-color: #84b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row204_col8, #T_02857_row205_col8, #T_02857_row222_col4, #T_02857_row223_col4, #T_02857_row271_col3, #T_02857_row272_col3, #T_02857_row313_col8, #T_02857_row314_col8, #T_02857_row434_col8, #T_02857_row435_col8, #T_02857_row474_col4, #T_02857_row475_col4, #T_02857_row509_col4, #T_02857_row510_col4 {\n",
              "  background-color: #81aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row261_col11, #T_02857_row262_col11, #T_02857_row319_col11, #T_02857_row320_col11 {\n",
              "  background-color: #ede8f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row271_col4, #T_02857_row272_col4, #T_02857_row285_col3, #T_02857_row286_col3, #T_02857_row378_col4, #T_02857_row379_col4, #T_02857_row392_col3, #T_02857_row393_col3, #T_02857_row440_col8, #T_02857_row441_col8 {\n",
              "  background-color: #7eadd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row277_col11, #T_02857_row278_col11 {\n",
              "  background-color: #ece7f2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row285_col4, #T_02857_row286_col4, #T_02857_row390_col3, #T_02857_row391_col3, #T_02857_row400_col3, #T_02857_row401_col3, #T_02857_row448_col4, #T_02857_row449_col4 {\n",
              "  background-color: #80aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row301_col11, #T_02857_row302_col11, #T_02857_row309_col11, #T_02857_row310_col11 {\n",
              "  background-color: #e6e2ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row305_col11, #T_02857_row306_col11 {\n",
              "  background-color: #e8e4f0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row315_col11, #T_02857_row316_col11, #T_02857_row501_col11, #T_02857_row502_col11, #T_02857_row569_col11, #T_02857_row570_col11 {\n",
              "  background-color: #f2ecf5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row327_col11, #T_02857_row328_col11, #T_02857_row333_col11, #T_02857_row334_col11 {\n",
              "  background-color: #eee9f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row390_col4, #T_02857_row391_col4, #T_02857_row432_col8, #T_02857_row433_col8, #T_02857_row446_col8, #T_02857_row447_col8 {\n",
              "  background-color: #7bacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row404_col3, #T_02857_row405_col3 {\n",
              "  background-color: #79abd0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row404_col4, #T_02857_row405_col4, #T_02857_row442_col8, #T_02857_row443_col8 {\n",
              "  background-color: #7dacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_02857_row515_col11, #T_02857_row516_col11 {\n",
              "  background-color: #f1ebf5;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_02857\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_02857_level0_col0\" class=\"col_heading level0 col0\" >Train Language</th>\n",
              "      <th id=\"T_02857_level0_col1\" class=\"col_heading level0 col1\" >Train LLM</th>\n",
              "      <th id=\"T_02857_level0_col2\" class=\"col_heading level0 col2\" >Model</th>\n",
              "      <th id=\"T_02857_level0_col3\" class=\"col_heading level0 col3\" >gpt-4</th>\n",
              "      <th id=\"T_02857_level0_col4\" class=\"col_heading level0 col4\" >gpt-3.5-turbo</th>\n",
              "      <th id=\"T_02857_level0_col5\" class=\"col_heading level0 col5\" >text-davinci-003</th>\n",
              "      <th id=\"T_02857_level0_col6\" class=\"col_heading level0 col6\" >vicuna-13b</th>\n",
              "      <th id=\"T_02857_level0_col7\" class=\"col_heading level0 col7\" >alpaca-lora-30b</th>\n",
              "      <th id=\"T_02857_level0_col8\" class=\"col_heading level0 col8\" >opt-iml-max-1.3b</th>\n",
              "      <th id=\"T_02857_level0_col9\" class=\"col_heading level0 col9\" >llama-65b</th>\n",
              "      <th id=\"T_02857_level0_col10\" class=\"col_heading level0 col10\" >opt-66b</th>\n",
              "      <th id=\"T_02857_level0_col11\" class=\"col_heading level0 col11\" >all</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_02857_row0_col0\" class=\"data row0 col0\" >en</td>\n",
              "      <td id=\"T_02857_row0_col1\" class=\"data row0 col1\" >all</td>\n",
              "      <td id=\"T_02857_row0_col2\" class=\"data row0 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row0_col3\" class=\"data row0 col3\" >0.6131</td>\n",
              "      <td id=\"T_02857_row0_col4\" class=\"data row0 col4\" >0.6136</td>\n",
              "      <td id=\"T_02857_row0_col5\" class=\"data row0 col5\" >0.6056</td>\n",
              "      <td id=\"T_02857_row0_col6\" class=\"data row0 col6\" >0.6044</td>\n",
              "      <td id=\"T_02857_row0_col7\" class=\"data row0 col7\" >0.5911</td>\n",
              "      <td id=\"T_02857_row0_col8\" class=\"data row0 col8\" >0.5216</td>\n",
              "      <td id=\"T_02857_row0_col9\" class=\"data row0 col9\" >0.6052</td>\n",
              "      <td id=\"T_02857_row0_col10\" class=\"data row0 col10\" >0.5457</td>\n",
              "      <td id=\"T_02857_row0_col11\" class=\"data row0 col11\" >0.6283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_02857_row1_col0\" class=\"data row1 col0\" >en</td>\n",
              "      <td id=\"T_02857_row1_col1\" class=\"data row1 col1\" >all</td>\n",
              "      <td id=\"T_02857_row1_col2\" class=\"data row1 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row1_col3\" class=\"data row1 col3\" >0.5720</td>\n",
              "      <td id=\"T_02857_row1_col4\" class=\"data row1 col4\" >0.5249</td>\n",
              "      <td id=\"T_02857_row1_col5\" class=\"data row1 col5\" >0.4995</td>\n",
              "      <td id=\"T_02857_row1_col6\" class=\"data row1 col6\" >0.5699</td>\n",
              "      <td id=\"T_02857_row1_col7\" class=\"data row1 col7\" >0.5244</td>\n",
              "      <td id=\"T_02857_row1_col8\" class=\"data row1 col8\" >0.5978</td>\n",
              "      <td id=\"T_02857_row1_col9\" class=\"data row1 col9\" >0.6275</td>\n",
              "      <td id=\"T_02857_row1_col10\" class=\"data row1 col10\" >0.5608</td>\n",
              "      <td id=\"T_02857_row1_col11\" class=\"data row1 col11\" >0.5559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_02857_row2_col0\" class=\"data row2 col0\" >en</td>\n",
              "      <td id=\"T_02857_row2_col1\" class=\"data row2 col1\" >all</td>\n",
              "      <td id=\"T_02857_row2_col2\" class=\"data row2 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row2_col3\" class=\"data row2 col3\" >0.4900</td>\n",
              "      <td id=\"T_02857_row2_col4\" class=\"data row2 col4\" >0.4872</td>\n",
              "      <td id=\"T_02857_row2_col5\" class=\"data row2 col5\" >0.4860</td>\n",
              "      <td id=\"T_02857_row2_col6\" class=\"data row2 col6\" >0.5357</td>\n",
              "      <td id=\"T_02857_row2_col7\" class=\"data row2 col7\" >0.4720</td>\n",
              "      <td id=\"T_02857_row2_col8\" class=\"data row2 col8\" >0.5678</td>\n",
              "      <td id=\"T_02857_row2_col9\" class=\"data row2 col9\" >0.6341</td>\n",
              "      <td id=\"T_02857_row2_col10\" class=\"data row2 col10\" >0.5553</td>\n",
              "      <td id=\"T_02857_row2_col11\" class=\"data row2 col11\" >0.4849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_02857_row3_col0\" class=\"data row3 col0\" >en</td>\n",
              "      <td id=\"T_02857_row3_col1\" class=\"data row3 col1\" >all</td>\n",
              "      <td id=\"T_02857_row3_col2\" class=\"data row3 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row3_col3\" class=\"data row3 col3\" >0.5180</td>\n",
              "      <td id=\"T_02857_row3_col4\" class=\"data row3 col4\" >0.5193</td>\n",
              "      <td id=\"T_02857_row3_col5\" class=\"data row3 col5\" >0.5151</td>\n",
              "      <td id=\"T_02857_row3_col6\" class=\"data row3 col6\" >0.5218</td>\n",
              "      <td id=\"T_02857_row3_col7\" class=\"data row3 col7\" >0.5126</td>\n",
              "      <td id=\"T_02857_row3_col8\" class=\"data row3 col8\" >0.4558</td>\n",
              "      <td id=\"T_02857_row3_col9\" class=\"data row3 col9\" >0.5237</td>\n",
              "      <td id=\"T_02857_row3_col10\" class=\"data row3 col10\" >0.4964</td>\n",
              "      <td id=\"T_02857_row3_col11\" class=\"data row3 col11\" >0.5727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_02857_row4_col0\" class=\"data row4 col0\" >en</td>\n",
              "      <td id=\"T_02857_row4_col1\" class=\"data row4 col1\" >all</td>\n",
              "      <td id=\"T_02857_row4_col2\" class=\"data row4 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row4_col3\" class=\"data row4 col3\" >0.5759</td>\n",
              "      <td id=\"T_02857_row4_col4\" class=\"data row4 col4\" >0.5751</td>\n",
              "      <td id=\"T_02857_row4_col5\" class=\"data row4 col5\" >0.5663</td>\n",
              "      <td id=\"T_02857_row4_col6\" class=\"data row4 col6\" >0.5686</td>\n",
              "      <td id=\"T_02857_row4_col7\" class=\"data row4 col7\" >0.5437</td>\n",
              "      <td id=\"T_02857_row4_col8\" class=\"data row4 col8\" >0.5099</td>\n",
              "      <td id=\"T_02857_row4_col9\" class=\"data row4 col9\" >0.5683</td>\n",
              "      <td id=\"T_02857_row4_col10\" class=\"data row4 col10\" >0.5203</td>\n",
              "      <td id=\"T_02857_row4_col11\" class=\"data row4 col11\" >0.6148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_02857_row5_col0\" class=\"data row5 col0\" >en</td>\n",
              "      <td id=\"T_02857_row5_col1\" class=\"data row5 col1\" >all</td>\n",
              "      <td id=\"T_02857_row5_col2\" class=\"data row5 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row5_col3\" class=\"data row5 col3\" >0.4271</td>\n",
              "      <td id=\"T_02857_row5_col4\" class=\"data row5 col4\" >0.4273</td>\n",
              "      <td id=\"T_02857_row5_col5\" class=\"data row5 col5\" >0.4272</td>\n",
              "      <td id=\"T_02857_row5_col6\" class=\"data row5 col6\" >0.4261</td>\n",
              "      <td id=\"T_02857_row5_col7\" class=\"data row5 col7\" >0.4269</td>\n",
              "      <td id=\"T_02857_row5_col8\" class=\"data row5 col8\" >0.4247</td>\n",
              "      <td id=\"T_02857_row5_col9\" class=\"data row5 col9\" >0.4249</td>\n",
              "      <td id=\"T_02857_row5_col10\" class=\"data row5 col10\" >0.4259</td>\n",
              "      <td id=\"T_02857_row5_col11\" class=\"data row5 col11\" >0.5541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_02857_row6_col0\" class=\"data row6 col0\" >en</td>\n",
              "      <td id=\"T_02857_row6_col1\" class=\"data row6 col1\" >all</td>\n",
              "      <td id=\"T_02857_row6_col2\" class=\"data row6 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row6_col3\" class=\"data row6 col3\" >0.4479</td>\n",
              "      <td id=\"T_02857_row6_col4\" class=\"data row6 col4\" >0.4474</td>\n",
              "      <td id=\"T_02857_row6_col5\" class=\"data row6 col5\" >0.4479</td>\n",
              "      <td id=\"T_02857_row6_col6\" class=\"data row6 col6\" >0.4478</td>\n",
              "      <td id=\"T_02857_row6_col7\" class=\"data row6 col7\" >0.4476</td>\n",
              "      <td id=\"T_02857_row6_col8\" class=\"data row6 col8\" >0.4406</td>\n",
              "      <td id=\"T_02857_row6_col9\" class=\"data row6 col9\" >0.4455</td>\n",
              "      <td id=\"T_02857_row6_col10\" class=\"data row6 col10\" >0.4379</td>\n",
              "      <td id=\"T_02857_row6_col11\" class=\"data row6 col11\" >0.5679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_02857_row7_col0\" class=\"data row7 col0\" >en</td>\n",
              "      <td id=\"T_02857_row7_col1\" class=\"data row7 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row7_col2\" class=\"data row7 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row7_col3\" class=\"data row7 col3\" >0.7964</td>\n",
              "      <td id=\"T_02857_row7_col4\" class=\"data row7 col4\" >0.8237</td>\n",
              "      <td id=\"T_02857_row7_col5\" class=\"data row7 col5\" >0.8102</td>\n",
              "      <td id=\"T_02857_row7_col6\" class=\"data row7 col6\" >0.7622</td>\n",
              "      <td id=\"T_02857_row7_col7\" class=\"data row7 col7\" >0.7889</td>\n",
              "      <td id=\"T_02857_row7_col8\" class=\"data row7 col8\" >0.4863</td>\n",
              "      <td id=\"T_02857_row7_col9\" class=\"data row7 col9\" >0.4336</td>\n",
              "      <td id=\"T_02857_row7_col10\" class=\"data row7 col10\" >0.4315</td>\n",
              "      <td id=\"T_02857_row7_col11\" class=\"data row7 col11\" >0.5401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "      <td id=\"T_02857_row8_col0\" class=\"data row8 col0\" >en</td>\n",
              "      <td id=\"T_02857_row8_col1\" class=\"data row8 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row8_col2\" class=\"data row8 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row8_col3\" class=\"data row8 col3\" >0.7964</td>\n",
              "      <td id=\"T_02857_row8_col4\" class=\"data row8 col4\" >0.8237</td>\n",
              "      <td id=\"T_02857_row8_col5\" class=\"data row8 col5\" >0.8102</td>\n",
              "      <td id=\"T_02857_row8_col6\" class=\"data row8 col6\" >0.7622</td>\n",
              "      <td id=\"T_02857_row8_col7\" class=\"data row8 col7\" >0.7889</td>\n",
              "      <td id=\"T_02857_row8_col8\" class=\"data row8 col8\" >0.4863</td>\n",
              "      <td id=\"T_02857_row8_col9\" class=\"data row8 col9\" >0.4336</td>\n",
              "      <td id=\"T_02857_row8_col10\" class=\"data row8 col10\" >0.4315</td>\n",
              "      <td id=\"T_02857_row8_col11\" class=\"data row8 col11\" >0.5401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "      <td id=\"T_02857_row9_col0\" class=\"data row9 col0\" >en</td>\n",
              "      <td id=\"T_02857_row9_col1\" class=\"data row9 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row9_col2\" class=\"data row9 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row9_col3\" class=\"data row9 col3\" >0.5001</td>\n",
              "      <td id=\"T_02857_row9_col4\" class=\"data row9 col4\" >0.5243</td>\n",
              "      <td id=\"T_02857_row9_col5\" class=\"data row9 col5\" >0.5356</td>\n",
              "      <td id=\"T_02857_row9_col6\" class=\"data row9 col6\" >0.5460</td>\n",
              "      <td id=\"T_02857_row9_col7\" class=\"data row9 col7\" >0.5698</td>\n",
              "      <td id=\"T_02857_row9_col8\" class=\"data row9 col8\" >0.5168</td>\n",
              "      <td id=\"T_02857_row9_col9\" class=\"data row9 col9\" >0.4060</td>\n",
              "      <td id=\"T_02857_row9_col10\" class=\"data row9 col10\" >0.4595</td>\n",
              "      <td id=\"T_02857_row9_col11\" class=\"data row9 col11\" >0.3148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "      <td id=\"T_02857_row10_col0\" class=\"data row10 col0\" >en</td>\n",
              "      <td id=\"T_02857_row10_col1\" class=\"data row10 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row10_col2\" class=\"data row10 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row10_col3\" class=\"data row10 col3\" >0.5001</td>\n",
              "      <td id=\"T_02857_row10_col4\" class=\"data row10 col4\" >0.5243</td>\n",
              "      <td id=\"T_02857_row10_col5\" class=\"data row10 col5\" >0.5356</td>\n",
              "      <td id=\"T_02857_row10_col6\" class=\"data row10 col6\" >0.5460</td>\n",
              "      <td id=\"T_02857_row10_col7\" class=\"data row10 col7\" >0.5698</td>\n",
              "      <td id=\"T_02857_row10_col8\" class=\"data row10 col8\" >0.5168</td>\n",
              "      <td id=\"T_02857_row10_col9\" class=\"data row10 col9\" >0.4060</td>\n",
              "      <td id=\"T_02857_row10_col10\" class=\"data row10 col10\" >0.4595</td>\n",
              "      <td id=\"T_02857_row10_col11\" class=\"data row10 col11\" >0.3148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
              "      <td id=\"T_02857_row11_col0\" class=\"data row11 col0\" >en</td>\n",
              "      <td id=\"T_02857_row11_col1\" class=\"data row11 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row11_col2\" class=\"data row11 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row11_col3\" class=\"data row11 col3\" >0.4438</td>\n",
              "      <td id=\"T_02857_row11_col4\" class=\"data row11 col4\" >0.4589</td>\n",
              "      <td id=\"T_02857_row11_col5\" class=\"data row11 col5\" >0.5267</td>\n",
              "      <td id=\"T_02857_row11_col6\" class=\"data row11 col6\" >0.4768</td>\n",
              "      <td id=\"T_02857_row11_col7\" class=\"data row11 col7\" >0.5426</td>\n",
              "      <td id=\"T_02857_row11_col8\" class=\"data row11 col8\" >0.4939</td>\n",
              "      <td id=\"T_02857_row11_col9\" class=\"data row11 col9\" >0.5626</td>\n",
              "      <td id=\"T_02857_row11_col10\" class=\"data row11 col10\" >0.4401</td>\n",
              "      <td id=\"T_02857_row11_col11\" class=\"data row11 col11\" >0.2790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
              "      <td id=\"T_02857_row12_col0\" class=\"data row12 col0\" >en</td>\n",
              "      <td id=\"T_02857_row12_col1\" class=\"data row12 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row12_col2\" class=\"data row12 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row12_col3\" class=\"data row12 col3\" >0.4438</td>\n",
              "      <td id=\"T_02857_row12_col4\" class=\"data row12 col4\" >0.4589</td>\n",
              "      <td id=\"T_02857_row12_col5\" class=\"data row12 col5\" >0.5267</td>\n",
              "      <td id=\"T_02857_row12_col6\" class=\"data row12 col6\" >0.4768</td>\n",
              "      <td id=\"T_02857_row12_col7\" class=\"data row12 col7\" >0.5426</td>\n",
              "      <td id=\"T_02857_row12_col8\" class=\"data row12 col8\" >0.4939</td>\n",
              "      <td id=\"T_02857_row12_col9\" class=\"data row12 col9\" >0.5626</td>\n",
              "      <td id=\"T_02857_row12_col10\" class=\"data row12 col10\" >0.4401</td>\n",
              "      <td id=\"T_02857_row12_col11\" class=\"data row12 col11\" >0.2790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
              "      <td id=\"T_02857_row13_col0\" class=\"data row13 col0\" >en</td>\n",
              "      <td id=\"T_02857_row13_col1\" class=\"data row13 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row13_col2\" class=\"data row13 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row13_col3\" class=\"data row13 col3\" >0.7437</td>\n",
              "      <td id=\"T_02857_row13_col4\" class=\"data row13 col4\" >0.8022</td>\n",
              "      <td id=\"T_02857_row13_col5\" class=\"data row13 col5\" >0.7656</td>\n",
              "      <td id=\"T_02857_row13_col6\" class=\"data row13 col6\" >0.7396</td>\n",
              "      <td id=\"T_02857_row13_col7\" class=\"data row13 col7\" >0.7556</td>\n",
              "      <td id=\"T_02857_row13_col8\" class=\"data row13 col8\" >0.3624</td>\n",
              "      <td id=\"T_02857_row13_col9\" class=\"data row13 col9\" >0.4427</td>\n",
              "      <td id=\"T_02857_row13_col10\" class=\"data row13 col10\" >0.4386</td>\n",
              "      <td id=\"T_02857_row13_col11\" class=\"data row13 col11\" >0.4981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
              "      <td id=\"T_02857_row14_col0\" class=\"data row14 col0\" >en</td>\n",
              "      <td id=\"T_02857_row14_col1\" class=\"data row14 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row14_col2\" class=\"data row14 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row14_col3\" class=\"data row14 col3\" >0.7437</td>\n",
              "      <td id=\"T_02857_row14_col4\" class=\"data row14 col4\" >0.8022</td>\n",
              "      <td id=\"T_02857_row14_col5\" class=\"data row14 col5\" >0.7656</td>\n",
              "      <td id=\"T_02857_row14_col6\" class=\"data row14 col6\" >0.7396</td>\n",
              "      <td id=\"T_02857_row14_col7\" class=\"data row14 col7\" >0.7556</td>\n",
              "      <td id=\"T_02857_row14_col8\" class=\"data row14 col8\" >0.3624</td>\n",
              "      <td id=\"T_02857_row14_col9\" class=\"data row14 col9\" >0.4427</td>\n",
              "      <td id=\"T_02857_row14_col10\" class=\"data row14 col10\" >0.4386</td>\n",
              "      <td id=\"T_02857_row14_col11\" class=\"data row14 col11\" >0.4981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
              "      <td id=\"T_02857_row15_col0\" class=\"data row15 col0\" >en</td>\n",
              "      <td id=\"T_02857_row15_col1\" class=\"data row15 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row15_col2\" class=\"data row15 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row15_col3\" class=\"data row15 col3\" >0.8242</td>\n",
              "      <td id=\"T_02857_row15_col4\" class=\"data row15 col4\" >0.8118</td>\n",
              "      <td id=\"T_02857_row15_col5\" class=\"data row15 col5\" >0.7795</td>\n",
              "      <td id=\"T_02857_row15_col6\" class=\"data row15 col6\" >0.7404</td>\n",
              "      <td id=\"T_02857_row15_col7\" class=\"data row15 col7\" >0.7428</td>\n",
              "      <td id=\"T_02857_row15_col8\" class=\"data row15 col8\" >0.3175</td>\n",
              "      <td id=\"T_02857_row15_col9\" class=\"data row15 col9\" >0.3676</td>\n",
              "      <td id=\"T_02857_row15_col10\" class=\"data row15 col10\" >0.3285</td>\n",
              "      <td id=\"T_02857_row15_col11\" class=\"data row15 col11\" >0.4803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
              "      <td id=\"T_02857_row16_col0\" class=\"data row16 col0\" >en</td>\n",
              "      <td id=\"T_02857_row16_col1\" class=\"data row16 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row16_col2\" class=\"data row16 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row16_col3\" class=\"data row16 col3\" >0.8242</td>\n",
              "      <td id=\"T_02857_row16_col4\" class=\"data row16 col4\" >0.8118</td>\n",
              "      <td id=\"T_02857_row16_col5\" class=\"data row16 col5\" >0.7795</td>\n",
              "      <td id=\"T_02857_row16_col6\" class=\"data row16 col6\" >0.7404</td>\n",
              "      <td id=\"T_02857_row16_col7\" class=\"data row16 col7\" >0.7428</td>\n",
              "      <td id=\"T_02857_row16_col8\" class=\"data row16 col8\" >0.3175</td>\n",
              "      <td id=\"T_02857_row16_col9\" class=\"data row16 col9\" >0.3676</td>\n",
              "      <td id=\"T_02857_row16_col10\" class=\"data row16 col10\" >0.3285</td>\n",
              "      <td id=\"T_02857_row16_col11\" class=\"data row16 col11\" >0.4803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
              "      <td id=\"T_02857_row17_col0\" class=\"data row17 col0\" >en</td>\n",
              "      <td id=\"T_02857_row17_col1\" class=\"data row17 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row17_col2\" class=\"data row17 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row17_col3\" class=\"data row17 col3\" >0.5646</td>\n",
              "      <td id=\"T_02857_row17_col4\" class=\"data row17 col4\" >0.5660</td>\n",
              "      <td id=\"T_02857_row17_col5\" class=\"data row17 col5\" >0.5663</td>\n",
              "      <td id=\"T_02857_row17_col6\" class=\"data row17 col6\" >0.5556</td>\n",
              "      <td id=\"T_02857_row17_col7\" class=\"data row17 col7\" >0.5627</td>\n",
              "      <td id=\"T_02857_row17_col8\" class=\"data row17 col8\" >0.5061</td>\n",
              "      <td id=\"T_02857_row17_col9\" class=\"data row17 col9\" >0.4595</td>\n",
              "      <td id=\"T_02857_row17_col10\" class=\"data row17 col10\" >0.5272</td>\n",
              "      <td id=\"T_02857_row17_col11\" class=\"data row17 col11\" >0.5940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
              "      <td id=\"T_02857_row18_col0\" class=\"data row18 col0\" >en</td>\n",
              "      <td id=\"T_02857_row18_col1\" class=\"data row18 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row18_col2\" class=\"data row18 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row18_col3\" class=\"data row18 col3\" >0.5646</td>\n",
              "      <td id=\"T_02857_row18_col4\" class=\"data row18 col4\" >0.5660</td>\n",
              "      <td id=\"T_02857_row18_col5\" class=\"data row18 col5\" >0.5663</td>\n",
              "      <td id=\"T_02857_row18_col6\" class=\"data row18 col6\" >0.5556</td>\n",
              "      <td id=\"T_02857_row18_col7\" class=\"data row18 col7\" >0.5627</td>\n",
              "      <td id=\"T_02857_row18_col8\" class=\"data row18 col8\" >0.5061</td>\n",
              "      <td id=\"T_02857_row18_col9\" class=\"data row18 col9\" >0.4595</td>\n",
              "      <td id=\"T_02857_row18_col10\" class=\"data row18 col10\" >0.5272</td>\n",
              "      <td id=\"T_02857_row18_col11\" class=\"data row18 col11\" >0.5940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
              "      <td id=\"T_02857_row19_col0\" class=\"data row19 col0\" >en</td>\n",
              "      <td id=\"T_02857_row19_col1\" class=\"data row19 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row19_col2\" class=\"data row19 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row19_col3\" class=\"data row19 col3\" >0.8029</td>\n",
              "      <td id=\"T_02857_row19_col4\" class=\"data row19 col4\" >0.8264</td>\n",
              "      <td id=\"T_02857_row19_col5\" class=\"data row19 col5\" >0.7940</td>\n",
              "      <td id=\"T_02857_row19_col6\" class=\"data row19 col6\" >0.7542</td>\n",
              "      <td id=\"T_02857_row19_col7\" class=\"data row19 col7\" >0.7643</td>\n",
              "      <td id=\"T_02857_row19_col8\" class=\"data row19 col8\" >0.5107</td>\n",
              "      <td id=\"T_02857_row19_col9\" class=\"data row19 col9\" >0.4912</td>\n",
              "      <td id=\"T_02857_row19_col10\" class=\"data row19 col10\" >0.4660</td>\n",
              "      <td id=\"T_02857_row19_col11\" class=\"data row19 col11\" >0.5526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
              "      <td id=\"T_02857_row20_col0\" class=\"data row20 col0\" >en</td>\n",
              "      <td id=\"T_02857_row20_col1\" class=\"data row20 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row20_col2\" class=\"data row20 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row20_col3\" class=\"data row20 col3\" >0.8029</td>\n",
              "      <td id=\"T_02857_row20_col4\" class=\"data row20 col4\" >0.8264</td>\n",
              "      <td id=\"T_02857_row20_col5\" class=\"data row20 col5\" >0.7940</td>\n",
              "      <td id=\"T_02857_row20_col6\" class=\"data row20 col6\" >0.7542</td>\n",
              "      <td id=\"T_02857_row20_col7\" class=\"data row20 col7\" >0.7643</td>\n",
              "      <td id=\"T_02857_row20_col8\" class=\"data row20 col8\" >0.5107</td>\n",
              "      <td id=\"T_02857_row20_col9\" class=\"data row20 col9\" >0.4912</td>\n",
              "      <td id=\"T_02857_row20_col10\" class=\"data row20 col10\" >0.4660</td>\n",
              "      <td id=\"T_02857_row20_col11\" class=\"data row20 col11\" >0.5526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
              "      <td id=\"T_02857_row21_col0\" class=\"data row21 col0\" >en</td>\n",
              "      <td id=\"T_02857_row21_col1\" class=\"data row21 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row21_col2\" class=\"data row21 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row21_col3\" class=\"data row21 col3\" >0.8841</td>\n",
              "      <td id=\"T_02857_row21_col4\" class=\"data row21 col4\" >0.9106</td>\n",
              "      <td id=\"T_02857_row21_col5\" class=\"data row21 col5\" >0.8512</td>\n",
              "      <td id=\"T_02857_row21_col6\" class=\"data row21 col6\" >0.8281</td>\n",
              "      <td id=\"T_02857_row21_col7\" class=\"data row21 col7\" >0.7797</td>\n",
              "      <td id=\"T_02857_row21_col8\" class=\"data row21 col8\" >0.3959</td>\n",
              "      <td id=\"T_02857_row21_col9\" class=\"data row21 col9\" >0.3758</td>\n",
              "      <td id=\"T_02857_row21_col10\" class=\"data row21 col10\" >0.4023</td>\n",
              "      <td id=\"T_02857_row21_col11\" class=\"data row21 col11\" >0.5103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
              "      <td id=\"T_02857_row22_col0\" class=\"data row22 col0\" >en</td>\n",
              "      <td id=\"T_02857_row22_col1\" class=\"data row22 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row22_col2\" class=\"data row22 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row22_col3\" class=\"data row22 col3\" >0.8841</td>\n",
              "      <td id=\"T_02857_row22_col4\" class=\"data row22 col4\" >0.9106</td>\n",
              "      <td id=\"T_02857_row22_col5\" class=\"data row22 col5\" >0.8512</td>\n",
              "      <td id=\"T_02857_row22_col6\" class=\"data row22 col6\" >0.8281</td>\n",
              "      <td id=\"T_02857_row22_col7\" class=\"data row22 col7\" >0.7797</td>\n",
              "      <td id=\"T_02857_row22_col8\" class=\"data row22 col8\" >0.3959</td>\n",
              "      <td id=\"T_02857_row22_col9\" class=\"data row22 col9\" >0.3758</td>\n",
              "      <td id=\"T_02857_row22_col10\" class=\"data row22 col10\" >0.4023</td>\n",
              "      <td id=\"T_02857_row22_col11\" class=\"data row22 col11\" >0.5103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
              "      <td id=\"T_02857_row23_col0\" class=\"data row23 col0\" >en</td>\n",
              "      <td id=\"T_02857_row23_col1\" class=\"data row23 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row23_col2\" class=\"data row23 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row23_col3\" class=\"data row23 col3\" >0.5879</td>\n",
              "      <td id=\"T_02857_row23_col4\" class=\"data row23 col4\" >0.6052</td>\n",
              "      <td id=\"T_02857_row23_col5\" class=\"data row23 col5\" >0.5937</td>\n",
              "      <td id=\"T_02857_row23_col6\" class=\"data row23 col6\" >0.6087</td>\n",
              "      <td id=\"T_02857_row23_col7\" class=\"data row23 col7\" >0.6016</td>\n",
              "      <td id=\"T_02857_row23_col8\" class=\"data row23 col8\" >0.4864</td>\n",
              "      <td id=\"T_02857_row23_col9\" class=\"data row23 col9\" >0.4381</td>\n",
              "      <td id=\"T_02857_row23_col10\" class=\"data row23 col10\" >0.4511</td>\n",
              "      <td id=\"T_02857_row23_col11\" class=\"data row23 col11\" >0.3503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
              "      <td id=\"T_02857_row24_col0\" class=\"data row24 col0\" >en</td>\n",
              "      <td id=\"T_02857_row24_col1\" class=\"data row24 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row24_col2\" class=\"data row24 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row24_col3\" class=\"data row24 col3\" >0.5879</td>\n",
              "      <td id=\"T_02857_row24_col4\" class=\"data row24 col4\" >0.6052</td>\n",
              "      <td id=\"T_02857_row24_col5\" class=\"data row24 col5\" >0.5937</td>\n",
              "      <td id=\"T_02857_row24_col6\" class=\"data row24 col6\" >0.6087</td>\n",
              "      <td id=\"T_02857_row24_col7\" class=\"data row24 col7\" >0.6016</td>\n",
              "      <td id=\"T_02857_row24_col8\" class=\"data row24 col8\" >0.4864</td>\n",
              "      <td id=\"T_02857_row24_col9\" class=\"data row24 col9\" >0.4381</td>\n",
              "      <td id=\"T_02857_row24_col10\" class=\"data row24 col10\" >0.4511</td>\n",
              "      <td id=\"T_02857_row24_col11\" class=\"data row24 col11\" >0.3503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
              "      <td id=\"T_02857_row25_col0\" class=\"data row25 col0\" >en</td>\n",
              "      <td id=\"T_02857_row25_col1\" class=\"data row25 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row25_col2\" class=\"data row25 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row25_col3\" class=\"data row25 col3\" >0.4382</td>\n",
              "      <td id=\"T_02857_row25_col4\" class=\"data row25 col4\" >0.4302</td>\n",
              "      <td id=\"T_02857_row25_col5\" class=\"data row25 col5\" >0.4147</td>\n",
              "      <td id=\"T_02857_row25_col6\" class=\"data row25 col6\" >0.4532</td>\n",
              "      <td id=\"T_02857_row25_col7\" class=\"data row25 col7\" >0.4215</td>\n",
              "      <td id=\"T_02857_row25_col8\" class=\"data row25 col8\" >0.4162</td>\n",
              "      <td id=\"T_02857_row25_col9\" class=\"data row25 col9\" >0.4995</td>\n",
              "      <td id=\"T_02857_row25_col10\" class=\"data row25 col10\" >0.3672</td>\n",
              "      <td id=\"T_02857_row25_col11\" class=\"data row25 col11\" >0.1975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
              "      <td id=\"T_02857_row26_col0\" class=\"data row26 col0\" >en</td>\n",
              "      <td id=\"T_02857_row26_col1\" class=\"data row26 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row26_col2\" class=\"data row26 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row26_col3\" class=\"data row26 col3\" >0.4382</td>\n",
              "      <td id=\"T_02857_row26_col4\" class=\"data row26 col4\" >0.4302</td>\n",
              "      <td id=\"T_02857_row26_col5\" class=\"data row26 col5\" >0.4147</td>\n",
              "      <td id=\"T_02857_row26_col6\" class=\"data row26 col6\" >0.4532</td>\n",
              "      <td id=\"T_02857_row26_col7\" class=\"data row26 col7\" >0.4215</td>\n",
              "      <td id=\"T_02857_row26_col8\" class=\"data row26 col8\" >0.4162</td>\n",
              "      <td id=\"T_02857_row26_col9\" class=\"data row26 col9\" >0.4995</td>\n",
              "      <td id=\"T_02857_row26_col10\" class=\"data row26 col10\" >0.3672</td>\n",
              "      <td id=\"T_02857_row26_col11\" class=\"data row26 col11\" >0.1975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
              "      <td id=\"T_02857_row27_col0\" class=\"data row27 col0\" >en</td>\n",
              "      <td id=\"T_02857_row27_col1\" class=\"data row27 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row27_col2\" class=\"data row27 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row27_col3\" class=\"data row27 col3\" >0.7858</td>\n",
              "      <td id=\"T_02857_row27_col4\" class=\"data row27 col4\" >0.8286</td>\n",
              "      <td id=\"T_02857_row27_col5\" class=\"data row27 col5\" >0.7572</td>\n",
              "      <td id=\"T_02857_row27_col6\" class=\"data row27 col6\" >0.7503</td>\n",
              "      <td id=\"T_02857_row27_col7\" class=\"data row27 col7\" >0.6841</td>\n",
              "      <td id=\"T_02857_row27_col8\" class=\"data row27 col8\" >0.3457</td>\n",
              "      <td id=\"T_02857_row27_col9\" class=\"data row27 col9\" >0.3726</td>\n",
              "      <td id=\"T_02857_row27_col10\" class=\"data row27 col10\" >0.3879</td>\n",
              "      <td id=\"T_02857_row27_col11\" class=\"data row27 col11\" >0.4459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
              "      <td id=\"T_02857_row28_col0\" class=\"data row28 col0\" >en</td>\n",
              "      <td id=\"T_02857_row28_col1\" class=\"data row28 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row28_col2\" class=\"data row28 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row28_col3\" class=\"data row28 col3\" >0.7858</td>\n",
              "      <td id=\"T_02857_row28_col4\" class=\"data row28 col4\" >0.8286</td>\n",
              "      <td id=\"T_02857_row28_col5\" class=\"data row28 col5\" >0.7572</td>\n",
              "      <td id=\"T_02857_row28_col6\" class=\"data row28 col6\" >0.7503</td>\n",
              "      <td id=\"T_02857_row28_col7\" class=\"data row28 col7\" >0.6841</td>\n",
              "      <td id=\"T_02857_row28_col8\" class=\"data row28 col8\" >0.3457</td>\n",
              "      <td id=\"T_02857_row28_col9\" class=\"data row28 col9\" >0.3726</td>\n",
              "      <td id=\"T_02857_row28_col10\" class=\"data row28 col10\" >0.3879</td>\n",
              "      <td id=\"T_02857_row28_col11\" class=\"data row28 col11\" >0.4459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
              "      <td id=\"T_02857_row29_col0\" class=\"data row29 col0\" >en</td>\n",
              "      <td id=\"T_02857_row29_col1\" class=\"data row29 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row29_col2\" class=\"data row29 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row29_col3\" class=\"data row29 col3\" >0.7313</td>\n",
              "      <td id=\"T_02857_row29_col4\" class=\"data row29 col4\" >0.7136</td>\n",
              "      <td id=\"T_02857_row29_col5\" class=\"data row29 col5\" >0.6966</td>\n",
              "      <td id=\"T_02857_row29_col6\" class=\"data row29 col6\" >0.6704</td>\n",
              "      <td id=\"T_02857_row29_col7\" class=\"data row29 col7\" >0.6586</td>\n",
              "      <td id=\"T_02857_row29_col8\" class=\"data row29 col8\" >0.2729</td>\n",
              "      <td id=\"T_02857_row29_col9\" class=\"data row29 col9\" >0.3248</td>\n",
              "      <td id=\"T_02857_row29_col10\" class=\"data row29 col10\" >0.2977</td>\n",
              "      <td id=\"T_02857_row29_col11\" class=\"data row29 col11\" >0.4377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
              "      <td id=\"T_02857_row30_col0\" class=\"data row30 col0\" >en</td>\n",
              "      <td id=\"T_02857_row30_col1\" class=\"data row30 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row30_col2\" class=\"data row30 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row30_col3\" class=\"data row30 col3\" >0.7313</td>\n",
              "      <td id=\"T_02857_row30_col4\" class=\"data row30 col4\" >0.7136</td>\n",
              "      <td id=\"T_02857_row30_col5\" class=\"data row30 col5\" >0.6966</td>\n",
              "      <td id=\"T_02857_row30_col6\" class=\"data row30 col6\" >0.6704</td>\n",
              "      <td id=\"T_02857_row30_col7\" class=\"data row30 col7\" >0.6586</td>\n",
              "      <td id=\"T_02857_row30_col8\" class=\"data row30 col8\" >0.2729</td>\n",
              "      <td id=\"T_02857_row30_col9\" class=\"data row30 col9\" >0.3248</td>\n",
              "      <td id=\"T_02857_row30_col10\" class=\"data row30 col10\" >0.2977</td>\n",
              "      <td id=\"T_02857_row30_col11\" class=\"data row30 col11\" >0.4377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
              "      <td id=\"T_02857_row31_col0\" class=\"data row31 col0\" >en</td>\n",
              "      <td id=\"T_02857_row31_col1\" class=\"data row31 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row31_col2\" class=\"data row31 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row31_col3\" class=\"data row31 col3\" >0.6105</td>\n",
              "      <td id=\"T_02857_row31_col4\" class=\"data row31 col4\" >0.6141</td>\n",
              "      <td id=\"T_02857_row31_col5\" class=\"data row31 col5\" >0.6167</td>\n",
              "      <td id=\"T_02857_row31_col6\" class=\"data row31 col6\" >0.5940</td>\n",
              "      <td id=\"T_02857_row31_col7\" class=\"data row31 col7\" >0.6013</td>\n",
              "      <td id=\"T_02857_row31_col8\" class=\"data row31 col8\" >0.4714</td>\n",
              "      <td id=\"T_02857_row31_col9\" class=\"data row31 col9\" >0.4211</td>\n",
              "      <td id=\"T_02857_row31_col10\" class=\"data row31 col10\" >0.5461</td>\n",
              "      <td id=\"T_02857_row31_col11\" class=\"data row31 col11\" >0.5604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
              "      <td id=\"T_02857_row32_col0\" class=\"data row32 col0\" >en</td>\n",
              "      <td id=\"T_02857_row32_col1\" class=\"data row32 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row32_col2\" class=\"data row32 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row32_col3\" class=\"data row32 col3\" >0.6105</td>\n",
              "      <td id=\"T_02857_row32_col4\" class=\"data row32 col4\" >0.6141</td>\n",
              "      <td id=\"T_02857_row32_col5\" class=\"data row32 col5\" >0.6167</td>\n",
              "      <td id=\"T_02857_row32_col6\" class=\"data row32 col6\" >0.5940</td>\n",
              "      <td id=\"T_02857_row32_col7\" class=\"data row32 col7\" >0.6013</td>\n",
              "      <td id=\"T_02857_row32_col8\" class=\"data row32 col8\" >0.4714</td>\n",
              "      <td id=\"T_02857_row32_col9\" class=\"data row32 col9\" >0.4211</td>\n",
              "      <td id=\"T_02857_row32_col10\" class=\"data row32 col10\" >0.5461</td>\n",
              "      <td id=\"T_02857_row32_col11\" class=\"data row32 col11\" >0.5604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
              "      <td id=\"T_02857_row33_col0\" class=\"data row33 col0\" >en</td>\n",
              "      <td id=\"T_02857_row33_col1\" class=\"data row33 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row33_col2\" class=\"data row33 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row33_col3\" class=\"data row33 col3\" >0.7862</td>\n",
              "      <td id=\"T_02857_row33_col4\" class=\"data row33 col4\" >0.8308</td>\n",
              "      <td id=\"T_02857_row33_col5\" class=\"data row33 col5\" >0.7198</td>\n",
              "      <td id=\"T_02857_row33_col6\" class=\"data row33 col6\" >0.7215</td>\n",
              "      <td id=\"T_02857_row33_col7\" class=\"data row33 col7\" >0.6829</td>\n",
              "      <td id=\"T_02857_row33_col8\" class=\"data row33 col8\" >0.3881</td>\n",
              "      <td id=\"T_02857_row33_col9\" class=\"data row33 col9\" >0.3967</td>\n",
              "      <td id=\"T_02857_row33_col10\" class=\"data row33 col10\" >0.4055</td>\n",
              "      <td id=\"T_02857_row33_col11\" class=\"data row33 col11\" >0.4189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
              "      <td id=\"T_02857_row34_col0\" class=\"data row34 col0\" >en</td>\n",
              "      <td id=\"T_02857_row34_col1\" class=\"data row34 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row34_col2\" class=\"data row34 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row34_col3\" class=\"data row34 col3\" >0.7862</td>\n",
              "      <td id=\"T_02857_row34_col4\" class=\"data row34 col4\" >0.8308</td>\n",
              "      <td id=\"T_02857_row34_col5\" class=\"data row34 col5\" >0.7198</td>\n",
              "      <td id=\"T_02857_row34_col6\" class=\"data row34 col6\" >0.7215</td>\n",
              "      <td id=\"T_02857_row34_col7\" class=\"data row34 col7\" >0.6829</td>\n",
              "      <td id=\"T_02857_row34_col8\" class=\"data row34 col8\" >0.3881</td>\n",
              "      <td id=\"T_02857_row34_col9\" class=\"data row34 col9\" >0.3967</td>\n",
              "      <td id=\"T_02857_row34_col10\" class=\"data row34 col10\" >0.4055</td>\n",
              "      <td id=\"T_02857_row34_col11\" class=\"data row34 col11\" >0.4189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
              "      <td id=\"T_02857_row35_col0\" class=\"data row35 col0\" >en</td>\n",
              "      <td id=\"T_02857_row35_col1\" class=\"data row35 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row35_col2\" class=\"data row35 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row35_col3\" class=\"data row35 col3\" >0.8458</td>\n",
              "      <td id=\"T_02857_row35_col4\" class=\"data row35 col4\" >0.7954</td>\n",
              "      <td id=\"T_02857_row35_col5\" class=\"data row35 col5\" >0.7090</td>\n",
              "      <td id=\"T_02857_row35_col6\" class=\"data row35 col6\" >0.6632</td>\n",
              "      <td id=\"T_02857_row35_col7\" class=\"data row35 col7\" >0.6428</td>\n",
              "      <td id=\"T_02857_row35_col8\" class=\"data row35 col8\" >0.3585</td>\n",
              "      <td id=\"T_02857_row35_col9\" class=\"data row35 col9\" >0.3744</td>\n",
              "      <td id=\"T_02857_row35_col10\" class=\"data row35 col10\" >0.3749</td>\n",
              "      <td id=\"T_02857_row35_col11\" class=\"data row35 col11\" >0.4027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
              "      <td id=\"T_02857_row36_col0\" class=\"data row36 col0\" >en</td>\n",
              "      <td id=\"T_02857_row36_col1\" class=\"data row36 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row36_col2\" class=\"data row36 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row36_col3\" class=\"data row36 col3\" >0.8458</td>\n",
              "      <td id=\"T_02857_row36_col4\" class=\"data row36 col4\" >0.7954</td>\n",
              "      <td id=\"T_02857_row36_col5\" class=\"data row36 col5\" >0.7090</td>\n",
              "      <td id=\"T_02857_row36_col6\" class=\"data row36 col6\" >0.6632</td>\n",
              "      <td id=\"T_02857_row36_col7\" class=\"data row36 col7\" >0.6428</td>\n",
              "      <td id=\"T_02857_row36_col8\" class=\"data row36 col8\" >0.3585</td>\n",
              "      <td id=\"T_02857_row36_col9\" class=\"data row36 col9\" >0.3744</td>\n",
              "      <td id=\"T_02857_row36_col10\" class=\"data row36 col10\" >0.3749</td>\n",
              "      <td id=\"T_02857_row36_col11\" class=\"data row36 col11\" >0.4027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
              "      <td id=\"T_02857_row37_col0\" class=\"data row37 col0\" >en</td>\n",
              "      <td id=\"T_02857_row37_col1\" class=\"data row37 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row37_col2\" class=\"data row37 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row37_col3\" class=\"data row37 col3\" >0.5362</td>\n",
              "      <td id=\"T_02857_row37_col4\" class=\"data row37 col4\" >0.5225</td>\n",
              "      <td id=\"T_02857_row37_col5\" class=\"data row37 col5\" >0.4972</td>\n",
              "      <td id=\"T_02857_row37_col6\" class=\"data row37 col6\" >0.5156</td>\n",
              "      <td id=\"T_02857_row37_col7\" class=\"data row37 col7\" >0.4759</td>\n",
              "      <td id=\"T_02857_row37_col8\" class=\"data row37 col8\" >0.3755</td>\n",
              "      <td id=\"T_02857_row37_col9\" class=\"data row37 col9\" >0.3720</td>\n",
              "      <td id=\"T_02857_row37_col10\" class=\"data row37 col10\" >0.3757</td>\n",
              "      <td id=\"T_02857_row37_col11\" class=\"data row37 col11\" >0.2432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
              "      <td id=\"T_02857_row38_col0\" class=\"data row38 col0\" >en</td>\n",
              "      <td id=\"T_02857_row38_col1\" class=\"data row38 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row38_col2\" class=\"data row38 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row38_col3\" class=\"data row38 col3\" >0.5362</td>\n",
              "      <td id=\"T_02857_row38_col4\" class=\"data row38 col4\" >0.5225</td>\n",
              "      <td id=\"T_02857_row38_col5\" class=\"data row38 col5\" >0.4972</td>\n",
              "      <td id=\"T_02857_row38_col6\" class=\"data row38 col6\" >0.5156</td>\n",
              "      <td id=\"T_02857_row38_col7\" class=\"data row38 col7\" >0.4759</td>\n",
              "      <td id=\"T_02857_row38_col8\" class=\"data row38 col8\" >0.3755</td>\n",
              "      <td id=\"T_02857_row38_col9\" class=\"data row38 col9\" >0.3720</td>\n",
              "      <td id=\"T_02857_row38_col10\" class=\"data row38 col10\" >0.3757</td>\n",
              "      <td id=\"T_02857_row38_col11\" class=\"data row38 col11\" >0.2432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
              "      <td id=\"T_02857_row39_col0\" class=\"data row39 col0\" >en</td>\n",
              "      <td id=\"T_02857_row39_col1\" class=\"data row39 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row39_col2\" class=\"data row39 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row39_col3\" class=\"data row39 col3\" >0.4460</td>\n",
              "      <td id=\"T_02857_row39_col4\" class=\"data row39 col4\" >0.4334</td>\n",
              "      <td id=\"T_02857_row39_col5\" class=\"data row39 col5\" >0.4037</td>\n",
              "      <td id=\"T_02857_row39_col6\" class=\"data row39 col6\" >0.4387</td>\n",
              "      <td id=\"T_02857_row39_col7\" class=\"data row39 col7\" >0.4199</td>\n",
              "      <td id=\"T_02857_row39_col8\" class=\"data row39 col8\" >0.4035</td>\n",
              "      <td id=\"T_02857_row39_col9\" class=\"data row39 col9\" >0.5207</td>\n",
              "      <td id=\"T_02857_row39_col10\" class=\"data row39 col10\" >0.3628</td>\n",
              "      <td id=\"T_02857_row39_col11\" class=\"data row39 col11\" >0.1977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
              "      <td id=\"T_02857_row40_col0\" class=\"data row40 col0\" >en</td>\n",
              "      <td id=\"T_02857_row40_col1\" class=\"data row40 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row40_col2\" class=\"data row40 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row40_col3\" class=\"data row40 col3\" >0.4460</td>\n",
              "      <td id=\"T_02857_row40_col4\" class=\"data row40 col4\" >0.4334</td>\n",
              "      <td id=\"T_02857_row40_col5\" class=\"data row40 col5\" >0.4037</td>\n",
              "      <td id=\"T_02857_row40_col6\" class=\"data row40 col6\" >0.4387</td>\n",
              "      <td id=\"T_02857_row40_col7\" class=\"data row40 col7\" >0.4199</td>\n",
              "      <td id=\"T_02857_row40_col8\" class=\"data row40 col8\" >0.4035</td>\n",
              "      <td id=\"T_02857_row40_col9\" class=\"data row40 col9\" >0.5207</td>\n",
              "      <td id=\"T_02857_row40_col10\" class=\"data row40 col10\" >0.3628</td>\n",
              "      <td id=\"T_02857_row40_col11\" class=\"data row40 col11\" >0.1977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
              "      <td id=\"T_02857_row41_col0\" class=\"data row41 col0\" >en</td>\n",
              "      <td id=\"T_02857_row41_col1\" class=\"data row41 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row41_col2\" class=\"data row41 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row41_col3\" class=\"data row41 col3\" >0.7657</td>\n",
              "      <td id=\"T_02857_row41_col4\" class=\"data row41 col4\" >0.7616</td>\n",
              "      <td id=\"T_02857_row41_col5\" class=\"data row41 col5\" >0.7058</td>\n",
              "      <td id=\"T_02857_row41_col6\" class=\"data row41 col6\" >0.6981</td>\n",
              "      <td id=\"T_02857_row41_col7\" class=\"data row41 col7\" >0.6387</td>\n",
              "      <td id=\"T_02857_row41_col8\" class=\"data row41 col8\" >0.3558</td>\n",
              "      <td id=\"T_02857_row41_col9\" class=\"data row41 col9\" >0.4186</td>\n",
              "      <td id=\"T_02857_row41_col10\" class=\"data row41 col10\" >0.4255</td>\n",
              "      <td id=\"T_02857_row41_col11\" class=\"data row41 col11\" >0.4428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
              "      <td id=\"T_02857_row42_col0\" class=\"data row42 col0\" >en</td>\n",
              "      <td id=\"T_02857_row42_col1\" class=\"data row42 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row42_col2\" class=\"data row42 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row42_col3\" class=\"data row42 col3\" >0.7657</td>\n",
              "      <td id=\"T_02857_row42_col4\" class=\"data row42 col4\" >0.7616</td>\n",
              "      <td id=\"T_02857_row42_col5\" class=\"data row42 col5\" >0.7058</td>\n",
              "      <td id=\"T_02857_row42_col6\" class=\"data row42 col6\" >0.6981</td>\n",
              "      <td id=\"T_02857_row42_col7\" class=\"data row42 col7\" >0.6387</td>\n",
              "      <td id=\"T_02857_row42_col8\" class=\"data row42 col8\" >0.3558</td>\n",
              "      <td id=\"T_02857_row42_col9\" class=\"data row42 col9\" >0.4186</td>\n",
              "      <td id=\"T_02857_row42_col10\" class=\"data row42 col10\" >0.4255</td>\n",
              "      <td id=\"T_02857_row42_col11\" class=\"data row42 col11\" >0.4428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
              "      <td id=\"T_02857_row43_col0\" class=\"data row43 col0\" >en</td>\n",
              "      <td id=\"T_02857_row43_col1\" class=\"data row43 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row43_col2\" class=\"data row43 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row43_col3\" class=\"data row43 col3\" >0.8469</td>\n",
              "      <td id=\"T_02857_row43_col4\" class=\"data row43 col4\" >0.8077</td>\n",
              "      <td id=\"T_02857_row43_col5\" class=\"data row43 col5\" >0.6201</td>\n",
              "      <td id=\"T_02857_row43_col6\" class=\"data row43 col6\" >0.6150</td>\n",
              "      <td id=\"T_02857_row43_col7\" class=\"data row43 col7\" >0.5331</td>\n",
              "      <td id=\"T_02857_row43_col8\" class=\"data row43 col8\" >0.3357</td>\n",
              "      <td id=\"T_02857_row43_col9\" class=\"data row43 col9\" >0.3346</td>\n",
              "      <td id=\"T_02857_row43_col10\" class=\"data row43 col10\" >0.3385</td>\n",
              "      <td id=\"T_02857_row43_col11\" class=\"data row43 col11\" >0.3509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
              "      <td id=\"T_02857_row44_col0\" class=\"data row44 col0\" >en</td>\n",
              "      <td id=\"T_02857_row44_col1\" class=\"data row44 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row44_col2\" class=\"data row44 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row44_col3\" class=\"data row44 col3\" >0.8469</td>\n",
              "      <td id=\"T_02857_row44_col4\" class=\"data row44 col4\" >0.8077</td>\n",
              "      <td id=\"T_02857_row44_col5\" class=\"data row44 col5\" >0.6201</td>\n",
              "      <td id=\"T_02857_row44_col6\" class=\"data row44 col6\" >0.6150</td>\n",
              "      <td id=\"T_02857_row44_col7\" class=\"data row44 col7\" >0.5331</td>\n",
              "      <td id=\"T_02857_row44_col8\" class=\"data row44 col8\" >0.3357</td>\n",
              "      <td id=\"T_02857_row44_col9\" class=\"data row44 col9\" >0.3346</td>\n",
              "      <td id=\"T_02857_row44_col10\" class=\"data row44 col10\" >0.3385</td>\n",
              "      <td id=\"T_02857_row44_col11\" class=\"data row44 col11\" >0.3509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
              "      <td id=\"T_02857_row45_col0\" class=\"data row45 col0\" >en</td>\n",
              "      <td id=\"T_02857_row45_col1\" class=\"data row45 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row45_col2\" class=\"data row45 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row45_col3\" class=\"data row45 col3\" >0.6136</td>\n",
              "      <td id=\"T_02857_row45_col4\" class=\"data row45 col4\" >0.6131</td>\n",
              "      <td id=\"T_02857_row45_col5\" class=\"data row45 col5\" >0.6070</td>\n",
              "      <td id=\"T_02857_row45_col6\" class=\"data row45 col6\" >0.5889</td>\n",
              "      <td id=\"T_02857_row45_col7\" class=\"data row45 col7\" >0.5804</td>\n",
              "      <td id=\"T_02857_row45_col8\" class=\"data row45 col8\" >0.3777</td>\n",
              "      <td id=\"T_02857_row45_col9\" class=\"data row45 col9\" >0.3577</td>\n",
              "      <td id=\"T_02857_row45_col10\" class=\"data row45 col10\" >0.5085</td>\n",
              "      <td id=\"T_02857_row45_col11\" class=\"data row45 col11\" >0.5035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
              "      <td id=\"T_02857_row46_col0\" class=\"data row46 col0\" >en</td>\n",
              "      <td id=\"T_02857_row46_col1\" class=\"data row46 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row46_col2\" class=\"data row46 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row46_col3\" class=\"data row46 col3\" >0.6136</td>\n",
              "      <td id=\"T_02857_row46_col4\" class=\"data row46 col4\" >0.6131</td>\n",
              "      <td id=\"T_02857_row46_col5\" class=\"data row46 col5\" >0.6070</td>\n",
              "      <td id=\"T_02857_row46_col6\" class=\"data row46 col6\" >0.5889</td>\n",
              "      <td id=\"T_02857_row46_col7\" class=\"data row46 col7\" >0.5804</td>\n",
              "      <td id=\"T_02857_row46_col8\" class=\"data row46 col8\" >0.3777</td>\n",
              "      <td id=\"T_02857_row46_col9\" class=\"data row46 col9\" >0.3577</td>\n",
              "      <td id=\"T_02857_row46_col10\" class=\"data row46 col10\" >0.5085</td>\n",
              "      <td id=\"T_02857_row46_col11\" class=\"data row46 col11\" >0.5035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row47\" class=\"row_heading level0 row47\" >47</th>\n",
              "      <td id=\"T_02857_row47_col0\" class=\"data row47 col0\" >en</td>\n",
              "      <td id=\"T_02857_row47_col1\" class=\"data row47 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row47_col2\" class=\"data row47 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row47_col3\" class=\"data row47 col3\" >0.7742</td>\n",
              "      <td id=\"T_02857_row47_col4\" class=\"data row47 col4\" >0.7151</td>\n",
              "      <td id=\"T_02857_row47_col5\" class=\"data row47 col5\" >0.6526</td>\n",
              "      <td id=\"T_02857_row47_col6\" class=\"data row47 col6\" >0.6754</td>\n",
              "      <td id=\"T_02857_row47_col7\" class=\"data row47 col7\" >0.5707</td>\n",
              "      <td id=\"T_02857_row47_col8\" class=\"data row47 col8\" >0.3743</td>\n",
              "      <td id=\"T_02857_row47_col9\" class=\"data row47 col9\" >0.3815</td>\n",
              "      <td id=\"T_02857_row47_col10\" class=\"data row47 col10\" >0.3821</td>\n",
              "      <td id=\"T_02857_row47_col11\" class=\"data row47 col11\" >0.3546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row48\" class=\"row_heading level0 row48\" >48</th>\n",
              "      <td id=\"T_02857_row48_col0\" class=\"data row48 col0\" >en</td>\n",
              "      <td id=\"T_02857_row48_col1\" class=\"data row48 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row48_col2\" class=\"data row48 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row48_col3\" class=\"data row48 col3\" >0.7742</td>\n",
              "      <td id=\"T_02857_row48_col4\" class=\"data row48 col4\" >0.7151</td>\n",
              "      <td id=\"T_02857_row48_col5\" class=\"data row48 col5\" >0.6526</td>\n",
              "      <td id=\"T_02857_row48_col6\" class=\"data row48 col6\" >0.6754</td>\n",
              "      <td id=\"T_02857_row48_col7\" class=\"data row48 col7\" >0.5707</td>\n",
              "      <td id=\"T_02857_row48_col8\" class=\"data row48 col8\" >0.3743</td>\n",
              "      <td id=\"T_02857_row48_col9\" class=\"data row48 col9\" >0.3815</td>\n",
              "      <td id=\"T_02857_row48_col10\" class=\"data row48 col10\" >0.3821</td>\n",
              "      <td id=\"T_02857_row48_col11\" class=\"data row48 col11\" >0.3546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row49\" class=\"row_heading level0 row49\" >49</th>\n",
              "      <td id=\"T_02857_row49_col0\" class=\"data row49 col0\" >en</td>\n",
              "      <td id=\"T_02857_row49_col1\" class=\"data row49 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row49_col2\" class=\"data row49 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row49_col3\" class=\"data row49 col3\" >0.4327</td>\n",
              "      <td id=\"T_02857_row49_col4\" class=\"data row49 col4\" >0.3174</td>\n",
              "      <td id=\"T_02857_row49_col5\" class=\"data row49 col5\" >0.2969</td>\n",
              "      <td id=\"T_02857_row49_col6\" class=\"data row49 col6\" >0.4522</td>\n",
              "      <td id=\"T_02857_row49_col7\" class=\"data row49 col7\" >0.3828</td>\n",
              "      <td id=\"T_02857_row49_col8\" class=\"data row49 col8\" >0.6403</td>\n",
              "      <td id=\"T_02857_row49_col9\" class=\"data row49 col9\" >0.6841</td>\n",
              "      <td id=\"T_02857_row49_col10\" class=\"data row49 col10\" >0.6015</td>\n",
              "      <td id=\"T_02857_row49_col11\" class=\"data row49 col11\" >0.4113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row50\" class=\"row_heading level0 row50\" >50</th>\n",
              "      <td id=\"T_02857_row50_col0\" class=\"data row50 col0\" >en</td>\n",
              "      <td id=\"T_02857_row50_col1\" class=\"data row50 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row50_col2\" class=\"data row50 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row50_col3\" class=\"data row50 col3\" >0.4327</td>\n",
              "      <td id=\"T_02857_row50_col4\" class=\"data row50 col4\" >0.3174</td>\n",
              "      <td id=\"T_02857_row50_col5\" class=\"data row50 col5\" >0.2969</td>\n",
              "      <td id=\"T_02857_row50_col6\" class=\"data row50 col6\" >0.4522</td>\n",
              "      <td id=\"T_02857_row50_col7\" class=\"data row50 col7\" >0.3828</td>\n",
              "      <td id=\"T_02857_row50_col8\" class=\"data row50 col8\" >0.6403</td>\n",
              "      <td id=\"T_02857_row50_col9\" class=\"data row50 col9\" >0.6841</td>\n",
              "      <td id=\"T_02857_row50_col10\" class=\"data row50 col10\" >0.6015</td>\n",
              "      <td id=\"T_02857_row50_col11\" class=\"data row50 col11\" >0.4113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row51\" class=\"row_heading level0 row51\" >51</th>\n",
              "      <td id=\"T_02857_row51_col0\" class=\"data row51 col0\" >en</td>\n",
              "      <td id=\"T_02857_row51_col1\" class=\"data row51 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row51_col2\" class=\"data row51 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row51_col3\" class=\"data row51 col3\" >0.4806</td>\n",
              "      <td id=\"T_02857_row51_col4\" class=\"data row51 col4\" >0.4843</td>\n",
              "      <td id=\"T_02857_row51_col5\" class=\"data row51 col5\" >0.5096</td>\n",
              "      <td id=\"T_02857_row51_col6\" class=\"data row51 col6\" >0.4921</td>\n",
              "      <td id=\"T_02857_row51_col7\" class=\"data row51 col7\" >0.4937</td>\n",
              "      <td id=\"T_02857_row51_col8\" class=\"data row51 col8\" >0.5095</td>\n",
              "      <td id=\"T_02857_row51_col9\" class=\"data row51 col9\" >0.5111</td>\n",
              "      <td id=\"T_02857_row51_col10\" class=\"data row51 col10\" >0.5108</td>\n",
              "      <td id=\"T_02857_row51_col11\" class=\"data row51 col11\" >0.5314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row52\" class=\"row_heading level0 row52\" >52</th>\n",
              "      <td id=\"T_02857_row52_col0\" class=\"data row52 col0\" >en</td>\n",
              "      <td id=\"T_02857_row52_col1\" class=\"data row52 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row52_col2\" class=\"data row52 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row52_col3\" class=\"data row52 col3\" >0.4806</td>\n",
              "      <td id=\"T_02857_row52_col4\" class=\"data row52 col4\" >0.4843</td>\n",
              "      <td id=\"T_02857_row52_col5\" class=\"data row52 col5\" >0.5096</td>\n",
              "      <td id=\"T_02857_row52_col6\" class=\"data row52 col6\" >0.4921</td>\n",
              "      <td id=\"T_02857_row52_col7\" class=\"data row52 col7\" >0.4937</td>\n",
              "      <td id=\"T_02857_row52_col8\" class=\"data row52 col8\" >0.5095</td>\n",
              "      <td id=\"T_02857_row52_col9\" class=\"data row52 col9\" >0.5111</td>\n",
              "      <td id=\"T_02857_row52_col10\" class=\"data row52 col10\" >0.5108</td>\n",
              "      <td id=\"T_02857_row52_col11\" class=\"data row52 col11\" >0.5314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row53\" class=\"row_heading level0 row53\" >53</th>\n",
              "      <td id=\"T_02857_row53_col0\" class=\"data row53 col0\" >en</td>\n",
              "      <td id=\"T_02857_row53_col1\" class=\"data row53 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row53_col2\" class=\"data row53 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row53_col3\" class=\"data row53 col3\" >0.4092</td>\n",
              "      <td id=\"T_02857_row53_col4\" class=\"data row53 col4\" >0.4221</td>\n",
              "      <td id=\"T_02857_row53_col5\" class=\"data row53 col5\" >0.4531</td>\n",
              "      <td id=\"T_02857_row53_col6\" class=\"data row53 col6\" >0.4644</td>\n",
              "      <td id=\"T_02857_row53_col7\" class=\"data row53 col7\" >0.4193</td>\n",
              "      <td id=\"T_02857_row53_col8\" class=\"data row53 col8\" >0.4954</td>\n",
              "      <td id=\"T_02857_row53_col9\" class=\"data row53 col9\" >0.5337</td>\n",
              "      <td id=\"T_02857_row53_col10\" class=\"data row53 col10\" >0.4957</td>\n",
              "      <td id=\"T_02857_row53_col11\" class=\"data row53 col11\" >0.4669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row54\" class=\"row_heading level0 row54\" >54</th>\n",
              "      <td id=\"T_02857_row54_col0\" class=\"data row54 col0\" >en</td>\n",
              "      <td id=\"T_02857_row54_col1\" class=\"data row54 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row54_col2\" class=\"data row54 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row54_col3\" class=\"data row54 col3\" >0.4092</td>\n",
              "      <td id=\"T_02857_row54_col4\" class=\"data row54 col4\" >0.4221</td>\n",
              "      <td id=\"T_02857_row54_col5\" class=\"data row54 col5\" >0.4531</td>\n",
              "      <td id=\"T_02857_row54_col6\" class=\"data row54 col6\" >0.4644</td>\n",
              "      <td id=\"T_02857_row54_col7\" class=\"data row54 col7\" >0.4193</td>\n",
              "      <td id=\"T_02857_row54_col8\" class=\"data row54 col8\" >0.4954</td>\n",
              "      <td id=\"T_02857_row54_col9\" class=\"data row54 col9\" >0.5337</td>\n",
              "      <td id=\"T_02857_row54_col10\" class=\"data row54 col10\" >0.4957</td>\n",
              "      <td id=\"T_02857_row54_col11\" class=\"data row54 col11\" >0.4669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row55\" class=\"row_heading level0 row55\" >55</th>\n",
              "      <td id=\"T_02857_row55_col0\" class=\"data row55 col0\" >en</td>\n",
              "      <td id=\"T_02857_row55_col1\" class=\"data row55 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row55_col2\" class=\"data row55 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row55_col3\" class=\"data row55 col3\" >0.5086</td>\n",
              "      <td id=\"T_02857_row55_col4\" class=\"data row55 col4\" >0.4824</td>\n",
              "      <td id=\"T_02857_row55_col5\" class=\"data row55 col5\" >0.4849</td>\n",
              "      <td id=\"T_02857_row55_col6\" class=\"data row55 col6\" >0.5308</td>\n",
              "      <td id=\"T_02857_row55_col7\" class=\"data row55 col7\" >0.4783</td>\n",
              "      <td id=\"T_02857_row55_col8\" class=\"data row55 col8\" >0.5410</td>\n",
              "      <td id=\"T_02857_row55_col9\" class=\"data row55 col9\" >0.5608</td>\n",
              "      <td id=\"T_02857_row55_col10\" class=\"data row55 col10\" >0.5337</td>\n",
              "      <td id=\"T_02857_row55_col11\" class=\"data row55 col11\" >0.5417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row56\" class=\"row_heading level0 row56\" >56</th>\n",
              "      <td id=\"T_02857_row56_col0\" class=\"data row56 col0\" >en</td>\n",
              "      <td id=\"T_02857_row56_col1\" class=\"data row56 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row56_col2\" class=\"data row56 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row56_col3\" class=\"data row56 col3\" >0.5086</td>\n",
              "      <td id=\"T_02857_row56_col4\" class=\"data row56 col4\" >0.4824</td>\n",
              "      <td id=\"T_02857_row56_col5\" class=\"data row56 col5\" >0.4849</td>\n",
              "      <td id=\"T_02857_row56_col6\" class=\"data row56 col6\" >0.5308</td>\n",
              "      <td id=\"T_02857_row56_col7\" class=\"data row56 col7\" >0.4783</td>\n",
              "      <td id=\"T_02857_row56_col8\" class=\"data row56 col8\" >0.5410</td>\n",
              "      <td id=\"T_02857_row56_col9\" class=\"data row56 col9\" >0.5608</td>\n",
              "      <td id=\"T_02857_row56_col10\" class=\"data row56 col10\" >0.5337</td>\n",
              "      <td id=\"T_02857_row56_col11\" class=\"data row56 col11\" >0.5417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row57\" class=\"row_heading level0 row57\" >57</th>\n",
              "      <td id=\"T_02857_row57_col0\" class=\"data row57 col0\" >en</td>\n",
              "      <td id=\"T_02857_row57_col1\" class=\"data row57 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row57_col2\" class=\"data row57 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row57_col3\" class=\"data row57 col3\" >0.5626</td>\n",
              "      <td id=\"T_02857_row57_col4\" class=\"data row57 col4\" >0.5356</td>\n",
              "      <td id=\"T_02857_row57_col5\" class=\"data row57 col5\" >0.5439</td>\n",
              "      <td id=\"T_02857_row57_col6\" class=\"data row57 col6\" >0.6054</td>\n",
              "      <td id=\"T_02857_row57_col7\" class=\"data row57 col7\" >0.5473</td>\n",
              "      <td id=\"T_02857_row57_col8\" class=\"data row57 col8\" >0.6318</td>\n",
              "      <td id=\"T_02857_row57_col9\" class=\"data row57 col9\" >0.6348</td>\n",
              "      <td id=\"T_02857_row57_col10\" class=\"data row57 col10\" >0.6075</td>\n",
              "      <td id=\"T_02857_row57_col11\" class=\"data row57 col11\" >0.5883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row58\" class=\"row_heading level0 row58\" >58</th>\n",
              "      <td id=\"T_02857_row58_col0\" class=\"data row58 col0\" >en</td>\n",
              "      <td id=\"T_02857_row58_col1\" class=\"data row58 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row58_col2\" class=\"data row58 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row58_col3\" class=\"data row58 col3\" >0.5626</td>\n",
              "      <td id=\"T_02857_row58_col4\" class=\"data row58 col4\" >0.5356</td>\n",
              "      <td id=\"T_02857_row58_col5\" class=\"data row58 col5\" >0.5439</td>\n",
              "      <td id=\"T_02857_row58_col6\" class=\"data row58 col6\" >0.6054</td>\n",
              "      <td id=\"T_02857_row58_col7\" class=\"data row58 col7\" >0.5473</td>\n",
              "      <td id=\"T_02857_row58_col8\" class=\"data row58 col8\" >0.6318</td>\n",
              "      <td id=\"T_02857_row58_col9\" class=\"data row58 col9\" >0.6348</td>\n",
              "      <td id=\"T_02857_row58_col10\" class=\"data row58 col10\" >0.6075</td>\n",
              "      <td id=\"T_02857_row58_col11\" class=\"data row58 col11\" >0.5883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row59\" class=\"row_heading level0 row59\" >59</th>\n",
              "      <td id=\"T_02857_row59_col0\" class=\"data row59 col0\" >en</td>\n",
              "      <td id=\"T_02857_row59_col1\" class=\"data row59 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row59_col2\" class=\"data row59 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row59_col3\" class=\"data row59 col3\" >0.4157</td>\n",
              "      <td id=\"T_02857_row59_col4\" class=\"data row59 col4\" >0.4153</td>\n",
              "      <td id=\"T_02857_row59_col5\" class=\"data row59 col5\" >0.4157</td>\n",
              "      <td id=\"T_02857_row59_col6\" class=\"data row59 col6\" >0.4133</td>\n",
              "      <td id=\"T_02857_row59_col7\" class=\"data row59 col7\" >0.4131</td>\n",
              "      <td id=\"T_02857_row59_col8\" class=\"data row59 col8\" >0.4060</td>\n",
              "      <td id=\"T_02857_row59_col9\" class=\"data row59 col9\" >0.4124</td>\n",
              "      <td id=\"T_02857_row59_col10\" class=\"data row59 col10\" >0.4099</td>\n",
              "      <td id=\"T_02857_row59_col11\" class=\"data row59 col11\" >0.5378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row60\" class=\"row_heading level0 row60\" >60</th>\n",
              "      <td id=\"T_02857_row60_col0\" class=\"data row60 col0\" >en</td>\n",
              "      <td id=\"T_02857_row60_col1\" class=\"data row60 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row60_col2\" class=\"data row60 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row60_col3\" class=\"data row60 col3\" >0.4157</td>\n",
              "      <td id=\"T_02857_row60_col4\" class=\"data row60 col4\" >0.4153</td>\n",
              "      <td id=\"T_02857_row60_col5\" class=\"data row60 col5\" >0.4157</td>\n",
              "      <td id=\"T_02857_row60_col6\" class=\"data row60 col6\" >0.4133</td>\n",
              "      <td id=\"T_02857_row60_col7\" class=\"data row60 col7\" >0.4131</td>\n",
              "      <td id=\"T_02857_row60_col8\" class=\"data row60 col8\" >0.4060</td>\n",
              "      <td id=\"T_02857_row60_col9\" class=\"data row60 col9\" >0.4124</td>\n",
              "      <td id=\"T_02857_row60_col10\" class=\"data row60 col10\" >0.4099</td>\n",
              "      <td id=\"T_02857_row60_col11\" class=\"data row60 col11\" >0.5378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row61\" class=\"row_heading level0 row61\" >61</th>\n",
              "      <td id=\"T_02857_row61_col0\" class=\"data row61 col0\" >en</td>\n",
              "      <td id=\"T_02857_row61_col1\" class=\"data row61 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row61_col2\" class=\"data row61 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row61_col3\" class=\"data row61 col3\" >0.5183</td>\n",
              "      <td id=\"T_02857_row61_col4\" class=\"data row61 col4\" >0.4691</td>\n",
              "      <td id=\"T_02857_row61_col5\" class=\"data row61 col5\" >0.4817</td>\n",
              "      <td id=\"T_02857_row61_col6\" class=\"data row61 col6\" >0.5757</td>\n",
              "      <td id=\"T_02857_row61_col7\" class=\"data row61 col7\" >0.5133</td>\n",
              "      <td id=\"T_02857_row61_col8\" class=\"data row61 col8\" >0.6553</td>\n",
              "      <td id=\"T_02857_row61_col9\" class=\"data row61 col9\" >0.6625</td>\n",
              "      <td id=\"T_02857_row61_col10\" class=\"data row61 col10\" >0.6145</td>\n",
              "      <td id=\"T_02857_row61_col11\" class=\"data row61 col11\" >0.5276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row62\" class=\"row_heading level0 row62\" >62</th>\n",
              "      <td id=\"T_02857_row62_col0\" class=\"data row62 col0\" >en</td>\n",
              "      <td id=\"T_02857_row62_col1\" class=\"data row62 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row62_col2\" class=\"data row62 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row62_col3\" class=\"data row62 col3\" >0.5183</td>\n",
              "      <td id=\"T_02857_row62_col4\" class=\"data row62 col4\" >0.4691</td>\n",
              "      <td id=\"T_02857_row62_col5\" class=\"data row62 col5\" >0.4817</td>\n",
              "      <td id=\"T_02857_row62_col6\" class=\"data row62 col6\" >0.5757</td>\n",
              "      <td id=\"T_02857_row62_col7\" class=\"data row62 col7\" >0.5133</td>\n",
              "      <td id=\"T_02857_row62_col8\" class=\"data row62 col8\" >0.6553</td>\n",
              "      <td id=\"T_02857_row62_col9\" class=\"data row62 col9\" >0.6625</td>\n",
              "      <td id=\"T_02857_row62_col10\" class=\"data row62 col10\" >0.6145</td>\n",
              "      <td id=\"T_02857_row62_col11\" class=\"data row62 col11\" >0.5276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row63\" class=\"row_heading level0 row63\" >63</th>\n",
              "      <td id=\"T_02857_row63_col0\" class=\"data row63 col0\" >en</td>\n",
              "      <td id=\"T_02857_row63_col1\" class=\"data row63 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row63_col2\" class=\"data row63 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row63_col3\" class=\"data row63 col3\" >0.5264</td>\n",
              "      <td id=\"T_02857_row63_col4\" class=\"data row63 col4\" >0.4443</td>\n",
              "      <td id=\"T_02857_row63_col5\" class=\"data row63 col5\" >0.4144</td>\n",
              "      <td id=\"T_02857_row63_col6\" class=\"data row63 col6\" >0.5037</td>\n",
              "      <td id=\"T_02857_row63_col7\" class=\"data row63 col7\" >0.4583</td>\n",
              "      <td id=\"T_02857_row63_col8\" class=\"data row63 col8\" >0.5766</td>\n",
              "      <td id=\"T_02857_row63_col9\" class=\"data row63 col9\" >0.6291</td>\n",
              "      <td id=\"T_02857_row63_col10\" class=\"data row63 col10\" >0.5623</td>\n",
              "      <td id=\"T_02857_row63_col11\" class=\"data row63 col11\" >0.4825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row64\" class=\"row_heading level0 row64\" >64</th>\n",
              "      <td id=\"T_02857_row64_col0\" class=\"data row64 col0\" >en</td>\n",
              "      <td id=\"T_02857_row64_col1\" class=\"data row64 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row64_col2\" class=\"data row64 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row64_col3\" class=\"data row64 col3\" >0.5264</td>\n",
              "      <td id=\"T_02857_row64_col4\" class=\"data row64 col4\" >0.4443</td>\n",
              "      <td id=\"T_02857_row64_col5\" class=\"data row64 col5\" >0.4144</td>\n",
              "      <td id=\"T_02857_row64_col6\" class=\"data row64 col6\" >0.5037</td>\n",
              "      <td id=\"T_02857_row64_col7\" class=\"data row64 col7\" >0.4583</td>\n",
              "      <td id=\"T_02857_row64_col8\" class=\"data row64 col8\" >0.5766</td>\n",
              "      <td id=\"T_02857_row64_col9\" class=\"data row64 col9\" >0.6291</td>\n",
              "      <td id=\"T_02857_row64_col10\" class=\"data row64 col10\" >0.5623</td>\n",
              "      <td id=\"T_02857_row64_col11\" class=\"data row64 col11\" >0.4825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row65\" class=\"row_heading level0 row65\" >65</th>\n",
              "      <td id=\"T_02857_row65_col0\" class=\"data row65 col0\" >en</td>\n",
              "      <td id=\"T_02857_row65_col1\" class=\"data row65 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row65_col2\" class=\"data row65 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row65_col3\" class=\"data row65 col3\" >0.5217</td>\n",
              "      <td id=\"T_02857_row65_col4\" class=\"data row65 col4\" >0.4710</td>\n",
              "      <td id=\"T_02857_row65_col5\" class=\"data row65 col5\" >0.4734</td>\n",
              "      <td id=\"T_02857_row65_col6\" class=\"data row65 col6\" >0.5515</td>\n",
              "      <td id=\"T_02857_row65_col7\" class=\"data row65 col7\" >0.5028</td>\n",
              "      <td id=\"T_02857_row65_col8\" class=\"data row65 col8\" >0.6285</td>\n",
              "      <td id=\"T_02857_row65_col9\" class=\"data row65 col9\" >0.6779</td>\n",
              "      <td id=\"T_02857_row65_col10\" class=\"data row65 col10\" >0.5974</td>\n",
              "      <td id=\"T_02857_row65_col11\" class=\"data row65 col11\" >0.4708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row66\" class=\"row_heading level0 row66\" >66</th>\n",
              "      <td id=\"T_02857_row66_col0\" class=\"data row66 col0\" >en</td>\n",
              "      <td id=\"T_02857_row66_col1\" class=\"data row66 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row66_col2\" class=\"data row66 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row66_col3\" class=\"data row66 col3\" >0.5217</td>\n",
              "      <td id=\"T_02857_row66_col4\" class=\"data row66 col4\" >0.4710</td>\n",
              "      <td id=\"T_02857_row66_col5\" class=\"data row66 col5\" >0.4734</td>\n",
              "      <td id=\"T_02857_row66_col6\" class=\"data row66 col6\" >0.5515</td>\n",
              "      <td id=\"T_02857_row66_col7\" class=\"data row66 col7\" >0.5028</td>\n",
              "      <td id=\"T_02857_row66_col8\" class=\"data row66 col8\" >0.6285</td>\n",
              "      <td id=\"T_02857_row66_col9\" class=\"data row66 col9\" >0.6779</td>\n",
              "      <td id=\"T_02857_row66_col10\" class=\"data row66 col10\" >0.5974</td>\n",
              "      <td id=\"T_02857_row66_col11\" class=\"data row66 col11\" >0.4708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row67\" class=\"row_heading level0 row67\" >67</th>\n",
              "      <td id=\"T_02857_row67_col0\" class=\"data row67 col0\" >en</td>\n",
              "      <td id=\"T_02857_row67_col1\" class=\"data row67 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row67_col2\" class=\"data row67 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row67_col3\" class=\"data row67 col3\" >0.4574</td>\n",
              "      <td id=\"T_02857_row67_col4\" class=\"data row67 col4\" >0.4556</td>\n",
              "      <td id=\"T_02857_row67_col5\" class=\"data row67 col5\" >0.5210</td>\n",
              "      <td id=\"T_02857_row67_col6\" class=\"data row67 col6\" >0.4977</td>\n",
              "      <td id=\"T_02857_row67_col7\" class=\"data row67 col7\" >0.4769</td>\n",
              "      <td id=\"T_02857_row67_col8\" class=\"data row67 col8\" >0.5676</td>\n",
              "      <td id=\"T_02857_row67_col9\" class=\"data row67 col9\" >0.6650</td>\n",
              "      <td id=\"T_02857_row67_col10\" class=\"data row67 col10\" >0.5379</td>\n",
              "      <td id=\"T_02857_row67_col11\" class=\"data row67 col11\" >0.3452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row68\" class=\"row_heading level0 row68\" >68</th>\n",
              "      <td id=\"T_02857_row68_col0\" class=\"data row68 col0\" >en</td>\n",
              "      <td id=\"T_02857_row68_col1\" class=\"data row68 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row68_col2\" class=\"data row68 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row68_col3\" class=\"data row68 col3\" >0.4574</td>\n",
              "      <td id=\"T_02857_row68_col4\" class=\"data row68 col4\" >0.4556</td>\n",
              "      <td id=\"T_02857_row68_col5\" class=\"data row68 col5\" >0.5210</td>\n",
              "      <td id=\"T_02857_row68_col6\" class=\"data row68 col6\" >0.4977</td>\n",
              "      <td id=\"T_02857_row68_col7\" class=\"data row68 col7\" >0.4769</td>\n",
              "      <td id=\"T_02857_row68_col8\" class=\"data row68 col8\" >0.5676</td>\n",
              "      <td id=\"T_02857_row68_col9\" class=\"data row68 col9\" >0.6650</td>\n",
              "      <td id=\"T_02857_row68_col10\" class=\"data row68 col10\" >0.5379</td>\n",
              "      <td id=\"T_02857_row68_col11\" class=\"data row68 col11\" >0.3452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row69\" class=\"row_heading level0 row69\" >69</th>\n",
              "      <td id=\"T_02857_row69_col0\" class=\"data row69 col0\" >en</td>\n",
              "      <td id=\"T_02857_row69_col1\" class=\"data row69 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row69_col2\" class=\"data row69 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row69_col3\" class=\"data row69 col3\" >0.6248</td>\n",
              "      <td id=\"T_02857_row69_col4\" class=\"data row69 col4\" >0.6185</td>\n",
              "      <td id=\"T_02857_row69_col5\" class=\"data row69 col5\" >0.5835</td>\n",
              "      <td id=\"T_02857_row69_col6\" class=\"data row69 col6\" >0.6169</td>\n",
              "      <td id=\"T_02857_row69_col7\" class=\"data row69 col7\" >0.4870</td>\n",
              "      <td id=\"T_02857_row69_col8\" class=\"data row69 col8\" >0.4388</td>\n",
              "      <td id=\"T_02857_row69_col9\" class=\"data row69 col9\" >0.6016</td>\n",
              "      <td id=\"T_02857_row69_col10\" class=\"data row69 col10\" >0.5344</td>\n",
              "      <td id=\"T_02857_row69_col11\" class=\"data row69 col11\" >0.4773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row70\" class=\"row_heading level0 row70\" >70</th>\n",
              "      <td id=\"T_02857_row70_col0\" class=\"data row70 col0\" >en</td>\n",
              "      <td id=\"T_02857_row70_col1\" class=\"data row70 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row70_col2\" class=\"data row70 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row70_col3\" class=\"data row70 col3\" >0.6248</td>\n",
              "      <td id=\"T_02857_row70_col4\" class=\"data row70 col4\" >0.6185</td>\n",
              "      <td id=\"T_02857_row70_col5\" class=\"data row70 col5\" >0.5835</td>\n",
              "      <td id=\"T_02857_row70_col6\" class=\"data row70 col6\" >0.6169</td>\n",
              "      <td id=\"T_02857_row70_col7\" class=\"data row70 col7\" >0.4870</td>\n",
              "      <td id=\"T_02857_row70_col8\" class=\"data row70 col8\" >0.4388</td>\n",
              "      <td id=\"T_02857_row70_col9\" class=\"data row70 col9\" >0.6016</td>\n",
              "      <td id=\"T_02857_row70_col10\" class=\"data row70 col10\" >0.5344</td>\n",
              "      <td id=\"T_02857_row70_col11\" class=\"data row70 col11\" >0.4773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row71\" class=\"row_heading level0 row71\" >71</th>\n",
              "      <td id=\"T_02857_row71_col0\" class=\"data row71 col0\" >en</td>\n",
              "      <td id=\"T_02857_row71_col1\" class=\"data row71 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row71_col2\" class=\"data row71 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row71_col3\" class=\"data row71 col3\" >0.5924</td>\n",
              "      <td id=\"T_02857_row71_col4\" class=\"data row71 col4\" >0.5203</td>\n",
              "      <td id=\"T_02857_row71_col5\" class=\"data row71 col5\" >0.4586</td>\n",
              "      <td id=\"T_02857_row71_col6\" class=\"data row71 col6\" >0.6025</td>\n",
              "      <td id=\"T_02857_row71_col7\" class=\"data row71 col7\" >0.4266</td>\n",
              "      <td id=\"T_02857_row71_col8\" class=\"data row71 col8\" >0.5744</td>\n",
              "      <td id=\"T_02857_row71_col9\" class=\"data row71 col9\" >0.6556</td>\n",
              "      <td id=\"T_02857_row71_col10\" class=\"data row71 col10\" >0.5969</td>\n",
              "      <td id=\"T_02857_row71_col11\" class=\"data row71 col11\" >0.4436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row72\" class=\"row_heading level0 row72\" >72</th>\n",
              "      <td id=\"T_02857_row72_col0\" class=\"data row72 col0\" >en</td>\n",
              "      <td id=\"T_02857_row72_col1\" class=\"data row72 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row72_col2\" class=\"data row72 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row72_col3\" class=\"data row72 col3\" >0.5924</td>\n",
              "      <td id=\"T_02857_row72_col4\" class=\"data row72 col4\" >0.5203</td>\n",
              "      <td id=\"T_02857_row72_col5\" class=\"data row72 col5\" >0.4586</td>\n",
              "      <td id=\"T_02857_row72_col6\" class=\"data row72 col6\" >0.6025</td>\n",
              "      <td id=\"T_02857_row72_col7\" class=\"data row72 col7\" >0.4266</td>\n",
              "      <td id=\"T_02857_row72_col8\" class=\"data row72 col8\" >0.5744</td>\n",
              "      <td id=\"T_02857_row72_col9\" class=\"data row72 col9\" >0.6556</td>\n",
              "      <td id=\"T_02857_row72_col10\" class=\"data row72 col10\" >0.5969</td>\n",
              "      <td id=\"T_02857_row72_col11\" class=\"data row72 col11\" >0.4436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row73\" class=\"row_heading level0 row73\" >73</th>\n",
              "      <td id=\"T_02857_row73_col0\" class=\"data row73 col0\" >en</td>\n",
              "      <td id=\"T_02857_row73_col1\" class=\"data row73 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row73_col2\" class=\"data row73 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row73_col3\" class=\"data row73 col3\" >0.4787</td>\n",
              "      <td id=\"T_02857_row73_col4\" class=\"data row73 col4\" >0.4776</td>\n",
              "      <td id=\"T_02857_row73_col5\" class=\"data row73 col5\" >0.4785</td>\n",
              "      <td id=\"T_02857_row73_col6\" class=\"data row73 col6\" >0.4726</td>\n",
              "      <td id=\"T_02857_row73_col7\" class=\"data row73 col7\" >0.4731</td>\n",
              "      <td id=\"T_02857_row73_col8\" class=\"data row73 col8\" >0.4719</td>\n",
              "      <td id=\"T_02857_row73_col9\" class=\"data row73 col9\" >0.4619</td>\n",
              "      <td id=\"T_02857_row73_col10\" class=\"data row73 col10\" >0.4708</td>\n",
              "      <td id=\"T_02857_row73_col11\" class=\"data row73 col11\" >0.5800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row74\" class=\"row_heading level0 row74\" >74</th>\n",
              "      <td id=\"T_02857_row74_col0\" class=\"data row74 col0\" >en</td>\n",
              "      <td id=\"T_02857_row74_col1\" class=\"data row74 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row74_col2\" class=\"data row74 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row74_col3\" class=\"data row74 col3\" >0.4787</td>\n",
              "      <td id=\"T_02857_row74_col4\" class=\"data row74 col4\" >0.4776</td>\n",
              "      <td id=\"T_02857_row74_col5\" class=\"data row74 col5\" >0.4785</td>\n",
              "      <td id=\"T_02857_row74_col6\" class=\"data row74 col6\" >0.4726</td>\n",
              "      <td id=\"T_02857_row74_col7\" class=\"data row74 col7\" >0.4731</td>\n",
              "      <td id=\"T_02857_row74_col8\" class=\"data row74 col8\" >0.4719</td>\n",
              "      <td id=\"T_02857_row74_col9\" class=\"data row74 col9\" >0.4619</td>\n",
              "      <td id=\"T_02857_row74_col10\" class=\"data row74 col10\" >0.4708</td>\n",
              "      <td id=\"T_02857_row74_col11\" class=\"data row74 col11\" >0.5800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row75\" class=\"row_heading level0 row75\" >75</th>\n",
              "      <td id=\"T_02857_row75_col0\" class=\"data row75 col0\" >en</td>\n",
              "      <td id=\"T_02857_row75_col1\" class=\"data row75 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row75_col2\" class=\"data row75 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row75_col3\" class=\"data row75 col3\" >0.5545</td>\n",
              "      <td id=\"T_02857_row75_col4\" class=\"data row75 col4\" >0.5048</td>\n",
              "      <td id=\"T_02857_row75_col5\" class=\"data row75 col5\" >0.5180</td>\n",
              "      <td id=\"T_02857_row75_col6\" class=\"data row75 col6\" >0.5929</td>\n",
              "      <td id=\"T_02857_row75_col7\" class=\"data row75 col7\" >0.4707</td>\n",
              "      <td id=\"T_02857_row75_col8\" class=\"data row75 col8\" >0.6762</td>\n",
              "      <td id=\"T_02857_row75_col9\" class=\"data row75 col9\" >0.7085</td>\n",
              "      <td id=\"T_02857_row75_col10\" class=\"data row75 col10\" >0.6116</td>\n",
              "      <td id=\"T_02857_row75_col11\" class=\"data row75 col11\" >0.4983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row76\" class=\"row_heading level0 row76\" >76</th>\n",
              "      <td id=\"T_02857_row76_col0\" class=\"data row76 col0\" >en</td>\n",
              "      <td id=\"T_02857_row76_col1\" class=\"data row76 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row76_col2\" class=\"data row76 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row76_col3\" class=\"data row76 col3\" >0.5545</td>\n",
              "      <td id=\"T_02857_row76_col4\" class=\"data row76 col4\" >0.5048</td>\n",
              "      <td id=\"T_02857_row76_col5\" class=\"data row76 col5\" >0.5180</td>\n",
              "      <td id=\"T_02857_row76_col6\" class=\"data row76 col6\" >0.5929</td>\n",
              "      <td id=\"T_02857_row76_col7\" class=\"data row76 col7\" >0.4707</td>\n",
              "      <td id=\"T_02857_row76_col8\" class=\"data row76 col8\" >0.6762</td>\n",
              "      <td id=\"T_02857_row76_col9\" class=\"data row76 col9\" >0.7085</td>\n",
              "      <td id=\"T_02857_row76_col10\" class=\"data row76 col10\" >0.6116</td>\n",
              "      <td id=\"T_02857_row76_col11\" class=\"data row76 col11\" >0.4983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row77\" class=\"row_heading level0 row77\" >77</th>\n",
              "      <td id=\"T_02857_row77_col0\" class=\"data row77 col0\" >en</td>\n",
              "      <td id=\"T_02857_row77_col1\" class=\"data row77 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row77_col2\" class=\"data row77 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row77_col3\" class=\"data row77 col3\" >0.5330</td>\n",
              "      <td id=\"T_02857_row77_col4\" class=\"data row77 col4\" >0.5289</td>\n",
              "      <td id=\"T_02857_row77_col5\" class=\"data row77 col5\" >0.5108</td>\n",
              "      <td id=\"T_02857_row77_col6\" class=\"data row77 col6\" >0.5442</td>\n",
              "      <td id=\"T_02857_row77_col7\" class=\"data row77 col7\" >0.5173</td>\n",
              "      <td id=\"T_02857_row77_col8\" class=\"data row77 col8\" >0.5626</td>\n",
              "      <td id=\"T_02857_row77_col9\" class=\"data row77 col9\" >0.5635</td>\n",
              "      <td id=\"T_02857_row77_col10\" class=\"data row77 col10\" >0.5450</td>\n",
              "      <td id=\"T_02857_row77_col11\" class=\"data row77 col11\" >0.5799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row78\" class=\"row_heading level0 row78\" >78</th>\n",
              "      <td id=\"T_02857_row78_col0\" class=\"data row78 col0\" >en</td>\n",
              "      <td id=\"T_02857_row78_col1\" class=\"data row78 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row78_col2\" class=\"data row78 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row78_col3\" class=\"data row78 col3\" >0.5330</td>\n",
              "      <td id=\"T_02857_row78_col4\" class=\"data row78 col4\" >0.5289</td>\n",
              "      <td id=\"T_02857_row78_col5\" class=\"data row78 col5\" >0.5108</td>\n",
              "      <td id=\"T_02857_row78_col6\" class=\"data row78 col6\" >0.5442</td>\n",
              "      <td id=\"T_02857_row78_col7\" class=\"data row78 col7\" >0.5173</td>\n",
              "      <td id=\"T_02857_row78_col8\" class=\"data row78 col8\" >0.5626</td>\n",
              "      <td id=\"T_02857_row78_col9\" class=\"data row78 col9\" >0.5635</td>\n",
              "      <td id=\"T_02857_row78_col10\" class=\"data row78 col10\" >0.5450</td>\n",
              "      <td id=\"T_02857_row78_col11\" class=\"data row78 col11\" >0.5799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row79\" class=\"row_heading level0 row79\" >79</th>\n",
              "      <td id=\"T_02857_row79_col0\" class=\"data row79 col0\" >en</td>\n",
              "      <td id=\"T_02857_row79_col1\" class=\"data row79 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row79_col2\" class=\"data row79 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row79_col3\" class=\"data row79 col3\" >0.4788</td>\n",
              "      <td id=\"T_02857_row79_col4\" class=\"data row79 col4\" >0.4613</td>\n",
              "      <td id=\"T_02857_row79_col5\" class=\"data row79 col5\" >0.4745</td>\n",
              "      <td id=\"T_02857_row79_col6\" class=\"data row79 col6\" >0.5009</td>\n",
              "      <td id=\"T_02857_row79_col7\" class=\"data row79 col7\" >0.5011</td>\n",
              "      <td id=\"T_02857_row79_col8\" class=\"data row79 col8\" >0.6240</td>\n",
              "      <td id=\"T_02857_row79_col9\" class=\"data row79 col9\" >0.6021</td>\n",
              "      <td id=\"T_02857_row79_col10\" class=\"data row79 col10\" >0.5657</td>\n",
              "      <td id=\"T_02857_row79_col11\" class=\"data row79 col11\" >0.5098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row80\" class=\"row_heading level0 row80\" >80</th>\n",
              "      <td id=\"T_02857_row80_col0\" class=\"data row80 col0\" >en</td>\n",
              "      <td id=\"T_02857_row80_col1\" class=\"data row80 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row80_col2\" class=\"data row80 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row80_col3\" class=\"data row80 col3\" >0.4788</td>\n",
              "      <td id=\"T_02857_row80_col4\" class=\"data row80 col4\" >0.4613</td>\n",
              "      <td id=\"T_02857_row80_col5\" class=\"data row80 col5\" >0.4745</td>\n",
              "      <td id=\"T_02857_row80_col6\" class=\"data row80 col6\" >0.5009</td>\n",
              "      <td id=\"T_02857_row80_col7\" class=\"data row80 col7\" >0.5011</td>\n",
              "      <td id=\"T_02857_row80_col8\" class=\"data row80 col8\" >0.6240</td>\n",
              "      <td id=\"T_02857_row80_col9\" class=\"data row80 col9\" >0.6021</td>\n",
              "      <td id=\"T_02857_row80_col10\" class=\"data row80 col10\" >0.5657</td>\n",
              "      <td id=\"T_02857_row80_col11\" class=\"data row80 col11\" >0.5098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row81\" class=\"row_heading level0 row81\" >81</th>\n",
              "      <td id=\"T_02857_row81_col0\" class=\"data row81 col0\" >en</td>\n",
              "      <td id=\"T_02857_row81_col1\" class=\"data row81 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row81_col2\" class=\"data row81 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row81_col3\" class=\"data row81 col3\" >0.5053</td>\n",
              "      <td id=\"T_02857_row81_col4\" class=\"data row81 col4\" >0.4936</td>\n",
              "      <td id=\"T_02857_row81_col5\" class=\"data row81 col5\" >0.5220</td>\n",
              "      <td id=\"T_02857_row81_col6\" class=\"data row81 col6\" >0.5431</td>\n",
              "      <td id=\"T_02857_row81_col7\" class=\"data row81 col7\" >0.5192</td>\n",
              "      <td id=\"T_02857_row81_col8\" class=\"data row81 col8\" >0.6141</td>\n",
              "      <td id=\"T_02857_row81_col9\" class=\"data row81 col9\" >0.6894</td>\n",
              "      <td id=\"T_02857_row81_col10\" class=\"data row81 col10\" >0.5497</td>\n",
              "      <td id=\"T_02857_row81_col11\" class=\"data row81 col11\" >0.4132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row82\" class=\"row_heading level0 row82\" >82</th>\n",
              "      <td id=\"T_02857_row82_col0\" class=\"data row82 col0\" >en</td>\n",
              "      <td id=\"T_02857_row82_col1\" class=\"data row82 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row82_col2\" class=\"data row82 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row82_col3\" class=\"data row82 col3\" >0.5053</td>\n",
              "      <td id=\"T_02857_row82_col4\" class=\"data row82 col4\" >0.4936</td>\n",
              "      <td id=\"T_02857_row82_col5\" class=\"data row82 col5\" >0.5220</td>\n",
              "      <td id=\"T_02857_row82_col6\" class=\"data row82 col6\" >0.5431</td>\n",
              "      <td id=\"T_02857_row82_col7\" class=\"data row82 col7\" >0.5192</td>\n",
              "      <td id=\"T_02857_row82_col8\" class=\"data row82 col8\" >0.6141</td>\n",
              "      <td id=\"T_02857_row82_col9\" class=\"data row82 col9\" >0.6894</td>\n",
              "      <td id=\"T_02857_row82_col10\" class=\"data row82 col10\" >0.5497</td>\n",
              "      <td id=\"T_02857_row82_col11\" class=\"data row82 col11\" >0.4132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row83\" class=\"row_heading level0 row83\" >83</th>\n",
              "      <td id=\"T_02857_row83_col0\" class=\"data row83 col0\" >en</td>\n",
              "      <td id=\"T_02857_row83_col1\" class=\"data row83 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row83_col2\" class=\"data row83 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row83_col3\" class=\"data row83 col3\" >0.6061</td>\n",
              "      <td id=\"T_02857_row83_col4\" class=\"data row83 col4\" >0.6200</td>\n",
              "      <td id=\"T_02857_row83_col5\" class=\"data row83 col5\" >0.6135</td>\n",
              "      <td id=\"T_02857_row83_col6\" class=\"data row83 col6\" >0.6331</td>\n",
              "      <td id=\"T_02857_row83_col7\" class=\"data row83 col7\" >0.6004</td>\n",
              "      <td id=\"T_02857_row83_col8\" class=\"data row83 col8\" >0.6172</td>\n",
              "      <td id=\"T_02857_row83_col9\" class=\"data row83 col9\" >0.6238</td>\n",
              "      <td id=\"T_02857_row83_col10\" class=\"data row83 col10\" >0.5904</td>\n",
              "      <td id=\"T_02857_row83_col11\" class=\"data row83 col11\" >0.6035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row84\" class=\"row_heading level0 row84\" >84</th>\n",
              "      <td id=\"T_02857_row84_col0\" class=\"data row84 col0\" >en</td>\n",
              "      <td id=\"T_02857_row84_col1\" class=\"data row84 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row84_col2\" class=\"data row84 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row84_col3\" class=\"data row84 col3\" >0.6061</td>\n",
              "      <td id=\"T_02857_row84_col4\" class=\"data row84 col4\" >0.6200</td>\n",
              "      <td id=\"T_02857_row84_col5\" class=\"data row84 col5\" >0.6135</td>\n",
              "      <td id=\"T_02857_row84_col6\" class=\"data row84 col6\" >0.6331</td>\n",
              "      <td id=\"T_02857_row84_col7\" class=\"data row84 col7\" >0.6004</td>\n",
              "      <td id=\"T_02857_row84_col8\" class=\"data row84 col8\" >0.6172</td>\n",
              "      <td id=\"T_02857_row84_col9\" class=\"data row84 col9\" >0.6238</td>\n",
              "      <td id=\"T_02857_row84_col10\" class=\"data row84 col10\" >0.5904</td>\n",
              "      <td id=\"T_02857_row84_col11\" class=\"data row84 col11\" >0.6035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row85\" class=\"row_heading level0 row85\" >85</th>\n",
              "      <td id=\"T_02857_row85_col0\" class=\"data row85 col0\" >en</td>\n",
              "      <td id=\"T_02857_row85_col1\" class=\"data row85 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row85_col2\" class=\"data row85 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row85_col3\" class=\"data row85 col3\" >0.5093</td>\n",
              "      <td id=\"T_02857_row85_col4\" class=\"data row85 col4\" >0.5374</td>\n",
              "      <td id=\"T_02857_row85_col5\" class=\"data row85 col5\" >0.5972</td>\n",
              "      <td id=\"T_02857_row85_col6\" class=\"data row85 col6\" >0.7953</td>\n",
              "      <td id=\"T_02857_row85_col7\" class=\"data row85 col7\" >0.6664</td>\n",
              "      <td id=\"T_02857_row85_col8\" class=\"data row85 col8\" >0.9138</td>\n",
              "      <td id=\"T_02857_row85_col9\" class=\"data row85 col9\" >0.8235</td>\n",
              "      <td id=\"T_02857_row85_col10\" class=\"data row85 col10\" >0.8139</td>\n",
              "      <td id=\"T_02857_row85_col11\" class=\"data row85 col11\" >0.5335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row86\" class=\"row_heading level0 row86\" >86</th>\n",
              "      <td id=\"T_02857_row86_col0\" class=\"data row86 col0\" >en</td>\n",
              "      <td id=\"T_02857_row86_col1\" class=\"data row86 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row86_col2\" class=\"data row86 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row86_col3\" class=\"data row86 col3\" >0.5093</td>\n",
              "      <td id=\"T_02857_row86_col4\" class=\"data row86 col4\" >0.5374</td>\n",
              "      <td id=\"T_02857_row86_col5\" class=\"data row86 col5\" >0.5972</td>\n",
              "      <td id=\"T_02857_row86_col6\" class=\"data row86 col6\" >0.7953</td>\n",
              "      <td id=\"T_02857_row86_col7\" class=\"data row86 col7\" >0.6664</td>\n",
              "      <td id=\"T_02857_row86_col8\" class=\"data row86 col8\" >0.9138</td>\n",
              "      <td id=\"T_02857_row86_col9\" class=\"data row86 col9\" >0.8235</td>\n",
              "      <td id=\"T_02857_row86_col10\" class=\"data row86 col10\" >0.8139</td>\n",
              "      <td id=\"T_02857_row86_col11\" class=\"data row86 col11\" >0.5335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row87\" class=\"row_heading level0 row87\" >87</th>\n",
              "      <td id=\"T_02857_row87_col0\" class=\"data row87 col0\" >en</td>\n",
              "      <td id=\"T_02857_row87_col1\" class=\"data row87 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row87_col2\" class=\"data row87 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row87_col3\" class=\"data row87 col3\" >0.6491</td>\n",
              "      <td id=\"T_02857_row87_col4\" class=\"data row87 col4\" >0.6591</td>\n",
              "      <td id=\"T_02857_row87_col5\" class=\"data row87 col5\" >0.6543</td>\n",
              "      <td id=\"T_02857_row87_col6\" class=\"data row87 col6\" >0.6535</td>\n",
              "      <td id=\"T_02857_row87_col7\" class=\"data row87 col7\" >0.6438</td>\n",
              "      <td id=\"T_02857_row87_col8\" class=\"data row87 col8\" >0.6772</td>\n",
              "      <td id=\"T_02857_row87_col9\" class=\"data row87 col9\" >0.5722</td>\n",
              "      <td id=\"T_02857_row87_col10\" class=\"data row87 col10\" >0.6484</td>\n",
              "      <td id=\"T_02857_row87_col11\" class=\"data row87 col11\" >0.6524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row88\" class=\"row_heading level0 row88\" >88</th>\n",
              "      <td id=\"T_02857_row88_col0\" class=\"data row88 col0\" >en</td>\n",
              "      <td id=\"T_02857_row88_col1\" class=\"data row88 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row88_col2\" class=\"data row88 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row88_col3\" class=\"data row88 col3\" >0.6491</td>\n",
              "      <td id=\"T_02857_row88_col4\" class=\"data row88 col4\" >0.6591</td>\n",
              "      <td id=\"T_02857_row88_col5\" class=\"data row88 col5\" >0.6543</td>\n",
              "      <td id=\"T_02857_row88_col6\" class=\"data row88 col6\" >0.6535</td>\n",
              "      <td id=\"T_02857_row88_col7\" class=\"data row88 col7\" >0.6438</td>\n",
              "      <td id=\"T_02857_row88_col8\" class=\"data row88 col8\" >0.6772</td>\n",
              "      <td id=\"T_02857_row88_col9\" class=\"data row88 col9\" >0.5722</td>\n",
              "      <td id=\"T_02857_row88_col10\" class=\"data row88 col10\" >0.6484</td>\n",
              "      <td id=\"T_02857_row88_col11\" class=\"data row88 col11\" >0.6524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row89\" class=\"row_heading level0 row89\" >89</th>\n",
              "      <td id=\"T_02857_row89_col0\" class=\"data row89 col0\" >en</td>\n",
              "      <td id=\"T_02857_row89_col1\" class=\"data row89 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row89_col2\" class=\"data row89 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row89_col3\" class=\"data row89 col3\" >0.5196</td>\n",
              "      <td id=\"T_02857_row89_col4\" class=\"data row89 col4\" >0.5218</td>\n",
              "      <td id=\"T_02857_row89_col5\" class=\"data row89 col5\" >0.5576</td>\n",
              "      <td id=\"T_02857_row89_col6\" class=\"data row89 col6\" >0.6675</td>\n",
              "      <td id=\"T_02857_row89_col7\" class=\"data row89 col7\" >0.5736</td>\n",
              "      <td id=\"T_02857_row89_col8\" class=\"data row89 col8\" >0.7731</td>\n",
              "      <td id=\"T_02857_row89_col9\" class=\"data row89 col9\" >0.7572</td>\n",
              "      <td id=\"T_02857_row89_col10\" class=\"data row89 col10\" >0.6830</td>\n",
              "      <td id=\"T_02857_row89_col11\" class=\"data row89 col11\" >0.5179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row90\" class=\"row_heading level0 row90\" >90</th>\n",
              "      <td id=\"T_02857_row90_col0\" class=\"data row90 col0\" >en</td>\n",
              "      <td id=\"T_02857_row90_col1\" class=\"data row90 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row90_col2\" class=\"data row90 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row90_col3\" class=\"data row90 col3\" >0.5196</td>\n",
              "      <td id=\"T_02857_row90_col4\" class=\"data row90 col4\" >0.5218</td>\n",
              "      <td id=\"T_02857_row90_col5\" class=\"data row90 col5\" >0.5576</td>\n",
              "      <td id=\"T_02857_row90_col6\" class=\"data row90 col6\" >0.6675</td>\n",
              "      <td id=\"T_02857_row90_col7\" class=\"data row90 col7\" >0.5736</td>\n",
              "      <td id=\"T_02857_row90_col8\" class=\"data row90 col8\" >0.7731</td>\n",
              "      <td id=\"T_02857_row90_col9\" class=\"data row90 col9\" >0.7572</td>\n",
              "      <td id=\"T_02857_row90_col10\" class=\"data row90 col10\" >0.6830</td>\n",
              "      <td id=\"T_02857_row90_col11\" class=\"data row90 col11\" >0.5179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row91\" class=\"row_heading level0 row91\" >91</th>\n",
              "      <td id=\"T_02857_row91_col0\" class=\"data row91 col0\" >en</td>\n",
              "      <td id=\"T_02857_row91_col1\" class=\"data row91 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row91_col2\" class=\"data row91 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row91_col3\" class=\"data row91 col3\" >0.8116</td>\n",
              "      <td id=\"T_02857_row91_col4\" class=\"data row91 col4\" >0.8692</td>\n",
              "      <td id=\"T_02857_row91_col5\" class=\"data row91 col5\" >0.8289</td>\n",
              "      <td id=\"T_02857_row91_col6\" class=\"data row91 col6\" >0.7640</td>\n",
              "      <td id=\"T_02857_row91_col7\" class=\"data row91 col7\" >0.7427</td>\n",
              "      <td id=\"T_02857_row91_col8\" class=\"data row91 col8\" >0.3703</td>\n",
              "      <td id=\"T_02857_row91_col9\" class=\"data row91 col9\" >0.3490</td>\n",
              "      <td id=\"T_02857_row91_col10\" class=\"data row91 col10\" >0.3876</td>\n",
              "      <td id=\"T_02857_row91_col11\" class=\"data row91 col11\" >0.4663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row92\" class=\"row_heading level0 row92\" >92</th>\n",
              "      <td id=\"T_02857_row92_col0\" class=\"data row92 col0\" >en</td>\n",
              "      <td id=\"T_02857_row92_col1\" class=\"data row92 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row92_col2\" class=\"data row92 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row92_col3\" class=\"data row92 col3\" >0.8116</td>\n",
              "      <td id=\"T_02857_row92_col4\" class=\"data row92 col4\" >0.8692</td>\n",
              "      <td id=\"T_02857_row92_col5\" class=\"data row92 col5\" >0.8289</td>\n",
              "      <td id=\"T_02857_row92_col6\" class=\"data row92 col6\" >0.7640</td>\n",
              "      <td id=\"T_02857_row92_col7\" class=\"data row92 col7\" >0.7427</td>\n",
              "      <td id=\"T_02857_row92_col8\" class=\"data row92 col8\" >0.3703</td>\n",
              "      <td id=\"T_02857_row92_col9\" class=\"data row92 col9\" >0.3490</td>\n",
              "      <td id=\"T_02857_row92_col10\" class=\"data row92 col10\" >0.3876</td>\n",
              "      <td id=\"T_02857_row92_col11\" class=\"data row92 col11\" >0.4663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row93\" class=\"row_heading level0 row93\" >93</th>\n",
              "      <td id=\"T_02857_row93_col0\" class=\"data row93 col0\" >en</td>\n",
              "      <td id=\"T_02857_row93_col1\" class=\"data row93 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row93_col2\" class=\"data row93 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row93_col3\" class=\"data row93 col3\" >0.5310</td>\n",
              "      <td id=\"T_02857_row93_col4\" class=\"data row93 col4\" >0.5924</td>\n",
              "      <td id=\"T_02857_row93_col5\" class=\"data row93 col5\" >0.5878</td>\n",
              "      <td id=\"T_02857_row93_col6\" class=\"data row93 col6\" >0.5942</td>\n",
              "      <td id=\"T_02857_row93_col7\" class=\"data row93 col7\" >0.5775</td>\n",
              "      <td id=\"T_02857_row93_col8\" class=\"data row93 col8\" >0.5302</td>\n",
              "      <td id=\"T_02857_row93_col9\" class=\"data row93 col9\" >0.4013</td>\n",
              "      <td id=\"T_02857_row93_col10\" class=\"data row93 col10\" >0.4614</td>\n",
              "      <td id=\"T_02857_row93_col11\" class=\"data row93 col11\" >0.3526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row94\" class=\"row_heading level0 row94\" >94</th>\n",
              "      <td id=\"T_02857_row94_col0\" class=\"data row94 col0\" >en</td>\n",
              "      <td id=\"T_02857_row94_col1\" class=\"data row94 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row94_col2\" class=\"data row94 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row94_col3\" class=\"data row94 col3\" >0.5310</td>\n",
              "      <td id=\"T_02857_row94_col4\" class=\"data row94 col4\" >0.5924</td>\n",
              "      <td id=\"T_02857_row94_col5\" class=\"data row94 col5\" >0.5878</td>\n",
              "      <td id=\"T_02857_row94_col6\" class=\"data row94 col6\" >0.5942</td>\n",
              "      <td id=\"T_02857_row94_col7\" class=\"data row94 col7\" >0.5775</td>\n",
              "      <td id=\"T_02857_row94_col8\" class=\"data row94 col8\" >0.5302</td>\n",
              "      <td id=\"T_02857_row94_col9\" class=\"data row94 col9\" >0.4013</td>\n",
              "      <td id=\"T_02857_row94_col10\" class=\"data row94 col10\" >0.4614</td>\n",
              "      <td id=\"T_02857_row94_col11\" class=\"data row94 col11\" >0.3526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row95\" class=\"row_heading level0 row95\" >95</th>\n",
              "      <td id=\"T_02857_row95_col0\" class=\"data row95 col0\" >en</td>\n",
              "      <td id=\"T_02857_row95_col1\" class=\"data row95 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row95_col2\" class=\"data row95 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row95_col3\" class=\"data row95 col3\" >0.4378</td>\n",
              "      <td id=\"T_02857_row95_col4\" class=\"data row95 col4\" >0.4284</td>\n",
              "      <td id=\"T_02857_row95_col5\" class=\"data row95 col5\" >0.4172</td>\n",
              "      <td id=\"T_02857_row95_col6\" class=\"data row95 col6\" >0.4411</td>\n",
              "      <td id=\"T_02857_row95_col7\" class=\"data row95 col7\" >0.4176</td>\n",
              "      <td id=\"T_02857_row95_col8\" class=\"data row95 col8\" >0.4088</td>\n",
              "      <td id=\"T_02857_row95_col9\" class=\"data row95 col9\" >0.4916</td>\n",
              "      <td id=\"T_02857_row95_col10\" class=\"data row95 col10\" >0.3651</td>\n",
              "      <td id=\"T_02857_row95_col11\" class=\"data row95 col11\" >0.1930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row96\" class=\"row_heading level0 row96\" >96</th>\n",
              "      <td id=\"T_02857_row96_col0\" class=\"data row96 col0\" >en</td>\n",
              "      <td id=\"T_02857_row96_col1\" class=\"data row96 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row96_col2\" class=\"data row96 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row96_col3\" class=\"data row96 col3\" >0.4378</td>\n",
              "      <td id=\"T_02857_row96_col4\" class=\"data row96 col4\" >0.4284</td>\n",
              "      <td id=\"T_02857_row96_col5\" class=\"data row96 col5\" >0.4172</td>\n",
              "      <td id=\"T_02857_row96_col6\" class=\"data row96 col6\" >0.4411</td>\n",
              "      <td id=\"T_02857_row96_col7\" class=\"data row96 col7\" >0.4176</td>\n",
              "      <td id=\"T_02857_row96_col8\" class=\"data row96 col8\" >0.4088</td>\n",
              "      <td id=\"T_02857_row96_col9\" class=\"data row96 col9\" >0.4916</td>\n",
              "      <td id=\"T_02857_row96_col10\" class=\"data row96 col10\" >0.3651</td>\n",
              "      <td id=\"T_02857_row96_col11\" class=\"data row96 col11\" >0.1930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row97\" class=\"row_heading level0 row97\" >97</th>\n",
              "      <td id=\"T_02857_row97_col0\" class=\"data row97 col0\" >en</td>\n",
              "      <td id=\"T_02857_row97_col1\" class=\"data row97 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row97_col2\" class=\"data row97 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row97_col3\" class=\"data row97 col3\" >0.7709</td>\n",
              "      <td id=\"T_02857_row97_col4\" class=\"data row97 col4\" >0.8044</td>\n",
              "      <td id=\"T_02857_row97_col5\" class=\"data row97 col5\" >0.7636</td>\n",
              "      <td id=\"T_02857_row97_col6\" class=\"data row97 col6\" >0.7306</td>\n",
              "      <td id=\"T_02857_row97_col7\" class=\"data row97 col7\" >0.6834</td>\n",
              "      <td id=\"T_02857_row97_col8\" class=\"data row97 col8\" >0.3518</td>\n",
              "      <td id=\"T_02857_row97_col9\" class=\"data row97 col9\" >0.3624</td>\n",
              "      <td id=\"T_02857_row97_col10\" class=\"data row97 col10\" >0.4103</td>\n",
              "      <td id=\"T_02857_row97_col11\" class=\"data row97 col11\" >0.4625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row98\" class=\"row_heading level0 row98\" >98</th>\n",
              "      <td id=\"T_02857_row98_col0\" class=\"data row98 col0\" >en</td>\n",
              "      <td id=\"T_02857_row98_col1\" class=\"data row98 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row98_col2\" class=\"data row98 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row98_col3\" class=\"data row98 col3\" >0.7709</td>\n",
              "      <td id=\"T_02857_row98_col4\" class=\"data row98 col4\" >0.8044</td>\n",
              "      <td id=\"T_02857_row98_col5\" class=\"data row98 col5\" >0.7636</td>\n",
              "      <td id=\"T_02857_row98_col6\" class=\"data row98 col6\" >0.7306</td>\n",
              "      <td id=\"T_02857_row98_col7\" class=\"data row98 col7\" >0.6834</td>\n",
              "      <td id=\"T_02857_row98_col8\" class=\"data row98 col8\" >0.3518</td>\n",
              "      <td id=\"T_02857_row98_col9\" class=\"data row98 col9\" >0.3624</td>\n",
              "      <td id=\"T_02857_row98_col10\" class=\"data row98 col10\" >0.4103</td>\n",
              "      <td id=\"T_02857_row98_col11\" class=\"data row98 col11\" >0.4625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row99\" class=\"row_heading level0 row99\" >99</th>\n",
              "      <td id=\"T_02857_row99_col0\" class=\"data row99 col0\" >en</td>\n",
              "      <td id=\"T_02857_row99_col1\" class=\"data row99 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row99_col2\" class=\"data row99 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row99_col3\" class=\"data row99 col3\" >0.7533</td>\n",
              "      <td id=\"T_02857_row99_col4\" class=\"data row99 col4\" >0.7398</td>\n",
              "      <td id=\"T_02857_row99_col5\" class=\"data row99 col5\" >0.7193</td>\n",
              "      <td id=\"T_02857_row99_col6\" class=\"data row99 col6\" >0.6873</td>\n",
              "      <td id=\"T_02857_row99_col7\" class=\"data row99 col7\" >0.6600</td>\n",
              "      <td id=\"T_02857_row99_col8\" class=\"data row99 col8\" >0.2649</td>\n",
              "      <td id=\"T_02857_row99_col9\" class=\"data row99 col9\" >0.3190</td>\n",
              "      <td id=\"T_02857_row99_col10\" class=\"data row99 col10\" >0.3248</td>\n",
              "      <td id=\"T_02857_row99_col11\" class=\"data row99 col11\" >0.4781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row100\" class=\"row_heading level0 row100\" >100</th>\n",
              "      <td id=\"T_02857_row100_col0\" class=\"data row100 col0\" >en</td>\n",
              "      <td id=\"T_02857_row100_col1\" class=\"data row100 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row100_col2\" class=\"data row100 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row100_col3\" class=\"data row100 col3\" >0.7533</td>\n",
              "      <td id=\"T_02857_row100_col4\" class=\"data row100 col4\" >0.7398</td>\n",
              "      <td id=\"T_02857_row100_col5\" class=\"data row100 col5\" >0.7193</td>\n",
              "      <td id=\"T_02857_row100_col6\" class=\"data row100 col6\" >0.6873</td>\n",
              "      <td id=\"T_02857_row100_col7\" class=\"data row100 col7\" >0.6600</td>\n",
              "      <td id=\"T_02857_row100_col8\" class=\"data row100 col8\" >0.2649</td>\n",
              "      <td id=\"T_02857_row100_col9\" class=\"data row100 col9\" >0.3190</td>\n",
              "      <td id=\"T_02857_row100_col10\" class=\"data row100 col10\" >0.3248</td>\n",
              "      <td id=\"T_02857_row100_col11\" class=\"data row100 col11\" >0.4781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row101\" class=\"row_heading level0 row101\" >101</th>\n",
              "      <td id=\"T_02857_row101_col0\" class=\"data row101 col0\" >en</td>\n",
              "      <td id=\"T_02857_row101_col1\" class=\"data row101 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row101_col2\" class=\"data row101 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row101_col3\" class=\"data row101 col3\" >0.5671</td>\n",
              "      <td id=\"T_02857_row101_col4\" class=\"data row101 col4\" >0.5691</td>\n",
              "      <td id=\"T_02857_row101_col5\" class=\"data row101 col5\" >0.5722</td>\n",
              "      <td id=\"T_02857_row101_col6\" class=\"data row101 col6\" >0.5534</td>\n",
              "      <td id=\"T_02857_row101_col7\" class=\"data row101 col7\" >0.5618</td>\n",
              "      <td id=\"T_02857_row101_col8\" class=\"data row101 col8\" >0.4817</td>\n",
              "      <td id=\"T_02857_row101_col9\" class=\"data row101 col9\" >0.4024</td>\n",
              "      <td id=\"T_02857_row101_col10\" class=\"data row101 col10\" >0.5216</td>\n",
              "      <td id=\"T_02857_row101_col11\" class=\"data row101 col11\" >0.5647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row102\" class=\"row_heading level0 row102\" >102</th>\n",
              "      <td id=\"T_02857_row102_col0\" class=\"data row102 col0\" >en</td>\n",
              "      <td id=\"T_02857_row102_col1\" class=\"data row102 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row102_col2\" class=\"data row102 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row102_col3\" class=\"data row102 col3\" >0.5671</td>\n",
              "      <td id=\"T_02857_row102_col4\" class=\"data row102 col4\" >0.5691</td>\n",
              "      <td id=\"T_02857_row102_col5\" class=\"data row102 col5\" >0.5722</td>\n",
              "      <td id=\"T_02857_row102_col6\" class=\"data row102 col6\" >0.5534</td>\n",
              "      <td id=\"T_02857_row102_col7\" class=\"data row102 col7\" >0.5618</td>\n",
              "      <td id=\"T_02857_row102_col8\" class=\"data row102 col8\" >0.4817</td>\n",
              "      <td id=\"T_02857_row102_col9\" class=\"data row102 col9\" >0.4024</td>\n",
              "      <td id=\"T_02857_row102_col10\" class=\"data row102 col10\" >0.5216</td>\n",
              "      <td id=\"T_02857_row102_col11\" class=\"data row102 col11\" >0.5647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row103\" class=\"row_heading level0 row103\" >103</th>\n",
              "      <td id=\"T_02857_row103_col0\" class=\"data row103 col0\" >en</td>\n",
              "      <td id=\"T_02857_row103_col1\" class=\"data row103 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row103_col2\" class=\"data row103 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row103_col3\" class=\"data row103 col3\" >0.7469</td>\n",
              "      <td id=\"T_02857_row103_col4\" class=\"data row103 col4\" >0.7991</td>\n",
              "      <td id=\"T_02857_row103_col5\" class=\"data row103 col5\" >0.7430</td>\n",
              "      <td id=\"T_02857_row103_col6\" class=\"data row103 col6\" >0.7062</td>\n",
              "      <td id=\"T_02857_row103_col7\" class=\"data row103 col7\" >0.6847</td>\n",
              "      <td id=\"T_02857_row103_col8\" class=\"data row103 col8\" >0.5216</td>\n",
              "      <td id=\"T_02857_row103_col9\" class=\"data row103 col9\" >0.4158</td>\n",
              "      <td id=\"T_02857_row103_col10\" class=\"data row103 col10\" >0.4656</td>\n",
              "      <td id=\"T_02857_row103_col11\" class=\"data row103 col11\" >0.4522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row104\" class=\"row_heading level0 row104\" >104</th>\n",
              "      <td id=\"T_02857_row104_col0\" class=\"data row104 col0\" >en</td>\n",
              "      <td id=\"T_02857_row104_col1\" class=\"data row104 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row104_col2\" class=\"data row104 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row104_col3\" class=\"data row104 col3\" >0.7469</td>\n",
              "      <td id=\"T_02857_row104_col4\" class=\"data row104 col4\" >0.7991</td>\n",
              "      <td id=\"T_02857_row104_col5\" class=\"data row104 col5\" >0.7430</td>\n",
              "      <td id=\"T_02857_row104_col6\" class=\"data row104 col6\" >0.7062</td>\n",
              "      <td id=\"T_02857_row104_col7\" class=\"data row104 col7\" >0.6847</td>\n",
              "      <td id=\"T_02857_row104_col8\" class=\"data row104 col8\" >0.5216</td>\n",
              "      <td id=\"T_02857_row104_col9\" class=\"data row104 col9\" >0.4158</td>\n",
              "      <td id=\"T_02857_row104_col10\" class=\"data row104 col10\" >0.4656</td>\n",
              "      <td id=\"T_02857_row104_col11\" class=\"data row104 col11\" >0.4522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row105\" class=\"row_heading level0 row105\" >105</th>\n",
              "      <td id=\"T_02857_row105_col0\" class=\"data row105 col0\" >en</td>\n",
              "      <td id=\"T_02857_row105_col1\" class=\"data row105 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row105_col2\" class=\"data row105 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row105_col3\" class=\"data row105 col3\" >0.8672</td>\n",
              "      <td id=\"T_02857_row105_col4\" class=\"data row105 col4\" >0.8745</td>\n",
              "      <td id=\"T_02857_row105_col5\" class=\"data row105 col5\" >0.8212</td>\n",
              "      <td id=\"T_02857_row105_col6\" class=\"data row105 col6\" >0.8353</td>\n",
              "      <td id=\"T_02857_row105_col7\" class=\"data row105 col7\" >0.7552</td>\n",
              "      <td id=\"T_02857_row105_col8\" class=\"data row105 col8\" >0.4256</td>\n",
              "      <td id=\"T_02857_row105_col9\" class=\"data row105 col9\" >0.4419</td>\n",
              "      <td id=\"T_02857_row105_col10\" class=\"data row105 col10\" >0.4167</td>\n",
              "      <td id=\"T_02857_row105_col11\" class=\"data row105 col11\" >0.5294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row106\" class=\"row_heading level0 row106\" >106</th>\n",
              "      <td id=\"T_02857_row106_col0\" class=\"data row106 col0\" >en</td>\n",
              "      <td id=\"T_02857_row106_col1\" class=\"data row106 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row106_col2\" class=\"data row106 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row106_col3\" class=\"data row106 col3\" >0.8672</td>\n",
              "      <td id=\"T_02857_row106_col4\" class=\"data row106 col4\" >0.8745</td>\n",
              "      <td id=\"T_02857_row106_col5\" class=\"data row106 col5\" >0.8212</td>\n",
              "      <td id=\"T_02857_row106_col6\" class=\"data row106 col6\" >0.8353</td>\n",
              "      <td id=\"T_02857_row106_col7\" class=\"data row106 col7\" >0.7552</td>\n",
              "      <td id=\"T_02857_row106_col8\" class=\"data row106 col8\" >0.4256</td>\n",
              "      <td id=\"T_02857_row106_col9\" class=\"data row106 col9\" >0.4419</td>\n",
              "      <td id=\"T_02857_row106_col10\" class=\"data row106 col10\" >0.4167</td>\n",
              "      <td id=\"T_02857_row106_col11\" class=\"data row106 col11\" >0.5294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row107\" class=\"row_heading level0 row107\" >107</th>\n",
              "      <td id=\"T_02857_row107_col0\" class=\"data row107 col0\" >en</td>\n",
              "      <td id=\"T_02857_row107_col1\" class=\"data row107 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row107_col2\" class=\"data row107 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row107_col3\" class=\"data row107 col3\" >0.5669</td>\n",
              "      <td id=\"T_02857_row107_col4\" class=\"data row107 col4\" >0.5769</td>\n",
              "      <td id=\"T_02857_row107_col5\" class=\"data row107 col5\" >0.5655</td>\n",
              "      <td id=\"T_02857_row107_col6\" class=\"data row107 col6\" >0.5920</td>\n",
              "      <td id=\"T_02857_row107_col7\" class=\"data row107 col7\" >0.5823</td>\n",
              "      <td id=\"T_02857_row107_col8\" class=\"data row107 col8\" >0.6136</td>\n",
              "      <td id=\"T_02857_row107_col9\" class=\"data row107 col9\" >0.5197</td>\n",
              "      <td id=\"T_02857_row107_col10\" class=\"data row107 col10\" >0.5161</td>\n",
              "      <td id=\"T_02857_row107_col11\" class=\"data row107 col11\" >0.4454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row108\" class=\"row_heading level0 row108\" >108</th>\n",
              "      <td id=\"T_02857_row108_col0\" class=\"data row108 col0\" >en</td>\n",
              "      <td id=\"T_02857_row108_col1\" class=\"data row108 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row108_col2\" class=\"data row108 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row108_col3\" class=\"data row108 col3\" >0.5669</td>\n",
              "      <td id=\"T_02857_row108_col4\" class=\"data row108 col4\" >0.5769</td>\n",
              "      <td id=\"T_02857_row108_col5\" class=\"data row108 col5\" >0.5655</td>\n",
              "      <td id=\"T_02857_row108_col6\" class=\"data row108 col6\" >0.5920</td>\n",
              "      <td id=\"T_02857_row108_col7\" class=\"data row108 col7\" >0.5823</td>\n",
              "      <td id=\"T_02857_row108_col8\" class=\"data row108 col8\" >0.6136</td>\n",
              "      <td id=\"T_02857_row108_col9\" class=\"data row108 col9\" >0.5197</td>\n",
              "      <td id=\"T_02857_row108_col10\" class=\"data row108 col10\" >0.5161</td>\n",
              "      <td id=\"T_02857_row108_col11\" class=\"data row108 col11\" >0.4454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row109\" class=\"row_heading level0 row109\" >109</th>\n",
              "      <td id=\"T_02857_row109_col0\" class=\"data row109 col0\" >en</td>\n",
              "      <td id=\"T_02857_row109_col1\" class=\"data row109 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row109_col2\" class=\"data row109 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row109_col3\" class=\"data row109 col3\" >0.4421</td>\n",
              "      <td id=\"T_02857_row109_col4\" class=\"data row109 col4\" >0.4521</td>\n",
              "      <td id=\"T_02857_row109_col5\" class=\"data row109 col5\" >0.5558</td>\n",
              "      <td id=\"T_02857_row109_col6\" class=\"data row109 col6\" >0.5204</td>\n",
              "      <td id=\"T_02857_row109_col7\" class=\"data row109 col7\" >0.5117</td>\n",
              "      <td id=\"T_02857_row109_col8\" class=\"data row109 col8\" >0.5329</td>\n",
              "      <td id=\"T_02857_row109_col9\" class=\"data row109 col9\" >0.5360</td>\n",
              "      <td id=\"T_02857_row109_col10\" class=\"data row109 col10\" >0.4994</td>\n",
              "      <td id=\"T_02857_row109_col11\" class=\"data row109 col11\" >0.3132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row110\" class=\"row_heading level0 row110\" >110</th>\n",
              "      <td id=\"T_02857_row110_col0\" class=\"data row110 col0\" >en</td>\n",
              "      <td id=\"T_02857_row110_col1\" class=\"data row110 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row110_col2\" class=\"data row110 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row110_col3\" class=\"data row110 col3\" >0.4421</td>\n",
              "      <td id=\"T_02857_row110_col4\" class=\"data row110 col4\" >0.4521</td>\n",
              "      <td id=\"T_02857_row110_col5\" class=\"data row110 col5\" >0.5558</td>\n",
              "      <td id=\"T_02857_row110_col6\" class=\"data row110 col6\" >0.5204</td>\n",
              "      <td id=\"T_02857_row110_col7\" class=\"data row110 col7\" >0.5117</td>\n",
              "      <td id=\"T_02857_row110_col8\" class=\"data row110 col8\" >0.5329</td>\n",
              "      <td id=\"T_02857_row110_col9\" class=\"data row110 col9\" >0.5360</td>\n",
              "      <td id=\"T_02857_row110_col10\" class=\"data row110 col10\" >0.4994</td>\n",
              "      <td id=\"T_02857_row110_col11\" class=\"data row110 col11\" >0.3132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row111\" class=\"row_heading level0 row111\" >111</th>\n",
              "      <td id=\"T_02857_row111_col0\" class=\"data row111 col0\" >en</td>\n",
              "      <td id=\"T_02857_row111_col1\" class=\"data row111 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row111_col2\" class=\"data row111 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row111_col3\" class=\"data row111 col3\" >0.7366</td>\n",
              "      <td id=\"T_02857_row111_col4\" class=\"data row111 col4\" >0.7568</td>\n",
              "      <td id=\"T_02857_row111_col5\" class=\"data row111 col5\" >0.7242</td>\n",
              "      <td id=\"T_02857_row111_col6\" class=\"data row111 col6\" >0.7312</td>\n",
              "      <td id=\"T_02857_row111_col7\" class=\"data row111 col7\" >0.6741</td>\n",
              "      <td id=\"T_02857_row111_col8\" class=\"data row111 col8\" >0.3665</td>\n",
              "      <td id=\"T_02857_row111_col9\" class=\"data row111 col9\" >0.4275</td>\n",
              "      <td id=\"T_02857_row111_col10\" class=\"data row111 col10\" >0.4706</td>\n",
              "      <td id=\"T_02857_row111_col11\" class=\"data row111 col11\" >0.5007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row112\" class=\"row_heading level0 row112\" >112</th>\n",
              "      <td id=\"T_02857_row112_col0\" class=\"data row112 col0\" >en</td>\n",
              "      <td id=\"T_02857_row112_col1\" class=\"data row112 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row112_col2\" class=\"data row112 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row112_col3\" class=\"data row112 col3\" >0.7366</td>\n",
              "      <td id=\"T_02857_row112_col4\" class=\"data row112 col4\" >0.7568</td>\n",
              "      <td id=\"T_02857_row112_col5\" class=\"data row112 col5\" >0.7242</td>\n",
              "      <td id=\"T_02857_row112_col6\" class=\"data row112 col6\" >0.7312</td>\n",
              "      <td id=\"T_02857_row112_col7\" class=\"data row112 col7\" >0.6741</td>\n",
              "      <td id=\"T_02857_row112_col8\" class=\"data row112 col8\" >0.3665</td>\n",
              "      <td id=\"T_02857_row112_col9\" class=\"data row112 col9\" >0.4275</td>\n",
              "      <td id=\"T_02857_row112_col10\" class=\"data row112 col10\" >0.4706</td>\n",
              "      <td id=\"T_02857_row112_col11\" class=\"data row112 col11\" >0.5007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row113\" class=\"row_heading level0 row113\" >113</th>\n",
              "      <td id=\"T_02857_row113_col0\" class=\"data row113 col0\" >en</td>\n",
              "      <td id=\"T_02857_row113_col1\" class=\"data row113 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row113_col2\" class=\"data row113 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row113_col3\" class=\"data row113 col3\" >0.5925</td>\n",
              "      <td id=\"T_02857_row113_col4\" class=\"data row113 col4\" >0.5773</td>\n",
              "      <td id=\"T_02857_row113_col5\" class=\"data row113 col5\" >0.5538</td>\n",
              "      <td id=\"T_02857_row113_col6\" class=\"data row113 col6\" >0.5328</td>\n",
              "      <td id=\"T_02857_row113_col7\" class=\"data row113 col7\" >0.5141</td>\n",
              "      <td id=\"T_02857_row113_col8\" class=\"data row113 col8\" >0.2131</td>\n",
              "      <td id=\"T_02857_row113_col9\" class=\"data row113 col9\" >0.4013</td>\n",
              "      <td id=\"T_02857_row113_col10\" class=\"data row113 col10\" >0.2795</td>\n",
              "      <td id=\"T_02857_row113_col11\" class=\"data row113 col11\" >0.4531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row114\" class=\"row_heading level0 row114\" >114</th>\n",
              "      <td id=\"T_02857_row114_col0\" class=\"data row114 col0\" >en</td>\n",
              "      <td id=\"T_02857_row114_col1\" class=\"data row114 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row114_col2\" class=\"data row114 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row114_col3\" class=\"data row114 col3\" >0.5925</td>\n",
              "      <td id=\"T_02857_row114_col4\" class=\"data row114 col4\" >0.5773</td>\n",
              "      <td id=\"T_02857_row114_col5\" class=\"data row114 col5\" >0.5538</td>\n",
              "      <td id=\"T_02857_row114_col6\" class=\"data row114 col6\" >0.5328</td>\n",
              "      <td id=\"T_02857_row114_col7\" class=\"data row114 col7\" >0.5141</td>\n",
              "      <td id=\"T_02857_row114_col8\" class=\"data row114 col8\" >0.2131</td>\n",
              "      <td id=\"T_02857_row114_col9\" class=\"data row114 col9\" >0.4013</td>\n",
              "      <td id=\"T_02857_row114_col10\" class=\"data row114 col10\" >0.2795</td>\n",
              "      <td id=\"T_02857_row114_col11\" class=\"data row114 col11\" >0.4531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row115\" class=\"row_heading level0 row115\" >115</th>\n",
              "      <td id=\"T_02857_row115_col0\" class=\"data row115 col0\" >en</td>\n",
              "      <td id=\"T_02857_row115_col1\" class=\"data row115 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row115_col2\" class=\"data row115 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row115_col3\" class=\"data row115 col3\" >0.6093</td>\n",
              "      <td id=\"T_02857_row115_col4\" class=\"data row115 col4\" >0.6150</td>\n",
              "      <td id=\"T_02857_row115_col5\" class=\"data row115 col5\" >0.6170</td>\n",
              "      <td id=\"T_02857_row115_col6\" class=\"data row115 col6\" >0.5980</td>\n",
              "      <td id=\"T_02857_row115_col7\" class=\"data row115 col7\" >0.6029</td>\n",
              "      <td id=\"T_02857_row115_col8\" class=\"data row115 col8\" >0.5310</td>\n",
              "      <td id=\"T_02857_row115_col9\" class=\"data row115 col9\" >0.4421</td>\n",
              "      <td id=\"T_02857_row115_col10\" class=\"data row115 col10\" >0.5623</td>\n",
              "      <td id=\"T_02857_row115_col11\" class=\"data row115 col11\" >0.5792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row116\" class=\"row_heading level0 row116\" >116</th>\n",
              "      <td id=\"T_02857_row116_col0\" class=\"data row116 col0\" >en</td>\n",
              "      <td id=\"T_02857_row116_col1\" class=\"data row116 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row116_col2\" class=\"data row116 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row116_col3\" class=\"data row116 col3\" >0.6093</td>\n",
              "      <td id=\"T_02857_row116_col4\" class=\"data row116 col4\" >0.6150</td>\n",
              "      <td id=\"T_02857_row116_col5\" class=\"data row116 col5\" >0.6170</td>\n",
              "      <td id=\"T_02857_row116_col6\" class=\"data row116 col6\" >0.5980</td>\n",
              "      <td id=\"T_02857_row116_col7\" class=\"data row116 col7\" >0.6029</td>\n",
              "      <td id=\"T_02857_row116_col8\" class=\"data row116 col8\" >0.5310</td>\n",
              "      <td id=\"T_02857_row116_col9\" class=\"data row116 col9\" >0.4421</td>\n",
              "      <td id=\"T_02857_row116_col10\" class=\"data row116 col10\" >0.5623</td>\n",
              "      <td id=\"T_02857_row116_col11\" class=\"data row116 col11\" >0.5792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row117\" class=\"row_heading level0 row117\" >117</th>\n",
              "      <td id=\"T_02857_row117_col0\" class=\"data row117 col0\" >en</td>\n",
              "      <td id=\"T_02857_row117_col1\" class=\"data row117 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row117_col2\" class=\"data row117 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row117_col3\" class=\"data row117 col3\" >0.7476</td>\n",
              "      <td id=\"T_02857_row117_col4\" class=\"data row117 col4\" >0.7807</td>\n",
              "      <td id=\"T_02857_row117_col5\" class=\"data row117 col5\" >0.7038</td>\n",
              "      <td id=\"T_02857_row117_col6\" class=\"data row117 col6\" >0.7234</td>\n",
              "      <td id=\"T_02857_row117_col7\" class=\"data row117 col7\" >0.6598</td>\n",
              "      <td id=\"T_02857_row117_col8\" class=\"data row117 col8\" >0.5116</td>\n",
              "      <td id=\"T_02857_row117_col9\" class=\"data row117 col9\" >0.4878</td>\n",
              "      <td id=\"T_02857_row117_col10\" class=\"data row117 col10\" >0.4896</td>\n",
              "      <td id=\"T_02857_row117_col11\" class=\"data row117 col11\" >0.4653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row118\" class=\"row_heading level0 row118\" >118</th>\n",
              "      <td id=\"T_02857_row118_col0\" class=\"data row118 col0\" >en</td>\n",
              "      <td id=\"T_02857_row118_col1\" class=\"data row118 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row118_col2\" class=\"data row118 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row118_col3\" class=\"data row118 col3\" >0.7476</td>\n",
              "      <td id=\"T_02857_row118_col4\" class=\"data row118 col4\" >0.7807</td>\n",
              "      <td id=\"T_02857_row118_col5\" class=\"data row118 col5\" >0.7038</td>\n",
              "      <td id=\"T_02857_row118_col6\" class=\"data row118 col6\" >0.7234</td>\n",
              "      <td id=\"T_02857_row118_col7\" class=\"data row118 col7\" >0.6598</td>\n",
              "      <td id=\"T_02857_row118_col8\" class=\"data row118 col8\" >0.5116</td>\n",
              "      <td id=\"T_02857_row118_col9\" class=\"data row118 col9\" >0.4878</td>\n",
              "      <td id=\"T_02857_row118_col10\" class=\"data row118 col10\" >0.4896</td>\n",
              "      <td id=\"T_02857_row118_col11\" class=\"data row118 col11\" >0.4653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row119\" class=\"row_heading level0 row119\" >119</th>\n",
              "      <td id=\"T_02857_row119_col0\" class=\"data row119 col0\" >es</td>\n",
              "      <td id=\"T_02857_row119_col1\" class=\"data row119 col1\" >all</td>\n",
              "      <td id=\"T_02857_row119_col2\" class=\"data row119 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row119_col3\" class=\"data row119 col3\" >0.7564</td>\n",
              "      <td id=\"T_02857_row119_col4\" class=\"data row119 col4\" >0.7745</td>\n",
              "      <td id=\"T_02857_row119_col5\" class=\"data row119 col5\" >0.7620</td>\n",
              "      <td id=\"T_02857_row119_col6\" class=\"data row119 col6\" >0.7703</td>\n",
              "      <td id=\"T_02857_row119_col7\" class=\"data row119 col7\" >0.7478</td>\n",
              "      <td id=\"T_02857_row119_col8\" class=\"data row119 col8\" >0.7461</td>\n",
              "      <td id=\"T_02857_row119_col9\" class=\"data row119 col9\" >0.7624</td>\n",
              "      <td id=\"T_02857_row119_col10\" class=\"data row119 col10\" >0.6835</td>\n",
              "      <td id=\"T_02857_row119_col11\" class=\"data row119 col11\" >0.7400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row120\" class=\"row_heading level0 row120\" >120</th>\n",
              "      <td id=\"T_02857_row120_col0\" class=\"data row120 col0\" >es</td>\n",
              "      <td id=\"T_02857_row120_col1\" class=\"data row120 col1\" >all</td>\n",
              "      <td id=\"T_02857_row120_col2\" class=\"data row120 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row120_col3\" class=\"data row120 col3\" >0.4082</td>\n",
              "      <td id=\"T_02857_row120_col4\" class=\"data row120 col4\" >0.4130</td>\n",
              "      <td id=\"T_02857_row120_col5\" class=\"data row120 col5\" >0.4355</td>\n",
              "      <td id=\"T_02857_row120_col6\" class=\"data row120 col6\" >0.4274</td>\n",
              "      <td id=\"T_02857_row120_col7\" class=\"data row120 col7\" >0.4274</td>\n",
              "      <td id=\"T_02857_row120_col8\" class=\"data row120 col8\" >0.4265</td>\n",
              "      <td id=\"T_02857_row120_col9\" class=\"data row120 col9\" >0.4411</td>\n",
              "      <td id=\"T_02857_row120_col10\" class=\"data row120 col10\" >0.4351</td>\n",
              "      <td id=\"T_02857_row120_col11\" class=\"data row120 col11\" >0.4915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row121\" class=\"row_heading level0 row121\" >121</th>\n",
              "      <td id=\"T_02857_row121_col0\" class=\"data row121 col0\" >es</td>\n",
              "      <td id=\"T_02857_row121_col1\" class=\"data row121 col1\" >all</td>\n",
              "      <td id=\"T_02857_row121_col2\" class=\"data row121 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row121_col3\" class=\"data row121 col3\" >0.5694</td>\n",
              "      <td id=\"T_02857_row121_col4\" class=\"data row121 col4\" >0.6103</td>\n",
              "      <td id=\"T_02857_row121_col5\" class=\"data row121 col5\" >0.6296</td>\n",
              "      <td id=\"T_02857_row121_col6\" class=\"data row121 col6\" >0.6217</td>\n",
              "      <td id=\"T_02857_row121_col7\" class=\"data row121 col7\" >0.6392</td>\n",
              "      <td id=\"T_02857_row121_col8\" class=\"data row121 col8\" >0.6401</td>\n",
              "      <td id=\"T_02857_row121_col9\" class=\"data row121 col9\" >0.6458</td>\n",
              "      <td id=\"T_02857_row121_col10\" class=\"data row121 col10\" >0.5655</td>\n",
              "      <td id=\"T_02857_row121_col11\" class=\"data row121 col11\" >0.5680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row122\" class=\"row_heading level0 row122\" >122</th>\n",
              "      <td id=\"T_02857_row122_col0\" class=\"data row122 col0\" >es</td>\n",
              "      <td id=\"T_02857_row122_col1\" class=\"data row122 col1\" >all</td>\n",
              "      <td id=\"T_02857_row122_col2\" class=\"data row122 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row122_col3\" class=\"data row122 col3\" >0.7177</td>\n",
              "      <td id=\"T_02857_row122_col4\" class=\"data row122 col4\" >0.7341</td>\n",
              "      <td id=\"T_02857_row122_col5\" class=\"data row122 col5\" >0.7193</td>\n",
              "      <td id=\"T_02857_row122_col6\" class=\"data row122 col6\" >0.7246</td>\n",
              "      <td id=\"T_02857_row122_col7\" class=\"data row122 col7\" >0.7136</td>\n",
              "      <td id=\"T_02857_row122_col8\" class=\"data row122 col8\" >0.7053</td>\n",
              "      <td id=\"T_02857_row122_col9\" class=\"data row122 col9\" >0.7285</td>\n",
              "      <td id=\"T_02857_row122_col10\" class=\"data row122 col10\" >0.6411</td>\n",
              "      <td id=\"T_02857_row122_col11\" class=\"data row122 col11\" >0.6849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row123\" class=\"row_heading level0 row123\" >123</th>\n",
              "      <td id=\"T_02857_row123_col0\" class=\"data row123 col0\" >es</td>\n",
              "      <td id=\"T_02857_row123_col1\" class=\"data row123 col1\" >all</td>\n",
              "      <td id=\"T_02857_row123_col2\" class=\"data row123 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row123_col3\" class=\"data row123 col3\" >0.7692</td>\n",
              "      <td id=\"T_02857_row123_col4\" class=\"data row123 col4\" >0.7731</td>\n",
              "      <td id=\"T_02857_row123_col5\" class=\"data row123 col5\" >0.7721</td>\n",
              "      <td id=\"T_02857_row123_col6\" class=\"data row123 col6\" >0.7722</td>\n",
              "      <td id=\"T_02857_row123_col7\" class=\"data row123 col7\" >0.7680</td>\n",
              "      <td id=\"T_02857_row123_col8\" class=\"data row123 col8\" >0.7768</td>\n",
              "      <td id=\"T_02857_row123_col9\" class=\"data row123 col9\" >0.7611</td>\n",
              "      <td id=\"T_02857_row123_col10\" class=\"data row123 col10\" >0.7375</td>\n",
              "      <td id=\"T_02857_row123_col11\" class=\"data row123 col11\" >0.7961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row124\" class=\"row_heading level0 row124\" >124</th>\n",
              "      <td id=\"T_02857_row124_col0\" class=\"data row124 col0\" >es</td>\n",
              "      <td id=\"T_02857_row124_col1\" class=\"data row124 col1\" >all</td>\n",
              "      <td id=\"T_02857_row124_col2\" class=\"data row124 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row124_col3\" class=\"data row124 col3\" >0.6559</td>\n",
              "      <td id=\"T_02857_row124_col4\" class=\"data row124 col4\" >0.6799</td>\n",
              "      <td id=\"T_02857_row124_col5\" class=\"data row124 col5\" >0.6689</td>\n",
              "      <td id=\"T_02857_row124_col6\" class=\"data row124 col6\" >0.6813</td>\n",
              "      <td id=\"T_02857_row124_col7\" class=\"data row124 col7\" >0.6709</td>\n",
              "      <td id=\"T_02857_row124_col8\" class=\"data row124 col8\" >0.7291</td>\n",
              "      <td id=\"T_02857_row124_col9\" class=\"data row124 col9\" >0.7130</td>\n",
              "      <td id=\"T_02857_row124_col10\" class=\"data row124 col10\" >0.6555</td>\n",
              "      <td id=\"T_02857_row124_col11\" class=\"data row124 col11\" >0.6349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row125\" class=\"row_heading level0 row125\" >125</th>\n",
              "      <td id=\"T_02857_row125_col0\" class=\"data row125 col0\" >es</td>\n",
              "      <td id=\"T_02857_row125_col1\" class=\"data row125 col1\" >all</td>\n",
              "      <td id=\"T_02857_row125_col2\" class=\"data row125 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row125_col3\" class=\"data row125 col3\" >0.8228</td>\n",
              "      <td id=\"T_02857_row125_col4\" class=\"data row125 col4\" >0.8280</td>\n",
              "      <td id=\"T_02857_row125_col5\" class=\"data row125 col5\" >0.8242</td>\n",
              "      <td id=\"T_02857_row125_col6\" class=\"data row125 col6\" >0.8227</td>\n",
              "      <td id=\"T_02857_row125_col7\" class=\"data row125 col7\" >0.8117</td>\n",
              "      <td id=\"T_02857_row125_col8\" class=\"data row125 col8\" >0.7920</td>\n",
              "      <td id=\"T_02857_row125_col9\" class=\"data row125 col9\" >0.8136</td>\n",
              "      <td id=\"T_02857_row125_col10\" class=\"data row125 col10\" >0.7613</td>\n",
              "      <td id=\"T_02857_row125_col11\" class=\"data row125 col11\" >0.8110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row126\" class=\"row_heading level0 row126\" >126</th>\n",
              "      <td id=\"T_02857_row126_col0\" class=\"data row126 col0\" >es</td>\n",
              "      <td id=\"T_02857_row126_col1\" class=\"data row126 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row126_col2\" class=\"data row126 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row126_col3\" class=\"data row126 col3\" >0.8078</td>\n",
              "      <td id=\"T_02857_row126_col4\" class=\"data row126 col4\" >0.8905</td>\n",
              "      <td id=\"T_02857_row126_col5\" class=\"data row126 col5\" >0.8581</td>\n",
              "      <td id=\"T_02857_row126_col6\" class=\"data row126 col6\" >0.8268</td>\n",
              "      <td id=\"T_02857_row126_col7\" class=\"data row126 col7\" >0.8295</td>\n",
              "      <td id=\"T_02857_row126_col8\" class=\"data row126 col8\" >0.5286</td>\n",
              "      <td id=\"T_02857_row126_col9\" class=\"data row126 col9\" >0.5157</td>\n",
              "      <td id=\"T_02857_row126_col10\" class=\"data row126 col10\" >0.4523</td>\n",
              "      <td id=\"T_02857_row126_col11\" class=\"data row126 col11\" >0.5447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row127\" class=\"row_heading level0 row127\" >127</th>\n",
              "      <td id=\"T_02857_row127_col0\" class=\"data row127 col0\" >es</td>\n",
              "      <td id=\"T_02857_row127_col1\" class=\"data row127 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row127_col2\" class=\"data row127 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row127_col3\" class=\"data row127 col3\" >0.8078</td>\n",
              "      <td id=\"T_02857_row127_col4\" class=\"data row127 col4\" >0.8905</td>\n",
              "      <td id=\"T_02857_row127_col5\" class=\"data row127 col5\" >0.8581</td>\n",
              "      <td id=\"T_02857_row127_col6\" class=\"data row127 col6\" >0.8268</td>\n",
              "      <td id=\"T_02857_row127_col7\" class=\"data row127 col7\" >0.8295</td>\n",
              "      <td id=\"T_02857_row127_col8\" class=\"data row127 col8\" >0.5286</td>\n",
              "      <td id=\"T_02857_row127_col9\" class=\"data row127 col9\" >0.5157</td>\n",
              "      <td id=\"T_02857_row127_col10\" class=\"data row127 col10\" >0.4523</td>\n",
              "      <td id=\"T_02857_row127_col11\" class=\"data row127 col11\" >0.5447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row128\" class=\"row_heading level0 row128\" >128</th>\n",
              "      <td id=\"T_02857_row128_col0\" class=\"data row128 col0\" >es</td>\n",
              "      <td id=\"T_02857_row128_col1\" class=\"data row128 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row128_col2\" class=\"data row128 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row128_col3\" class=\"data row128 col3\" >0.5105</td>\n",
              "      <td id=\"T_02857_row128_col4\" class=\"data row128 col4\" >0.5414</td>\n",
              "      <td id=\"T_02857_row128_col5\" class=\"data row128 col5\" >0.5473</td>\n",
              "      <td id=\"T_02857_row128_col6\" class=\"data row128 col6\" >0.5624</td>\n",
              "      <td id=\"T_02857_row128_col7\" class=\"data row128 col7\" >0.5562</td>\n",
              "      <td id=\"T_02857_row128_col8\" class=\"data row128 col8\" >0.4795</td>\n",
              "      <td id=\"T_02857_row128_col9\" class=\"data row128 col9\" >0.5363</td>\n",
              "      <td id=\"T_02857_row128_col10\" class=\"data row128 col10\" >0.4681</td>\n",
              "      <td id=\"T_02857_row128_col11\" class=\"data row128 col11\" >0.3351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row129\" class=\"row_heading level0 row129\" >129</th>\n",
              "      <td id=\"T_02857_row129_col0\" class=\"data row129 col0\" >es</td>\n",
              "      <td id=\"T_02857_row129_col1\" class=\"data row129 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row129_col2\" class=\"data row129 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row129_col3\" class=\"data row129 col3\" >0.5105</td>\n",
              "      <td id=\"T_02857_row129_col4\" class=\"data row129 col4\" >0.5414</td>\n",
              "      <td id=\"T_02857_row129_col5\" class=\"data row129 col5\" >0.5473</td>\n",
              "      <td id=\"T_02857_row129_col6\" class=\"data row129 col6\" >0.5624</td>\n",
              "      <td id=\"T_02857_row129_col7\" class=\"data row129 col7\" >0.5562</td>\n",
              "      <td id=\"T_02857_row129_col8\" class=\"data row129 col8\" >0.4795</td>\n",
              "      <td id=\"T_02857_row129_col9\" class=\"data row129 col9\" >0.5363</td>\n",
              "      <td id=\"T_02857_row129_col10\" class=\"data row129 col10\" >0.4681</td>\n",
              "      <td id=\"T_02857_row129_col11\" class=\"data row129 col11\" >0.3351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row130\" class=\"row_heading level0 row130\" >130</th>\n",
              "      <td id=\"T_02857_row130_col0\" class=\"data row130 col0\" >es</td>\n",
              "      <td id=\"T_02857_row130_col1\" class=\"data row130 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row130_col2\" class=\"data row130 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row130_col3\" class=\"data row130 col3\" >0.4491</td>\n",
              "      <td id=\"T_02857_row130_col4\" class=\"data row130 col4\" >0.5975</td>\n",
              "      <td id=\"T_02857_row130_col5\" class=\"data row130 col5\" >0.6630</td>\n",
              "      <td id=\"T_02857_row130_col6\" class=\"data row130 col6\" >0.5730</td>\n",
              "      <td id=\"T_02857_row130_col7\" class=\"data row130 col7\" >0.7259</td>\n",
              "      <td id=\"T_02857_row130_col8\" class=\"data row130 col8\" >0.5837</td>\n",
              "      <td id=\"T_02857_row130_col9\" class=\"data row130 col9\" >0.5714</td>\n",
              "      <td id=\"T_02857_row130_col10\" class=\"data row130 col10\" >0.4916</td>\n",
              "      <td id=\"T_02857_row130_col11\" class=\"data row130 col11\" >0.3963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row131\" class=\"row_heading level0 row131\" >131</th>\n",
              "      <td id=\"T_02857_row131_col0\" class=\"data row131 col0\" >es</td>\n",
              "      <td id=\"T_02857_row131_col1\" class=\"data row131 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row131_col2\" class=\"data row131 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row131_col3\" class=\"data row131 col3\" >0.4491</td>\n",
              "      <td id=\"T_02857_row131_col4\" class=\"data row131 col4\" >0.5975</td>\n",
              "      <td id=\"T_02857_row131_col5\" class=\"data row131 col5\" >0.6630</td>\n",
              "      <td id=\"T_02857_row131_col6\" class=\"data row131 col6\" >0.5730</td>\n",
              "      <td id=\"T_02857_row131_col7\" class=\"data row131 col7\" >0.7259</td>\n",
              "      <td id=\"T_02857_row131_col8\" class=\"data row131 col8\" >0.5837</td>\n",
              "      <td id=\"T_02857_row131_col9\" class=\"data row131 col9\" >0.5714</td>\n",
              "      <td id=\"T_02857_row131_col10\" class=\"data row131 col10\" >0.4916</td>\n",
              "      <td id=\"T_02857_row131_col11\" class=\"data row131 col11\" >0.3963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row132\" class=\"row_heading level0 row132\" >132</th>\n",
              "      <td id=\"T_02857_row132_col0\" class=\"data row132 col0\" >es</td>\n",
              "      <td id=\"T_02857_row132_col1\" class=\"data row132 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row132_col2\" class=\"data row132 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row132_col3\" class=\"data row132 col3\" >0.6966</td>\n",
              "      <td id=\"T_02857_row132_col4\" class=\"data row132 col4\" >0.8005</td>\n",
              "      <td id=\"T_02857_row132_col5\" class=\"data row132 col5\" >0.7748</td>\n",
              "      <td id=\"T_02857_row132_col6\" class=\"data row132 col6\" >0.7508</td>\n",
              "      <td id=\"T_02857_row132_col7\" class=\"data row132 col7\" >0.8025</td>\n",
              "      <td id=\"T_02857_row132_col8\" class=\"data row132 col8\" >0.5592</td>\n",
              "      <td id=\"T_02857_row132_col9\" class=\"data row132 col9\" >0.6114</td>\n",
              "      <td id=\"T_02857_row132_col10\" class=\"data row132 col10\" >0.5314</td>\n",
              "      <td id=\"T_02857_row132_col11\" class=\"data row132 col11\" >0.5555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row133\" class=\"row_heading level0 row133\" >133</th>\n",
              "      <td id=\"T_02857_row133_col0\" class=\"data row133 col0\" >es</td>\n",
              "      <td id=\"T_02857_row133_col1\" class=\"data row133 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row133_col2\" class=\"data row133 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row133_col3\" class=\"data row133 col3\" >0.6966</td>\n",
              "      <td id=\"T_02857_row133_col4\" class=\"data row133 col4\" >0.8005</td>\n",
              "      <td id=\"T_02857_row133_col5\" class=\"data row133 col5\" >0.7748</td>\n",
              "      <td id=\"T_02857_row133_col6\" class=\"data row133 col6\" >0.7508</td>\n",
              "      <td id=\"T_02857_row133_col7\" class=\"data row133 col7\" >0.8025</td>\n",
              "      <td id=\"T_02857_row133_col8\" class=\"data row133 col8\" >0.5592</td>\n",
              "      <td id=\"T_02857_row133_col9\" class=\"data row133 col9\" >0.6114</td>\n",
              "      <td id=\"T_02857_row133_col10\" class=\"data row133 col10\" >0.5314</td>\n",
              "      <td id=\"T_02857_row133_col11\" class=\"data row133 col11\" >0.5555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row134\" class=\"row_heading level0 row134\" >134</th>\n",
              "      <td id=\"T_02857_row134_col0\" class=\"data row134 col0\" >es</td>\n",
              "      <td id=\"T_02857_row134_col1\" class=\"data row134 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row134_col2\" class=\"data row134 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row134_col3\" class=\"data row134 col3\" >0.8237</td>\n",
              "      <td id=\"T_02857_row134_col4\" class=\"data row134 col4\" >0.8505</td>\n",
              "      <td id=\"T_02857_row134_col5\" class=\"data row134 col5\" >0.8632</td>\n",
              "      <td id=\"T_02857_row134_col6\" class=\"data row134 col6\" >0.8484</td>\n",
              "      <td id=\"T_02857_row134_col7\" class=\"data row134 col7\" >0.8465</td>\n",
              "      <td id=\"T_02857_row134_col8\" class=\"data row134 col8\" >0.6004</td>\n",
              "      <td id=\"T_02857_row134_col9\" class=\"data row134 col9\" >0.4766</td>\n",
              "      <td id=\"T_02857_row134_col10\" class=\"data row134 col10\" >0.5173</td>\n",
              "      <td id=\"T_02857_row134_col11\" class=\"data row134 col11\" >0.5463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row135\" class=\"row_heading level0 row135\" >135</th>\n",
              "      <td id=\"T_02857_row135_col0\" class=\"data row135 col0\" >es</td>\n",
              "      <td id=\"T_02857_row135_col1\" class=\"data row135 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row135_col2\" class=\"data row135 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row135_col3\" class=\"data row135 col3\" >0.8237</td>\n",
              "      <td id=\"T_02857_row135_col4\" class=\"data row135 col4\" >0.8505</td>\n",
              "      <td id=\"T_02857_row135_col5\" class=\"data row135 col5\" >0.8632</td>\n",
              "      <td id=\"T_02857_row135_col6\" class=\"data row135 col6\" >0.8484</td>\n",
              "      <td id=\"T_02857_row135_col7\" class=\"data row135 col7\" >0.8465</td>\n",
              "      <td id=\"T_02857_row135_col8\" class=\"data row135 col8\" >0.6004</td>\n",
              "      <td id=\"T_02857_row135_col9\" class=\"data row135 col9\" >0.4766</td>\n",
              "      <td id=\"T_02857_row135_col10\" class=\"data row135 col10\" >0.5173</td>\n",
              "      <td id=\"T_02857_row135_col11\" class=\"data row135 col11\" >0.5463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row136\" class=\"row_heading level0 row136\" >136</th>\n",
              "      <td id=\"T_02857_row136_col0\" class=\"data row136 col0\" >es</td>\n",
              "      <td id=\"T_02857_row136_col1\" class=\"data row136 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row136_col2\" class=\"data row136 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row136_col3\" class=\"data row136 col3\" >0.6452</td>\n",
              "      <td id=\"T_02857_row136_col4\" class=\"data row136 col4\" >0.6847</td>\n",
              "      <td id=\"T_02857_row136_col5\" class=\"data row136 col5\" >0.6667</td>\n",
              "      <td id=\"T_02857_row136_col6\" class=\"data row136 col6\" >0.6629</td>\n",
              "      <td id=\"T_02857_row136_col7\" class=\"data row136 col7\" >0.6807</td>\n",
              "      <td id=\"T_02857_row136_col8\" class=\"data row136 col8\" >0.3758</td>\n",
              "      <td id=\"T_02857_row136_col9\" class=\"data row136 col9\" >0.5254</td>\n",
              "      <td id=\"T_02857_row136_col10\" class=\"data row136 col10\" >0.4764</td>\n",
              "      <td id=\"T_02857_row136_col11\" class=\"data row136 col11\" >0.4067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row137\" class=\"row_heading level0 row137\" >137</th>\n",
              "      <td id=\"T_02857_row137_col0\" class=\"data row137 col0\" >es</td>\n",
              "      <td id=\"T_02857_row137_col1\" class=\"data row137 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row137_col2\" class=\"data row137 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row137_col3\" class=\"data row137 col3\" >0.6452</td>\n",
              "      <td id=\"T_02857_row137_col4\" class=\"data row137 col4\" >0.6847</td>\n",
              "      <td id=\"T_02857_row137_col5\" class=\"data row137 col5\" >0.6667</td>\n",
              "      <td id=\"T_02857_row137_col6\" class=\"data row137 col6\" >0.6629</td>\n",
              "      <td id=\"T_02857_row137_col7\" class=\"data row137 col7\" >0.6807</td>\n",
              "      <td id=\"T_02857_row137_col8\" class=\"data row137 col8\" >0.3758</td>\n",
              "      <td id=\"T_02857_row137_col9\" class=\"data row137 col9\" >0.5254</td>\n",
              "      <td id=\"T_02857_row137_col10\" class=\"data row137 col10\" >0.4764</td>\n",
              "      <td id=\"T_02857_row137_col11\" class=\"data row137 col11\" >0.4067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row138\" class=\"row_heading level0 row138\" >138</th>\n",
              "      <td id=\"T_02857_row138_col0\" class=\"data row138 col0\" >es</td>\n",
              "      <td id=\"T_02857_row138_col1\" class=\"data row138 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row138_col2\" class=\"data row138 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row138_col3\" class=\"data row138 col3\" >0.6452</td>\n",
              "      <td id=\"T_02857_row138_col4\" class=\"data row138 col4\" >0.8136</td>\n",
              "      <td id=\"T_02857_row138_col5\" class=\"data row138 col5\" >0.8169</td>\n",
              "      <td id=\"T_02857_row138_col6\" class=\"data row138 col6\" >0.7653</td>\n",
              "      <td id=\"T_02857_row138_col7\" class=\"data row138 col7\" >0.8599</td>\n",
              "      <td id=\"T_02857_row138_col8\" class=\"data row138 col8\" >0.5307</td>\n",
              "      <td id=\"T_02857_row138_col9\" class=\"data row138 col9\" >0.4279</td>\n",
              "      <td id=\"T_02857_row138_col10\" class=\"data row138 col10\" >0.4353</td>\n",
              "      <td id=\"T_02857_row138_col11\" class=\"data row138 col11\" >0.4647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row139\" class=\"row_heading level0 row139\" >139</th>\n",
              "      <td id=\"T_02857_row139_col0\" class=\"data row139 col0\" >es</td>\n",
              "      <td id=\"T_02857_row139_col1\" class=\"data row139 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row139_col2\" class=\"data row139 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row139_col3\" class=\"data row139 col3\" >0.6452</td>\n",
              "      <td id=\"T_02857_row139_col4\" class=\"data row139 col4\" >0.8136</td>\n",
              "      <td id=\"T_02857_row139_col5\" class=\"data row139 col5\" >0.8169</td>\n",
              "      <td id=\"T_02857_row139_col6\" class=\"data row139 col6\" >0.7653</td>\n",
              "      <td id=\"T_02857_row139_col7\" class=\"data row139 col7\" >0.8599</td>\n",
              "      <td id=\"T_02857_row139_col8\" class=\"data row139 col8\" >0.5307</td>\n",
              "      <td id=\"T_02857_row139_col9\" class=\"data row139 col9\" >0.4279</td>\n",
              "      <td id=\"T_02857_row139_col10\" class=\"data row139 col10\" >0.4353</td>\n",
              "      <td id=\"T_02857_row139_col11\" class=\"data row139 col11\" >0.4647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row140\" class=\"row_heading level0 row140\" >140</th>\n",
              "      <td id=\"T_02857_row140_col0\" class=\"data row140 col0\" >es</td>\n",
              "      <td id=\"T_02857_row140_col1\" class=\"data row140 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row140_col2\" class=\"data row140 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row140_col3\" class=\"data row140 col3\" >0.8290</td>\n",
              "      <td id=\"T_02857_row140_col4\" class=\"data row140 col4\" >0.9011</td>\n",
              "      <td id=\"T_02857_row140_col5\" class=\"data row140 col5\" >0.8364</td>\n",
              "      <td id=\"T_02857_row140_col6\" class=\"data row140 col6\" >0.7909</td>\n",
              "      <td id=\"T_02857_row140_col7\" class=\"data row140 col7\" >0.7524</td>\n",
              "      <td id=\"T_02857_row140_col8\" class=\"data row140 col8\" >0.3736</td>\n",
              "      <td id=\"T_02857_row140_col9\" class=\"data row140 col9\" >0.3473</td>\n",
              "      <td id=\"T_02857_row140_col10\" class=\"data row140 col10\" >0.3842</td>\n",
              "      <td id=\"T_02857_row140_col11\" class=\"data row140 col11\" >0.4715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row141\" class=\"row_heading level0 row141\" >141</th>\n",
              "      <td id=\"T_02857_row141_col0\" class=\"data row141 col0\" >es</td>\n",
              "      <td id=\"T_02857_row141_col1\" class=\"data row141 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row141_col2\" class=\"data row141 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row141_col3\" class=\"data row141 col3\" >0.8290</td>\n",
              "      <td id=\"T_02857_row141_col4\" class=\"data row141 col4\" >0.9011</td>\n",
              "      <td id=\"T_02857_row141_col5\" class=\"data row141 col5\" >0.8364</td>\n",
              "      <td id=\"T_02857_row141_col6\" class=\"data row141 col6\" >0.7909</td>\n",
              "      <td id=\"T_02857_row141_col7\" class=\"data row141 col7\" >0.7524</td>\n",
              "      <td id=\"T_02857_row141_col8\" class=\"data row141 col8\" >0.3736</td>\n",
              "      <td id=\"T_02857_row141_col9\" class=\"data row141 col9\" >0.3473</td>\n",
              "      <td id=\"T_02857_row141_col10\" class=\"data row141 col10\" >0.3842</td>\n",
              "      <td id=\"T_02857_row141_col11\" class=\"data row141 col11\" >0.4715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row142\" class=\"row_heading level0 row142\" >142</th>\n",
              "      <td id=\"T_02857_row142_col0\" class=\"data row142 col0\" >es</td>\n",
              "      <td id=\"T_02857_row142_col1\" class=\"data row142 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row142_col2\" class=\"data row142 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row142_col3\" class=\"data row142 col3\" >0.5620</td>\n",
              "      <td id=\"T_02857_row142_col4\" class=\"data row142 col4\" >0.5958</td>\n",
              "      <td id=\"T_02857_row142_col5\" class=\"data row142 col5\" >0.5457</td>\n",
              "      <td id=\"T_02857_row142_col6\" class=\"data row142 col6\" >0.5732</td>\n",
              "      <td id=\"T_02857_row142_col7\" class=\"data row142 col7\" >0.5250</td>\n",
              "      <td id=\"T_02857_row142_col8\" class=\"data row142 col8\" >0.4277</td>\n",
              "      <td id=\"T_02857_row142_col9\" class=\"data row142 col9\" >0.4545</td>\n",
              "      <td id=\"T_02857_row142_col10\" class=\"data row142 col10\" >0.4451</td>\n",
              "      <td id=\"T_02857_row142_col11\" class=\"data row142 col11\" >0.3193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row143\" class=\"row_heading level0 row143\" >143</th>\n",
              "      <td id=\"T_02857_row143_col0\" class=\"data row143 col0\" >es</td>\n",
              "      <td id=\"T_02857_row143_col1\" class=\"data row143 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row143_col2\" class=\"data row143 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row143_col3\" class=\"data row143 col3\" >0.5620</td>\n",
              "      <td id=\"T_02857_row143_col4\" class=\"data row143 col4\" >0.5958</td>\n",
              "      <td id=\"T_02857_row143_col5\" class=\"data row143 col5\" >0.5457</td>\n",
              "      <td id=\"T_02857_row143_col6\" class=\"data row143 col6\" >0.5732</td>\n",
              "      <td id=\"T_02857_row143_col7\" class=\"data row143 col7\" >0.5250</td>\n",
              "      <td id=\"T_02857_row143_col8\" class=\"data row143 col8\" >0.4277</td>\n",
              "      <td id=\"T_02857_row143_col9\" class=\"data row143 col9\" >0.4545</td>\n",
              "      <td id=\"T_02857_row143_col10\" class=\"data row143 col10\" >0.4451</td>\n",
              "      <td id=\"T_02857_row143_col11\" class=\"data row143 col11\" >0.3193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row144\" class=\"row_heading level0 row144\" >144</th>\n",
              "      <td id=\"T_02857_row144_col0\" class=\"data row144 col0\" >es</td>\n",
              "      <td id=\"T_02857_row144_col1\" class=\"data row144 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row144_col2\" class=\"data row144 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row144_col3\" class=\"data row144 col3\" >0.5022</td>\n",
              "      <td id=\"T_02857_row144_col4\" class=\"data row144 col4\" >0.5785</td>\n",
              "      <td id=\"T_02857_row144_col5\" class=\"data row144 col5\" >0.5610</td>\n",
              "      <td id=\"T_02857_row144_col6\" class=\"data row144 col6\" >0.5366</td>\n",
              "      <td id=\"T_02857_row144_col7\" class=\"data row144 col7\" >0.5185</td>\n",
              "      <td id=\"T_02857_row144_col8\" class=\"data row144 col8\" >0.5121</td>\n",
              "      <td id=\"T_02857_row144_col9\" class=\"data row144 col9\" >0.5299</td>\n",
              "      <td id=\"T_02857_row144_col10\" class=\"data row144 col10\" >0.3966</td>\n",
              "      <td id=\"T_02857_row144_col11\" class=\"data row144 col11\" >0.2926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row145\" class=\"row_heading level0 row145\" >145</th>\n",
              "      <td id=\"T_02857_row145_col0\" class=\"data row145 col0\" >es</td>\n",
              "      <td id=\"T_02857_row145_col1\" class=\"data row145 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row145_col2\" class=\"data row145 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row145_col3\" class=\"data row145 col3\" >0.5022</td>\n",
              "      <td id=\"T_02857_row145_col4\" class=\"data row145 col4\" >0.5785</td>\n",
              "      <td id=\"T_02857_row145_col5\" class=\"data row145 col5\" >0.5610</td>\n",
              "      <td id=\"T_02857_row145_col6\" class=\"data row145 col6\" >0.5366</td>\n",
              "      <td id=\"T_02857_row145_col7\" class=\"data row145 col7\" >0.5185</td>\n",
              "      <td id=\"T_02857_row145_col8\" class=\"data row145 col8\" >0.5121</td>\n",
              "      <td id=\"T_02857_row145_col9\" class=\"data row145 col9\" >0.5299</td>\n",
              "      <td id=\"T_02857_row145_col10\" class=\"data row145 col10\" >0.3966</td>\n",
              "      <td id=\"T_02857_row145_col11\" class=\"data row145 col11\" >0.2926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row146\" class=\"row_heading level0 row146\" >146</th>\n",
              "      <td id=\"T_02857_row146_col0\" class=\"data row146 col0\" >es</td>\n",
              "      <td id=\"T_02857_row146_col1\" class=\"data row146 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row146_col2\" class=\"data row146 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row146_col3\" class=\"data row146 col3\" >0.8033</td>\n",
              "      <td id=\"T_02857_row146_col4\" class=\"data row146 col4\" >0.8596</td>\n",
              "      <td id=\"T_02857_row146_col5\" class=\"data row146 col5\" >0.7977</td>\n",
              "      <td id=\"T_02857_row146_col6\" class=\"data row146 col6\" >0.7721</td>\n",
              "      <td id=\"T_02857_row146_col7\" class=\"data row146 col7\" >0.7172</td>\n",
              "      <td id=\"T_02857_row146_col8\" class=\"data row146 col8\" >0.3702</td>\n",
              "      <td id=\"T_02857_row146_col9\" class=\"data row146 col9\" >0.3934</td>\n",
              "      <td id=\"T_02857_row146_col10\" class=\"data row146 col10\" >0.4093</td>\n",
              "      <td id=\"T_02857_row146_col11\" class=\"data row146 col11\" >0.4647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row147\" class=\"row_heading level0 row147\" >147</th>\n",
              "      <td id=\"T_02857_row147_col0\" class=\"data row147 col0\" >es</td>\n",
              "      <td id=\"T_02857_row147_col1\" class=\"data row147 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row147_col2\" class=\"data row147 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row147_col3\" class=\"data row147 col3\" >0.8033</td>\n",
              "      <td id=\"T_02857_row147_col4\" class=\"data row147 col4\" >0.8596</td>\n",
              "      <td id=\"T_02857_row147_col5\" class=\"data row147 col5\" >0.7977</td>\n",
              "      <td id=\"T_02857_row147_col6\" class=\"data row147 col6\" >0.7721</td>\n",
              "      <td id=\"T_02857_row147_col7\" class=\"data row147 col7\" >0.7172</td>\n",
              "      <td id=\"T_02857_row147_col8\" class=\"data row147 col8\" >0.3702</td>\n",
              "      <td id=\"T_02857_row147_col9\" class=\"data row147 col9\" >0.3934</td>\n",
              "      <td id=\"T_02857_row147_col10\" class=\"data row147 col10\" >0.4093</td>\n",
              "      <td id=\"T_02857_row147_col11\" class=\"data row147 col11\" >0.4647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row148\" class=\"row_heading level0 row148\" >148</th>\n",
              "      <td id=\"T_02857_row148_col0\" class=\"data row148 col0\" >es</td>\n",
              "      <td id=\"T_02857_row148_col1\" class=\"data row148 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row148_col2\" class=\"data row148 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row148_col3\" class=\"data row148 col3\" >0.9040</td>\n",
              "      <td id=\"T_02857_row148_col4\" class=\"data row148 col4\" >0.9266</td>\n",
              "      <td id=\"T_02857_row148_col5\" class=\"data row148 col5\" >0.8451</td>\n",
              "      <td id=\"T_02857_row148_col6\" class=\"data row148 col6\" >0.8030</td>\n",
              "      <td id=\"T_02857_row148_col7\" class=\"data row148 col7\" >0.7635</td>\n",
              "      <td id=\"T_02857_row148_col8\" class=\"data row148 col8\" >0.3264</td>\n",
              "      <td id=\"T_02857_row148_col9\" class=\"data row148 col9\" >0.3472</td>\n",
              "      <td id=\"T_02857_row148_col10\" class=\"data row148 col10\" >0.3405</td>\n",
              "      <td id=\"T_02857_row148_col11\" class=\"data row148 col11\" >0.4948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row149\" class=\"row_heading level0 row149\" >149</th>\n",
              "      <td id=\"T_02857_row149_col0\" class=\"data row149 col0\" >es</td>\n",
              "      <td id=\"T_02857_row149_col1\" class=\"data row149 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row149_col2\" class=\"data row149 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row149_col3\" class=\"data row149 col3\" >0.9040</td>\n",
              "      <td id=\"T_02857_row149_col4\" class=\"data row149 col4\" >0.9266</td>\n",
              "      <td id=\"T_02857_row149_col5\" class=\"data row149 col5\" >0.8451</td>\n",
              "      <td id=\"T_02857_row149_col6\" class=\"data row149 col6\" >0.8030</td>\n",
              "      <td id=\"T_02857_row149_col7\" class=\"data row149 col7\" >0.7635</td>\n",
              "      <td id=\"T_02857_row149_col8\" class=\"data row149 col8\" >0.3264</td>\n",
              "      <td id=\"T_02857_row149_col9\" class=\"data row149 col9\" >0.3472</td>\n",
              "      <td id=\"T_02857_row149_col10\" class=\"data row149 col10\" >0.3405</td>\n",
              "      <td id=\"T_02857_row149_col11\" class=\"data row149 col11\" >0.4948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row150\" class=\"row_heading level0 row150\" >150</th>\n",
              "      <td id=\"T_02857_row150_col0\" class=\"data row150 col0\" >es</td>\n",
              "      <td id=\"T_02857_row150_col1\" class=\"data row150 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row150_col2\" class=\"data row150 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row150_col3\" class=\"data row150 col3\" >0.6875</td>\n",
              "      <td id=\"T_02857_row150_col4\" class=\"data row150 col4\" >0.6981</td>\n",
              "      <td id=\"T_02857_row150_col5\" class=\"data row150 col5\" >0.6670</td>\n",
              "      <td id=\"T_02857_row150_col6\" class=\"data row150 col6\" >0.6554</td>\n",
              "      <td id=\"T_02857_row150_col7\" class=\"data row150 col7\" >0.6202</td>\n",
              "      <td id=\"T_02857_row150_col8\" class=\"data row150 col8\" >0.3329</td>\n",
              "      <td id=\"T_02857_row150_col9\" class=\"data row150 col9\" >0.4468</td>\n",
              "      <td id=\"T_02857_row150_col10\" class=\"data row150 col10\" >0.4257</td>\n",
              "      <td id=\"T_02857_row150_col11\" class=\"data row150 col11\" >0.3936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row151\" class=\"row_heading level0 row151\" >151</th>\n",
              "      <td id=\"T_02857_row151_col0\" class=\"data row151 col0\" >es</td>\n",
              "      <td id=\"T_02857_row151_col1\" class=\"data row151 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row151_col2\" class=\"data row151 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row151_col3\" class=\"data row151 col3\" >0.6875</td>\n",
              "      <td id=\"T_02857_row151_col4\" class=\"data row151 col4\" >0.6981</td>\n",
              "      <td id=\"T_02857_row151_col5\" class=\"data row151 col5\" >0.6670</td>\n",
              "      <td id=\"T_02857_row151_col6\" class=\"data row151 col6\" >0.6554</td>\n",
              "      <td id=\"T_02857_row151_col7\" class=\"data row151 col7\" >0.6202</td>\n",
              "      <td id=\"T_02857_row151_col8\" class=\"data row151 col8\" >0.3329</td>\n",
              "      <td id=\"T_02857_row151_col9\" class=\"data row151 col9\" >0.4468</td>\n",
              "      <td id=\"T_02857_row151_col10\" class=\"data row151 col10\" >0.4257</td>\n",
              "      <td id=\"T_02857_row151_col11\" class=\"data row151 col11\" >0.3936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row152\" class=\"row_heading level0 row152\" >152</th>\n",
              "      <td id=\"T_02857_row152_col0\" class=\"data row152 col0\" >es</td>\n",
              "      <td id=\"T_02857_row152_col1\" class=\"data row152 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row152_col2\" class=\"data row152 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row152_col3\" class=\"data row152 col3\" >0.8010</td>\n",
              "      <td id=\"T_02857_row152_col4\" class=\"data row152 col4\" >0.8939</td>\n",
              "      <td id=\"T_02857_row152_col5\" class=\"data row152 col5\" >0.8127</td>\n",
              "      <td id=\"T_02857_row152_col6\" class=\"data row152 col6\" >0.7738</td>\n",
              "      <td id=\"T_02857_row152_col7\" class=\"data row152 col7\" >0.7125</td>\n",
              "      <td id=\"T_02857_row152_col8\" class=\"data row152 col8\" >0.4005</td>\n",
              "      <td id=\"T_02857_row152_col9\" class=\"data row152 col9\" >0.3816</td>\n",
              "      <td id=\"T_02857_row152_col10\" class=\"data row152 col10\" >0.3816</td>\n",
              "      <td id=\"T_02857_row152_col11\" class=\"data row152 col11\" >0.4412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row153\" class=\"row_heading level0 row153\" >153</th>\n",
              "      <td id=\"T_02857_row153_col0\" class=\"data row153 col0\" >es</td>\n",
              "      <td id=\"T_02857_row153_col1\" class=\"data row153 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row153_col2\" class=\"data row153 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row153_col3\" class=\"data row153 col3\" >0.8010</td>\n",
              "      <td id=\"T_02857_row153_col4\" class=\"data row153 col4\" >0.8939</td>\n",
              "      <td id=\"T_02857_row153_col5\" class=\"data row153 col5\" >0.8127</td>\n",
              "      <td id=\"T_02857_row153_col6\" class=\"data row153 col6\" >0.7738</td>\n",
              "      <td id=\"T_02857_row153_col7\" class=\"data row153 col7\" >0.7125</td>\n",
              "      <td id=\"T_02857_row153_col8\" class=\"data row153 col8\" >0.4005</td>\n",
              "      <td id=\"T_02857_row153_col9\" class=\"data row153 col9\" >0.3816</td>\n",
              "      <td id=\"T_02857_row153_col10\" class=\"data row153 col10\" >0.3816</td>\n",
              "      <td id=\"T_02857_row153_col11\" class=\"data row153 col11\" >0.4412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row154\" class=\"row_heading level0 row154\" >154</th>\n",
              "      <td id=\"T_02857_row154_col0\" class=\"data row154 col0\" >es</td>\n",
              "      <td id=\"T_02857_row154_col1\" class=\"data row154 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row154_col2\" class=\"data row154 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row154_col3\" class=\"data row154 col3\" >0.8802</td>\n",
              "      <td id=\"T_02857_row154_col4\" class=\"data row154 col4\" >0.8702</td>\n",
              "      <td id=\"T_02857_row154_col5\" class=\"data row154 col5\" >0.8240</td>\n",
              "      <td id=\"T_02857_row154_col6\" class=\"data row154 col6\" >0.7969</td>\n",
              "      <td id=\"T_02857_row154_col7\" class=\"data row154 col7\" >0.7418</td>\n",
              "      <td id=\"T_02857_row154_col8\" class=\"data row154 col8\" >0.3798</td>\n",
              "      <td id=\"T_02857_row154_col9\" class=\"data row154 col9\" >0.4285</td>\n",
              "      <td id=\"T_02857_row154_col10\" class=\"data row154 col10\" >0.4251</td>\n",
              "      <td id=\"T_02857_row154_col11\" class=\"data row154 col11\" >0.5155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row155\" class=\"row_heading level0 row155\" >155</th>\n",
              "      <td id=\"T_02857_row155_col0\" class=\"data row155 col0\" >es</td>\n",
              "      <td id=\"T_02857_row155_col1\" class=\"data row155 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row155_col2\" class=\"data row155 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row155_col3\" class=\"data row155 col3\" >0.8802</td>\n",
              "      <td id=\"T_02857_row155_col4\" class=\"data row155 col4\" >0.8702</td>\n",
              "      <td id=\"T_02857_row155_col5\" class=\"data row155 col5\" >0.8240</td>\n",
              "      <td id=\"T_02857_row155_col6\" class=\"data row155 col6\" >0.7969</td>\n",
              "      <td id=\"T_02857_row155_col7\" class=\"data row155 col7\" >0.7418</td>\n",
              "      <td id=\"T_02857_row155_col8\" class=\"data row155 col8\" >0.3798</td>\n",
              "      <td id=\"T_02857_row155_col9\" class=\"data row155 col9\" >0.4285</td>\n",
              "      <td id=\"T_02857_row155_col10\" class=\"data row155 col10\" >0.4251</td>\n",
              "      <td id=\"T_02857_row155_col11\" class=\"data row155 col11\" >0.5155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row156\" class=\"row_heading level0 row156\" >156</th>\n",
              "      <td id=\"T_02857_row156_col0\" class=\"data row156 col0\" >es</td>\n",
              "      <td id=\"T_02857_row156_col1\" class=\"data row156 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row156_col2\" class=\"data row156 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row156_col3\" class=\"data row156 col3\" >0.5785</td>\n",
              "      <td id=\"T_02857_row156_col4\" class=\"data row156 col4\" >0.5537</td>\n",
              "      <td id=\"T_02857_row156_col5\" class=\"data row156 col5\" >0.5456</td>\n",
              "      <td id=\"T_02857_row156_col6\" class=\"data row156 col6\" >0.5510</td>\n",
              "      <td id=\"T_02857_row156_col7\" class=\"data row156 col7\" >0.5060</td>\n",
              "      <td id=\"T_02857_row156_col8\" class=\"data row156 col8\" >0.4512</td>\n",
              "      <td id=\"T_02857_row156_col9\" class=\"data row156 col9\" >0.4553</td>\n",
              "      <td id=\"T_02857_row156_col10\" class=\"data row156 col10\" >0.4632</td>\n",
              "      <td id=\"T_02857_row156_col11\" class=\"data row156 col11\" >0.3259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row157\" class=\"row_heading level0 row157\" >157</th>\n",
              "      <td id=\"T_02857_row157_col0\" class=\"data row157 col0\" >es</td>\n",
              "      <td id=\"T_02857_row157_col1\" class=\"data row157 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row157_col2\" class=\"data row157 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row157_col3\" class=\"data row157 col3\" >0.5785</td>\n",
              "      <td id=\"T_02857_row157_col4\" class=\"data row157 col4\" >0.5537</td>\n",
              "      <td id=\"T_02857_row157_col5\" class=\"data row157 col5\" >0.5456</td>\n",
              "      <td id=\"T_02857_row157_col6\" class=\"data row157 col6\" >0.5510</td>\n",
              "      <td id=\"T_02857_row157_col7\" class=\"data row157 col7\" >0.5060</td>\n",
              "      <td id=\"T_02857_row157_col8\" class=\"data row157 col8\" >0.4512</td>\n",
              "      <td id=\"T_02857_row157_col9\" class=\"data row157 col9\" >0.4553</td>\n",
              "      <td id=\"T_02857_row157_col10\" class=\"data row157 col10\" >0.4632</td>\n",
              "      <td id=\"T_02857_row157_col11\" class=\"data row157 col11\" >0.3259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row158\" class=\"row_heading level0 row158\" >158</th>\n",
              "      <td id=\"T_02857_row158_col0\" class=\"data row158 col0\" >es</td>\n",
              "      <td id=\"T_02857_row158_col1\" class=\"data row158 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row158_col2\" class=\"data row158 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row158_col3\" class=\"data row158 col3\" >0.5551</td>\n",
              "      <td id=\"T_02857_row158_col4\" class=\"data row158 col4\" >0.5059</td>\n",
              "      <td id=\"T_02857_row158_col5\" class=\"data row158 col5\" >0.4109</td>\n",
              "      <td id=\"T_02857_row158_col6\" class=\"data row158 col6\" >0.4680</td>\n",
              "      <td id=\"T_02857_row158_col7\" class=\"data row158 col7\" >0.4236</td>\n",
              "      <td id=\"T_02857_row158_col8\" class=\"data row158 col8\" >0.4303</td>\n",
              "      <td id=\"T_02857_row158_col9\" class=\"data row158 col9\" >0.5172</td>\n",
              "      <td id=\"T_02857_row158_col10\" class=\"data row158 col10\" >0.3713</td>\n",
              "      <td id=\"T_02857_row158_col11\" class=\"data row158 col11\" >0.2333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row159\" class=\"row_heading level0 row159\" >159</th>\n",
              "      <td id=\"T_02857_row159_col0\" class=\"data row159 col0\" >es</td>\n",
              "      <td id=\"T_02857_row159_col1\" class=\"data row159 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row159_col2\" class=\"data row159 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row159_col3\" class=\"data row159 col3\" >0.5551</td>\n",
              "      <td id=\"T_02857_row159_col4\" class=\"data row159 col4\" >0.5059</td>\n",
              "      <td id=\"T_02857_row159_col5\" class=\"data row159 col5\" >0.4109</td>\n",
              "      <td id=\"T_02857_row159_col6\" class=\"data row159 col6\" >0.4680</td>\n",
              "      <td id=\"T_02857_row159_col7\" class=\"data row159 col7\" >0.4236</td>\n",
              "      <td id=\"T_02857_row159_col8\" class=\"data row159 col8\" >0.4303</td>\n",
              "      <td id=\"T_02857_row159_col9\" class=\"data row159 col9\" >0.5172</td>\n",
              "      <td id=\"T_02857_row159_col10\" class=\"data row159 col10\" >0.3713</td>\n",
              "      <td id=\"T_02857_row159_col11\" class=\"data row159 col11\" >0.2333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row160\" class=\"row_heading level0 row160\" >160</th>\n",
              "      <td id=\"T_02857_row160_col0\" class=\"data row160 col0\" >es</td>\n",
              "      <td id=\"T_02857_row160_col1\" class=\"data row160 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row160_col2\" class=\"data row160 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row160_col3\" class=\"data row160 col3\" >0.8526</td>\n",
              "      <td id=\"T_02857_row160_col4\" class=\"data row160 col4\" >0.8420</td>\n",
              "      <td id=\"T_02857_row160_col5\" class=\"data row160 col5\" >0.7428</td>\n",
              "      <td id=\"T_02857_row160_col6\" class=\"data row160 col6\" >0.7372</td>\n",
              "      <td id=\"T_02857_row160_col7\" class=\"data row160 col7\" >0.6301</td>\n",
              "      <td id=\"T_02857_row160_col8\" class=\"data row160 col8\" >0.3614</td>\n",
              "      <td id=\"T_02857_row160_col9\" class=\"data row160 col9\" >0.4196</td>\n",
              "      <td id=\"T_02857_row160_col10\" class=\"data row160 col10\" >0.3823</td>\n",
              "      <td id=\"T_02857_row160_col11\" class=\"data row160 col11\" >0.4304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row161\" class=\"row_heading level0 row161\" >161</th>\n",
              "      <td id=\"T_02857_row161_col0\" class=\"data row161 col0\" >es</td>\n",
              "      <td id=\"T_02857_row161_col1\" class=\"data row161 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row161_col2\" class=\"data row161 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row161_col3\" class=\"data row161 col3\" >0.8526</td>\n",
              "      <td id=\"T_02857_row161_col4\" class=\"data row161 col4\" >0.8420</td>\n",
              "      <td id=\"T_02857_row161_col5\" class=\"data row161 col5\" >0.7428</td>\n",
              "      <td id=\"T_02857_row161_col6\" class=\"data row161 col6\" >0.7372</td>\n",
              "      <td id=\"T_02857_row161_col7\" class=\"data row161 col7\" >0.6301</td>\n",
              "      <td id=\"T_02857_row161_col8\" class=\"data row161 col8\" >0.3614</td>\n",
              "      <td id=\"T_02857_row161_col9\" class=\"data row161 col9\" >0.4196</td>\n",
              "      <td id=\"T_02857_row161_col10\" class=\"data row161 col10\" >0.3823</td>\n",
              "      <td id=\"T_02857_row161_col11\" class=\"data row161 col11\" >0.4304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row162\" class=\"row_heading level0 row162\" >162</th>\n",
              "      <td id=\"T_02857_row162_col0\" class=\"data row162 col0\" >es</td>\n",
              "      <td id=\"T_02857_row162_col1\" class=\"data row162 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row162_col2\" class=\"data row162 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row162_col3\" class=\"data row162 col3\" >0.9113</td>\n",
              "      <td id=\"T_02857_row162_col4\" class=\"data row162 col4\" >0.9095</td>\n",
              "      <td id=\"T_02857_row162_col5\" class=\"data row162 col5\" >0.7977</td>\n",
              "      <td id=\"T_02857_row162_col6\" class=\"data row162 col6\" >0.7768</td>\n",
              "      <td id=\"T_02857_row162_col7\" class=\"data row162 col7\" >0.6895</td>\n",
              "      <td id=\"T_02857_row162_col8\" class=\"data row162 col8\" >0.3371</td>\n",
              "      <td id=\"T_02857_row162_col9\" class=\"data row162 col9\" >0.3573</td>\n",
              "      <td id=\"T_02857_row162_col10\" class=\"data row162 col10\" >0.3471</td>\n",
              "      <td id=\"T_02857_row162_col11\" class=\"data row162 col11\" >0.4730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row163\" class=\"row_heading level0 row163\" >163</th>\n",
              "      <td id=\"T_02857_row163_col0\" class=\"data row163 col0\" >es</td>\n",
              "      <td id=\"T_02857_row163_col1\" class=\"data row163 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row163_col2\" class=\"data row163 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row163_col3\" class=\"data row163 col3\" >0.9113</td>\n",
              "      <td id=\"T_02857_row163_col4\" class=\"data row163 col4\" >0.9095</td>\n",
              "      <td id=\"T_02857_row163_col5\" class=\"data row163 col5\" >0.7977</td>\n",
              "      <td id=\"T_02857_row163_col6\" class=\"data row163 col6\" >0.7768</td>\n",
              "      <td id=\"T_02857_row163_col7\" class=\"data row163 col7\" >0.6895</td>\n",
              "      <td id=\"T_02857_row163_col8\" class=\"data row163 col8\" >0.3371</td>\n",
              "      <td id=\"T_02857_row163_col9\" class=\"data row163 col9\" >0.3573</td>\n",
              "      <td id=\"T_02857_row163_col10\" class=\"data row163 col10\" >0.3471</td>\n",
              "      <td id=\"T_02857_row163_col11\" class=\"data row163 col11\" >0.4730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row164\" class=\"row_heading level0 row164\" >164</th>\n",
              "      <td id=\"T_02857_row164_col0\" class=\"data row164 col0\" >es</td>\n",
              "      <td id=\"T_02857_row164_col1\" class=\"data row164 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row164_col2\" class=\"data row164 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row164_col3\" class=\"data row164 col3\" >0.7063</td>\n",
              "      <td id=\"T_02857_row164_col4\" class=\"data row164 col4\" >0.6892</td>\n",
              "      <td id=\"T_02857_row164_col5\" class=\"data row164 col5\" >0.6422</td>\n",
              "      <td id=\"T_02857_row164_col6\" class=\"data row164 col6\" >0.6375</td>\n",
              "      <td id=\"T_02857_row164_col7\" class=\"data row164 col7\" >0.5845</td>\n",
              "      <td id=\"T_02857_row164_col8\" class=\"data row164 col8\" >0.3187</td>\n",
              "      <td id=\"T_02857_row164_col9\" class=\"data row164 col9\" >0.4231</td>\n",
              "      <td id=\"T_02857_row164_col10\" class=\"data row164 col10\" >0.3862</td>\n",
              "      <td id=\"T_02857_row164_col11\" class=\"data row164 col11\" >0.3657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row165\" class=\"row_heading level0 row165\" >165</th>\n",
              "      <td id=\"T_02857_row165_col0\" class=\"data row165 col0\" >es</td>\n",
              "      <td id=\"T_02857_row165_col1\" class=\"data row165 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row165_col2\" class=\"data row165 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row165_col3\" class=\"data row165 col3\" >0.7063</td>\n",
              "      <td id=\"T_02857_row165_col4\" class=\"data row165 col4\" >0.6892</td>\n",
              "      <td id=\"T_02857_row165_col5\" class=\"data row165 col5\" >0.6422</td>\n",
              "      <td id=\"T_02857_row165_col6\" class=\"data row165 col6\" >0.6375</td>\n",
              "      <td id=\"T_02857_row165_col7\" class=\"data row165 col7\" >0.5845</td>\n",
              "      <td id=\"T_02857_row165_col8\" class=\"data row165 col8\" >0.3187</td>\n",
              "      <td id=\"T_02857_row165_col9\" class=\"data row165 col9\" >0.4231</td>\n",
              "      <td id=\"T_02857_row165_col10\" class=\"data row165 col10\" >0.3862</td>\n",
              "      <td id=\"T_02857_row165_col11\" class=\"data row165 col11\" >0.3657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row166\" class=\"row_heading level0 row166\" >166</th>\n",
              "      <td id=\"T_02857_row166_col0\" class=\"data row166 col0\" >es</td>\n",
              "      <td id=\"T_02857_row166_col1\" class=\"data row166 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row166_col2\" class=\"data row166 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row166_col3\" class=\"data row166 col3\" >0.7680</td>\n",
              "      <td id=\"T_02857_row166_col4\" class=\"data row166 col4\" >0.7627</td>\n",
              "      <td id=\"T_02857_row166_col5\" class=\"data row166 col5\" >0.6783</td>\n",
              "      <td id=\"T_02857_row166_col6\" class=\"data row166 col6\" >0.7000</td>\n",
              "      <td id=\"T_02857_row166_col7\" class=\"data row166 col7\" >0.6233</td>\n",
              "      <td id=\"T_02857_row166_col8\" class=\"data row166 col8\" >0.3783</td>\n",
              "      <td id=\"T_02857_row166_col9\" class=\"data row166 col9\" >0.3695</td>\n",
              "      <td id=\"T_02857_row166_col10\" class=\"data row166 col10\" >0.3670</td>\n",
              "      <td id=\"T_02857_row166_col11\" class=\"data row166 col11\" >0.3658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row167\" class=\"row_heading level0 row167\" >167</th>\n",
              "      <td id=\"T_02857_row167_col0\" class=\"data row167 col0\" >es</td>\n",
              "      <td id=\"T_02857_row167_col1\" class=\"data row167 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row167_col2\" class=\"data row167 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row167_col3\" class=\"data row167 col3\" >0.7680</td>\n",
              "      <td id=\"T_02857_row167_col4\" class=\"data row167 col4\" >0.7627</td>\n",
              "      <td id=\"T_02857_row167_col5\" class=\"data row167 col5\" >0.6783</td>\n",
              "      <td id=\"T_02857_row167_col6\" class=\"data row167 col6\" >0.7000</td>\n",
              "      <td id=\"T_02857_row167_col7\" class=\"data row167 col7\" >0.6233</td>\n",
              "      <td id=\"T_02857_row167_col8\" class=\"data row167 col8\" >0.3783</td>\n",
              "      <td id=\"T_02857_row167_col9\" class=\"data row167 col9\" >0.3695</td>\n",
              "      <td id=\"T_02857_row167_col10\" class=\"data row167 col10\" >0.3670</td>\n",
              "      <td id=\"T_02857_row167_col11\" class=\"data row167 col11\" >0.3658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row168\" class=\"row_heading level0 row168\" >168</th>\n",
              "      <td id=\"T_02857_row168_col0\" class=\"data row168 col0\" >es</td>\n",
              "      <td id=\"T_02857_row168_col1\" class=\"data row168 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row168_col2\" class=\"data row168 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row168_col3\" class=\"data row168 col3\" >0.4299</td>\n",
              "      <td id=\"T_02857_row168_col4\" class=\"data row168 col4\" >0.3815</td>\n",
              "      <td id=\"T_02857_row168_col5\" class=\"data row168 col5\" >0.3458</td>\n",
              "      <td id=\"T_02857_row168_col6\" class=\"data row168 col6\" >0.5081</td>\n",
              "      <td id=\"T_02857_row168_col7\" class=\"data row168 col7\" >0.4424</td>\n",
              "      <td id=\"T_02857_row168_col8\" class=\"data row168 col8\" >0.7356</td>\n",
              "      <td id=\"T_02857_row168_col9\" class=\"data row168 col9\" >0.8214</td>\n",
              "      <td id=\"T_02857_row168_col10\" class=\"data row168 col10\" >0.6687</td>\n",
              "      <td id=\"T_02857_row168_col11\" class=\"data row168 col11\" >0.4096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row169\" class=\"row_heading level0 row169\" >169</th>\n",
              "      <td id=\"T_02857_row169_col0\" class=\"data row169 col0\" >es</td>\n",
              "      <td id=\"T_02857_row169_col1\" class=\"data row169 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row169_col2\" class=\"data row169 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row169_col3\" class=\"data row169 col3\" >0.4299</td>\n",
              "      <td id=\"T_02857_row169_col4\" class=\"data row169 col4\" >0.3815</td>\n",
              "      <td id=\"T_02857_row169_col5\" class=\"data row169 col5\" >0.3458</td>\n",
              "      <td id=\"T_02857_row169_col6\" class=\"data row169 col6\" >0.5081</td>\n",
              "      <td id=\"T_02857_row169_col7\" class=\"data row169 col7\" >0.4424</td>\n",
              "      <td id=\"T_02857_row169_col8\" class=\"data row169 col8\" >0.7356</td>\n",
              "      <td id=\"T_02857_row169_col9\" class=\"data row169 col9\" >0.8214</td>\n",
              "      <td id=\"T_02857_row169_col10\" class=\"data row169 col10\" >0.6687</td>\n",
              "      <td id=\"T_02857_row169_col11\" class=\"data row169 col11\" >0.4096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row170\" class=\"row_heading level0 row170\" >170</th>\n",
              "      <td id=\"T_02857_row170_col0\" class=\"data row170 col0\" >es</td>\n",
              "      <td id=\"T_02857_row170_col1\" class=\"data row170 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row170_col2\" class=\"data row170 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row170_col3\" class=\"data row170 col3\" >0.4809</td>\n",
              "      <td id=\"T_02857_row170_col4\" class=\"data row170 col4\" >0.4632</td>\n",
              "      <td id=\"T_02857_row170_col5\" class=\"data row170 col5\" >0.4849</td>\n",
              "      <td id=\"T_02857_row170_col6\" class=\"data row170 col6\" >0.5231</td>\n",
              "      <td id=\"T_02857_row170_col7\" class=\"data row170 col7\" >0.5045</td>\n",
              "      <td id=\"T_02857_row170_col8\" class=\"data row170 col8\" >0.6151</td>\n",
              "      <td id=\"T_02857_row170_col9\" class=\"data row170 col9\" >0.7180</td>\n",
              "      <td id=\"T_02857_row170_col10\" class=\"data row170 col10\" >0.5699</td>\n",
              "      <td id=\"T_02857_row170_col11\" class=\"data row170 col11\" >0.4479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row171\" class=\"row_heading level0 row171\" >171</th>\n",
              "      <td id=\"T_02857_row171_col0\" class=\"data row171 col0\" >es</td>\n",
              "      <td id=\"T_02857_row171_col1\" class=\"data row171 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row171_col2\" class=\"data row171 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row171_col3\" class=\"data row171 col3\" >0.4809</td>\n",
              "      <td id=\"T_02857_row171_col4\" class=\"data row171 col4\" >0.4632</td>\n",
              "      <td id=\"T_02857_row171_col5\" class=\"data row171 col5\" >0.4849</td>\n",
              "      <td id=\"T_02857_row171_col6\" class=\"data row171 col6\" >0.5231</td>\n",
              "      <td id=\"T_02857_row171_col7\" class=\"data row171 col7\" >0.5045</td>\n",
              "      <td id=\"T_02857_row171_col8\" class=\"data row171 col8\" >0.6151</td>\n",
              "      <td id=\"T_02857_row171_col9\" class=\"data row171 col9\" >0.7180</td>\n",
              "      <td id=\"T_02857_row171_col10\" class=\"data row171 col10\" >0.5699</td>\n",
              "      <td id=\"T_02857_row171_col11\" class=\"data row171 col11\" >0.4479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row172\" class=\"row_heading level0 row172\" >172</th>\n",
              "      <td id=\"T_02857_row172_col0\" class=\"data row172 col0\" >es</td>\n",
              "      <td id=\"T_02857_row172_col1\" class=\"data row172 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row172_col2\" class=\"data row172 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row172_col3\" class=\"data row172 col3\" >0.4721</td>\n",
              "      <td id=\"T_02857_row172_col4\" class=\"data row172 col4\" >0.4536</td>\n",
              "      <td id=\"T_02857_row172_col5\" class=\"data row172 col5\" >0.4832</td>\n",
              "      <td id=\"T_02857_row172_col6\" class=\"data row172 col6\" >0.5024</td>\n",
              "      <td id=\"T_02857_row172_col7\" class=\"data row172 col7\" >0.4963</td>\n",
              "      <td id=\"T_02857_row172_col8\" class=\"data row172 col8\" >0.5924</td>\n",
              "      <td id=\"T_02857_row172_col9\" class=\"data row172 col9\" >0.7405</td>\n",
              "      <td id=\"T_02857_row172_col10\" class=\"data row172 col10\" >0.5325</td>\n",
              "      <td id=\"T_02857_row172_col11\" class=\"data row172 col11\" >0.4210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row173\" class=\"row_heading level0 row173\" >173</th>\n",
              "      <td id=\"T_02857_row173_col0\" class=\"data row173 col0\" >es</td>\n",
              "      <td id=\"T_02857_row173_col1\" class=\"data row173 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row173_col2\" class=\"data row173 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row173_col3\" class=\"data row173 col3\" >0.4721</td>\n",
              "      <td id=\"T_02857_row173_col4\" class=\"data row173 col4\" >0.4536</td>\n",
              "      <td id=\"T_02857_row173_col5\" class=\"data row173 col5\" >0.4832</td>\n",
              "      <td id=\"T_02857_row173_col6\" class=\"data row173 col6\" >0.5024</td>\n",
              "      <td id=\"T_02857_row173_col7\" class=\"data row173 col7\" >0.4963</td>\n",
              "      <td id=\"T_02857_row173_col8\" class=\"data row173 col8\" >0.5924</td>\n",
              "      <td id=\"T_02857_row173_col9\" class=\"data row173 col9\" >0.7405</td>\n",
              "      <td id=\"T_02857_row173_col10\" class=\"data row173 col10\" >0.5325</td>\n",
              "      <td id=\"T_02857_row173_col11\" class=\"data row173 col11\" >0.4210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row174\" class=\"row_heading level0 row174\" >174</th>\n",
              "      <td id=\"T_02857_row174_col0\" class=\"data row174 col0\" >es</td>\n",
              "      <td id=\"T_02857_row174_col1\" class=\"data row174 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row174_col2\" class=\"data row174 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row174_col3\" class=\"data row174 col3\" >0.5199</td>\n",
              "      <td id=\"T_02857_row174_col4\" class=\"data row174 col4\" >0.5447</td>\n",
              "      <td id=\"T_02857_row174_col5\" class=\"data row174 col5\" >0.5262</td>\n",
              "      <td id=\"T_02857_row174_col6\" class=\"data row174 col6\" >0.5742</td>\n",
              "      <td id=\"T_02857_row174_col7\" class=\"data row174 col7\" >0.5776</td>\n",
              "      <td id=\"T_02857_row174_col8\" class=\"data row174 col8\" >0.6419</td>\n",
              "      <td id=\"T_02857_row174_col9\" class=\"data row174 col9\" >0.6943</td>\n",
              "      <td id=\"T_02857_row174_col10\" class=\"data row174 col10\" >0.5542</td>\n",
              "      <td id=\"T_02857_row174_col11\" class=\"data row174 col11\" >0.5248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row175\" class=\"row_heading level0 row175\" >175</th>\n",
              "      <td id=\"T_02857_row175_col0\" class=\"data row175 col0\" >es</td>\n",
              "      <td id=\"T_02857_row175_col1\" class=\"data row175 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row175_col2\" class=\"data row175 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row175_col3\" class=\"data row175 col3\" >0.5199</td>\n",
              "      <td id=\"T_02857_row175_col4\" class=\"data row175 col4\" >0.5447</td>\n",
              "      <td id=\"T_02857_row175_col5\" class=\"data row175 col5\" >0.5262</td>\n",
              "      <td id=\"T_02857_row175_col6\" class=\"data row175 col6\" >0.5742</td>\n",
              "      <td id=\"T_02857_row175_col7\" class=\"data row175 col7\" >0.5776</td>\n",
              "      <td id=\"T_02857_row175_col8\" class=\"data row175 col8\" >0.6419</td>\n",
              "      <td id=\"T_02857_row175_col9\" class=\"data row175 col9\" >0.6943</td>\n",
              "      <td id=\"T_02857_row175_col10\" class=\"data row175 col10\" >0.5542</td>\n",
              "      <td id=\"T_02857_row175_col11\" class=\"data row175 col11\" >0.5248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row176\" class=\"row_heading level0 row176\" >176</th>\n",
              "      <td id=\"T_02857_row176_col0\" class=\"data row176 col0\" >es</td>\n",
              "      <td id=\"T_02857_row176_col1\" class=\"data row176 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row176_col2\" class=\"data row176 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row176_col3\" class=\"data row176 col3\" >0.4382</td>\n",
              "      <td id=\"T_02857_row176_col4\" class=\"data row176 col4\" >0.4224</td>\n",
              "      <td id=\"T_02857_row176_col5\" class=\"data row176 col5\" >0.4746</td>\n",
              "      <td id=\"T_02857_row176_col6\" class=\"data row176 col6\" >0.6442</td>\n",
              "      <td id=\"T_02857_row176_col7\" class=\"data row176 col7\" >0.5887</td>\n",
              "      <td id=\"T_02857_row176_col8\" class=\"data row176 col8\" >0.8579</td>\n",
              "      <td id=\"T_02857_row176_col9\" class=\"data row176 col9\" >0.8678</td>\n",
              "      <td id=\"T_02857_row176_col10\" class=\"data row176 col10\" >0.7715</td>\n",
              "      <td id=\"T_02857_row176_col11\" class=\"data row176 col11\" >0.4843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row177\" class=\"row_heading level0 row177\" >177</th>\n",
              "      <td id=\"T_02857_row177_col0\" class=\"data row177 col0\" >es</td>\n",
              "      <td id=\"T_02857_row177_col1\" class=\"data row177 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row177_col2\" class=\"data row177 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row177_col3\" class=\"data row177 col3\" >0.4382</td>\n",
              "      <td id=\"T_02857_row177_col4\" class=\"data row177 col4\" >0.4224</td>\n",
              "      <td id=\"T_02857_row177_col5\" class=\"data row177 col5\" >0.4746</td>\n",
              "      <td id=\"T_02857_row177_col6\" class=\"data row177 col6\" >0.6442</td>\n",
              "      <td id=\"T_02857_row177_col7\" class=\"data row177 col7\" >0.5887</td>\n",
              "      <td id=\"T_02857_row177_col8\" class=\"data row177 col8\" >0.8579</td>\n",
              "      <td id=\"T_02857_row177_col9\" class=\"data row177 col9\" >0.8678</td>\n",
              "      <td id=\"T_02857_row177_col10\" class=\"data row177 col10\" >0.7715</td>\n",
              "      <td id=\"T_02857_row177_col11\" class=\"data row177 col11\" >0.4843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row178\" class=\"row_heading level0 row178\" >178</th>\n",
              "      <td id=\"T_02857_row178_col0\" class=\"data row178 col0\" >es</td>\n",
              "      <td id=\"T_02857_row178_col1\" class=\"data row178 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row178_col2\" class=\"data row178 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row178_col3\" class=\"data row178 col3\" >0.6282</td>\n",
              "      <td id=\"T_02857_row178_col4\" class=\"data row178 col4\" >0.6331</td>\n",
              "      <td id=\"T_02857_row178_col5\" class=\"data row178 col5\" >0.6111</td>\n",
              "      <td id=\"T_02857_row178_col6\" class=\"data row178 col6\" >0.6229</td>\n",
              "      <td id=\"T_02857_row178_col7\" class=\"data row178 col7\" >0.5793</td>\n",
              "      <td id=\"T_02857_row178_col8\" class=\"data row178 col8\" >0.4934</td>\n",
              "      <td id=\"T_02857_row178_col9\" class=\"data row178 col9\" >0.6602</td>\n",
              "      <td id=\"T_02857_row178_col10\" class=\"data row178 col10\" >0.5422</td>\n",
              "      <td id=\"T_02857_row178_col11\" class=\"data row178 col11\" >0.5791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row179\" class=\"row_heading level0 row179\" >179</th>\n",
              "      <td id=\"T_02857_row179_col0\" class=\"data row179 col0\" >es</td>\n",
              "      <td id=\"T_02857_row179_col1\" class=\"data row179 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row179_col2\" class=\"data row179 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row179_col3\" class=\"data row179 col3\" >0.6282</td>\n",
              "      <td id=\"T_02857_row179_col4\" class=\"data row179 col4\" >0.6331</td>\n",
              "      <td id=\"T_02857_row179_col5\" class=\"data row179 col5\" >0.6111</td>\n",
              "      <td id=\"T_02857_row179_col6\" class=\"data row179 col6\" >0.6229</td>\n",
              "      <td id=\"T_02857_row179_col7\" class=\"data row179 col7\" >0.5793</td>\n",
              "      <td id=\"T_02857_row179_col8\" class=\"data row179 col8\" >0.4934</td>\n",
              "      <td id=\"T_02857_row179_col9\" class=\"data row179 col9\" >0.6602</td>\n",
              "      <td id=\"T_02857_row179_col10\" class=\"data row179 col10\" >0.5422</td>\n",
              "      <td id=\"T_02857_row179_col11\" class=\"data row179 col11\" >0.5791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row180\" class=\"row_heading level0 row180\" >180</th>\n",
              "      <td id=\"T_02857_row180_col0\" class=\"data row180 col0\" >es</td>\n",
              "      <td id=\"T_02857_row180_col1\" class=\"data row180 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row180_col2\" class=\"data row180 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row180_col3\" class=\"data row180 col3\" >0.4451</td>\n",
              "      <td id=\"T_02857_row180_col4\" class=\"data row180 col4\" >0.4284</td>\n",
              "      <td id=\"T_02857_row180_col5\" class=\"data row180 col5\" >0.4259</td>\n",
              "      <td id=\"T_02857_row180_col6\" class=\"data row180 col6\" >0.6168</td>\n",
              "      <td id=\"T_02857_row180_col7\" class=\"data row180 col7\" >0.5757</td>\n",
              "      <td id=\"T_02857_row180_col8\" class=\"data row180 col8\" >0.8332</td>\n",
              "      <td id=\"T_02857_row180_col9\" class=\"data row180 col9\" >0.8565</td>\n",
              "      <td id=\"T_02857_row180_col10\" class=\"data row180 col10\" >0.7240</td>\n",
              "      <td id=\"T_02857_row180_col11\" class=\"data row180 col11\" >0.4656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row181\" class=\"row_heading level0 row181\" >181</th>\n",
              "      <td id=\"T_02857_row181_col0\" class=\"data row181 col0\" >es</td>\n",
              "      <td id=\"T_02857_row181_col1\" class=\"data row181 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row181_col2\" class=\"data row181 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row181_col3\" class=\"data row181 col3\" >0.4451</td>\n",
              "      <td id=\"T_02857_row181_col4\" class=\"data row181 col4\" >0.4284</td>\n",
              "      <td id=\"T_02857_row181_col5\" class=\"data row181 col5\" >0.4259</td>\n",
              "      <td id=\"T_02857_row181_col6\" class=\"data row181 col6\" >0.6168</td>\n",
              "      <td id=\"T_02857_row181_col7\" class=\"data row181 col7\" >0.5757</td>\n",
              "      <td id=\"T_02857_row181_col8\" class=\"data row181 col8\" >0.8332</td>\n",
              "      <td id=\"T_02857_row181_col9\" class=\"data row181 col9\" >0.8565</td>\n",
              "      <td id=\"T_02857_row181_col10\" class=\"data row181 col10\" >0.7240</td>\n",
              "      <td id=\"T_02857_row181_col11\" class=\"data row181 col11\" >0.4656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row182\" class=\"row_heading level0 row182\" >182</th>\n",
              "      <td id=\"T_02857_row182_col0\" class=\"data row182 col0\" >es</td>\n",
              "      <td id=\"T_02857_row182_col1\" class=\"data row182 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row182_col2\" class=\"data row182 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row182_col3\" class=\"data row182 col3\" >0.5370</td>\n",
              "      <td id=\"T_02857_row182_col4\" class=\"data row182 col4\" >0.5420</td>\n",
              "      <td id=\"T_02857_row182_col5\" class=\"data row182 col5\" >0.5182</td>\n",
              "      <td id=\"T_02857_row182_col6\" class=\"data row182 col6\" >0.6400</td>\n",
              "      <td id=\"T_02857_row182_col7\" class=\"data row182 col7\" >0.5378</td>\n",
              "      <td id=\"T_02857_row182_col8\" class=\"data row182 col8\" >0.7397</td>\n",
              "      <td id=\"T_02857_row182_col9\" class=\"data row182 col9\" >0.7476</td>\n",
              "      <td id=\"T_02857_row182_col10\" class=\"data row182 col10\" >0.7074</td>\n",
              "      <td id=\"T_02857_row182_col11\" class=\"data row182 col11\" >0.5314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row183\" class=\"row_heading level0 row183\" >183</th>\n",
              "      <td id=\"T_02857_row183_col0\" class=\"data row183 col0\" >es</td>\n",
              "      <td id=\"T_02857_row183_col1\" class=\"data row183 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row183_col2\" class=\"data row183 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row183_col3\" class=\"data row183 col3\" >0.5370</td>\n",
              "      <td id=\"T_02857_row183_col4\" class=\"data row183 col4\" >0.5420</td>\n",
              "      <td id=\"T_02857_row183_col5\" class=\"data row183 col5\" >0.5182</td>\n",
              "      <td id=\"T_02857_row183_col6\" class=\"data row183 col6\" >0.6400</td>\n",
              "      <td id=\"T_02857_row183_col7\" class=\"data row183 col7\" >0.5378</td>\n",
              "      <td id=\"T_02857_row183_col8\" class=\"data row183 col8\" >0.7397</td>\n",
              "      <td id=\"T_02857_row183_col9\" class=\"data row183 col9\" >0.7476</td>\n",
              "      <td id=\"T_02857_row183_col10\" class=\"data row183 col10\" >0.7074</td>\n",
              "      <td id=\"T_02857_row183_col11\" class=\"data row183 col11\" >0.5314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row184\" class=\"row_heading level0 row184\" >184</th>\n",
              "      <td id=\"T_02857_row184_col0\" class=\"data row184 col0\" >es</td>\n",
              "      <td id=\"T_02857_row184_col1\" class=\"data row184 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row184_col2\" class=\"data row184 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row184_col3\" class=\"data row184 col3\" >0.4785</td>\n",
              "      <td id=\"T_02857_row184_col4\" class=\"data row184 col4\" >0.4612</td>\n",
              "      <td id=\"T_02857_row184_col5\" class=\"data row184 col5\" >0.4604</td>\n",
              "      <td id=\"T_02857_row184_col6\" class=\"data row184 col6\" >0.5282</td>\n",
              "      <td id=\"T_02857_row184_col7\" class=\"data row184 col7\" >0.4740</td>\n",
              "      <td id=\"T_02857_row184_col8\" class=\"data row184 col8\" >0.6205</td>\n",
              "      <td id=\"T_02857_row184_col9\" class=\"data row184 col9\" >0.6504</td>\n",
              "      <td id=\"T_02857_row184_col10\" class=\"data row184 col10\" >0.6009</td>\n",
              "      <td id=\"T_02857_row184_col11\" class=\"data row184 col11\" >0.4788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row185\" class=\"row_heading level0 row185\" >185</th>\n",
              "      <td id=\"T_02857_row185_col0\" class=\"data row185 col0\" >es</td>\n",
              "      <td id=\"T_02857_row185_col1\" class=\"data row185 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row185_col2\" class=\"data row185 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row185_col3\" class=\"data row185 col3\" >0.4785</td>\n",
              "      <td id=\"T_02857_row185_col4\" class=\"data row185 col4\" >0.4612</td>\n",
              "      <td id=\"T_02857_row185_col5\" class=\"data row185 col5\" >0.4604</td>\n",
              "      <td id=\"T_02857_row185_col6\" class=\"data row185 col6\" >0.5282</td>\n",
              "      <td id=\"T_02857_row185_col7\" class=\"data row185 col7\" >0.4740</td>\n",
              "      <td id=\"T_02857_row185_col8\" class=\"data row185 col8\" >0.6205</td>\n",
              "      <td id=\"T_02857_row185_col9\" class=\"data row185 col9\" >0.6504</td>\n",
              "      <td id=\"T_02857_row185_col10\" class=\"data row185 col10\" >0.6009</td>\n",
              "      <td id=\"T_02857_row185_col11\" class=\"data row185 col11\" >0.4788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row186\" class=\"row_heading level0 row186\" >186</th>\n",
              "      <td id=\"T_02857_row186_col0\" class=\"data row186 col0\" >es</td>\n",
              "      <td id=\"T_02857_row186_col1\" class=\"data row186 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row186_col2\" class=\"data row186 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row186_col3\" class=\"data row186 col3\" >0.4575</td>\n",
              "      <td id=\"T_02857_row186_col4\" class=\"data row186 col4\" >0.5033</td>\n",
              "      <td id=\"T_02857_row186_col5\" class=\"data row186 col5\" >0.5741</td>\n",
              "      <td id=\"T_02857_row186_col6\" class=\"data row186 col6\" >0.5645</td>\n",
              "      <td id=\"T_02857_row186_col7\" class=\"data row186 col7\" >0.5371</td>\n",
              "      <td id=\"T_02857_row186_col8\" class=\"data row186 col8\" >0.6506</td>\n",
              "      <td id=\"T_02857_row186_col9\" class=\"data row186 col9\" >0.6102</td>\n",
              "      <td id=\"T_02857_row186_col10\" class=\"data row186 col10\" >0.6019</td>\n",
              "      <td id=\"T_02857_row186_col11\" class=\"data row186 col11\" >0.4371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row187\" class=\"row_heading level0 row187\" >187</th>\n",
              "      <td id=\"T_02857_row187_col0\" class=\"data row187 col0\" >es</td>\n",
              "      <td id=\"T_02857_row187_col1\" class=\"data row187 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row187_col2\" class=\"data row187 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row187_col3\" class=\"data row187 col3\" >0.4575</td>\n",
              "      <td id=\"T_02857_row187_col4\" class=\"data row187 col4\" >0.5033</td>\n",
              "      <td id=\"T_02857_row187_col5\" class=\"data row187 col5\" >0.5741</td>\n",
              "      <td id=\"T_02857_row187_col6\" class=\"data row187 col6\" >0.5645</td>\n",
              "      <td id=\"T_02857_row187_col7\" class=\"data row187 col7\" >0.5371</td>\n",
              "      <td id=\"T_02857_row187_col8\" class=\"data row187 col8\" >0.6506</td>\n",
              "      <td id=\"T_02857_row187_col9\" class=\"data row187 col9\" >0.6102</td>\n",
              "      <td id=\"T_02857_row187_col10\" class=\"data row187 col10\" >0.6019</td>\n",
              "      <td id=\"T_02857_row187_col11\" class=\"data row187 col11\" >0.4371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row188\" class=\"row_heading level0 row188\" >188</th>\n",
              "      <td id=\"T_02857_row188_col0\" class=\"data row188 col0\" >es</td>\n",
              "      <td id=\"T_02857_row188_col1\" class=\"data row188 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row188_col2\" class=\"data row188 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row188_col3\" class=\"data row188 col3\" >0.5390</td>\n",
              "      <td id=\"T_02857_row188_col4\" class=\"data row188 col4\" >0.5752</td>\n",
              "      <td id=\"T_02857_row188_col5\" class=\"data row188 col5\" >0.5684</td>\n",
              "      <td id=\"T_02857_row188_col6\" class=\"data row188 col6\" >0.5986</td>\n",
              "      <td id=\"T_02857_row188_col7\" class=\"data row188 col7\" >0.5698</td>\n",
              "      <td id=\"T_02857_row188_col8\" class=\"data row188 col8\" >0.6038</td>\n",
              "      <td id=\"T_02857_row188_col9\" class=\"data row188 col9\" >0.5700</td>\n",
              "      <td id=\"T_02857_row188_col10\" class=\"data row188 col10\" >0.5972</td>\n",
              "      <td id=\"T_02857_row188_col11\" class=\"data row188 col11\" >0.5491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row189\" class=\"row_heading level0 row189\" >189</th>\n",
              "      <td id=\"T_02857_row189_col0\" class=\"data row189 col0\" >es</td>\n",
              "      <td id=\"T_02857_row189_col1\" class=\"data row189 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row189_col2\" class=\"data row189 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row189_col3\" class=\"data row189 col3\" >0.5390</td>\n",
              "      <td id=\"T_02857_row189_col4\" class=\"data row189 col4\" >0.5752</td>\n",
              "      <td id=\"T_02857_row189_col5\" class=\"data row189 col5\" >0.5684</td>\n",
              "      <td id=\"T_02857_row189_col6\" class=\"data row189 col6\" >0.5986</td>\n",
              "      <td id=\"T_02857_row189_col7\" class=\"data row189 col7\" >0.5698</td>\n",
              "      <td id=\"T_02857_row189_col8\" class=\"data row189 col8\" >0.6038</td>\n",
              "      <td id=\"T_02857_row189_col9\" class=\"data row189 col9\" >0.5700</td>\n",
              "      <td id=\"T_02857_row189_col10\" class=\"data row189 col10\" >0.5972</td>\n",
              "      <td id=\"T_02857_row189_col11\" class=\"data row189 col11\" >0.5491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row190\" class=\"row_heading level0 row190\" >190</th>\n",
              "      <td id=\"T_02857_row190_col0\" class=\"data row190 col0\" >es</td>\n",
              "      <td id=\"T_02857_row190_col1\" class=\"data row190 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row190_col2\" class=\"data row190 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row190_col3\" class=\"data row190 col3\" >0.4000</td>\n",
              "      <td id=\"T_02857_row190_col4\" class=\"data row190 col4\" >0.4019</td>\n",
              "      <td id=\"T_02857_row190_col5\" class=\"data row190 col5\" >0.6073</td>\n",
              "      <td id=\"T_02857_row190_col6\" class=\"data row190 col6\" >0.7419</td>\n",
              "      <td id=\"T_02857_row190_col7\" class=\"data row190 col7\" >0.7470</td>\n",
              "      <td id=\"T_02857_row190_col8\" class=\"data row190 col8\" >0.8861</td>\n",
              "      <td id=\"T_02857_row190_col9\" class=\"data row190 col9\" >0.8027</td>\n",
              "      <td id=\"T_02857_row190_col10\" class=\"data row190 col10\" >0.8651</td>\n",
              "      <td id=\"T_02857_row190_col11\" class=\"data row190 col11\" >0.5283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row191\" class=\"row_heading level0 row191\" >191</th>\n",
              "      <td id=\"T_02857_row191_col0\" class=\"data row191 col0\" >es</td>\n",
              "      <td id=\"T_02857_row191_col1\" class=\"data row191 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row191_col2\" class=\"data row191 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row191_col3\" class=\"data row191 col3\" >0.4000</td>\n",
              "      <td id=\"T_02857_row191_col4\" class=\"data row191 col4\" >0.4019</td>\n",
              "      <td id=\"T_02857_row191_col5\" class=\"data row191 col5\" >0.6073</td>\n",
              "      <td id=\"T_02857_row191_col6\" class=\"data row191 col6\" >0.7419</td>\n",
              "      <td id=\"T_02857_row191_col7\" class=\"data row191 col7\" >0.7470</td>\n",
              "      <td id=\"T_02857_row191_col8\" class=\"data row191 col8\" >0.8861</td>\n",
              "      <td id=\"T_02857_row191_col9\" class=\"data row191 col9\" >0.8027</td>\n",
              "      <td id=\"T_02857_row191_col10\" class=\"data row191 col10\" >0.8651</td>\n",
              "      <td id=\"T_02857_row191_col11\" class=\"data row191 col11\" >0.5283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row192\" class=\"row_heading level0 row192\" >192</th>\n",
              "      <td id=\"T_02857_row192_col0\" class=\"data row192 col0\" >es</td>\n",
              "      <td id=\"T_02857_row192_col1\" class=\"data row192 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row192_col2\" class=\"data row192 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row192_col3\" class=\"data row192 col3\" >0.6341</td>\n",
              "      <td id=\"T_02857_row192_col4\" class=\"data row192 col4\" >0.6478</td>\n",
              "      <td id=\"T_02857_row192_col5\" class=\"data row192 col5\" >0.6509</td>\n",
              "      <td id=\"T_02857_row192_col6\" class=\"data row192 col6\" >0.6360</td>\n",
              "      <td id=\"T_02857_row192_col7\" class=\"data row192 col7\" >0.6311</td>\n",
              "      <td id=\"T_02857_row192_col8\" class=\"data row192 col8\" >0.6683</td>\n",
              "      <td id=\"T_02857_row192_col9\" class=\"data row192 col9\" >0.6075</td>\n",
              "      <td id=\"T_02857_row192_col10\" class=\"data row192 col10\" >0.6683</td>\n",
              "      <td id=\"T_02857_row192_col11\" class=\"data row192 col11\" >0.5426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row193\" class=\"row_heading level0 row193\" >193</th>\n",
              "      <td id=\"T_02857_row193_col0\" class=\"data row193 col0\" >es</td>\n",
              "      <td id=\"T_02857_row193_col1\" class=\"data row193 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row193_col2\" class=\"data row193 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row193_col3\" class=\"data row193 col3\" >0.6341</td>\n",
              "      <td id=\"T_02857_row193_col4\" class=\"data row193 col4\" >0.6478</td>\n",
              "      <td id=\"T_02857_row193_col5\" class=\"data row193 col5\" >0.6509</td>\n",
              "      <td id=\"T_02857_row193_col6\" class=\"data row193 col6\" >0.6360</td>\n",
              "      <td id=\"T_02857_row193_col7\" class=\"data row193 col7\" >0.6311</td>\n",
              "      <td id=\"T_02857_row193_col8\" class=\"data row193 col8\" >0.6683</td>\n",
              "      <td id=\"T_02857_row193_col9\" class=\"data row193 col9\" >0.6075</td>\n",
              "      <td id=\"T_02857_row193_col10\" class=\"data row193 col10\" >0.6683</td>\n",
              "      <td id=\"T_02857_row193_col11\" class=\"data row193 col11\" >0.5426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row194\" class=\"row_heading level0 row194\" >194</th>\n",
              "      <td id=\"T_02857_row194_col0\" class=\"data row194 col0\" >es</td>\n",
              "      <td id=\"T_02857_row194_col1\" class=\"data row194 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row194_col2\" class=\"data row194 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row194_col3\" class=\"data row194 col3\" >0.4654</td>\n",
              "      <td id=\"T_02857_row194_col4\" class=\"data row194 col4\" >0.4781</td>\n",
              "      <td id=\"T_02857_row194_col5\" class=\"data row194 col5\" >0.5889</td>\n",
              "      <td id=\"T_02857_row194_col6\" class=\"data row194 col6\" >0.6626</td>\n",
              "      <td id=\"T_02857_row194_col7\" class=\"data row194 col7\" >0.6340</td>\n",
              "      <td id=\"T_02857_row194_col8\" class=\"data row194 col8\" >0.7086</td>\n",
              "      <td id=\"T_02857_row194_col9\" class=\"data row194 col9\" >0.7066</td>\n",
              "      <td id=\"T_02857_row194_col10\" class=\"data row194 col10\" >0.7053</td>\n",
              "      <td id=\"T_02857_row194_col11\" class=\"data row194 col11\" >0.5780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row195\" class=\"row_heading level0 row195\" >195</th>\n",
              "      <td id=\"T_02857_row195_col0\" class=\"data row195 col0\" >es</td>\n",
              "      <td id=\"T_02857_row195_col1\" class=\"data row195 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row195_col2\" class=\"data row195 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row195_col3\" class=\"data row195 col3\" >0.4654</td>\n",
              "      <td id=\"T_02857_row195_col4\" class=\"data row195 col4\" >0.4781</td>\n",
              "      <td id=\"T_02857_row195_col5\" class=\"data row195 col5\" >0.5889</td>\n",
              "      <td id=\"T_02857_row195_col6\" class=\"data row195 col6\" >0.6626</td>\n",
              "      <td id=\"T_02857_row195_col7\" class=\"data row195 col7\" >0.6340</td>\n",
              "      <td id=\"T_02857_row195_col8\" class=\"data row195 col8\" >0.7086</td>\n",
              "      <td id=\"T_02857_row195_col9\" class=\"data row195 col9\" >0.7066</td>\n",
              "      <td id=\"T_02857_row195_col10\" class=\"data row195 col10\" >0.7053</td>\n",
              "      <td id=\"T_02857_row195_col11\" class=\"data row195 col11\" >0.5780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row196\" class=\"row_heading level0 row196\" >196</th>\n",
              "      <td id=\"T_02857_row196_col0\" class=\"data row196 col0\" >es</td>\n",
              "      <td id=\"T_02857_row196_col1\" class=\"data row196 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row196_col2\" class=\"data row196 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row196_col3\" class=\"data row196 col3\" >0.3455</td>\n",
              "      <td id=\"T_02857_row196_col4\" class=\"data row196 col4\" >0.3595</td>\n",
              "      <td id=\"T_02857_row196_col5\" class=\"data row196 col5\" >0.3503</td>\n",
              "      <td id=\"T_02857_row196_col6\" class=\"data row196 col6\" >0.5012</td>\n",
              "      <td id=\"T_02857_row196_col7\" class=\"data row196 col7\" >0.4341</td>\n",
              "      <td id=\"T_02857_row196_col8\" class=\"data row196 col8\" >0.9178</td>\n",
              "      <td id=\"T_02857_row196_col9\" class=\"data row196 col9\" >0.7651</td>\n",
              "      <td id=\"T_02857_row196_col10\" class=\"data row196 col10\" >0.6947</td>\n",
              "      <td id=\"T_02857_row196_col11\" class=\"data row196 col11\" >0.3484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row197\" class=\"row_heading level0 row197\" >197</th>\n",
              "      <td id=\"T_02857_row197_col0\" class=\"data row197 col0\" >es</td>\n",
              "      <td id=\"T_02857_row197_col1\" class=\"data row197 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row197_col2\" class=\"data row197 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row197_col3\" class=\"data row197 col3\" >0.3455</td>\n",
              "      <td id=\"T_02857_row197_col4\" class=\"data row197 col4\" >0.3595</td>\n",
              "      <td id=\"T_02857_row197_col5\" class=\"data row197 col5\" >0.3503</td>\n",
              "      <td id=\"T_02857_row197_col6\" class=\"data row197 col6\" >0.5012</td>\n",
              "      <td id=\"T_02857_row197_col7\" class=\"data row197 col7\" >0.4341</td>\n",
              "      <td id=\"T_02857_row197_col8\" class=\"data row197 col8\" >0.9178</td>\n",
              "      <td id=\"T_02857_row197_col9\" class=\"data row197 col9\" >0.7651</td>\n",
              "      <td id=\"T_02857_row197_col10\" class=\"data row197 col10\" >0.6947</td>\n",
              "      <td id=\"T_02857_row197_col11\" class=\"data row197 col11\" >0.3484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row198\" class=\"row_heading level0 row198\" >198</th>\n",
              "      <td id=\"T_02857_row198_col0\" class=\"data row198 col0\" >es</td>\n",
              "      <td id=\"T_02857_row198_col1\" class=\"data row198 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row198_col2\" class=\"data row198 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row198_col3\" class=\"data row198 col3\" >0.4909</td>\n",
              "      <td id=\"T_02857_row198_col4\" class=\"data row198 col4\" >0.5251</td>\n",
              "      <td id=\"T_02857_row198_col5\" class=\"data row198 col5\" >0.5284</td>\n",
              "      <td id=\"T_02857_row198_col6\" class=\"data row198 col6\" >0.5738</td>\n",
              "      <td id=\"T_02857_row198_col7\" class=\"data row198 col7\" >0.5445</td>\n",
              "      <td id=\"T_02857_row198_col8\" class=\"data row198 col8\" >0.7075</td>\n",
              "      <td id=\"T_02857_row198_col9\" class=\"data row198 col9\" >0.6557</td>\n",
              "      <td id=\"T_02857_row198_col10\" class=\"data row198 col10\" >0.5849</td>\n",
              "      <td id=\"T_02857_row198_col11\" class=\"data row198 col11\" >0.5019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row199\" class=\"row_heading level0 row199\" >199</th>\n",
              "      <td id=\"T_02857_row199_col0\" class=\"data row199 col0\" >es</td>\n",
              "      <td id=\"T_02857_row199_col1\" class=\"data row199 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row199_col2\" class=\"data row199 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row199_col3\" class=\"data row199 col3\" >0.4909</td>\n",
              "      <td id=\"T_02857_row199_col4\" class=\"data row199 col4\" >0.5251</td>\n",
              "      <td id=\"T_02857_row199_col5\" class=\"data row199 col5\" >0.5284</td>\n",
              "      <td id=\"T_02857_row199_col6\" class=\"data row199 col6\" >0.5738</td>\n",
              "      <td id=\"T_02857_row199_col7\" class=\"data row199 col7\" >0.5445</td>\n",
              "      <td id=\"T_02857_row199_col8\" class=\"data row199 col8\" >0.7075</td>\n",
              "      <td id=\"T_02857_row199_col9\" class=\"data row199 col9\" >0.6557</td>\n",
              "      <td id=\"T_02857_row199_col10\" class=\"data row199 col10\" >0.5849</td>\n",
              "      <td id=\"T_02857_row199_col11\" class=\"data row199 col11\" >0.5019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row200\" class=\"row_heading level0 row200\" >200</th>\n",
              "      <td id=\"T_02857_row200_col0\" class=\"data row200 col0\" >es</td>\n",
              "      <td id=\"T_02857_row200_col1\" class=\"data row200 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row200_col2\" class=\"data row200 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row200_col3\" class=\"data row200 col3\" >0.4399</td>\n",
              "      <td id=\"T_02857_row200_col4\" class=\"data row200 col4\" >0.4668</td>\n",
              "      <td id=\"T_02857_row200_col5\" class=\"data row200 col5\" >0.5215</td>\n",
              "      <td id=\"T_02857_row200_col6\" class=\"data row200 col6\" >0.5111</td>\n",
              "      <td id=\"T_02857_row200_col7\" class=\"data row200 col7\" >0.4763</td>\n",
              "      <td id=\"T_02857_row200_col8\" class=\"data row200 col8\" >0.6770</td>\n",
              "      <td id=\"T_02857_row200_col9\" class=\"data row200 col9\" >0.5719</td>\n",
              "      <td id=\"T_02857_row200_col10\" class=\"data row200 col10\" >0.4972</td>\n",
              "      <td id=\"T_02857_row200_col11\" class=\"data row200 col11\" >0.3355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row201\" class=\"row_heading level0 row201\" >201</th>\n",
              "      <td id=\"T_02857_row201_col0\" class=\"data row201 col0\" >es</td>\n",
              "      <td id=\"T_02857_row201_col1\" class=\"data row201 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row201_col2\" class=\"data row201 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row201_col3\" class=\"data row201 col3\" >0.4399</td>\n",
              "      <td id=\"T_02857_row201_col4\" class=\"data row201 col4\" >0.4668</td>\n",
              "      <td id=\"T_02857_row201_col5\" class=\"data row201 col5\" >0.5215</td>\n",
              "      <td id=\"T_02857_row201_col6\" class=\"data row201 col6\" >0.5111</td>\n",
              "      <td id=\"T_02857_row201_col7\" class=\"data row201 col7\" >0.4763</td>\n",
              "      <td id=\"T_02857_row201_col8\" class=\"data row201 col8\" >0.6770</td>\n",
              "      <td id=\"T_02857_row201_col9\" class=\"data row201 col9\" >0.5719</td>\n",
              "      <td id=\"T_02857_row201_col10\" class=\"data row201 col10\" >0.4972</td>\n",
              "      <td id=\"T_02857_row201_col11\" class=\"data row201 col11\" >0.3355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row202\" class=\"row_heading level0 row202\" >202</th>\n",
              "      <td id=\"T_02857_row202_col0\" class=\"data row202 col0\" >es</td>\n",
              "      <td id=\"T_02857_row202_col1\" class=\"data row202 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row202_col2\" class=\"data row202 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row202_col3\" class=\"data row202 col3\" >0.3641</td>\n",
              "      <td id=\"T_02857_row202_col4\" class=\"data row202 col4\" >0.4060</td>\n",
              "      <td id=\"T_02857_row202_col5\" class=\"data row202 col5\" >0.4806</td>\n",
              "      <td id=\"T_02857_row202_col6\" class=\"data row202 col6\" >0.5902</td>\n",
              "      <td id=\"T_02857_row202_col7\" class=\"data row202 col7\" >0.6005</td>\n",
              "      <td id=\"T_02857_row202_col8\" class=\"data row202 col8\" >0.8787</td>\n",
              "      <td id=\"T_02857_row202_col9\" class=\"data row202 col9\" >0.7384</td>\n",
              "      <td id=\"T_02857_row202_col10\" class=\"data row202 col10\" >0.6777</td>\n",
              "      <td id=\"T_02857_row202_col11\" class=\"data row202 col11\" >0.4344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row203\" class=\"row_heading level0 row203\" >203</th>\n",
              "      <td id=\"T_02857_row203_col0\" class=\"data row203 col0\" >es</td>\n",
              "      <td id=\"T_02857_row203_col1\" class=\"data row203 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row203_col2\" class=\"data row203 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row203_col3\" class=\"data row203 col3\" >0.3641</td>\n",
              "      <td id=\"T_02857_row203_col4\" class=\"data row203 col4\" >0.4060</td>\n",
              "      <td id=\"T_02857_row203_col5\" class=\"data row203 col5\" >0.4806</td>\n",
              "      <td id=\"T_02857_row203_col6\" class=\"data row203 col6\" >0.5902</td>\n",
              "      <td id=\"T_02857_row203_col7\" class=\"data row203 col7\" >0.6005</td>\n",
              "      <td id=\"T_02857_row203_col8\" class=\"data row203 col8\" >0.8787</td>\n",
              "      <td id=\"T_02857_row203_col9\" class=\"data row203 col9\" >0.7384</td>\n",
              "      <td id=\"T_02857_row203_col10\" class=\"data row203 col10\" >0.6777</td>\n",
              "      <td id=\"T_02857_row203_col11\" class=\"data row203 col11\" >0.4344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row204\" class=\"row_heading level0 row204\" >204</th>\n",
              "      <td id=\"T_02857_row204_col0\" class=\"data row204 col0\" >es</td>\n",
              "      <td id=\"T_02857_row204_col1\" class=\"data row204 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row204_col2\" class=\"data row204 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row204_col3\" class=\"data row204 col3\" >0.3352</td>\n",
              "      <td id=\"T_02857_row204_col4\" class=\"data row204 col4\" >0.3456</td>\n",
              "      <td id=\"T_02857_row204_col5\" class=\"data row204 col5\" >0.4692</td>\n",
              "      <td id=\"T_02857_row204_col6\" class=\"data row204 col6\" >0.5674</td>\n",
              "      <td id=\"T_02857_row204_col7\" class=\"data row204 col7\" >0.5788</td>\n",
              "      <td id=\"T_02857_row204_col8\" class=\"data row204 col8\" >0.9323</td>\n",
              "      <td id=\"T_02857_row204_col9\" class=\"data row204 col9\" >0.5690</td>\n",
              "      <td id=\"T_02857_row204_col10\" class=\"data row204 col10\" >0.8046</td>\n",
              "      <td id=\"T_02857_row204_col11\" class=\"data row204 col11\" >0.3696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row205\" class=\"row_heading level0 row205\" >205</th>\n",
              "      <td id=\"T_02857_row205_col0\" class=\"data row205 col0\" >es</td>\n",
              "      <td id=\"T_02857_row205_col1\" class=\"data row205 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row205_col2\" class=\"data row205 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row205_col3\" class=\"data row205 col3\" >0.3352</td>\n",
              "      <td id=\"T_02857_row205_col4\" class=\"data row205 col4\" >0.3456</td>\n",
              "      <td id=\"T_02857_row205_col5\" class=\"data row205 col5\" >0.4692</td>\n",
              "      <td id=\"T_02857_row205_col6\" class=\"data row205 col6\" >0.5674</td>\n",
              "      <td id=\"T_02857_row205_col7\" class=\"data row205 col7\" >0.5788</td>\n",
              "      <td id=\"T_02857_row205_col8\" class=\"data row205 col8\" >0.9323</td>\n",
              "      <td id=\"T_02857_row205_col9\" class=\"data row205 col9\" >0.5690</td>\n",
              "      <td id=\"T_02857_row205_col10\" class=\"data row205 col10\" >0.8046</td>\n",
              "      <td id=\"T_02857_row205_col11\" class=\"data row205 col11\" >0.3696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row206\" class=\"row_heading level0 row206\" >206</th>\n",
              "      <td id=\"T_02857_row206_col0\" class=\"data row206 col0\" >es</td>\n",
              "      <td id=\"T_02857_row206_col1\" class=\"data row206 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row206_col2\" class=\"data row206 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row206_col3\" class=\"data row206 col3\" >0.5829</td>\n",
              "      <td id=\"T_02857_row206_col4\" class=\"data row206 col4\" >0.6232</td>\n",
              "      <td id=\"T_02857_row206_col5\" class=\"data row206 col5\" >0.6303</td>\n",
              "      <td id=\"T_02857_row206_col6\" class=\"data row206 col6\" >0.6424</td>\n",
              "      <td id=\"T_02857_row206_col7\" class=\"data row206 col7\" >0.6268</td>\n",
              "      <td id=\"T_02857_row206_col8\" class=\"data row206 col8\" >0.7347</td>\n",
              "      <td id=\"T_02857_row206_col9\" class=\"data row206 col9\" >0.5714</td>\n",
              "      <td id=\"T_02857_row206_col10\" class=\"data row206 col10\" >0.6498</td>\n",
              "      <td id=\"T_02857_row206_col11\" class=\"data row206 col11\" >0.5413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row207\" class=\"row_heading level0 row207\" >207</th>\n",
              "      <td id=\"T_02857_row207_col0\" class=\"data row207 col0\" >es</td>\n",
              "      <td id=\"T_02857_row207_col1\" class=\"data row207 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row207_col2\" class=\"data row207 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row207_col3\" class=\"data row207 col3\" >0.5829</td>\n",
              "      <td id=\"T_02857_row207_col4\" class=\"data row207 col4\" >0.6232</td>\n",
              "      <td id=\"T_02857_row207_col5\" class=\"data row207 col5\" >0.6303</td>\n",
              "      <td id=\"T_02857_row207_col6\" class=\"data row207 col6\" >0.6424</td>\n",
              "      <td id=\"T_02857_row207_col7\" class=\"data row207 col7\" >0.6268</td>\n",
              "      <td id=\"T_02857_row207_col8\" class=\"data row207 col8\" >0.7347</td>\n",
              "      <td id=\"T_02857_row207_col9\" class=\"data row207 col9\" >0.5714</td>\n",
              "      <td id=\"T_02857_row207_col10\" class=\"data row207 col10\" >0.6498</td>\n",
              "      <td id=\"T_02857_row207_col11\" class=\"data row207 col11\" >0.5413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row208\" class=\"row_heading level0 row208\" >208</th>\n",
              "      <td id=\"T_02857_row208_col0\" class=\"data row208 col0\" >es</td>\n",
              "      <td id=\"T_02857_row208_col1\" class=\"data row208 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row208_col2\" class=\"data row208 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row208_col3\" class=\"data row208 col3\" >0.3320</td>\n",
              "      <td id=\"T_02857_row208_col4\" class=\"data row208 col4\" >0.3310</td>\n",
              "      <td id=\"T_02857_row208_col5\" class=\"data row208 col5\" >0.3438</td>\n",
              "      <td id=\"T_02857_row208_col6\" class=\"data row208 col6\" >0.4500</td>\n",
              "      <td id=\"T_02857_row208_col7\" class=\"data row208 col7\" >0.4737</td>\n",
              "      <td id=\"T_02857_row208_col8\" class=\"data row208 col8\" >0.8856</td>\n",
              "      <td id=\"T_02857_row208_col9\" class=\"data row208 col9\" >0.5210</td>\n",
              "      <td id=\"T_02857_row208_col10\" class=\"data row208 col10\" >0.6757</td>\n",
              "      <td id=\"T_02857_row208_col11\" class=\"data row208 col11\" >0.2901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row209\" class=\"row_heading level0 row209\" >209</th>\n",
              "      <td id=\"T_02857_row209_col0\" class=\"data row209 col0\" >es</td>\n",
              "      <td id=\"T_02857_row209_col1\" class=\"data row209 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row209_col2\" class=\"data row209 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row209_col3\" class=\"data row209 col3\" >0.3320</td>\n",
              "      <td id=\"T_02857_row209_col4\" class=\"data row209 col4\" >0.3310</td>\n",
              "      <td id=\"T_02857_row209_col5\" class=\"data row209 col5\" >0.3438</td>\n",
              "      <td id=\"T_02857_row209_col6\" class=\"data row209 col6\" >0.4500</td>\n",
              "      <td id=\"T_02857_row209_col7\" class=\"data row209 col7\" >0.4737</td>\n",
              "      <td id=\"T_02857_row209_col8\" class=\"data row209 col8\" >0.8856</td>\n",
              "      <td id=\"T_02857_row209_col9\" class=\"data row209 col9\" >0.5210</td>\n",
              "      <td id=\"T_02857_row209_col10\" class=\"data row209 col10\" >0.6757</td>\n",
              "      <td id=\"T_02857_row209_col11\" class=\"data row209 col11\" >0.2901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row210\" class=\"row_heading level0 row210\" >210</th>\n",
              "      <td id=\"T_02857_row210_col0\" class=\"data row210 col0\" >es</td>\n",
              "      <td id=\"T_02857_row210_col1\" class=\"data row210 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row210_col2\" class=\"data row210 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row210_col3\" class=\"data row210 col3\" >0.7840</td>\n",
              "      <td id=\"T_02857_row210_col4\" class=\"data row210 col4\" >0.8700</td>\n",
              "      <td id=\"T_02857_row210_col5\" class=\"data row210 col5\" >0.8398</td>\n",
              "      <td id=\"T_02857_row210_col6\" class=\"data row210 col6\" >0.8105</td>\n",
              "      <td id=\"T_02857_row210_col7\" class=\"data row210 col7\" >0.7585</td>\n",
              "      <td id=\"T_02857_row210_col8\" class=\"data row210 col8\" >0.4992</td>\n",
              "      <td id=\"T_02857_row210_col9\" class=\"data row210 col9\" >0.3832</td>\n",
              "      <td id=\"T_02857_row210_col10\" class=\"data row210 col10\" >0.4459</td>\n",
              "      <td id=\"T_02857_row210_col11\" class=\"data row210 col11\" >0.5119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row211\" class=\"row_heading level0 row211\" >211</th>\n",
              "      <td id=\"T_02857_row211_col0\" class=\"data row211 col0\" >es</td>\n",
              "      <td id=\"T_02857_row211_col1\" class=\"data row211 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row211_col2\" class=\"data row211 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row211_col3\" class=\"data row211 col3\" >0.7840</td>\n",
              "      <td id=\"T_02857_row211_col4\" class=\"data row211 col4\" >0.8700</td>\n",
              "      <td id=\"T_02857_row211_col5\" class=\"data row211 col5\" >0.8398</td>\n",
              "      <td id=\"T_02857_row211_col6\" class=\"data row211 col6\" >0.8105</td>\n",
              "      <td id=\"T_02857_row211_col7\" class=\"data row211 col7\" >0.7585</td>\n",
              "      <td id=\"T_02857_row211_col8\" class=\"data row211 col8\" >0.4992</td>\n",
              "      <td id=\"T_02857_row211_col9\" class=\"data row211 col9\" >0.3832</td>\n",
              "      <td id=\"T_02857_row211_col10\" class=\"data row211 col10\" >0.4459</td>\n",
              "      <td id=\"T_02857_row211_col11\" class=\"data row211 col11\" >0.5119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row212\" class=\"row_heading level0 row212\" >212</th>\n",
              "      <td id=\"T_02857_row212_col0\" class=\"data row212 col0\" >es</td>\n",
              "      <td id=\"T_02857_row212_col1\" class=\"data row212 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row212_col2\" class=\"data row212 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row212_col3\" class=\"data row212 col3\" >0.5429</td>\n",
              "      <td id=\"T_02857_row212_col4\" class=\"data row212 col4\" >0.5807</td>\n",
              "      <td id=\"T_02857_row212_col5\" class=\"data row212 col5\" >0.5908</td>\n",
              "      <td id=\"T_02857_row212_col6\" class=\"data row212 col6\" >0.5745</td>\n",
              "      <td id=\"T_02857_row212_col7\" class=\"data row212 col7\" >0.5449</td>\n",
              "      <td id=\"T_02857_row212_col8\" class=\"data row212 col8\" >0.4590</td>\n",
              "      <td id=\"T_02857_row212_col9\" class=\"data row212 col9\" >0.4342</td>\n",
              "      <td id=\"T_02857_row212_col10\" class=\"data row212 col10\" >0.4713</td>\n",
              "      <td id=\"T_02857_row212_col11\" class=\"data row212 col11\" >0.3448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row213\" class=\"row_heading level0 row213\" >213</th>\n",
              "      <td id=\"T_02857_row213_col0\" class=\"data row213 col0\" >es</td>\n",
              "      <td id=\"T_02857_row213_col1\" class=\"data row213 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row213_col2\" class=\"data row213 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row213_col3\" class=\"data row213 col3\" >0.5429</td>\n",
              "      <td id=\"T_02857_row213_col4\" class=\"data row213 col4\" >0.5807</td>\n",
              "      <td id=\"T_02857_row213_col5\" class=\"data row213 col5\" >0.5908</td>\n",
              "      <td id=\"T_02857_row213_col6\" class=\"data row213 col6\" >0.5745</td>\n",
              "      <td id=\"T_02857_row213_col7\" class=\"data row213 col7\" >0.5449</td>\n",
              "      <td id=\"T_02857_row213_col8\" class=\"data row213 col8\" >0.4590</td>\n",
              "      <td id=\"T_02857_row213_col9\" class=\"data row213 col9\" >0.4342</td>\n",
              "      <td id=\"T_02857_row213_col10\" class=\"data row213 col10\" >0.4713</td>\n",
              "      <td id=\"T_02857_row213_col11\" class=\"data row213 col11\" >0.3448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row214\" class=\"row_heading level0 row214\" >214</th>\n",
              "      <td id=\"T_02857_row214_col0\" class=\"data row214 col0\" >es</td>\n",
              "      <td id=\"T_02857_row214_col1\" class=\"data row214 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row214_col2\" class=\"data row214 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row214_col3\" class=\"data row214 col3\" >0.4750</td>\n",
              "      <td id=\"T_02857_row214_col4\" class=\"data row214 col4\" >0.6582</td>\n",
              "      <td id=\"T_02857_row214_col5\" class=\"data row214 col5\" >0.7275</td>\n",
              "      <td id=\"T_02857_row214_col6\" class=\"data row214 col6\" >0.6159</td>\n",
              "      <td id=\"T_02857_row214_col7\" class=\"data row214 col7\" >0.6549</td>\n",
              "      <td id=\"T_02857_row214_col8\" class=\"data row214 col8\" >0.6132</td>\n",
              "      <td id=\"T_02857_row214_col9\" class=\"data row214 col9\" >0.4904</td>\n",
              "      <td id=\"T_02857_row214_col10\" class=\"data row214 col10\" >0.4822</td>\n",
              "      <td id=\"T_02857_row214_col11\" class=\"data row214 col11\" >0.3827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row215\" class=\"row_heading level0 row215\" >215</th>\n",
              "      <td id=\"T_02857_row215_col0\" class=\"data row215 col0\" >es</td>\n",
              "      <td id=\"T_02857_row215_col1\" class=\"data row215 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row215_col2\" class=\"data row215 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row215_col3\" class=\"data row215 col3\" >0.4750</td>\n",
              "      <td id=\"T_02857_row215_col4\" class=\"data row215 col4\" >0.6582</td>\n",
              "      <td id=\"T_02857_row215_col5\" class=\"data row215 col5\" >0.7275</td>\n",
              "      <td id=\"T_02857_row215_col6\" class=\"data row215 col6\" >0.6159</td>\n",
              "      <td id=\"T_02857_row215_col7\" class=\"data row215 col7\" >0.6549</td>\n",
              "      <td id=\"T_02857_row215_col8\" class=\"data row215 col8\" >0.6132</td>\n",
              "      <td id=\"T_02857_row215_col9\" class=\"data row215 col9\" >0.4904</td>\n",
              "      <td id=\"T_02857_row215_col10\" class=\"data row215 col10\" >0.4822</td>\n",
              "      <td id=\"T_02857_row215_col11\" class=\"data row215 col11\" >0.3827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row216\" class=\"row_heading level0 row216\" >216</th>\n",
              "      <td id=\"T_02857_row216_col0\" class=\"data row216 col0\" >es</td>\n",
              "      <td id=\"T_02857_row216_col1\" class=\"data row216 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row216_col2\" class=\"data row216 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row216_col3\" class=\"data row216 col3\" >0.7730</td>\n",
              "      <td id=\"T_02857_row216_col4\" class=\"data row216 col4\" >0.8604</td>\n",
              "      <td id=\"T_02857_row216_col5\" class=\"data row216 col5\" >0.8230</td>\n",
              "      <td id=\"T_02857_row216_col6\" class=\"data row216 col6\" >0.7823</td>\n",
              "      <td id=\"T_02857_row216_col7\" class=\"data row216 col7\" >0.7351</td>\n",
              "      <td id=\"T_02857_row216_col8\" class=\"data row216 col8\" >0.3777</td>\n",
              "      <td id=\"T_02857_row216_col9\" class=\"data row216 col9\" >0.4025</td>\n",
              "      <td id=\"T_02857_row216_col10\" class=\"data row216 col10\" >0.4022</td>\n",
              "      <td id=\"T_02857_row216_col11\" class=\"data row216 col11\" >0.4697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row217\" class=\"row_heading level0 row217\" >217</th>\n",
              "      <td id=\"T_02857_row217_col0\" class=\"data row217 col0\" >es</td>\n",
              "      <td id=\"T_02857_row217_col1\" class=\"data row217 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row217_col2\" class=\"data row217 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row217_col3\" class=\"data row217 col3\" >0.7730</td>\n",
              "      <td id=\"T_02857_row217_col4\" class=\"data row217 col4\" >0.8604</td>\n",
              "      <td id=\"T_02857_row217_col5\" class=\"data row217 col5\" >0.8230</td>\n",
              "      <td id=\"T_02857_row217_col6\" class=\"data row217 col6\" >0.7823</td>\n",
              "      <td id=\"T_02857_row217_col7\" class=\"data row217 col7\" >0.7351</td>\n",
              "      <td id=\"T_02857_row217_col8\" class=\"data row217 col8\" >0.3777</td>\n",
              "      <td id=\"T_02857_row217_col9\" class=\"data row217 col9\" >0.4025</td>\n",
              "      <td id=\"T_02857_row217_col10\" class=\"data row217 col10\" >0.4022</td>\n",
              "      <td id=\"T_02857_row217_col11\" class=\"data row217 col11\" >0.4697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row218\" class=\"row_heading level0 row218\" >218</th>\n",
              "      <td id=\"T_02857_row218_col0\" class=\"data row218 col0\" >es</td>\n",
              "      <td id=\"T_02857_row218_col1\" class=\"data row218 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row218_col2\" class=\"data row218 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row218_col3\" class=\"data row218 col3\" >0.8881</td>\n",
              "      <td id=\"T_02857_row218_col4\" class=\"data row218 col4\" >0.8898</td>\n",
              "      <td id=\"T_02857_row218_col5\" class=\"data row218 col5\" >0.8459</td>\n",
              "      <td id=\"T_02857_row218_col6\" class=\"data row218 col6\" >0.8136</td>\n",
              "      <td id=\"T_02857_row218_col7\" class=\"data row218 col7\" >0.7783</td>\n",
              "      <td id=\"T_02857_row218_col8\" class=\"data row218 col8\" >0.3517</td>\n",
              "      <td id=\"T_02857_row218_col9\" class=\"data row218 col9\" >0.3620</td>\n",
              "      <td id=\"T_02857_row218_col10\" class=\"data row218 col10\" >0.3554</td>\n",
              "      <td id=\"T_02857_row218_col11\" class=\"data row218 col11\" >0.4832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row219\" class=\"row_heading level0 row219\" >219</th>\n",
              "      <td id=\"T_02857_row219_col0\" class=\"data row219 col0\" >es</td>\n",
              "      <td id=\"T_02857_row219_col1\" class=\"data row219 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row219_col2\" class=\"data row219 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row219_col3\" class=\"data row219 col3\" >0.8881</td>\n",
              "      <td id=\"T_02857_row219_col4\" class=\"data row219 col4\" >0.8898</td>\n",
              "      <td id=\"T_02857_row219_col5\" class=\"data row219 col5\" >0.8459</td>\n",
              "      <td id=\"T_02857_row219_col6\" class=\"data row219 col6\" >0.8136</td>\n",
              "      <td id=\"T_02857_row219_col7\" class=\"data row219 col7\" >0.7783</td>\n",
              "      <td id=\"T_02857_row219_col8\" class=\"data row219 col8\" >0.3517</td>\n",
              "      <td id=\"T_02857_row219_col9\" class=\"data row219 col9\" >0.3620</td>\n",
              "      <td id=\"T_02857_row219_col10\" class=\"data row219 col10\" >0.3554</td>\n",
              "      <td id=\"T_02857_row219_col11\" class=\"data row219 col11\" >0.4832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row220\" class=\"row_heading level0 row220\" >220</th>\n",
              "      <td id=\"T_02857_row220_col0\" class=\"data row220 col0\" >es</td>\n",
              "      <td id=\"T_02857_row220_col1\" class=\"data row220 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row220_col2\" class=\"data row220 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row220_col3\" class=\"data row220 col3\" >0.6602</td>\n",
              "      <td id=\"T_02857_row220_col4\" class=\"data row220 col4\" >0.6923</td>\n",
              "      <td id=\"T_02857_row220_col5\" class=\"data row220 col5\" >0.6641</td>\n",
              "      <td id=\"T_02857_row220_col6\" class=\"data row220 col6\" >0.6459</td>\n",
              "      <td id=\"T_02857_row220_col7\" class=\"data row220 col7\" >0.6147</td>\n",
              "      <td id=\"T_02857_row220_col8\" class=\"data row220 col8\" >0.3379</td>\n",
              "      <td id=\"T_02857_row220_col9\" class=\"data row220 col9\" >0.4523</td>\n",
              "      <td id=\"T_02857_row220_col10\" class=\"data row220 col10\" >0.4150</td>\n",
              "      <td id=\"T_02857_row220_col11\" class=\"data row220 col11\" >0.3761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row221\" class=\"row_heading level0 row221\" >221</th>\n",
              "      <td id=\"T_02857_row221_col0\" class=\"data row221 col0\" >es</td>\n",
              "      <td id=\"T_02857_row221_col1\" class=\"data row221 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row221_col2\" class=\"data row221 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row221_col3\" class=\"data row221 col3\" >0.6602</td>\n",
              "      <td id=\"T_02857_row221_col4\" class=\"data row221 col4\" >0.6923</td>\n",
              "      <td id=\"T_02857_row221_col5\" class=\"data row221 col5\" >0.6641</td>\n",
              "      <td id=\"T_02857_row221_col6\" class=\"data row221 col6\" >0.6459</td>\n",
              "      <td id=\"T_02857_row221_col7\" class=\"data row221 col7\" >0.6147</td>\n",
              "      <td id=\"T_02857_row221_col8\" class=\"data row221 col8\" >0.3379</td>\n",
              "      <td id=\"T_02857_row221_col9\" class=\"data row221 col9\" >0.4523</td>\n",
              "      <td id=\"T_02857_row221_col10\" class=\"data row221 col10\" >0.4150</td>\n",
              "      <td id=\"T_02857_row221_col11\" class=\"data row221 col11\" >0.3761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row222\" class=\"row_heading level0 row222\" >222</th>\n",
              "      <td id=\"T_02857_row222_col0\" class=\"data row222 col0\" >es</td>\n",
              "      <td id=\"T_02857_row222_col1\" class=\"data row222 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row222_col2\" class=\"data row222 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row222_col3\" class=\"data row222 col3\" >0.8569</td>\n",
              "      <td id=\"T_02857_row222_col4\" class=\"data row222 col4\" >0.9345</td>\n",
              "      <td id=\"T_02857_row222_col5\" class=\"data row222 col5\" >0.9113</td>\n",
              "      <td id=\"T_02857_row222_col6\" class=\"data row222 col6\" >0.8627</td>\n",
              "      <td id=\"T_02857_row222_col7\" class=\"data row222 col7\" >0.7909</td>\n",
              "      <td id=\"T_02857_row222_col8\" class=\"data row222 col8\" >0.4358</td>\n",
              "      <td id=\"T_02857_row222_col9\" class=\"data row222 col9\" >0.3898</td>\n",
              "      <td id=\"T_02857_row222_col10\" class=\"data row222 col10\" >0.4001</td>\n",
              "      <td id=\"T_02857_row222_col11\" class=\"data row222 col11\" >0.5069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row223\" class=\"row_heading level0 row223\" >223</th>\n",
              "      <td id=\"T_02857_row223_col0\" class=\"data row223 col0\" >es</td>\n",
              "      <td id=\"T_02857_row223_col1\" class=\"data row223 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row223_col2\" class=\"data row223 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row223_col3\" class=\"data row223 col3\" >0.8569</td>\n",
              "      <td id=\"T_02857_row223_col4\" class=\"data row223 col4\" >0.9345</td>\n",
              "      <td id=\"T_02857_row223_col5\" class=\"data row223 col5\" >0.9113</td>\n",
              "      <td id=\"T_02857_row223_col6\" class=\"data row223 col6\" >0.8627</td>\n",
              "      <td id=\"T_02857_row223_col7\" class=\"data row223 col7\" >0.7909</td>\n",
              "      <td id=\"T_02857_row223_col8\" class=\"data row223 col8\" >0.4358</td>\n",
              "      <td id=\"T_02857_row223_col9\" class=\"data row223 col9\" >0.3898</td>\n",
              "      <td id=\"T_02857_row223_col10\" class=\"data row223 col10\" >0.4001</td>\n",
              "      <td id=\"T_02857_row223_col11\" class=\"data row223 col11\" >0.5069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row224\" class=\"row_heading level0 row224\" >224</th>\n",
              "      <td id=\"T_02857_row224_col0\" class=\"data row224 col0\" >es</td>\n",
              "      <td id=\"T_02857_row224_col1\" class=\"data row224 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row224_col2\" class=\"data row224 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row224_col3\" class=\"data row224 col3\" >0.8444</td>\n",
              "      <td id=\"T_02857_row224_col4\" class=\"data row224 col4\" >0.8954</td>\n",
              "      <td id=\"T_02857_row224_col5\" class=\"data row224 col5\" >0.8525</td>\n",
              "      <td id=\"T_02857_row224_col6\" class=\"data row224 col6\" >0.8483</td>\n",
              "      <td id=\"T_02857_row224_col7\" class=\"data row224 col7\" >0.7812</td>\n",
              "      <td id=\"T_02857_row224_col8\" class=\"data row224 col8\" >0.4992</td>\n",
              "      <td id=\"T_02857_row224_col9\" class=\"data row224 col9\" >0.4819</td>\n",
              "      <td id=\"T_02857_row224_col10\" class=\"data row224 col10\" >0.4452</td>\n",
              "      <td id=\"T_02857_row224_col11\" class=\"data row224 col11\" >0.5340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row225\" class=\"row_heading level0 row225\" >225</th>\n",
              "      <td id=\"T_02857_row225_col0\" class=\"data row225 col0\" >es</td>\n",
              "      <td id=\"T_02857_row225_col1\" class=\"data row225 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row225_col2\" class=\"data row225 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row225_col3\" class=\"data row225 col3\" >0.8444</td>\n",
              "      <td id=\"T_02857_row225_col4\" class=\"data row225 col4\" >0.8954</td>\n",
              "      <td id=\"T_02857_row225_col5\" class=\"data row225 col5\" >0.8525</td>\n",
              "      <td id=\"T_02857_row225_col6\" class=\"data row225 col6\" >0.8483</td>\n",
              "      <td id=\"T_02857_row225_col7\" class=\"data row225 col7\" >0.7812</td>\n",
              "      <td id=\"T_02857_row225_col8\" class=\"data row225 col8\" >0.4992</td>\n",
              "      <td id=\"T_02857_row225_col9\" class=\"data row225 col9\" >0.4819</td>\n",
              "      <td id=\"T_02857_row225_col10\" class=\"data row225 col10\" >0.4452</td>\n",
              "      <td id=\"T_02857_row225_col11\" class=\"data row225 col11\" >0.5340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row226\" class=\"row_heading level0 row226\" >226</th>\n",
              "      <td id=\"T_02857_row226_col0\" class=\"data row226 col0\" >es</td>\n",
              "      <td id=\"T_02857_row226_col1\" class=\"data row226 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row226_col2\" class=\"data row226 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row226_col3\" class=\"data row226 col3\" >0.5707</td>\n",
              "      <td id=\"T_02857_row226_col4\" class=\"data row226 col4\" >0.5950</td>\n",
              "      <td id=\"T_02857_row226_col5\" class=\"data row226 col5\" >0.5415</td>\n",
              "      <td id=\"T_02857_row226_col6\" class=\"data row226 col6\" >0.6017</td>\n",
              "      <td id=\"T_02857_row226_col7\" class=\"data row226 col7\" >0.5428</td>\n",
              "      <td id=\"T_02857_row226_col8\" class=\"data row226 col8\" >0.4876</td>\n",
              "      <td id=\"T_02857_row226_col9\" class=\"data row226 col9\" >0.5586</td>\n",
              "      <td id=\"T_02857_row226_col10\" class=\"data row226 col10\" >0.4745</td>\n",
              "      <td id=\"T_02857_row226_col11\" class=\"data row226 col11\" >0.3625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row227\" class=\"row_heading level0 row227\" >227</th>\n",
              "      <td id=\"T_02857_row227_col0\" class=\"data row227 col0\" >es</td>\n",
              "      <td id=\"T_02857_row227_col1\" class=\"data row227 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row227_col2\" class=\"data row227 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row227_col3\" class=\"data row227 col3\" >0.5707</td>\n",
              "      <td id=\"T_02857_row227_col4\" class=\"data row227 col4\" >0.5950</td>\n",
              "      <td id=\"T_02857_row227_col5\" class=\"data row227 col5\" >0.5415</td>\n",
              "      <td id=\"T_02857_row227_col6\" class=\"data row227 col6\" >0.6017</td>\n",
              "      <td id=\"T_02857_row227_col7\" class=\"data row227 col7\" >0.5428</td>\n",
              "      <td id=\"T_02857_row227_col8\" class=\"data row227 col8\" >0.4876</td>\n",
              "      <td id=\"T_02857_row227_col9\" class=\"data row227 col9\" >0.5586</td>\n",
              "      <td id=\"T_02857_row227_col10\" class=\"data row227 col10\" >0.4745</td>\n",
              "      <td id=\"T_02857_row227_col11\" class=\"data row227 col11\" >0.3625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row228\" class=\"row_heading level0 row228\" >228</th>\n",
              "      <td id=\"T_02857_row228_col0\" class=\"data row228 col0\" >es</td>\n",
              "      <td id=\"T_02857_row228_col1\" class=\"data row228 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row228_col2\" class=\"data row228 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row228_col3\" class=\"data row228 col3\" >0.5819</td>\n",
              "      <td id=\"T_02857_row228_col4\" class=\"data row228 col4\" >0.6886</td>\n",
              "      <td id=\"T_02857_row228_col5\" class=\"data row228 col5\" >0.7084</td>\n",
              "      <td id=\"T_02857_row228_col6\" class=\"data row228 col6\" >0.7066</td>\n",
              "      <td id=\"T_02857_row228_col7\" class=\"data row228 col7\" >0.6673</td>\n",
              "      <td id=\"T_02857_row228_col8\" class=\"data row228 col8\" >0.6613</td>\n",
              "      <td id=\"T_02857_row228_col9\" class=\"data row228 col9\" >0.5962</td>\n",
              "      <td id=\"T_02857_row228_col10\" class=\"data row228 col10\" >0.5040</td>\n",
              "      <td id=\"T_02857_row228_col11\" class=\"data row228 col11\" >0.4456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row229\" class=\"row_heading level0 row229\" >229</th>\n",
              "      <td id=\"T_02857_row229_col0\" class=\"data row229 col0\" >es</td>\n",
              "      <td id=\"T_02857_row229_col1\" class=\"data row229 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row229_col2\" class=\"data row229 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row229_col3\" class=\"data row229 col3\" >0.5819</td>\n",
              "      <td id=\"T_02857_row229_col4\" class=\"data row229 col4\" >0.6886</td>\n",
              "      <td id=\"T_02857_row229_col5\" class=\"data row229 col5\" >0.7084</td>\n",
              "      <td id=\"T_02857_row229_col6\" class=\"data row229 col6\" >0.7066</td>\n",
              "      <td id=\"T_02857_row229_col7\" class=\"data row229 col7\" >0.6673</td>\n",
              "      <td id=\"T_02857_row229_col8\" class=\"data row229 col8\" >0.6613</td>\n",
              "      <td id=\"T_02857_row229_col9\" class=\"data row229 col9\" >0.5962</td>\n",
              "      <td id=\"T_02857_row229_col10\" class=\"data row229 col10\" >0.5040</td>\n",
              "      <td id=\"T_02857_row229_col11\" class=\"data row229 col11\" >0.4456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row230\" class=\"row_heading level0 row230\" >230</th>\n",
              "      <td id=\"T_02857_row230_col0\" class=\"data row230 col0\" >es</td>\n",
              "      <td id=\"T_02857_row230_col1\" class=\"data row230 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row230_col2\" class=\"data row230 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row230_col3\" class=\"data row230 col3\" >0.7664</td>\n",
              "      <td id=\"T_02857_row230_col4\" class=\"data row230 col4\" >0.7967</td>\n",
              "      <td id=\"T_02857_row230_col5\" class=\"data row230 col5\" >0.7824</td>\n",
              "      <td id=\"T_02857_row230_col6\" class=\"data row230 col6\" >0.7815</td>\n",
              "      <td id=\"T_02857_row230_col7\" class=\"data row230 col7\" >0.7500</td>\n",
              "      <td id=\"T_02857_row230_col8\" class=\"data row230 col8\" >0.4828</td>\n",
              "      <td id=\"T_02857_row230_col9\" class=\"data row230 col9\" >0.6007</td>\n",
              "      <td id=\"T_02857_row230_col10\" class=\"data row230 col10\" >0.5111</td>\n",
              "      <td id=\"T_02857_row230_col11\" class=\"data row230 col11\" >0.5612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row231\" class=\"row_heading level0 row231\" >231</th>\n",
              "      <td id=\"T_02857_row231_col0\" class=\"data row231 col0\" >es</td>\n",
              "      <td id=\"T_02857_row231_col1\" class=\"data row231 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row231_col2\" class=\"data row231 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row231_col3\" class=\"data row231 col3\" >0.7664</td>\n",
              "      <td id=\"T_02857_row231_col4\" class=\"data row231 col4\" >0.7967</td>\n",
              "      <td id=\"T_02857_row231_col5\" class=\"data row231 col5\" >0.7824</td>\n",
              "      <td id=\"T_02857_row231_col6\" class=\"data row231 col6\" >0.7815</td>\n",
              "      <td id=\"T_02857_row231_col7\" class=\"data row231 col7\" >0.7500</td>\n",
              "      <td id=\"T_02857_row231_col8\" class=\"data row231 col8\" >0.4828</td>\n",
              "      <td id=\"T_02857_row231_col9\" class=\"data row231 col9\" >0.6007</td>\n",
              "      <td id=\"T_02857_row231_col10\" class=\"data row231 col10\" >0.5111</td>\n",
              "      <td id=\"T_02857_row231_col11\" class=\"data row231 col11\" >0.5612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row232\" class=\"row_heading level0 row232\" >232</th>\n",
              "      <td id=\"T_02857_row232_col0\" class=\"data row232 col0\" >es</td>\n",
              "      <td id=\"T_02857_row232_col1\" class=\"data row232 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row232_col2\" class=\"data row232 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row232_col3\" class=\"data row232 col3\" >0.8630</td>\n",
              "      <td id=\"T_02857_row232_col4\" class=\"data row232 col4\" >0.9009</td>\n",
              "      <td id=\"T_02857_row232_col5\" class=\"data row232 col5\" >0.8518</td>\n",
              "      <td id=\"T_02857_row232_col6\" class=\"data row232 col6\" >0.8656</td>\n",
              "      <td id=\"T_02857_row232_col7\" class=\"data row232 col7\" >0.7728</td>\n",
              "      <td id=\"T_02857_row232_col8\" class=\"data row232 col8\" >0.4794</td>\n",
              "      <td id=\"T_02857_row232_col9\" class=\"data row232 col9\" >0.4793</td>\n",
              "      <td id=\"T_02857_row232_col10\" class=\"data row232 col10\" >0.4083</td>\n",
              "      <td id=\"T_02857_row232_col11\" class=\"data row232 col11\" >0.5334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row233\" class=\"row_heading level0 row233\" >233</th>\n",
              "      <td id=\"T_02857_row233_col0\" class=\"data row233 col0\" >es</td>\n",
              "      <td id=\"T_02857_row233_col1\" class=\"data row233 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row233_col2\" class=\"data row233 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row233_col3\" class=\"data row233 col3\" >0.8630</td>\n",
              "      <td id=\"T_02857_row233_col4\" class=\"data row233 col4\" >0.9009</td>\n",
              "      <td id=\"T_02857_row233_col5\" class=\"data row233 col5\" >0.8518</td>\n",
              "      <td id=\"T_02857_row233_col6\" class=\"data row233 col6\" >0.8656</td>\n",
              "      <td id=\"T_02857_row233_col7\" class=\"data row233 col7\" >0.7728</td>\n",
              "      <td id=\"T_02857_row233_col8\" class=\"data row233 col8\" >0.4794</td>\n",
              "      <td id=\"T_02857_row233_col9\" class=\"data row233 col9\" >0.4793</td>\n",
              "      <td id=\"T_02857_row233_col10\" class=\"data row233 col10\" >0.4083</td>\n",
              "      <td id=\"T_02857_row233_col11\" class=\"data row233 col11\" >0.5334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row234\" class=\"row_heading level0 row234\" >234</th>\n",
              "      <td id=\"T_02857_row234_col0\" class=\"data row234 col0\" >es</td>\n",
              "      <td id=\"T_02857_row234_col1\" class=\"data row234 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row234_col2\" class=\"data row234 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row234_col3\" class=\"data row234 col3\" >0.6872</td>\n",
              "      <td id=\"T_02857_row234_col4\" class=\"data row234 col4\" >0.7304</td>\n",
              "      <td id=\"T_02857_row234_col5\" class=\"data row234 col5\" >0.6966</td>\n",
              "      <td id=\"T_02857_row234_col6\" class=\"data row234 col6\" >0.7141</td>\n",
              "      <td id=\"T_02857_row234_col7\" class=\"data row234 col7\" >0.6489</td>\n",
              "      <td id=\"T_02857_row234_col8\" class=\"data row234 col8\" >0.4041</td>\n",
              "      <td id=\"T_02857_row234_col9\" class=\"data row234 col9\" >0.5363</td>\n",
              "      <td id=\"T_02857_row234_col10\" class=\"data row234 col10\" >0.4789</td>\n",
              "      <td id=\"T_02857_row234_col11\" class=\"data row234 col11\" >0.4390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row235\" class=\"row_heading level0 row235\" >235</th>\n",
              "      <td id=\"T_02857_row235_col0\" class=\"data row235 col0\" >es</td>\n",
              "      <td id=\"T_02857_row235_col1\" class=\"data row235 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row235_col2\" class=\"data row235 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row235_col3\" class=\"data row235 col3\" >0.6872</td>\n",
              "      <td id=\"T_02857_row235_col4\" class=\"data row235 col4\" >0.7304</td>\n",
              "      <td id=\"T_02857_row235_col5\" class=\"data row235 col5\" >0.6966</td>\n",
              "      <td id=\"T_02857_row235_col6\" class=\"data row235 col6\" >0.7141</td>\n",
              "      <td id=\"T_02857_row235_col7\" class=\"data row235 col7\" >0.6489</td>\n",
              "      <td id=\"T_02857_row235_col8\" class=\"data row235 col8\" >0.4041</td>\n",
              "      <td id=\"T_02857_row235_col9\" class=\"data row235 col9\" >0.5363</td>\n",
              "      <td id=\"T_02857_row235_col10\" class=\"data row235 col10\" >0.4789</td>\n",
              "      <td id=\"T_02857_row235_col11\" class=\"data row235 col11\" >0.4390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row236\" class=\"row_heading level0 row236\" >236</th>\n",
              "      <td id=\"T_02857_row236_col0\" class=\"data row236 col0\" >es</td>\n",
              "      <td id=\"T_02857_row236_col1\" class=\"data row236 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row236_col2\" class=\"data row236 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row236_col3\" class=\"data row236 col3\" >0.6991</td>\n",
              "      <td id=\"T_02857_row236_col4\" class=\"data row236 col4\" >0.7924</td>\n",
              "      <td id=\"T_02857_row236_col5\" class=\"data row236 col5\" >0.7742</td>\n",
              "      <td id=\"T_02857_row236_col6\" class=\"data row236 col6\" >0.8133</td>\n",
              "      <td id=\"T_02857_row236_col7\" class=\"data row236 col7\" >0.7086</td>\n",
              "      <td id=\"T_02857_row236_col8\" class=\"data row236 col8\" >0.6679</td>\n",
              "      <td id=\"T_02857_row236_col9\" class=\"data row236 col9\" >0.5095</td>\n",
              "      <td id=\"T_02857_row236_col10\" class=\"data row236 col10\" >0.5060</td>\n",
              "      <td id=\"T_02857_row236_col11\" class=\"data row236 col11\" >0.4896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row237\" class=\"row_heading level0 row237\" >237</th>\n",
              "      <td id=\"T_02857_row237_col0\" class=\"data row237 col0\" >es</td>\n",
              "      <td id=\"T_02857_row237_col1\" class=\"data row237 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row237_col2\" class=\"data row237 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row237_col3\" class=\"data row237 col3\" >0.6991</td>\n",
              "      <td id=\"T_02857_row237_col4\" class=\"data row237 col4\" >0.7924</td>\n",
              "      <td id=\"T_02857_row237_col5\" class=\"data row237 col5\" >0.7742</td>\n",
              "      <td id=\"T_02857_row237_col6\" class=\"data row237 col6\" >0.8133</td>\n",
              "      <td id=\"T_02857_row237_col7\" class=\"data row237 col7\" >0.7086</td>\n",
              "      <td id=\"T_02857_row237_col8\" class=\"data row237 col8\" >0.6679</td>\n",
              "      <td id=\"T_02857_row237_col9\" class=\"data row237 col9\" >0.5095</td>\n",
              "      <td id=\"T_02857_row237_col10\" class=\"data row237 col10\" >0.5060</td>\n",
              "      <td id=\"T_02857_row237_col11\" class=\"data row237 col11\" >0.4896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row238\" class=\"row_heading level0 row238\" >238</th>\n",
              "      <td id=\"T_02857_row238_col0\" class=\"data row238 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row238_col1\" class=\"data row238 col1\" >all</td>\n",
              "      <td id=\"T_02857_row238_col2\" class=\"data row238 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row238_col3\" class=\"data row238 col3\" >0.5934</td>\n",
              "      <td id=\"T_02857_row238_col4\" class=\"data row238 col4\" >0.5956</td>\n",
              "      <td id=\"T_02857_row238_col5\" class=\"data row238 col5\" >0.5917</td>\n",
              "      <td id=\"T_02857_row238_col6\" class=\"data row238 col6\" >0.5939</td>\n",
              "      <td id=\"T_02857_row238_col7\" class=\"data row238 col7\" >0.5896</td>\n",
              "      <td id=\"T_02857_row238_col8\" class=\"data row238 col8\" >0.5792</td>\n",
              "      <td id=\"T_02857_row238_col9\" class=\"data row238 col9\" >0.5912</td>\n",
              "      <td id=\"T_02857_row238_col10\" class=\"data row238 col10\" >0.5702</td>\n",
              "      <td id=\"T_02857_row238_col11\" class=\"data row238 col11\" >0.6754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row239\" class=\"row_heading level0 row239\" >239</th>\n",
              "      <td id=\"T_02857_row239_col0\" class=\"data row239 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row239_col1\" class=\"data row239 col1\" >all</td>\n",
              "      <td id=\"T_02857_row239_col2\" class=\"data row239 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row239_col3\" class=\"data row239 col3\" >0.3349</td>\n",
              "      <td id=\"T_02857_row239_col4\" class=\"data row239 col4\" >0.3346</td>\n",
              "      <td id=\"T_02857_row239_col5\" class=\"data row239 col5\" >0.3347</td>\n",
              "      <td id=\"T_02857_row239_col6\" class=\"data row239 col6\" >0.3350</td>\n",
              "      <td id=\"T_02857_row239_col7\" class=\"data row239 col7\" >0.3350</td>\n",
              "      <td id=\"T_02857_row239_col8\" class=\"data row239 col8\" >0.3318</td>\n",
              "      <td id=\"T_02857_row239_col9\" class=\"data row239 col9\" >0.3336</td>\n",
              "      <td id=\"T_02857_row239_col10\" class=\"data row239 col10\" >0.3345</td>\n",
              "      <td id=\"T_02857_row239_col11\" class=\"data row239 col11\" >0.4709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row240\" class=\"row_heading level0 row240\" >240</th>\n",
              "      <td id=\"T_02857_row240_col0\" class=\"data row240 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row240_col1\" class=\"data row240 col1\" >all</td>\n",
              "      <td id=\"T_02857_row240_col2\" class=\"data row240 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row240_col3\" class=\"data row240 col3\" >0.5465</td>\n",
              "      <td id=\"T_02857_row240_col4\" class=\"data row240 col4\" >0.5622</td>\n",
              "      <td id=\"T_02857_row240_col5\" class=\"data row240 col5\" >0.5378</td>\n",
              "      <td id=\"T_02857_row240_col6\" class=\"data row240 col6\" >0.5623</td>\n",
              "      <td id=\"T_02857_row240_col7\" class=\"data row240 col7\" >0.5705</td>\n",
              "      <td id=\"T_02857_row240_col8\" class=\"data row240 col8\" >0.5705</td>\n",
              "      <td id=\"T_02857_row240_col9\" class=\"data row240 col9\" >0.5909</td>\n",
              "      <td id=\"T_02857_row240_col10\" class=\"data row240 col10\" >0.5487</td>\n",
              "      <td id=\"T_02857_row240_col11\" class=\"data row240 col11\" >0.5945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row241\" class=\"row_heading level0 row241\" >241</th>\n",
              "      <td id=\"T_02857_row241_col0\" class=\"data row241 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row241_col1\" class=\"data row241 col1\" >all</td>\n",
              "      <td id=\"T_02857_row241_col2\" class=\"data row241 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row241_col3\" class=\"data row241 col3\" >0.6760</td>\n",
              "      <td id=\"T_02857_row241_col4\" class=\"data row241 col4\" >0.6809</td>\n",
              "      <td id=\"T_02857_row241_col5\" class=\"data row241 col5\" >0.6788</td>\n",
              "      <td id=\"T_02857_row241_col6\" class=\"data row241 col6\" >0.6774</td>\n",
              "      <td id=\"T_02857_row241_col7\" class=\"data row241 col7\" >0.6771</td>\n",
              "      <td id=\"T_02857_row241_col8\" class=\"data row241 col8\" >0.6780</td>\n",
              "      <td id=\"T_02857_row241_col9\" class=\"data row241 col9\" >0.6821</td>\n",
              "      <td id=\"T_02857_row241_col10\" class=\"data row241 col10\" >0.6196</td>\n",
              "      <td id=\"T_02857_row241_col11\" class=\"data row241 col11\" >0.7219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row242\" class=\"row_heading level0 row242\" >242</th>\n",
              "      <td id=\"T_02857_row242_col0\" class=\"data row242 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row242_col1\" class=\"data row242 col1\" >all</td>\n",
              "      <td id=\"T_02857_row242_col2\" class=\"data row242 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row242_col3\" class=\"data row242 col3\" >0.6749</td>\n",
              "      <td id=\"T_02857_row242_col4\" class=\"data row242 col4\" >0.6738</td>\n",
              "      <td id=\"T_02857_row242_col5\" class=\"data row242 col5\" >0.6757</td>\n",
              "      <td id=\"T_02857_row242_col6\" class=\"data row242 col6\" >0.6743</td>\n",
              "      <td id=\"T_02857_row242_col7\" class=\"data row242 col7\" >0.6771</td>\n",
              "      <td id=\"T_02857_row242_col8\" class=\"data row242 col8\" >0.6714</td>\n",
              "      <td id=\"T_02857_row242_col9\" class=\"data row242 col9\" >0.6747</td>\n",
              "      <td id=\"T_02857_row242_col10\" class=\"data row242 col10\" >0.6667</td>\n",
              "      <td id=\"T_02857_row242_col11\" class=\"data row242 col11\" >0.7466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row243\" class=\"row_heading level0 row243\" >243</th>\n",
              "      <td id=\"T_02857_row243_col0\" class=\"data row243 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row243_col1\" class=\"data row243 col1\" >all</td>\n",
              "      <td id=\"T_02857_row243_col2\" class=\"data row243 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row243_col3\" class=\"data row243 col3\" >0.4447</td>\n",
              "      <td id=\"T_02857_row243_col4\" class=\"data row243 col4\" >0.4467</td>\n",
              "      <td id=\"T_02857_row243_col5\" class=\"data row243 col5\" >0.4457</td>\n",
              "      <td id=\"T_02857_row243_col6\" class=\"data row243 col6\" >0.4456</td>\n",
              "      <td id=\"T_02857_row243_col7\" class=\"data row243 col7\" >0.4456</td>\n",
              "      <td id=\"T_02857_row243_col8\" class=\"data row243 col8\" >0.4412</td>\n",
              "      <td id=\"T_02857_row243_col9\" class=\"data row243 col9\" >0.4466</td>\n",
              "      <td id=\"T_02857_row243_col10\" class=\"data row243 col10\" >0.4440</td>\n",
              "      <td id=\"T_02857_row243_col11\" class=\"data row243 col11\" >0.5642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row244\" class=\"row_heading level0 row244\" >244</th>\n",
              "      <td id=\"T_02857_row244_col0\" class=\"data row244 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row244_col1\" class=\"data row244 col1\" >all</td>\n",
              "      <td id=\"T_02857_row244_col2\" class=\"data row244 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row244_col3\" class=\"data row244 col3\" >0.7767</td>\n",
              "      <td id=\"T_02857_row244_col4\" class=\"data row244 col4\" >0.7797</td>\n",
              "      <td id=\"T_02857_row244_col5\" class=\"data row244 col5\" >0.7763</td>\n",
              "      <td id=\"T_02857_row244_col6\" class=\"data row244 col6\" >0.7748</td>\n",
              "      <td id=\"T_02857_row244_col7\" class=\"data row244 col7\" >0.7713</td>\n",
              "      <td id=\"T_02857_row244_col8\" class=\"data row244 col8\" >0.7832</td>\n",
              "      <td id=\"T_02857_row244_col9\" class=\"data row244 col9\" >0.7782</td>\n",
              "      <td id=\"T_02857_row244_col10\" class=\"data row244 col10\" >0.7338</td>\n",
              "      <td id=\"T_02857_row244_col11\" class=\"data row244 col11\" >0.7987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row245\" class=\"row_heading level0 row245\" >245</th>\n",
              "      <td id=\"T_02857_row245_col0\" class=\"data row245 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row245_col1\" class=\"data row245 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row245_col2\" class=\"data row245 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row245_col3\" class=\"data row245 col3\" >0.7276</td>\n",
              "      <td id=\"T_02857_row245_col4\" class=\"data row245 col4\" >0.7424</td>\n",
              "      <td id=\"T_02857_row245_col5\" class=\"data row245 col5\" >0.7279</td>\n",
              "      <td id=\"T_02857_row245_col6\" class=\"data row245 col6\" >0.7079</td>\n",
              "      <td id=\"T_02857_row245_col7\" class=\"data row245 col7\" >0.7127</td>\n",
              "      <td id=\"T_02857_row245_col8\" class=\"data row245 col8\" >0.5071</td>\n",
              "      <td id=\"T_02857_row245_col9\" class=\"data row245 col9\" >0.4906</td>\n",
              "      <td id=\"T_02857_row245_col10\" class=\"data row245 col10\" >0.4891</td>\n",
              "      <td id=\"T_02857_row245_col11\" class=\"data row245 col11\" >0.5679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row246\" class=\"row_heading level0 row246\" >246</th>\n",
              "      <td id=\"T_02857_row246_col0\" class=\"data row246 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row246_col1\" class=\"data row246 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row246_col2\" class=\"data row246 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row246_col3\" class=\"data row246 col3\" >0.7276</td>\n",
              "      <td id=\"T_02857_row246_col4\" class=\"data row246 col4\" >0.7424</td>\n",
              "      <td id=\"T_02857_row246_col5\" class=\"data row246 col5\" >0.7279</td>\n",
              "      <td id=\"T_02857_row246_col6\" class=\"data row246 col6\" >0.7079</td>\n",
              "      <td id=\"T_02857_row246_col7\" class=\"data row246 col7\" >0.7127</td>\n",
              "      <td id=\"T_02857_row246_col8\" class=\"data row246 col8\" >0.5071</td>\n",
              "      <td id=\"T_02857_row246_col9\" class=\"data row246 col9\" >0.4906</td>\n",
              "      <td id=\"T_02857_row246_col10\" class=\"data row246 col10\" >0.4891</td>\n",
              "      <td id=\"T_02857_row246_col11\" class=\"data row246 col11\" >0.5679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row247\" class=\"row_heading level0 row247\" >247</th>\n",
              "      <td id=\"T_02857_row247_col0\" class=\"data row247 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row247_col1\" class=\"data row247 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row247_col2\" class=\"data row247 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row247_col3\" class=\"data row247 col3\" >0.4906</td>\n",
              "      <td id=\"T_02857_row247_col4\" class=\"data row247 col4\" >0.4737</td>\n",
              "      <td id=\"T_02857_row247_col5\" class=\"data row247 col5\" >0.4746</td>\n",
              "      <td id=\"T_02857_row247_col6\" class=\"data row247 col6\" >0.4873</td>\n",
              "      <td id=\"T_02857_row247_col7\" class=\"data row247 col7\" >0.4889</td>\n",
              "      <td id=\"T_02857_row247_col8\" class=\"data row247 col8\" >0.5725</td>\n",
              "      <td id=\"T_02857_row247_col9\" class=\"data row247 col9\" >0.6388</td>\n",
              "      <td id=\"T_02857_row247_col10\" class=\"data row247 col10\" >0.5265</td>\n",
              "      <td id=\"T_02857_row247_col11\" class=\"data row247 col11\" >0.3700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row248\" class=\"row_heading level0 row248\" >248</th>\n",
              "      <td id=\"T_02857_row248_col0\" class=\"data row248 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row248_col1\" class=\"data row248 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row248_col2\" class=\"data row248 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row248_col3\" class=\"data row248 col3\" >0.4906</td>\n",
              "      <td id=\"T_02857_row248_col4\" class=\"data row248 col4\" >0.4737</td>\n",
              "      <td id=\"T_02857_row248_col5\" class=\"data row248 col5\" >0.4746</td>\n",
              "      <td id=\"T_02857_row248_col6\" class=\"data row248 col6\" >0.4873</td>\n",
              "      <td id=\"T_02857_row248_col7\" class=\"data row248 col7\" >0.4889</td>\n",
              "      <td id=\"T_02857_row248_col8\" class=\"data row248 col8\" >0.5725</td>\n",
              "      <td id=\"T_02857_row248_col9\" class=\"data row248 col9\" >0.6388</td>\n",
              "      <td id=\"T_02857_row248_col10\" class=\"data row248 col10\" >0.5265</td>\n",
              "      <td id=\"T_02857_row248_col11\" class=\"data row248 col11\" >0.3700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row249\" class=\"row_heading level0 row249\" >249</th>\n",
              "      <td id=\"T_02857_row249_col0\" class=\"data row249 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row249_col1\" class=\"data row249 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row249_col2\" class=\"data row249 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row249_col3\" class=\"data row249 col3\" >0.4338</td>\n",
              "      <td id=\"T_02857_row249_col4\" class=\"data row249 col4\" >0.4978</td>\n",
              "      <td id=\"T_02857_row249_col5\" class=\"data row249 col5\" >0.4154</td>\n",
              "      <td id=\"T_02857_row249_col6\" class=\"data row249 col6\" >0.4154</td>\n",
              "      <td id=\"T_02857_row249_col7\" class=\"data row249 col7\" >0.6095</td>\n",
              "      <td id=\"T_02857_row249_col8\" class=\"data row249 col8\" >0.4667</td>\n",
              "      <td id=\"T_02857_row249_col9\" class=\"data row249 col9\" >0.5750</td>\n",
              "      <td id=\"T_02857_row249_col10\" class=\"data row249 col10\" >0.3855</td>\n",
              "      <td id=\"T_02857_row249_col11\" class=\"data row249 col11\" >0.2498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row250\" class=\"row_heading level0 row250\" >250</th>\n",
              "      <td id=\"T_02857_row250_col0\" class=\"data row250 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row250_col1\" class=\"data row250 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row250_col2\" class=\"data row250 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row250_col3\" class=\"data row250 col3\" >0.4338</td>\n",
              "      <td id=\"T_02857_row250_col4\" class=\"data row250 col4\" >0.4978</td>\n",
              "      <td id=\"T_02857_row250_col5\" class=\"data row250 col5\" >0.4154</td>\n",
              "      <td id=\"T_02857_row250_col6\" class=\"data row250 col6\" >0.4154</td>\n",
              "      <td id=\"T_02857_row250_col7\" class=\"data row250 col7\" >0.6095</td>\n",
              "      <td id=\"T_02857_row250_col8\" class=\"data row250 col8\" >0.4667</td>\n",
              "      <td id=\"T_02857_row250_col9\" class=\"data row250 col9\" >0.5750</td>\n",
              "      <td id=\"T_02857_row250_col10\" class=\"data row250 col10\" >0.3855</td>\n",
              "      <td id=\"T_02857_row250_col11\" class=\"data row250 col11\" >0.2498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row251\" class=\"row_heading level0 row251\" >251</th>\n",
              "      <td id=\"T_02857_row251_col0\" class=\"data row251 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row251_col1\" class=\"data row251 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row251_col2\" class=\"data row251 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row251_col3\" class=\"data row251 col3\" >0.6610</td>\n",
              "      <td id=\"T_02857_row251_col4\" class=\"data row251 col4\" >0.7089</td>\n",
              "      <td id=\"T_02857_row251_col5\" class=\"data row251 col5\" >0.6885</td>\n",
              "      <td id=\"T_02857_row251_col6\" class=\"data row251 col6\" >0.6784</td>\n",
              "      <td id=\"T_02857_row251_col7\" class=\"data row251 col7\" >0.7077</td>\n",
              "      <td id=\"T_02857_row251_col8\" class=\"data row251 col8\" >0.4986</td>\n",
              "      <td id=\"T_02857_row251_col9\" class=\"data row251 col9\" >0.5698</td>\n",
              "      <td id=\"T_02857_row251_col10\" class=\"data row251 col10\" >0.5004</td>\n",
              "      <td id=\"T_02857_row251_col11\" class=\"data row251 col11\" >0.5444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row252\" class=\"row_heading level0 row252\" >252</th>\n",
              "      <td id=\"T_02857_row252_col0\" class=\"data row252 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row252_col1\" class=\"data row252 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row252_col2\" class=\"data row252 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row252_col3\" class=\"data row252 col3\" >0.6610</td>\n",
              "      <td id=\"T_02857_row252_col4\" class=\"data row252 col4\" >0.7089</td>\n",
              "      <td id=\"T_02857_row252_col5\" class=\"data row252 col5\" >0.6885</td>\n",
              "      <td id=\"T_02857_row252_col6\" class=\"data row252 col6\" >0.6784</td>\n",
              "      <td id=\"T_02857_row252_col7\" class=\"data row252 col7\" >0.7077</td>\n",
              "      <td id=\"T_02857_row252_col8\" class=\"data row252 col8\" >0.4986</td>\n",
              "      <td id=\"T_02857_row252_col9\" class=\"data row252 col9\" >0.5698</td>\n",
              "      <td id=\"T_02857_row252_col10\" class=\"data row252 col10\" >0.5004</td>\n",
              "      <td id=\"T_02857_row252_col11\" class=\"data row252 col11\" >0.5444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row253\" class=\"row_heading level0 row253\" >253</th>\n",
              "      <td id=\"T_02857_row253_col0\" class=\"data row253 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row253_col1\" class=\"data row253 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row253_col2\" class=\"data row253 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row253_col3\" class=\"data row253 col3\" >0.7959</td>\n",
              "      <td id=\"T_02857_row253_col4\" class=\"data row253 col4\" >0.8366</td>\n",
              "      <td id=\"T_02857_row253_col5\" class=\"data row253 col5\" >0.8700</td>\n",
              "      <td id=\"T_02857_row253_col6\" class=\"data row253 col6\" >0.8637</td>\n",
              "      <td id=\"T_02857_row253_col7\" class=\"data row253 col7\" >0.9018</td>\n",
              "      <td id=\"T_02857_row253_col8\" class=\"data row253 col8\" >0.8611</td>\n",
              "      <td id=\"T_02857_row253_col9\" class=\"data row253 col9\" >0.5788</td>\n",
              "      <td id=\"T_02857_row253_col10\" class=\"data row253 col10\" >0.7436</td>\n",
              "      <td id=\"T_02857_row253_col11\" class=\"data row253 col11\" >0.6593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row254\" class=\"row_heading level0 row254\" >254</th>\n",
              "      <td id=\"T_02857_row254_col0\" class=\"data row254 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row254_col1\" class=\"data row254 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row254_col2\" class=\"data row254 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row254_col3\" class=\"data row254 col3\" >0.7959</td>\n",
              "      <td id=\"T_02857_row254_col4\" class=\"data row254 col4\" >0.8366</td>\n",
              "      <td id=\"T_02857_row254_col5\" class=\"data row254 col5\" >0.8700</td>\n",
              "      <td id=\"T_02857_row254_col6\" class=\"data row254 col6\" >0.8637</td>\n",
              "      <td id=\"T_02857_row254_col7\" class=\"data row254 col7\" >0.9018</td>\n",
              "      <td id=\"T_02857_row254_col8\" class=\"data row254 col8\" >0.8611</td>\n",
              "      <td id=\"T_02857_row254_col9\" class=\"data row254 col9\" >0.5788</td>\n",
              "      <td id=\"T_02857_row254_col10\" class=\"data row254 col10\" >0.7436</td>\n",
              "      <td id=\"T_02857_row254_col11\" class=\"data row254 col11\" >0.6593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row255\" class=\"row_heading level0 row255\" >255</th>\n",
              "      <td id=\"T_02857_row255_col0\" class=\"data row255 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row255_col1\" class=\"data row255 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row255_col2\" class=\"data row255 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row255_col3\" class=\"data row255 col3\" >0.5139</td>\n",
              "      <td id=\"T_02857_row255_col4\" class=\"data row255 col4\" >0.5240</td>\n",
              "      <td id=\"T_02857_row255_col5\" class=\"data row255 col5\" >0.5134</td>\n",
              "      <td id=\"T_02857_row255_col6\" class=\"data row255 col6\" >0.4923</td>\n",
              "      <td id=\"T_02857_row255_col7\" class=\"data row255 col7\" >0.5359</td>\n",
              "      <td id=\"T_02857_row255_col8\" class=\"data row255 col8\" >0.4254</td>\n",
              "      <td id=\"T_02857_row255_col9\" class=\"data row255 col9\" >0.4890</td>\n",
              "      <td id=\"T_02857_row255_col10\" class=\"data row255 col10\" >0.4723</td>\n",
              "      <td id=\"T_02857_row255_col11\" class=\"data row255 col11\" >0.5059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row256\" class=\"row_heading level0 row256\" >256</th>\n",
              "      <td id=\"T_02857_row256_col0\" class=\"data row256 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row256_col1\" class=\"data row256 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row256_col2\" class=\"data row256 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row256_col3\" class=\"data row256 col3\" >0.5139</td>\n",
              "      <td id=\"T_02857_row256_col4\" class=\"data row256 col4\" >0.5240</td>\n",
              "      <td id=\"T_02857_row256_col5\" class=\"data row256 col5\" >0.5134</td>\n",
              "      <td id=\"T_02857_row256_col6\" class=\"data row256 col6\" >0.4923</td>\n",
              "      <td id=\"T_02857_row256_col7\" class=\"data row256 col7\" >0.5359</td>\n",
              "      <td id=\"T_02857_row256_col8\" class=\"data row256 col8\" >0.4254</td>\n",
              "      <td id=\"T_02857_row256_col9\" class=\"data row256 col9\" >0.4890</td>\n",
              "      <td id=\"T_02857_row256_col10\" class=\"data row256 col10\" >0.4723</td>\n",
              "      <td id=\"T_02857_row256_col11\" class=\"data row256 col11\" >0.5059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row257\" class=\"row_heading level0 row257\" >257</th>\n",
              "      <td id=\"T_02857_row257_col0\" class=\"data row257 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row257_col1\" class=\"data row257 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row257_col2\" class=\"data row257 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row257_col3\" class=\"data row257 col3\" >0.7762</td>\n",
              "      <td id=\"T_02857_row257_col4\" class=\"data row257 col4\" >0.8502</td>\n",
              "      <td id=\"T_02857_row257_col5\" class=\"data row257 col5\" >0.8739</td>\n",
              "      <td id=\"T_02857_row257_col6\" class=\"data row257 col6\" >0.8470</td>\n",
              "      <td id=\"T_02857_row257_col7\" class=\"data row257 col7\" >0.9212</td>\n",
              "      <td id=\"T_02857_row257_col8\" class=\"data row257 col8\" >0.8006</td>\n",
              "      <td id=\"T_02857_row257_col9\" class=\"data row257 col9\" >0.5112</td>\n",
              "      <td id=\"T_02857_row257_col10\" class=\"data row257 col10\" >0.5750</td>\n",
              "      <td id=\"T_02857_row257_col11\" class=\"data row257 col11\" >0.5794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row258\" class=\"row_heading level0 row258\" >258</th>\n",
              "      <td id=\"T_02857_row258_col0\" class=\"data row258 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row258_col1\" class=\"data row258 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row258_col2\" class=\"data row258 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row258_col3\" class=\"data row258 col3\" >0.7762</td>\n",
              "      <td id=\"T_02857_row258_col4\" class=\"data row258 col4\" >0.8502</td>\n",
              "      <td id=\"T_02857_row258_col5\" class=\"data row258 col5\" >0.8739</td>\n",
              "      <td id=\"T_02857_row258_col6\" class=\"data row258 col6\" >0.8470</td>\n",
              "      <td id=\"T_02857_row258_col7\" class=\"data row258 col7\" >0.9212</td>\n",
              "      <td id=\"T_02857_row258_col8\" class=\"data row258 col8\" >0.8006</td>\n",
              "      <td id=\"T_02857_row258_col9\" class=\"data row258 col9\" >0.5112</td>\n",
              "      <td id=\"T_02857_row258_col10\" class=\"data row258 col10\" >0.5750</td>\n",
              "      <td id=\"T_02857_row258_col11\" class=\"data row258 col11\" >0.5794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row259\" class=\"row_heading level0 row259\" >259</th>\n",
              "      <td id=\"T_02857_row259_col0\" class=\"data row259 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row259_col1\" class=\"data row259 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row259_col2\" class=\"data row259 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row259_col3\" class=\"data row259 col3\" >0.8548</td>\n",
              "      <td id=\"T_02857_row259_col4\" class=\"data row259 col4\" >0.8834</td>\n",
              "      <td id=\"T_02857_row259_col5\" class=\"data row259 col5\" >0.8358</td>\n",
              "      <td id=\"T_02857_row259_col6\" class=\"data row259 col6\" >0.8171</td>\n",
              "      <td id=\"T_02857_row259_col7\" class=\"data row259 col7\" >0.7617</td>\n",
              "      <td id=\"T_02857_row259_col8\" class=\"data row259 col8\" >0.4259</td>\n",
              "      <td id=\"T_02857_row259_col9\" class=\"data row259 col9\" >0.3808</td>\n",
              "      <td id=\"T_02857_row259_col10\" class=\"data row259 col10\" >0.4363</td>\n",
              "      <td id=\"T_02857_row259_col11\" class=\"data row259 col11\" >0.5259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row260\" class=\"row_heading level0 row260\" >260</th>\n",
              "      <td id=\"T_02857_row260_col0\" class=\"data row260 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row260_col1\" class=\"data row260 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row260_col2\" class=\"data row260 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row260_col3\" class=\"data row260 col3\" >0.8548</td>\n",
              "      <td id=\"T_02857_row260_col4\" class=\"data row260 col4\" >0.8834</td>\n",
              "      <td id=\"T_02857_row260_col5\" class=\"data row260 col5\" >0.8358</td>\n",
              "      <td id=\"T_02857_row260_col6\" class=\"data row260 col6\" >0.8171</td>\n",
              "      <td id=\"T_02857_row260_col7\" class=\"data row260 col7\" >0.7617</td>\n",
              "      <td id=\"T_02857_row260_col8\" class=\"data row260 col8\" >0.4259</td>\n",
              "      <td id=\"T_02857_row260_col9\" class=\"data row260 col9\" >0.3808</td>\n",
              "      <td id=\"T_02857_row260_col10\" class=\"data row260 col10\" >0.4363</td>\n",
              "      <td id=\"T_02857_row260_col11\" class=\"data row260 col11\" >0.5259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row261\" class=\"row_heading level0 row261\" >261</th>\n",
              "      <td id=\"T_02857_row261_col0\" class=\"data row261 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row261_col1\" class=\"data row261 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row261_col2\" class=\"data row261 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row261_col3\" class=\"data row261 col3\" >0.4983</td>\n",
              "      <td id=\"T_02857_row261_col4\" class=\"data row261 col4\" >0.4889</td>\n",
              "      <td id=\"T_02857_row261_col5\" class=\"data row261 col5\" >0.4347</td>\n",
              "      <td id=\"T_02857_row261_col6\" class=\"data row261 col6\" >0.4384</td>\n",
              "      <td id=\"T_02857_row261_col7\" class=\"data row261 col7\" >0.4408</td>\n",
              "      <td id=\"T_02857_row261_col8\" class=\"data row261 col8\" >0.3908</td>\n",
              "      <td id=\"T_02857_row261_col9\" class=\"data row261 col9\" >0.4084</td>\n",
              "      <td id=\"T_02857_row261_col10\" class=\"data row261 col10\" >0.3857</td>\n",
              "      <td id=\"T_02857_row261_col11\" class=\"data row261 col11\" >0.2358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row262\" class=\"row_heading level0 row262\" >262</th>\n",
              "      <td id=\"T_02857_row262_col0\" class=\"data row262 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row262_col1\" class=\"data row262 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row262_col2\" class=\"data row262 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row262_col3\" class=\"data row262 col3\" >0.4983</td>\n",
              "      <td id=\"T_02857_row262_col4\" class=\"data row262 col4\" >0.4889</td>\n",
              "      <td id=\"T_02857_row262_col5\" class=\"data row262 col5\" >0.4347</td>\n",
              "      <td id=\"T_02857_row262_col6\" class=\"data row262 col6\" >0.4384</td>\n",
              "      <td id=\"T_02857_row262_col7\" class=\"data row262 col7\" >0.4408</td>\n",
              "      <td id=\"T_02857_row262_col8\" class=\"data row262 col8\" >0.3908</td>\n",
              "      <td id=\"T_02857_row262_col9\" class=\"data row262 col9\" >0.4084</td>\n",
              "      <td id=\"T_02857_row262_col10\" class=\"data row262 col10\" >0.3857</td>\n",
              "      <td id=\"T_02857_row262_col11\" class=\"data row262 col11\" >0.2358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row263\" class=\"row_heading level0 row263\" >263</th>\n",
              "      <td id=\"T_02857_row263_col0\" class=\"data row263 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row263_col1\" class=\"data row263 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row263_col2\" class=\"data row263 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row263_col3\" class=\"data row263 col3\" >0.4599</td>\n",
              "      <td id=\"T_02857_row263_col4\" class=\"data row263 col4\" >0.4879</td>\n",
              "      <td id=\"T_02857_row263_col5\" class=\"data row263 col5\" >0.4234</td>\n",
              "      <td id=\"T_02857_row263_col6\" class=\"data row263 col6\" >0.4328</td>\n",
              "      <td id=\"T_02857_row263_col7\" class=\"data row263 col7\" >0.4539</td>\n",
              "      <td id=\"T_02857_row263_col8\" class=\"data row263 col8\" >0.4325</td>\n",
              "      <td id=\"T_02857_row263_col9\" class=\"data row263 col9\" >0.4395</td>\n",
              "      <td id=\"T_02857_row263_col10\" class=\"data row263 col10\" >0.3623</td>\n",
              "      <td id=\"T_02857_row263_col11\" class=\"data row263 col11\" >0.2059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row264\" class=\"row_heading level0 row264\" >264</th>\n",
              "      <td id=\"T_02857_row264_col0\" class=\"data row264 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row264_col1\" class=\"data row264 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row264_col2\" class=\"data row264 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row264_col3\" class=\"data row264 col3\" >0.4599</td>\n",
              "      <td id=\"T_02857_row264_col4\" class=\"data row264 col4\" >0.4879</td>\n",
              "      <td id=\"T_02857_row264_col5\" class=\"data row264 col5\" >0.4234</td>\n",
              "      <td id=\"T_02857_row264_col6\" class=\"data row264 col6\" >0.4328</td>\n",
              "      <td id=\"T_02857_row264_col7\" class=\"data row264 col7\" >0.4539</td>\n",
              "      <td id=\"T_02857_row264_col8\" class=\"data row264 col8\" >0.4325</td>\n",
              "      <td id=\"T_02857_row264_col9\" class=\"data row264 col9\" >0.4395</td>\n",
              "      <td id=\"T_02857_row264_col10\" class=\"data row264 col10\" >0.3623</td>\n",
              "      <td id=\"T_02857_row264_col11\" class=\"data row264 col11\" >0.2059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row265\" class=\"row_heading level0 row265\" >265</th>\n",
              "      <td id=\"T_02857_row265_col0\" class=\"data row265 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row265_col1\" class=\"data row265 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row265_col2\" class=\"data row265 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row265_col3\" class=\"data row265 col3\" >0.7840</td>\n",
              "      <td id=\"T_02857_row265_col4\" class=\"data row265 col4\" >0.8387</td>\n",
              "      <td id=\"T_02857_row265_col5\" class=\"data row265 col5\" >0.7712</td>\n",
              "      <td id=\"T_02857_row265_col6\" class=\"data row265 col6\" >0.7594</td>\n",
              "      <td id=\"T_02857_row265_col7\" class=\"data row265 col7\" >0.7107</td>\n",
              "      <td id=\"T_02857_row265_col8\" class=\"data row265 col8\" >0.3666</td>\n",
              "      <td id=\"T_02857_row265_col9\" class=\"data row265 col9\" >0.3828</td>\n",
              "      <td id=\"T_02857_row265_col10\" class=\"data row265 col10\" >0.4280</td>\n",
              "      <td id=\"T_02857_row265_col11\" class=\"data row265 col11\" >0.4576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row266\" class=\"row_heading level0 row266\" >266</th>\n",
              "      <td id=\"T_02857_row266_col0\" class=\"data row266 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row266_col1\" class=\"data row266 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row266_col2\" class=\"data row266 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row266_col3\" class=\"data row266 col3\" >0.7840</td>\n",
              "      <td id=\"T_02857_row266_col4\" class=\"data row266 col4\" >0.8387</td>\n",
              "      <td id=\"T_02857_row266_col5\" class=\"data row266 col5\" >0.7712</td>\n",
              "      <td id=\"T_02857_row266_col6\" class=\"data row266 col6\" >0.7594</td>\n",
              "      <td id=\"T_02857_row266_col7\" class=\"data row266 col7\" >0.7107</td>\n",
              "      <td id=\"T_02857_row266_col8\" class=\"data row266 col8\" >0.3666</td>\n",
              "      <td id=\"T_02857_row266_col9\" class=\"data row266 col9\" >0.3828</td>\n",
              "      <td id=\"T_02857_row266_col10\" class=\"data row266 col10\" >0.4280</td>\n",
              "      <td id=\"T_02857_row266_col11\" class=\"data row266 col11\" >0.4576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row267\" class=\"row_heading level0 row267\" >267</th>\n",
              "      <td id=\"T_02857_row267_col0\" class=\"data row267 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row267_col1\" class=\"data row267 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row267_col2\" class=\"data row267 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row267_col3\" class=\"data row267 col3\" >0.9074</td>\n",
              "      <td id=\"T_02857_row267_col4\" class=\"data row267 col4\" >0.9185</td>\n",
              "      <td id=\"T_02857_row267_col5\" class=\"data row267 col5\" >0.8608</td>\n",
              "      <td id=\"T_02857_row267_col6\" class=\"data row267 col6\" >0.8515</td>\n",
              "      <td id=\"T_02857_row267_col7\" class=\"data row267 col7\" >0.8127</td>\n",
              "      <td id=\"T_02857_row267_col8\" class=\"data row267 col8\" >0.3644</td>\n",
              "      <td id=\"T_02857_row267_col9\" class=\"data row267 col9\" >0.4025</td>\n",
              "      <td id=\"T_02857_row267_col10\" class=\"data row267 col10\" >0.3799</td>\n",
              "      <td id=\"T_02857_row267_col11\" class=\"data row267 col11\" >0.5133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row268\" class=\"row_heading level0 row268\" >268</th>\n",
              "      <td id=\"T_02857_row268_col0\" class=\"data row268 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row268_col1\" class=\"data row268 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row268_col2\" class=\"data row268 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row268_col3\" class=\"data row268 col3\" >0.9074</td>\n",
              "      <td id=\"T_02857_row268_col4\" class=\"data row268 col4\" >0.9185</td>\n",
              "      <td id=\"T_02857_row268_col5\" class=\"data row268 col5\" >0.8608</td>\n",
              "      <td id=\"T_02857_row268_col6\" class=\"data row268 col6\" >0.8515</td>\n",
              "      <td id=\"T_02857_row268_col7\" class=\"data row268 col7\" >0.8127</td>\n",
              "      <td id=\"T_02857_row268_col8\" class=\"data row268 col8\" >0.3644</td>\n",
              "      <td id=\"T_02857_row268_col9\" class=\"data row268 col9\" >0.4025</td>\n",
              "      <td id=\"T_02857_row268_col10\" class=\"data row268 col10\" >0.3799</td>\n",
              "      <td id=\"T_02857_row268_col11\" class=\"data row268 col11\" >0.5133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row269\" class=\"row_heading level0 row269\" >269</th>\n",
              "      <td id=\"T_02857_row269_col0\" class=\"data row269 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row269_col1\" class=\"data row269 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row269_col2\" class=\"data row269 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row269_col3\" class=\"data row269 col3\" >0.5247</td>\n",
              "      <td id=\"T_02857_row269_col4\" class=\"data row269 col4\" >0.5257</td>\n",
              "      <td id=\"T_02857_row269_col5\" class=\"data row269 col5\" >0.4998</td>\n",
              "      <td id=\"T_02857_row269_col6\" class=\"data row269 col6\" >0.4993</td>\n",
              "      <td id=\"T_02857_row269_col7\" class=\"data row269 col7\" >0.5126</td>\n",
              "      <td id=\"T_02857_row269_col8\" class=\"data row269 col8\" >0.3629</td>\n",
              "      <td id=\"T_02857_row269_col9\" class=\"data row269 col9\" >0.4612</td>\n",
              "      <td id=\"T_02857_row269_col10\" class=\"data row269 col10\" >0.4788</td>\n",
              "      <td id=\"T_02857_row269_col11\" class=\"data row269 col11\" >0.4490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row270\" class=\"row_heading level0 row270\" >270</th>\n",
              "      <td id=\"T_02857_row270_col0\" class=\"data row270 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row270_col1\" class=\"data row270 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row270_col2\" class=\"data row270 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row270_col3\" class=\"data row270 col3\" >0.5247</td>\n",
              "      <td id=\"T_02857_row270_col4\" class=\"data row270 col4\" >0.5257</td>\n",
              "      <td id=\"T_02857_row270_col5\" class=\"data row270 col5\" >0.4998</td>\n",
              "      <td id=\"T_02857_row270_col6\" class=\"data row270 col6\" >0.4993</td>\n",
              "      <td id=\"T_02857_row270_col7\" class=\"data row270 col7\" >0.5126</td>\n",
              "      <td id=\"T_02857_row270_col8\" class=\"data row270 col8\" >0.3629</td>\n",
              "      <td id=\"T_02857_row270_col9\" class=\"data row270 col9\" >0.4612</td>\n",
              "      <td id=\"T_02857_row270_col10\" class=\"data row270 col10\" >0.4788</td>\n",
              "      <td id=\"T_02857_row270_col11\" class=\"data row270 col11\" >0.4490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row271\" class=\"row_heading level0 row271\" >271</th>\n",
              "      <td id=\"T_02857_row271_col0\" class=\"data row271 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row271_col1\" class=\"data row271 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row271_col2\" class=\"data row271 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row271_col3\" class=\"data row271 col3\" >0.9320</td>\n",
              "      <td id=\"T_02857_row271_col4\" class=\"data row271 col4\" >0.9472</td>\n",
              "      <td id=\"T_02857_row271_col5\" class=\"data row271 col5\" >0.9057</td>\n",
              "      <td id=\"T_02857_row271_col6\" class=\"data row271 col6\" >0.8832</td>\n",
              "      <td id=\"T_02857_row271_col7\" class=\"data row271 col7\" >0.8523</td>\n",
              "      <td id=\"T_02857_row271_col8\" class=\"data row271 col8\" >0.4130</td>\n",
              "      <td id=\"T_02857_row271_col9\" class=\"data row271 col9\" >0.4311</td>\n",
              "      <td id=\"T_02857_row271_col10\" class=\"data row271 col10\" >0.4196</td>\n",
              "      <td id=\"T_02857_row271_col11\" class=\"data row271 col11\" >0.5434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row272\" class=\"row_heading level0 row272\" >272</th>\n",
              "      <td id=\"T_02857_row272_col0\" class=\"data row272 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row272_col1\" class=\"data row272 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row272_col2\" class=\"data row272 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row272_col3\" class=\"data row272 col3\" >0.9320</td>\n",
              "      <td id=\"T_02857_row272_col4\" class=\"data row272 col4\" >0.9472</td>\n",
              "      <td id=\"T_02857_row272_col5\" class=\"data row272 col5\" >0.9057</td>\n",
              "      <td id=\"T_02857_row272_col6\" class=\"data row272 col6\" >0.8832</td>\n",
              "      <td id=\"T_02857_row272_col7\" class=\"data row272 col7\" >0.8523</td>\n",
              "      <td id=\"T_02857_row272_col8\" class=\"data row272 col8\" >0.4130</td>\n",
              "      <td id=\"T_02857_row272_col9\" class=\"data row272 col9\" >0.4311</td>\n",
              "      <td id=\"T_02857_row272_col10\" class=\"data row272 col10\" >0.4196</td>\n",
              "      <td id=\"T_02857_row272_col11\" class=\"data row272 col11\" >0.5434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row273\" class=\"row_heading level0 row273\" >273</th>\n",
              "      <td id=\"T_02857_row273_col0\" class=\"data row273 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row273_col1\" class=\"data row273 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row273_col2\" class=\"data row273 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row273_col3\" class=\"data row273 col3\" >0.7747</td>\n",
              "      <td id=\"T_02857_row273_col4\" class=\"data row273 col4\" >0.7671</td>\n",
              "      <td id=\"T_02857_row273_col5\" class=\"data row273 col5\" >0.7354</td>\n",
              "      <td id=\"T_02857_row273_col6\" class=\"data row273 col6\" >0.7139</td>\n",
              "      <td id=\"T_02857_row273_col7\" class=\"data row273 col7\" >0.6645</td>\n",
              "      <td id=\"T_02857_row273_col8\" class=\"data row273 col8\" >0.4051</td>\n",
              "      <td id=\"T_02857_row273_col9\" class=\"data row273 col9\" >0.4094</td>\n",
              "      <td id=\"T_02857_row273_col10\" class=\"data row273 col10\" >0.4649</td>\n",
              "      <td id=\"T_02857_row273_col11\" class=\"data row273 col11\" >0.5220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row274\" class=\"row_heading level0 row274\" >274</th>\n",
              "      <td id=\"T_02857_row274_col0\" class=\"data row274 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row274_col1\" class=\"data row274 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row274_col2\" class=\"data row274 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row274_col3\" class=\"data row274 col3\" >0.7747</td>\n",
              "      <td id=\"T_02857_row274_col4\" class=\"data row274 col4\" >0.7671</td>\n",
              "      <td id=\"T_02857_row274_col5\" class=\"data row274 col5\" >0.7354</td>\n",
              "      <td id=\"T_02857_row274_col6\" class=\"data row274 col6\" >0.7139</td>\n",
              "      <td id=\"T_02857_row274_col7\" class=\"data row274 col7\" >0.6645</td>\n",
              "      <td id=\"T_02857_row274_col8\" class=\"data row274 col8\" >0.4051</td>\n",
              "      <td id=\"T_02857_row274_col9\" class=\"data row274 col9\" >0.4094</td>\n",
              "      <td id=\"T_02857_row274_col10\" class=\"data row274 col10\" >0.4649</td>\n",
              "      <td id=\"T_02857_row274_col11\" class=\"data row274 col11\" >0.5220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row275\" class=\"row_heading level0 row275\" >275</th>\n",
              "      <td id=\"T_02857_row275_col0\" class=\"data row275 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row275_col1\" class=\"data row275 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row275_col2\" class=\"data row275 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row275_col3\" class=\"data row275 col3\" >0.4788</td>\n",
              "      <td id=\"T_02857_row275_col4\" class=\"data row275 col4\" >0.4782</td>\n",
              "      <td id=\"T_02857_row275_col5\" class=\"data row275 col5\" >0.4297</td>\n",
              "      <td id=\"T_02857_row275_col6\" class=\"data row275 col6\" >0.4521</td>\n",
              "      <td id=\"T_02857_row275_col7\" class=\"data row275 col7\" >0.4560</td>\n",
              "      <td id=\"T_02857_row275_col8\" class=\"data row275 col8\" >0.4328</td>\n",
              "      <td id=\"T_02857_row275_col9\" class=\"data row275 col9\" >0.4829</td>\n",
              "      <td id=\"T_02857_row275_col10\" class=\"data row275 col10\" >0.4246</td>\n",
              "      <td id=\"T_02857_row275_col11\" class=\"data row275 col11\" >0.2609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row276\" class=\"row_heading level0 row276\" >276</th>\n",
              "      <td id=\"T_02857_row276_col0\" class=\"data row276 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row276_col1\" class=\"data row276 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row276_col2\" class=\"data row276 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row276_col3\" class=\"data row276 col3\" >0.4788</td>\n",
              "      <td id=\"T_02857_row276_col4\" class=\"data row276 col4\" >0.4782</td>\n",
              "      <td id=\"T_02857_row276_col5\" class=\"data row276 col5\" >0.4297</td>\n",
              "      <td id=\"T_02857_row276_col6\" class=\"data row276 col6\" >0.4521</td>\n",
              "      <td id=\"T_02857_row276_col7\" class=\"data row276 col7\" >0.4560</td>\n",
              "      <td id=\"T_02857_row276_col8\" class=\"data row276 col8\" >0.4328</td>\n",
              "      <td id=\"T_02857_row276_col9\" class=\"data row276 col9\" >0.4829</td>\n",
              "      <td id=\"T_02857_row276_col10\" class=\"data row276 col10\" >0.4246</td>\n",
              "      <td id=\"T_02857_row276_col11\" class=\"data row276 col11\" >0.2609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row277\" class=\"row_heading level0 row277\" >277</th>\n",
              "      <td id=\"T_02857_row277_col0\" class=\"data row277 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row277_col1\" class=\"data row277 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row277_col2\" class=\"data row277 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row277_col3\" class=\"data row277 col3\" >0.5003</td>\n",
              "      <td id=\"T_02857_row277_col4\" class=\"data row277 col4\" >0.4935</td>\n",
              "      <td id=\"T_02857_row277_col5\" class=\"data row277 col5\" >0.4549</td>\n",
              "      <td id=\"T_02857_row277_col6\" class=\"data row277 col6\" >0.4583</td>\n",
              "      <td id=\"T_02857_row277_col7\" class=\"data row277 col7\" >0.4893</td>\n",
              "      <td id=\"T_02857_row277_col8\" class=\"data row277 col8\" >0.4643</td>\n",
              "      <td id=\"T_02857_row277_col9\" class=\"data row277 col9\" >0.4802</td>\n",
              "      <td id=\"T_02857_row277_col10\" class=\"data row277 col10\" >0.4106</td>\n",
              "      <td id=\"T_02857_row277_col11\" class=\"data row277 col11\" >0.2520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row278\" class=\"row_heading level0 row278\" >278</th>\n",
              "      <td id=\"T_02857_row278_col0\" class=\"data row278 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row278_col1\" class=\"data row278 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row278_col2\" class=\"data row278 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row278_col3\" class=\"data row278 col3\" >0.5003</td>\n",
              "      <td id=\"T_02857_row278_col4\" class=\"data row278 col4\" >0.4935</td>\n",
              "      <td id=\"T_02857_row278_col5\" class=\"data row278 col5\" >0.4549</td>\n",
              "      <td id=\"T_02857_row278_col6\" class=\"data row278 col6\" >0.4583</td>\n",
              "      <td id=\"T_02857_row278_col7\" class=\"data row278 col7\" >0.4893</td>\n",
              "      <td id=\"T_02857_row278_col8\" class=\"data row278 col8\" >0.4643</td>\n",
              "      <td id=\"T_02857_row278_col9\" class=\"data row278 col9\" >0.4802</td>\n",
              "      <td id=\"T_02857_row278_col10\" class=\"data row278 col10\" >0.4106</td>\n",
              "      <td id=\"T_02857_row278_col11\" class=\"data row278 col11\" >0.2520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row279\" class=\"row_heading level0 row279\" >279</th>\n",
              "      <td id=\"T_02857_row279_col0\" class=\"data row279 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row279_col1\" class=\"data row279 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row279_col2\" class=\"data row279 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row279_col3\" class=\"data row279 col3\" >0.8566</td>\n",
              "      <td id=\"T_02857_row279_col4\" class=\"data row279 col4\" >0.8458</td>\n",
              "      <td id=\"T_02857_row279_col5\" class=\"data row279 col5\" >0.7582</td>\n",
              "      <td id=\"T_02857_row279_col6\" class=\"data row279 col6\" >0.7497</td>\n",
              "      <td id=\"T_02857_row279_col7\" class=\"data row279 col7\" >0.6444</td>\n",
              "      <td id=\"T_02857_row279_col8\" class=\"data row279 col8\" >0.3656</td>\n",
              "      <td id=\"T_02857_row279_col9\" class=\"data row279 col9\" >0.3996</td>\n",
              "      <td id=\"T_02857_row279_col10\" class=\"data row279 col10\" >0.4090</td>\n",
              "      <td id=\"T_02857_row279_col11\" class=\"data row279 col11\" >0.4407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row280\" class=\"row_heading level0 row280\" >280</th>\n",
              "      <td id=\"T_02857_row280_col0\" class=\"data row280 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row280_col1\" class=\"data row280 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row280_col2\" class=\"data row280 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row280_col3\" class=\"data row280 col3\" >0.8566</td>\n",
              "      <td id=\"T_02857_row280_col4\" class=\"data row280 col4\" >0.8458</td>\n",
              "      <td id=\"T_02857_row280_col5\" class=\"data row280 col5\" >0.7582</td>\n",
              "      <td id=\"T_02857_row280_col6\" class=\"data row280 col6\" >0.7497</td>\n",
              "      <td id=\"T_02857_row280_col7\" class=\"data row280 col7\" >0.6444</td>\n",
              "      <td id=\"T_02857_row280_col8\" class=\"data row280 col8\" >0.3656</td>\n",
              "      <td id=\"T_02857_row280_col9\" class=\"data row280 col9\" >0.3996</td>\n",
              "      <td id=\"T_02857_row280_col10\" class=\"data row280 col10\" >0.4090</td>\n",
              "      <td id=\"T_02857_row280_col11\" class=\"data row280 col11\" >0.4407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row281\" class=\"row_heading level0 row281\" >281</th>\n",
              "      <td id=\"T_02857_row281_col0\" class=\"data row281 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row281_col1\" class=\"data row281 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row281_col2\" class=\"data row281 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row281_col3\" class=\"data row281 col3\" >0.8134</td>\n",
              "      <td id=\"T_02857_row281_col4\" class=\"data row281 col4\" >0.8126</td>\n",
              "      <td id=\"T_02857_row281_col5\" class=\"data row281 col5\" >0.7805</td>\n",
              "      <td id=\"T_02857_row281_col6\" class=\"data row281 col6\" >0.7597</td>\n",
              "      <td id=\"T_02857_row281_col7\" class=\"data row281 col7\" >0.7199</td>\n",
              "      <td id=\"T_02857_row281_col8\" class=\"data row281 col8\" >0.3425</td>\n",
              "      <td id=\"T_02857_row281_col9\" class=\"data row281 col9\" >0.3770</td>\n",
              "      <td id=\"T_02857_row281_col10\" class=\"data row281 col10\" >0.3834</td>\n",
              "      <td id=\"T_02857_row281_col11\" class=\"data row281 col11\" >0.4564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row282\" class=\"row_heading level0 row282\" >282</th>\n",
              "      <td id=\"T_02857_row282_col0\" class=\"data row282 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row282_col1\" class=\"data row282 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row282_col2\" class=\"data row282 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row282_col3\" class=\"data row282 col3\" >0.8134</td>\n",
              "      <td id=\"T_02857_row282_col4\" class=\"data row282 col4\" >0.8126</td>\n",
              "      <td id=\"T_02857_row282_col5\" class=\"data row282 col5\" >0.7805</td>\n",
              "      <td id=\"T_02857_row282_col6\" class=\"data row282 col6\" >0.7597</td>\n",
              "      <td id=\"T_02857_row282_col7\" class=\"data row282 col7\" >0.7199</td>\n",
              "      <td id=\"T_02857_row282_col8\" class=\"data row282 col8\" >0.3425</td>\n",
              "      <td id=\"T_02857_row282_col9\" class=\"data row282 col9\" >0.3770</td>\n",
              "      <td id=\"T_02857_row282_col10\" class=\"data row282 col10\" >0.3834</td>\n",
              "      <td id=\"T_02857_row282_col11\" class=\"data row282 col11\" >0.4564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row283\" class=\"row_heading level0 row283\" >283</th>\n",
              "      <td id=\"T_02857_row283_col0\" class=\"data row283 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row283_col1\" class=\"data row283 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row283_col2\" class=\"data row283 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row283_col3\" class=\"data row283 col3\" >0.5355</td>\n",
              "      <td id=\"T_02857_row283_col4\" class=\"data row283 col4\" >0.4983</td>\n",
              "      <td id=\"T_02857_row283_col5\" class=\"data row283 col5\" >0.4814</td>\n",
              "      <td id=\"T_02857_row283_col6\" class=\"data row283 col6\" >0.4794</td>\n",
              "      <td id=\"T_02857_row283_col7\" class=\"data row283 col7\" >0.4818</td>\n",
              "      <td id=\"T_02857_row283_col8\" class=\"data row283 col8\" >0.3530</td>\n",
              "      <td id=\"T_02857_row283_col9\" class=\"data row283 col9\" >0.4322</td>\n",
              "      <td id=\"T_02857_row283_col10\" class=\"data row283 col10\" >0.4600</td>\n",
              "      <td id=\"T_02857_row283_col11\" class=\"data row283 col11\" >0.4725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row284\" class=\"row_heading level0 row284\" >284</th>\n",
              "      <td id=\"T_02857_row284_col0\" class=\"data row284 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row284_col1\" class=\"data row284 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row284_col2\" class=\"data row284 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row284_col3\" class=\"data row284 col3\" >0.5355</td>\n",
              "      <td id=\"T_02857_row284_col4\" class=\"data row284 col4\" >0.4983</td>\n",
              "      <td id=\"T_02857_row284_col5\" class=\"data row284 col5\" >0.4814</td>\n",
              "      <td id=\"T_02857_row284_col6\" class=\"data row284 col6\" >0.4794</td>\n",
              "      <td id=\"T_02857_row284_col7\" class=\"data row284 col7\" >0.4818</td>\n",
              "      <td id=\"T_02857_row284_col8\" class=\"data row284 col8\" >0.3530</td>\n",
              "      <td id=\"T_02857_row284_col9\" class=\"data row284 col9\" >0.4322</td>\n",
              "      <td id=\"T_02857_row284_col10\" class=\"data row284 col10\" >0.4600</td>\n",
              "      <td id=\"T_02857_row284_col11\" class=\"data row284 col11\" >0.4725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row285\" class=\"row_heading level0 row285\" >285</th>\n",
              "      <td id=\"T_02857_row285_col0\" class=\"data row285 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row285_col1\" class=\"data row285 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row285_col2\" class=\"data row285 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row285_col3\" class=\"data row285 col3\" >0.9499</td>\n",
              "      <td id=\"T_02857_row285_col4\" class=\"data row285 col4\" >0.9378</td>\n",
              "      <td id=\"T_02857_row285_col5\" class=\"data row285 col5\" >0.8743</td>\n",
              "      <td id=\"T_02857_row285_col6\" class=\"data row285 col6\" >0.8342</td>\n",
              "      <td id=\"T_02857_row285_col7\" class=\"data row285 col7\" >0.7668</td>\n",
              "      <td id=\"T_02857_row285_col8\" class=\"data row285 col8\" >0.3651</td>\n",
              "      <td id=\"T_02857_row285_col9\" class=\"data row285 col9\" >0.3855</td>\n",
              "      <td id=\"T_02857_row285_col10\" class=\"data row285 col10\" >0.3779</td>\n",
              "      <td id=\"T_02857_row285_col11\" class=\"data row285 col11\" >0.5037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row286\" class=\"row_heading level0 row286\" >286</th>\n",
              "      <td id=\"T_02857_row286_col0\" class=\"data row286 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row286_col1\" class=\"data row286 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row286_col2\" class=\"data row286 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row286_col3\" class=\"data row286 col3\" >0.9499</td>\n",
              "      <td id=\"T_02857_row286_col4\" class=\"data row286 col4\" >0.9378</td>\n",
              "      <td id=\"T_02857_row286_col5\" class=\"data row286 col5\" >0.8743</td>\n",
              "      <td id=\"T_02857_row286_col6\" class=\"data row286 col6\" >0.8342</td>\n",
              "      <td id=\"T_02857_row286_col7\" class=\"data row286 col7\" >0.7668</td>\n",
              "      <td id=\"T_02857_row286_col8\" class=\"data row286 col8\" >0.3651</td>\n",
              "      <td id=\"T_02857_row286_col9\" class=\"data row286 col9\" >0.3855</td>\n",
              "      <td id=\"T_02857_row286_col10\" class=\"data row286 col10\" >0.3779</td>\n",
              "      <td id=\"T_02857_row286_col11\" class=\"data row286 col11\" >0.5037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row287\" class=\"row_heading level0 row287\" >287</th>\n",
              "      <td id=\"T_02857_row287_col0\" class=\"data row287 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row287_col1\" class=\"data row287 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row287_col2\" class=\"data row287 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row287_col3\" class=\"data row287 col3\" >0.5551</td>\n",
              "      <td id=\"T_02857_row287_col4\" class=\"data row287 col4\" >0.4964</td>\n",
              "      <td id=\"T_02857_row287_col5\" class=\"data row287 col5\" >0.4444</td>\n",
              "      <td id=\"T_02857_row287_col6\" class=\"data row287 col6\" >0.5476</td>\n",
              "      <td id=\"T_02857_row287_col7\" class=\"data row287 col7\" >0.4455</td>\n",
              "      <td id=\"T_02857_row287_col8\" class=\"data row287 col8\" >0.6517</td>\n",
              "      <td id=\"T_02857_row287_col9\" class=\"data row287 col9\" >0.7071</td>\n",
              "      <td id=\"T_02857_row287_col10\" class=\"data row287 col10\" >0.6247</td>\n",
              "      <td id=\"T_02857_row287_col11\" class=\"data row287 col11\" >0.4956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row288\" class=\"row_heading level0 row288\" >288</th>\n",
              "      <td id=\"T_02857_row288_col0\" class=\"data row288 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row288_col1\" class=\"data row288 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row288_col2\" class=\"data row288 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row288_col3\" class=\"data row288 col3\" >0.5551</td>\n",
              "      <td id=\"T_02857_row288_col4\" class=\"data row288 col4\" >0.4964</td>\n",
              "      <td id=\"T_02857_row288_col5\" class=\"data row288 col5\" >0.4444</td>\n",
              "      <td id=\"T_02857_row288_col6\" class=\"data row288 col6\" >0.5476</td>\n",
              "      <td id=\"T_02857_row288_col7\" class=\"data row288 col7\" >0.4455</td>\n",
              "      <td id=\"T_02857_row288_col8\" class=\"data row288 col8\" >0.6517</td>\n",
              "      <td id=\"T_02857_row288_col9\" class=\"data row288 col9\" >0.7071</td>\n",
              "      <td id=\"T_02857_row288_col10\" class=\"data row288 col10\" >0.6247</td>\n",
              "      <td id=\"T_02857_row288_col11\" class=\"data row288 col11\" >0.4956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row289\" class=\"row_heading level0 row289\" >289</th>\n",
              "      <td id=\"T_02857_row289_col0\" class=\"data row289 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row289_col1\" class=\"data row289 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row289_col2\" class=\"data row289 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row289_col3\" class=\"data row289 col3\" >0.4546</td>\n",
              "      <td id=\"T_02857_row289_col4\" class=\"data row289 col4\" >0.4574</td>\n",
              "      <td id=\"T_02857_row289_col5\" class=\"data row289 col5\" >0.4662</td>\n",
              "      <td id=\"T_02857_row289_col6\" class=\"data row289 col6\" >0.4708</td>\n",
              "      <td id=\"T_02857_row289_col7\" class=\"data row289 col7\" >0.4690</td>\n",
              "      <td id=\"T_02857_row289_col8\" class=\"data row289 col8\" >0.4648</td>\n",
              "      <td id=\"T_02857_row289_col9\" class=\"data row289 col9\" >0.5003</td>\n",
              "      <td id=\"T_02857_row289_col10\" class=\"data row289 col10\" >0.4754</td>\n",
              "      <td id=\"T_02857_row289_col11\" class=\"data row289 col11\" >0.4897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row290\" class=\"row_heading level0 row290\" >290</th>\n",
              "      <td id=\"T_02857_row290_col0\" class=\"data row290 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row290_col1\" class=\"data row290 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row290_col2\" class=\"data row290 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row290_col3\" class=\"data row290 col3\" >0.4546</td>\n",
              "      <td id=\"T_02857_row290_col4\" class=\"data row290 col4\" >0.4574</td>\n",
              "      <td id=\"T_02857_row290_col5\" class=\"data row290 col5\" >0.4662</td>\n",
              "      <td id=\"T_02857_row290_col6\" class=\"data row290 col6\" >0.4708</td>\n",
              "      <td id=\"T_02857_row290_col7\" class=\"data row290 col7\" >0.4690</td>\n",
              "      <td id=\"T_02857_row290_col8\" class=\"data row290 col8\" >0.4648</td>\n",
              "      <td id=\"T_02857_row290_col9\" class=\"data row290 col9\" >0.5003</td>\n",
              "      <td id=\"T_02857_row290_col10\" class=\"data row290 col10\" >0.4754</td>\n",
              "      <td id=\"T_02857_row290_col11\" class=\"data row290 col11\" >0.4897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row291\" class=\"row_heading level0 row291\" >291</th>\n",
              "      <td id=\"T_02857_row291_col0\" class=\"data row291 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row291_col1\" class=\"data row291 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row291_col2\" class=\"data row291 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row291_col3\" class=\"data row291 col3\" >0.4093</td>\n",
              "      <td id=\"T_02857_row291_col4\" class=\"data row291 col4\" >0.4182</td>\n",
              "      <td id=\"T_02857_row291_col5\" class=\"data row291 col5\" >0.3999</td>\n",
              "      <td id=\"T_02857_row291_col6\" class=\"data row291 col6\" >0.4377</td>\n",
              "      <td id=\"T_02857_row291_col7\" class=\"data row291 col7\" >0.4423</td>\n",
              "      <td id=\"T_02857_row291_col8\" class=\"data row291 col8\" >0.4855</td>\n",
              "      <td id=\"T_02857_row291_col9\" class=\"data row291 col9\" >0.6506</td>\n",
              "      <td id=\"T_02857_row291_col10\" class=\"data row291 col10\" >0.4791</td>\n",
              "      <td id=\"T_02857_row291_col11\" class=\"data row291 col11\" >0.4042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row292\" class=\"row_heading level0 row292\" >292</th>\n",
              "      <td id=\"T_02857_row292_col0\" class=\"data row292 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row292_col1\" class=\"data row292 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row292_col2\" class=\"data row292 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row292_col3\" class=\"data row292 col3\" >0.4093</td>\n",
              "      <td id=\"T_02857_row292_col4\" class=\"data row292 col4\" >0.4182</td>\n",
              "      <td id=\"T_02857_row292_col5\" class=\"data row292 col5\" >0.3999</td>\n",
              "      <td id=\"T_02857_row292_col6\" class=\"data row292 col6\" >0.4377</td>\n",
              "      <td id=\"T_02857_row292_col7\" class=\"data row292 col7\" >0.4423</td>\n",
              "      <td id=\"T_02857_row292_col8\" class=\"data row292 col8\" >0.4855</td>\n",
              "      <td id=\"T_02857_row292_col9\" class=\"data row292 col9\" >0.6506</td>\n",
              "      <td id=\"T_02857_row292_col10\" class=\"data row292 col10\" >0.4791</td>\n",
              "      <td id=\"T_02857_row292_col11\" class=\"data row292 col11\" >0.4042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row293\" class=\"row_heading level0 row293\" >293</th>\n",
              "      <td id=\"T_02857_row293_col0\" class=\"data row293 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row293_col1\" class=\"data row293 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row293_col2\" class=\"data row293 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row293_col3\" class=\"data row293 col3\" >0.4891</td>\n",
              "      <td id=\"T_02857_row293_col4\" class=\"data row293 col4\" >0.4835</td>\n",
              "      <td id=\"T_02857_row293_col5\" class=\"data row293 col5\" >0.4844</td>\n",
              "      <td id=\"T_02857_row293_col6\" class=\"data row293 col6\" >0.5146</td>\n",
              "      <td id=\"T_02857_row293_col7\" class=\"data row293 col7\" >0.5283</td>\n",
              "      <td id=\"T_02857_row293_col8\" class=\"data row293 col8\" >0.5794</td>\n",
              "      <td id=\"T_02857_row293_col9\" class=\"data row293 col9\" >0.5925</td>\n",
              "      <td id=\"T_02857_row293_col10\" class=\"data row293 col10\" >0.5171</td>\n",
              "      <td id=\"T_02857_row293_col11\" class=\"data row293 col11\" >0.5354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row294\" class=\"row_heading level0 row294\" >294</th>\n",
              "      <td id=\"T_02857_row294_col0\" class=\"data row294 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row294_col1\" class=\"data row294 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row294_col2\" class=\"data row294 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row294_col3\" class=\"data row294 col3\" >0.4891</td>\n",
              "      <td id=\"T_02857_row294_col4\" class=\"data row294 col4\" >0.4835</td>\n",
              "      <td id=\"T_02857_row294_col5\" class=\"data row294 col5\" >0.4844</td>\n",
              "      <td id=\"T_02857_row294_col6\" class=\"data row294 col6\" >0.5146</td>\n",
              "      <td id=\"T_02857_row294_col7\" class=\"data row294 col7\" >0.5283</td>\n",
              "      <td id=\"T_02857_row294_col8\" class=\"data row294 col8\" >0.5794</td>\n",
              "      <td id=\"T_02857_row294_col9\" class=\"data row294 col9\" >0.5925</td>\n",
              "      <td id=\"T_02857_row294_col10\" class=\"data row294 col10\" >0.5171</td>\n",
              "      <td id=\"T_02857_row294_col11\" class=\"data row294 col11\" >0.5354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row295\" class=\"row_heading level0 row295\" >295</th>\n",
              "      <td id=\"T_02857_row295_col0\" class=\"data row295 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row295_col1\" class=\"data row295 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row295_col2\" class=\"data row295 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row295_col3\" class=\"data row295 col3\" >0.4816</td>\n",
              "      <td id=\"T_02857_row295_col4\" class=\"data row295 col4\" >0.4236</td>\n",
              "      <td id=\"T_02857_row295_col5\" class=\"data row295 col5\" >0.5546</td>\n",
              "      <td id=\"T_02857_row295_col6\" class=\"data row295 col6\" >0.6372</td>\n",
              "      <td id=\"T_02857_row295_col7\" class=\"data row295 col7\" >0.6273</td>\n",
              "      <td id=\"T_02857_row295_col8\" class=\"data row295 col8\" >0.7260</td>\n",
              "      <td id=\"T_02857_row295_col9\" class=\"data row295 col9\" >0.7263</td>\n",
              "      <td id=\"T_02857_row295_col10\" class=\"data row295 col10\" >0.7123</td>\n",
              "      <td id=\"T_02857_row295_col11\" class=\"data row295 col11\" >0.5514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row296\" class=\"row_heading level0 row296\" >296</th>\n",
              "      <td id=\"T_02857_row296_col0\" class=\"data row296 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row296_col1\" class=\"data row296 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row296_col2\" class=\"data row296 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row296_col3\" class=\"data row296 col3\" >0.4816</td>\n",
              "      <td id=\"T_02857_row296_col4\" class=\"data row296 col4\" >0.4236</td>\n",
              "      <td id=\"T_02857_row296_col5\" class=\"data row296 col5\" >0.5546</td>\n",
              "      <td id=\"T_02857_row296_col6\" class=\"data row296 col6\" >0.6372</td>\n",
              "      <td id=\"T_02857_row296_col7\" class=\"data row296 col7\" >0.6273</td>\n",
              "      <td id=\"T_02857_row296_col8\" class=\"data row296 col8\" >0.7260</td>\n",
              "      <td id=\"T_02857_row296_col9\" class=\"data row296 col9\" >0.7263</td>\n",
              "      <td id=\"T_02857_row296_col10\" class=\"data row296 col10\" >0.7123</td>\n",
              "      <td id=\"T_02857_row296_col11\" class=\"data row296 col11\" >0.5514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row297\" class=\"row_heading level0 row297\" >297</th>\n",
              "      <td id=\"T_02857_row297_col0\" class=\"data row297 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row297_col1\" class=\"data row297 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row297_col2\" class=\"data row297 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row297_col3\" class=\"data row297 col3\" >0.4487</td>\n",
              "      <td id=\"T_02857_row297_col4\" class=\"data row297 col4\" >0.4328</td>\n",
              "      <td id=\"T_02857_row297_col5\" class=\"data row297 col5\" >0.4379</td>\n",
              "      <td id=\"T_02857_row297_col6\" class=\"data row297 col6\" >0.4430</td>\n",
              "      <td id=\"T_02857_row297_col7\" class=\"data row297 col7\" >0.4540</td>\n",
              "      <td id=\"T_02857_row297_col8\" class=\"data row297 col8\" >0.4396</td>\n",
              "      <td id=\"T_02857_row297_col9\" class=\"data row297 col9\" >0.5352</td>\n",
              "      <td id=\"T_02857_row297_col10\" class=\"data row297 col10\" >0.4778</td>\n",
              "      <td id=\"T_02857_row297_col11\" class=\"data row297 col11\" >0.4817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row298\" class=\"row_heading level0 row298\" >298</th>\n",
              "      <td id=\"T_02857_row298_col0\" class=\"data row298 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row298_col1\" class=\"data row298 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row298_col2\" class=\"data row298 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row298_col3\" class=\"data row298 col3\" >0.4487</td>\n",
              "      <td id=\"T_02857_row298_col4\" class=\"data row298 col4\" >0.4328</td>\n",
              "      <td id=\"T_02857_row298_col5\" class=\"data row298 col5\" >0.4379</td>\n",
              "      <td id=\"T_02857_row298_col6\" class=\"data row298 col6\" >0.4430</td>\n",
              "      <td id=\"T_02857_row298_col7\" class=\"data row298 col7\" >0.4540</td>\n",
              "      <td id=\"T_02857_row298_col8\" class=\"data row298 col8\" >0.4396</td>\n",
              "      <td id=\"T_02857_row298_col9\" class=\"data row298 col9\" >0.5352</td>\n",
              "      <td id=\"T_02857_row298_col10\" class=\"data row298 col10\" >0.4778</td>\n",
              "      <td id=\"T_02857_row298_col11\" class=\"data row298 col11\" >0.4817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row299\" class=\"row_heading level0 row299\" >299</th>\n",
              "      <td id=\"T_02857_row299_col0\" class=\"data row299 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row299_col1\" class=\"data row299 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row299_col2\" class=\"data row299 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row299_col3\" class=\"data row299 col3\" >0.4730</td>\n",
              "      <td id=\"T_02857_row299_col4\" class=\"data row299 col4\" >0.4478</td>\n",
              "      <td id=\"T_02857_row299_col5\" class=\"data row299 col5\" >0.4767</td>\n",
              "      <td id=\"T_02857_row299_col6\" class=\"data row299 col6\" >0.5973</td>\n",
              "      <td id=\"T_02857_row299_col7\" class=\"data row299 col7\" >0.5810</td>\n",
              "      <td id=\"T_02857_row299_col8\" class=\"data row299 col8\" >0.7417</td>\n",
              "      <td id=\"T_02857_row299_col9\" class=\"data row299 col9\" >0.7443</td>\n",
              "      <td id=\"T_02857_row299_col10\" class=\"data row299 col10\" >0.7012</td>\n",
              "      <td id=\"T_02857_row299_col11\" class=\"data row299 col11\" >0.5171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row300\" class=\"row_heading level0 row300\" >300</th>\n",
              "      <td id=\"T_02857_row300_col0\" class=\"data row300 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row300_col1\" class=\"data row300 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row300_col2\" class=\"data row300 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row300_col3\" class=\"data row300 col3\" >0.4730</td>\n",
              "      <td id=\"T_02857_row300_col4\" class=\"data row300 col4\" >0.4478</td>\n",
              "      <td id=\"T_02857_row300_col5\" class=\"data row300 col5\" >0.4767</td>\n",
              "      <td id=\"T_02857_row300_col6\" class=\"data row300 col6\" >0.5973</td>\n",
              "      <td id=\"T_02857_row300_col7\" class=\"data row300 col7\" >0.5810</td>\n",
              "      <td id=\"T_02857_row300_col8\" class=\"data row300 col8\" >0.7417</td>\n",
              "      <td id=\"T_02857_row300_col9\" class=\"data row300 col9\" >0.7443</td>\n",
              "      <td id=\"T_02857_row300_col10\" class=\"data row300 col10\" >0.7012</td>\n",
              "      <td id=\"T_02857_row300_col11\" class=\"data row300 col11\" >0.5171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row301\" class=\"row_heading level0 row301\" >301</th>\n",
              "      <td id=\"T_02857_row301_col0\" class=\"data row301 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row301_col1\" class=\"data row301 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row301_col2\" class=\"data row301 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row301_col3\" class=\"data row301 col3\" >0.3592</td>\n",
              "      <td id=\"T_02857_row301_col4\" class=\"data row301 col4\" >0.3343</td>\n",
              "      <td id=\"T_02857_row301_col5\" class=\"data row301 col5\" >0.3359</td>\n",
              "      <td id=\"T_02857_row301_col6\" class=\"data row301 col6\" >0.4029</td>\n",
              "      <td id=\"T_02857_row301_col7\" class=\"data row301 col7\" >0.3989</td>\n",
              "      <td id=\"T_02857_row301_col8\" class=\"data row301 col8\" >0.7573</td>\n",
              "      <td id=\"T_02857_row301_col9\" class=\"data row301 col9\" >0.7943</td>\n",
              "      <td id=\"T_02857_row301_col10\" class=\"data row301 col10\" >0.7091</td>\n",
              "      <td id=\"T_02857_row301_col11\" class=\"data row301 col11\" >0.3096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row302\" class=\"row_heading level0 row302\" >302</th>\n",
              "      <td id=\"T_02857_row302_col0\" class=\"data row302 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row302_col1\" class=\"data row302 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row302_col2\" class=\"data row302 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row302_col3\" class=\"data row302 col3\" >0.3592</td>\n",
              "      <td id=\"T_02857_row302_col4\" class=\"data row302 col4\" >0.3343</td>\n",
              "      <td id=\"T_02857_row302_col5\" class=\"data row302 col5\" >0.3359</td>\n",
              "      <td id=\"T_02857_row302_col6\" class=\"data row302 col6\" >0.4029</td>\n",
              "      <td id=\"T_02857_row302_col7\" class=\"data row302 col7\" >0.3989</td>\n",
              "      <td id=\"T_02857_row302_col8\" class=\"data row302 col8\" >0.7573</td>\n",
              "      <td id=\"T_02857_row302_col9\" class=\"data row302 col9\" >0.7943</td>\n",
              "      <td id=\"T_02857_row302_col10\" class=\"data row302 col10\" >0.7091</td>\n",
              "      <td id=\"T_02857_row302_col11\" class=\"data row302 col11\" >0.3096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row303\" class=\"row_heading level0 row303\" >303</th>\n",
              "      <td id=\"T_02857_row303_col0\" class=\"data row303 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row303_col1\" class=\"data row303 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row303_col2\" class=\"data row303 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row303_col3\" class=\"data row303 col3\" >0.4189</td>\n",
              "      <td id=\"T_02857_row303_col4\" class=\"data row303 col4\" >0.4202</td>\n",
              "      <td id=\"T_02857_row303_col5\" class=\"data row303 col5\" >0.4473</td>\n",
              "      <td id=\"T_02857_row303_col6\" class=\"data row303 col6\" >0.4409</td>\n",
              "      <td id=\"T_02857_row303_col7\" class=\"data row303 col7\" >0.4424</td>\n",
              "      <td id=\"T_02857_row303_col8\" class=\"data row303 col8\" >0.4674</td>\n",
              "      <td id=\"T_02857_row303_col9\" class=\"data row303 col9\" >0.4619</td>\n",
              "      <td id=\"T_02857_row303_col10\" class=\"data row303 col10\" >0.4759</td>\n",
              "      <td id=\"T_02857_row303_col11\" class=\"data row303 col11\" >0.4992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row304\" class=\"row_heading level0 row304\" >304</th>\n",
              "      <td id=\"T_02857_row304_col0\" class=\"data row304 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row304_col1\" class=\"data row304 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row304_col2\" class=\"data row304 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row304_col3\" class=\"data row304 col3\" >0.4189</td>\n",
              "      <td id=\"T_02857_row304_col4\" class=\"data row304 col4\" >0.4202</td>\n",
              "      <td id=\"T_02857_row304_col5\" class=\"data row304 col5\" >0.4473</td>\n",
              "      <td id=\"T_02857_row304_col6\" class=\"data row304 col6\" >0.4409</td>\n",
              "      <td id=\"T_02857_row304_col7\" class=\"data row304 col7\" >0.4424</td>\n",
              "      <td id=\"T_02857_row304_col8\" class=\"data row304 col8\" >0.4674</td>\n",
              "      <td id=\"T_02857_row304_col9\" class=\"data row304 col9\" >0.4619</td>\n",
              "      <td id=\"T_02857_row304_col10\" class=\"data row304 col10\" >0.4759</td>\n",
              "      <td id=\"T_02857_row304_col11\" class=\"data row304 col11\" >0.4992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row305\" class=\"row_heading level0 row305\" >305</th>\n",
              "      <td id=\"T_02857_row305_col0\" class=\"data row305 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row305_col1\" class=\"data row305 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row305_col2\" class=\"data row305 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row305_col3\" class=\"data row305 col3\" >0.3950</td>\n",
              "      <td id=\"T_02857_row305_col4\" class=\"data row305 col4\" >0.4222</td>\n",
              "      <td id=\"T_02857_row305_col5\" class=\"data row305 col5\" >0.4902</td>\n",
              "      <td id=\"T_02857_row305_col6\" class=\"data row305 col6\" >0.4634</td>\n",
              "      <td id=\"T_02857_row305_col7\" class=\"data row305 col7\" >0.4765</td>\n",
              "      <td id=\"T_02857_row305_col8\" class=\"data row305 col8\" >0.5827</td>\n",
              "      <td id=\"T_02857_row305_col9\" class=\"data row305 col9\" >0.5932</td>\n",
              "      <td id=\"T_02857_row305_col10\" class=\"data row305 col10\" >0.5526</td>\n",
              "      <td id=\"T_02857_row305_col11\" class=\"data row305 col11\" >0.2888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row306\" class=\"row_heading level0 row306\" >306</th>\n",
              "      <td id=\"T_02857_row306_col0\" class=\"data row306 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row306_col1\" class=\"data row306 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row306_col2\" class=\"data row306 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row306_col3\" class=\"data row306 col3\" >0.3950</td>\n",
              "      <td id=\"T_02857_row306_col4\" class=\"data row306 col4\" >0.4222</td>\n",
              "      <td id=\"T_02857_row306_col5\" class=\"data row306 col5\" >0.4902</td>\n",
              "      <td id=\"T_02857_row306_col6\" class=\"data row306 col6\" >0.4634</td>\n",
              "      <td id=\"T_02857_row306_col7\" class=\"data row306 col7\" >0.4765</td>\n",
              "      <td id=\"T_02857_row306_col8\" class=\"data row306 col8\" >0.5827</td>\n",
              "      <td id=\"T_02857_row306_col9\" class=\"data row306 col9\" >0.5932</td>\n",
              "      <td id=\"T_02857_row306_col10\" class=\"data row306 col10\" >0.5526</td>\n",
              "      <td id=\"T_02857_row306_col11\" class=\"data row306 col11\" >0.2888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row307\" class=\"row_heading level0 row307\" >307</th>\n",
              "      <td id=\"T_02857_row307_col0\" class=\"data row307 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row307_col1\" class=\"data row307 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row307_col2\" class=\"data row307 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row307_col3\" class=\"data row307 col3\" >0.3596</td>\n",
              "      <td id=\"T_02857_row307_col4\" class=\"data row307 col4\" >0.3604</td>\n",
              "      <td id=\"T_02857_row307_col5\" class=\"data row307 col5\" >0.3959</td>\n",
              "      <td id=\"T_02857_row307_col6\" class=\"data row307 col6\" >0.4866</td>\n",
              "      <td id=\"T_02857_row307_col7\" class=\"data row307 col7\" >0.5127</td>\n",
              "      <td id=\"T_02857_row307_col8\" class=\"data row307 col8\" >0.8315</td>\n",
              "      <td id=\"T_02857_row307_col9\" class=\"data row307 col9\" >0.6481</td>\n",
              "      <td id=\"T_02857_row307_col10\" class=\"data row307 col10\" >0.6793</td>\n",
              "      <td id=\"T_02857_row307_col11\" class=\"data row307 col11\" >0.3718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row308\" class=\"row_heading level0 row308\" >308</th>\n",
              "      <td id=\"T_02857_row308_col0\" class=\"data row308 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row308_col1\" class=\"data row308 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row308_col2\" class=\"data row308 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row308_col3\" class=\"data row308 col3\" >0.3596</td>\n",
              "      <td id=\"T_02857_row308_col4\" class=\"data row308 col4\" >0.3604</td>\n",
              "      <td id=\"T_02857_row308_col5\" class=\"data row308 col5\" >0.3959</td>\n",
              "      <td id=\"T_02857_row308_col6\" class=\"data row308 col6\" >0.4866</td>\n",
              "      <td id=\"T_02857_row308_col7\" class=\"data row308 col7\" >0.5127</td>\n",
              "      <td id=\"T_02857_row308_col8\" class=\"data row308 col8\" >0.8315</td>\n",
              "      <td id=\"T_02857_row308_col9\" class=\"data row308 col9\" >0.6481</td>\n",
              "      <td id=\"T_02857_row308_col10\" class=\"data row308 col10\" >0.6793</td>\n",
              "      <td id=\"T_02857_row308_col11\" class=\"data row308 col11\" >0.3718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row309\" class=\"row_heading level0 row309\" >309</th>\n",
              "      <td id=\"T_02857_row309_col0\" class=\"data row309 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row309_col1\" class=\"data row309 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row309_col2\" class=\"data row309 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row309_col3\" class=\"data row309 col3\" >0.3375</td>\n",
              "      <td id=\"T_02857_row309_col4\" class=\"data row309 col4\" >0.3398</td>\n",
              "      <td id=\"T_02857_row309_col5\" class=\"data row309 col5\" >0.4262</td>\n",
              "      <td id=\"T_02857_row309_col6\" class=\"data row309 col6\" >0.4807</td>\n",
              "      <td id=\"T_02857_row309_col7\" class=\"data row309 col7\" >0.5127</td>\n",
              "      <td id=\"T_02857_row309_col8\" class=\"data row309 col8\" >0.8663</td>\n",
              "      <td id=\"T_02857_row309_col9\" class=\"data row309 col9\" >0.4615</td>\n",
              "      <td id=\"T_02857_row309_col10\" class=\"data row309 col10\" >0.7289</td>\n",
              "      <td id=\"T_02857_row309_col11\" class=\"data row309 col11\" >0.3085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row310\" class=\"row_heading level0 row310\" >310</th>\n",
              "      <td id=\"T_02857_row310_col0\" class=\"data row310 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row310_col1\" class=\"data row310 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row310_col2\" class=\"data row310 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row310_col3\" class=\"data row310 col3\" >0.3375</td>\n",
              "      <td id=\"T_02857_row310_col4\" class=\"data row310 col4\" >0.3398</td>\n",
              "      <td id=\"T_02857_row310_col5\" class=\"data row310 col5\" >0.4262</td>\n",
              "      <td id=\"T_02857_row310_col6\" class=\"data row310 col6\" >0.4807</td>\n",
              "      <td id=\"T_02857_row310_col7\" class=\"data row310 col7\" >0.5127</td>\n",
              "      <td id=\"T_02857_row310_col8\" class=\"data row310 col8\" >0.8663</td>\n",
              "      <td id=\"T_02857_row310_col9\" class=\"data row310 col9\" >0.4615</td>\n",
              "      <td id=\"T_02857_row310_col10\" class=\"data row310 col10\" >0.7289</td>\n",
              "      <td id=\"T_02857_row310_col11\" class=\"data row310 col11\" >0.3085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row311\" class=\"row_heading level0 row311\" >311</th>\n",
              "      <td id=\"T_02857_row311_col0\" class=\"data row311 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row311_col1\" class=\"data row311 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row311_col2\" class=\"data row311 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row311_col3\" class=\"data row311 col3\" >0.4359</td>\n",
              "      <td id=\"T_02857_row311_col4\" class=\"data row311 col4\" >0.4209</td>\n",
              "      <td id=\"T_02857_row311_col5\" class=\"data row311 col5\" >0.4435</td>\n",
              "      <td id=\"T_02857_row311_col6\" class=\"data row311 col6\" >0.4294</td>\n",
              "      <td id=\"T_02857_row311_col7\" class=\"data row311 col7\" >0.4382</td>\n",
              "      <td id=\"T_02857_row311_col8\" class=\"data row311 col8\" >0.4148</td>\n",
              "      <td id=\"T_02857_row311_col9\" class=\"data row311 col9\" >0.4511</td>\n",
              "      <td id=\"T_02857_row311_col10\" class=\"data row311 col10\" >0.4890</td>\n",
              "      <td id=\"T_02857_row311_col11\" class=\"data row311 col11\" >0.4859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row312\" class=\"row_heading level0 row312\" >312</th>\n",
              "      <td id=\"T_02857_row312_col0\" class=\"data row312 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row312_col1\" class=\"data row312 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row312_col2\" class=\"data row312 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row312_col3\" class=\"data row312 col3\" >0.4359</td>\n",
              "      <td id=\"T_02857_row312_col4\" class=\"data row312 col4\" >0.4209</td>\n",
              "      <td id=\"T_02857_row312_col5\" class=\"data row312 col5\" >0.4435</td>\n",
              "      <td id=\"T_02857_row312_col6\" class=\"data row312 col6\" >0.4294</td>\n",
              "      <td id=\"T_02857_row312_col7\" class=\"data row312 col7\" >0.4382</td>\n",
              "      <td id=\"T_02857_row312_col8\" class=\"data row312 col8\" >0.4148</td>\n",
              "      <td id=\"T_02857_row312_col9\" class=\"data row312 col9\" >0.4511</td>\n",
              "      <td id=\"T_02857_row312_col10\" class=\"data row312 col10\" >0.4890</td>\n",
              "      <td id=\"T_02857_row312_col11\" class=\"data row312 col11\" >0.4859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row313\" class=\"row_heading level0 row313\" >313</th>\n",
              "      <td id=\"T_02857_row313_col0\" class=\"data row313 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row313_col1\" class=\"data row313 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row313_col2\" class=\"data row313 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row313_col3\" class=\"data row313 col3\" >0.3320</td>\n",
              "      <td id=\"T_02857_row313_col4\" class=\"data row313 col4\" >0.3304</td>\n",
              "      <td id=\"T_02857_row313_col5\" class=\"data row313 col5\" >0.3934</td>\n",
              "      <td id=\"T_02857_row313_col6\" class=\"data row313 col6\" >0.4554</td>\n",
              "      <td id=\"T_02857_row313_col7\" class=\"data row313 col7\" >0.5074</td>\n",
              "      <td id=\"T_02857_row313_col8\" class=\"data row313 col8\" >0.9364</td>\n",
              "      <td id=\"T_02857_row313_col9\" class=\"data row313 col9\" >0.6092</td>\n",
              "      <td id=\"T_02857_row313_col10\" class=\"data row313 col10\" >0.7948</td>\n",
              "      <td id=\"T_02857_row313_col11\" class=\"data row313 col11\" >0.3404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row314\" class=\"row_heading level0 row314\" >314</th>\n",
              "      <td id=\"T_02857_row314_col0\" class=\"data row314 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row314_col1\" class=\"data row314 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row314_col2\" class=\"data row314 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row314_col3\" class=\"data row314 col3\" >0.3320</td>\n",
              "      <td id=\"T_02857_row314_col4\" class=\"data row314 col4\" >0.3304</td>\n",
              "      <td id=\"T_02857_row314_col5\" class=\"data row314 col5\" >0.3934</td>\n",
              "      <td id=\"T_02857_row314_col6\" class=\"data row314 col6\" >0.4554</td>\n",
              "      <td id=\"T_02857_row314_col7\" class=\"data row314 col7\" >0.5074</td>\n",
              "      <td id=\"T_02857_row314_col8\" class=\"data row314 col8\" >0.9364</td>\n",
              "      <td id=\"T_02857_row314_col9\" class=\"data row314 col9\" >0.6092</td>\n",
              "      <td id=\"T_02857_row314_col10\" class=\"data row314 col10\" >0.7948</td>\n",
              "      <td id=\"T_02857_row314_col11\" class=\"data row314 col11\" >0.3404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row315\" class=\"row_heading level0 row315\" >315</th>\n",
              "      <td id=\"T_02857_row315_col0\" class=\"data row315 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row315_col1\" class=\"data row315 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row315_col2\" class=\"data row315 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row315_col3\" class=\"data row315 col3\" >0.3327</td>\n",
              "      <td id=\"T_02857_row315_col4\" class=\"data row315 col4\" >0.3321</td>\n",
              "      <td id=\"T_02857_row315_col5\" class=\"data row315 col5\" >0.3321</td>\n",
              "      <td id=\"T_02857_row315_col6\" class=\"data row315 col6\" >0.3372</td>\n",
              "      <td id=\"T_02857_row315_col7\" class=\"data row315 col7\" >0.3456</td>\n",
              "      <td id=\"T_02857_row315_col8\" class=\"data row315 col8\" >0.7076</td>\n",
              "      <td id=\"T_02857_row315_col9\" class=\"data row315 col9\" >0.3776</td>\n",
              "      <td id=\"T_02857_row315_col10\" class=\"data row315 col10\" >0.4305</td>\n",
              "      <td id=\"T_02857_row315_col11\" class=\"data row315 col11\" >0.1766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row316\" class=\"row_heading level0 row316\" >316</th>\n",
              "      <td id=\"T_02857_row316_col0\" class=\"data row316 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row316_col1\" class=\"data row316 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row316_col2\" class=\"data row316 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row316_col3\" class=\"data row316 col3\" >0.3327</td>\n",
              "      <td id=\"T_02857_row316_col4\" class=\"data row316 col4\" >0.3321</td>\n",
              "      <td id=\"T_02857_row316_col5\" class=\"data row316 col5\" >0.3321</td>\n",
              "      <td id=\"T_02857_row316_col6\" class=\"data row316 col6\" >0.3372</td>\n",
              "      <td id=\"T_02857_row316_col7\" class=\"data row316 col7\" >0.3456</td>\n",
              "      <td id=\"T_02857_row316_col8\" class=\"data row316 col8\" >0.7076</td>\n",
              "      <td id=\"T_02857_row316_col9\" class=\"data row316 col9\" >0.3776</td>\n",
              "      <td id=\"T_02857_row316_col10\" class=\"data row316 col10\" >0.4305</td>\n",
              "      <td id=\"T_02857_row316_col11\" class=\"data row316 col11\" >0.1766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row317\" class=\"row_heading level0 row317\" >317</th>\n",
              "      <td id=\"T_02857_row317_col0\" class=\"data row317 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row317_col1\" class=\"data row317 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row317_col2\" class=\"data row317 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row317_col3\" class=\"data row317 col3\" >0.3585</td>\n",
              "      <td id=\"T_02857_row317_col4\" class=\"data row317 col4\" >0.4353</td>\n",
              "      <td id=\"T_02857_row317_col5\" class=\"data row317 col5\" >0.5100</td>\n",
              "      <td id=\"T_02857_row317_col6\" class=\"data row317 col6\" >0.4552</td>\n",
              "      <td id=\"T_02857_row317_col7\" class=\"data row317 col7\" >0.5607</td>\n",
              "      <td id=\"T_02857_row317_col8\" class=\"data row317 col8\" >0.5909</td>\n",
              "      <td id=\"T_02857_row317_col9\" class=\"data row317 col9\" >0.5110</td>\n",
              "      <td id=\"T_02857_row317_col10\" class=\"data row317 col10\" >0.5510</td>\n",
              "      <td id=\"T_02857_row317_col11\" class=\"data row317 col11\" >0.4334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row318\" class=\"row_heading level0 row318\" >318</th>\n",
              "      <td id=\"T_02857_row318_col0\" class=\"data row318 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row318_col1\" class=\"data row318 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row318_col2\" class=\"data row318 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row318_col3\" class=\"data row318 col3\" >0.3585</td>\n",
              "      <td id=\"T_02857_row318_col4\" class=\"data row318 col4\" >0.4353</td>\n",
              "      <td id=\"T_02857_row318_col5\" class=\"data row318 col5\" >0.5100</td>\n",
              "      <td id=\"T_02857_row318_col6\" class=\"data row318 col6\" >0.4552</td>\n",
              "      <td id=\"T_02857_row318_col7\" class=\"data row318 col7\" >0.5607</td>\n",
              "      <td id=\"T_02857_row318_col8\" class=\"data row318 col8\" >0.5909</td>\n",
              "      <td id=\"T_02857_row318_col9\" class=\"data row318 col9\" >0.5110</td>\n",
              "      <td id=\"T_02857_row318_col10\" class=\"data row318 col10\" >0.5510</td>\n",
              "      <td id=\"T_02857_row318_col11\" class=\"data row318 col11\" >0.4334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row319\" class=\"row_heading level0 row319\" >319</th>\n",
              "      <td id=\"T_02857_row319_col0\" class=\"data row319 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row319_col1\" class=\"data row319 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row319_col2\" class=\"data row319 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row319_col3\" class=\"data row319 col3\" >0.3737</td>\n",
              "      <td id=\"T_02857_row319_col4\" class=\"data row319 col4\" >0.3922</td>\n",
              "      <td id=\"T_02857_row319_col5\" class=\"data row319 col5\" >0.4438</td>\n",
              "      <td id=\"T_02857_row319_col6\" class=\"data row319 col6\" >0.4279</td>\n",
              "      <td id=\"T_02857_row319_col7\" class=\"data row319 col7\" >0.4431</td>\n",
              "      <td id=\"T_02857_row319_col8\" class=\"data row319 col8\" >0.5983</td>\n",
              "      <td id=\"T_02857_row319_col9\" class=\"data row319 col9\" >0.5229</td>\n",
              "      <td id=\"T_02857_row319_col10\" class=\"data row319 col10\" >0.4935</td>\n",
              "      <td id=\"T_02857_row319_col11\" class=\"data row319 col11\" >0.2366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row320\" class=\"row_heading level0 row320\" >320</th>\n",
              "      <td id=\"T_02857_row320_col0\" class=\"data row320 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row320_col1\" class=\"data row320 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row320_col2\" class=\"data row320 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row320_col3\" class=\"data row320 col3\" >0.3737</td>\n",
              "      <td id=\"T_02857_row320_col4\" class=\"data row320 col4\" >0.3922</td>\n",
              "      <td id=\"T_02857_row320_col5\" class=\"data row320 col5\" >0.4438</td>\n",
              "      <td id=\"T_02857_row320_col6\" class=\"data row320 col6\" >0.4279</td>\n",
              "      <td id=\"T_02857_row320_col7\" class=\"data row320 col7\" >0.4431</td>\n",
              "      <td id=\"T_02857_row320_col8\" class=\"data row320 col8\" >0.5983</td>\n",
              "      <td id=\"T_02857_row320_col9\" class=\"data row320 col9\" >0.5229</td>\n",
              "      <td id=\"T_02857_row320_col10\" class=\"data row320 col10\" >0.4935</td>\n",
              "      <td id=\"T_02857_row320_col11\" class=\"data row320 col11\" >0.2366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row321\" class=\"row_heading level0 row321\" >321</th>\n",
              "      <td id=\"T_02857_row321_col0\" class=\"data row321 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row321_col1\" class=\"data row321 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row321_col2\" class=\"data row321 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row321_col3\" class=\"data row321 col3\" >0.3381</td>\n",
              "      <td id=\"T_02857_row321_col4\" class=\"data row321 col4\" >0.3257</td>\n",
              "      <td id=\"T_02857_row321_col5\" class=\"data row321 col5\" >0.3341</td>\n",
              "      <td id=\"T_02857_row321_col6\" class=\"data row321 col6\" >0.3652</td>\n",
              "      <td id=\"T_02857_row321_col7\" class=\"data row321 col7\" >0.3707</td>\n",
              "      <td id=\"T_02857_row321_col8\" class=\"data row321 col8\" >0.7054</td>\n",
              "      <td id=\"T_02857_row321_col9\" class=\"data row321 col9\" >0.4328</td>\n",
              "      <td id=\"T_02857_row321_col10\" class=\"data row321 col10\" >0.4766</td>\n",
              "      <td id=\"T_02857_row321_col11\" class=\"data row321 col11\" >0.2031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row322\" class=\"row_heading level0 row322\" >322</th>\n",
              "      <td id=\"T_02857_row322_col0\" class=\"data row322 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row322_col1\" class=\"data row322 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row322_col2\" class=\"data row322 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row322_col3\" class=\"data row322 col3\" >0.3381</td>\n",
              "      <td id=\"T_02857_row322_col4\" class=\"data row322 col4\" >0.3257</td>\n",
              "      <td id=\"T_02857_row322_col5\" class=\"data row322 col5\" >0.3341</td>\n",
              "      <td id=\"T_02857_row322_col6\" class=\"data row322 col6\" >0.3652</td>\n",
              "      <td id=\"T_02857_row322_col7\" class=\"data row322 col7\" >0.3707</td>\n",
              "      <td id=\"T_02857_row322_col8\" class=\"data row322 col8\" >0.7054</td>\n",
              "      <td id=\"T_02857_row322_col9\" class=\"data row322 col9\" >0.4328</td>\n",
              "      <td id=\"T_02857_row322_col10\" class=\"data row322 col10\" >0.4766</td>\n",
              "      <td id=\"T_02857_row322_col11\" class=\"data row322 col11\" >0.2031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row323\" class=\"row_heading level0 row323\" >323</th>\n",
              "      <td id=\"T_02857_row323_col0\" class=\"data row323 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row323_col1\" class=\"data row323 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row323_col2\" class=\"data row323 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row323_col3\" class=\"data row323 col3\" >0.3371</td>\n",
              "      <td id=\"T_02857_row323_col4\" class=\"data row323 col4\" >0.3401</td>\n",
              "      <td id=\"T_02857_row323_col5\" class=\"data row323 col5\" >0.3482</td>\n",
              "      <td id=\"T_02857_row323_col6\" class=\"data row323 col6\" >0.3843</td>\n",
              "      <td id=\"T_02857_row323_col7\" class=\"data row323 col7\" >0.4485</td>\n",
              "      <td id=\"T_02857_row323_col8\" class=\"data row323 col8\" >0.7068</td>\n",
              "      <td id=\"T_02857_row323_col9\" class=\"data row323 col9\" >0.3611</td>\n",
              "      <td id=\"T_02857_row323_col10\" class=\"data row323 col10\" >0.5402</td>\n",
              "      <td id=\"T_02857_row323_col11\" class=\"data row323 col11\" >0.2098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row324\" class=\"row_heading level0 row324\" >324</th>\n",
              "      <td id=\"T_02857_row324_col0\" class=\"data row324 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row324_col1\" class=\"data row324 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row324_col2\" class=\"data row324 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row324_col3\" class=\"data row324 col3\" >0.3371</td>\n",
              "      <td id=\"T_02857_row324_col4\" class=\"data row324 col4\" >0.3401</td>\n",
              "      <td id=\"T_02857_row324_col5\" class=\"data row324 col5\" >0.3482</td>\n",
              "      <td id=\"T_02857_row324_col6\" class=\"data row324 col6\" >0.3843</td>\n",
              "      <td id=\"T_02857_row324_col7\" class=\"data row324 col7\" >0.4485</td>\n",
              "      <td id=\"T_02857_row324_col8\" class=\"data row324 col8\" >0.7068</td>\n",
              "      <td id=\"T_02857_row324_col9\" class=\"data row324 col9\" >0.3611</td>\n",
              "      <td id=\"T_02857_row324_col10\" class=\"data row324 col10\" >0.5402</td>\n",
              "      <td id=\"T_02857_row324_col11\" class=\"data row324 col11\" >0.2098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row325\" class=\"row_heading level0 row325\" >325</th>\n",
              "      <td id=\"T_02857_row325_col0\" class=\"data row325 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row325_col1\" class=\"data row325 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row325_col2\" class=\"data row325 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row325_col3\" class=\"data row325 col3\" >0.4789</td>\n",
              "      <td id=\"T_02857_row325_col4\" class=\"data row325 col4\" >0.4835</td>\n",
              "      <td id=\"T_02857_row325_col5\" class=\"data row325 col5\" >0.5199</td>\n",
              "      <td id=\"T_02857_row325_col6\" class=\"data row325 col6\" >0.4926</td>\n",
              "      <td id=\"T_02857_row325_col7\" class=\"data row325 col7\" >0.4998</td>\n",
              "      <td id=\"T_02857_row325_col8\" class=\"data row325 col8\" >0.5197</td>\n",
              "      <td id=\"T_02857_row325_col9\" class=\"data row325 col9\" >0.4773</td>\n",
              "      <td id=\"T_02857_row325_col10\" class=\"data row325 col10\" >0.5193</td>\n",
              "      <td id=\"T_02857_row325_col11\" class=\"data row325 col11\" >0.5213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row326\" class=\"row_heading level0 row326\" >326</th>\n",
              "      <td id=\"T_02857_row326_col0\" class=\"data row326 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row326_col1\" class=\"data row326 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row326_col2\" class=\"data row326 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row326_col3\" class=\"data row326 col3\" >0.4789</td>\n",
              "      <td id=\"T_02857_row326_col4\" class=\"data row326 col4\" >0.4835</td>\n",
              "      <td id=\"T_02857_row326_col5\" class=\"data row326 col5\" >0.5199</td>\n",
              "      <td id=\"T_02857_row326_col6\" class=\"data row326 col6\" >0.4926</td>\n",
              "      <td id=\"T_02857_row326_col7\" class=\"data row326 col7\" >0.4998</td>\n",
              "      <td id=\"T_02857_row326_col8\" class=\"data row326 col8\" >0.5197</td>\n",
              "      <td id=\"T_02857_row326_col9\" class=\"data row326 col9\" >0.4773</td>\n",
              "      <td id=\"T_02857_row326_col10\" class=\"data row326 col10\" >0.5193</td>\n",
              "      <td id=\"T_02857_row326_col11\" class=\"data row326 col11\" >0.5213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row327\" class=\"row_heading level0 row327\" >327</th>\n",
              "      <td id=\"T_02857_row327_col0\" class=\"data row327 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row327_col1\" class=\"data row327 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row327_col2\" class=\"data row327 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row327_col3\" class=\"data row327 col3\" >0.3312</td>\n",
              "      <td id=\"T_02857_row327_col4\" class=\"data row327 col4\" >0.3323</td>\n",
              "      <td id=\"T_02857_row327_col5\" class=\"data row327 col5\" >0.3360</td>\n",
              "      <td id=\"T_02857_row327_col6\" class=\"data row327 col6\" >0.3627</td>\n",
              "      <td id=\"T_02857_row327_col7\" class=\"data row327 col7\" >0.4210</td>\n",
              "      <td id=\"T_02857_row327_col8\" class=\"data row327 col8\" >0.7996</td>\n",
              "      <td id=\"T_02857_row327_col9\" class=\"data row327 col9\" >0.3791</td>\n",
              "      <td id=\"T_02857_row327_col10\" class=\"data row327 col10\" >0.5594</td>\n",
              "      <td id=\"T_02857_row327_col11\" class=\"data row327 col11\" >0.2230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row328\" class=\"row_heading level0 row328\" >328</th>\n",
              "      <td id=\"T_02857_row328_col0\" class=\"data row328 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row328_col1\" class=\"data row328 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row328_col2\" class=\"data row328 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row328_col3\" class=\"data row328 col3\" >0.3312</td>\n",
              "      <td id=\"T_02857_row328_col4\" class=\"data row328 col4\" >0.3323</td>\n",
              "      <td id=\"T_02857_row328_col5\" class=\"data row328 col5\" >0.3360</td>\n",
              "      <td id=\"T_02857_row328_col6\" class=\"data row328 col6\" >0.3627</td>\n",
              "      <td id=\"T_02857_row328_col7\" class=\"data row328 col7\" >0.4210</td>\n",
              "      <td id=\"T_02857_row328_col8\" class=\"data row328 col8\" >0.7996</td>\n",
              "      <td id=\"T_02857_row328_col9\" class=\"data row328 col9\" >0.3791</td>\n",
              "      <td id=\"T_02857_row328_col10\" class=\"data row328 col10\" >0.5594</td>\n",
              "      <td id=\"T_02857_row328_col11\" class=\"data row328 col11\" >0.2230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row329\" class=\"row_heading level0 row329\" >329</th>\n",
              "      <td id=\"T_02857_row329_col0\" class=\"data row329 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row329_col1\" class=\"data row329 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row329_col2\" class=\"data row329 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row329_col3\" class=\"data row329 col3\" >0.6553</td>\n",
              "      <td id=\"T_02857_row329_col4\" class=\"data row329 col4\" >0.7940</td>\n",
              "      <td id=\"T_02857_row329_col5\" class=\"data row329 col5\" >0.7808</td>\n",
              "      <td id=\"T_02857_row329_col6\" class=\"data row329 col6\" >0.6841</td>\n",
              "      <td id=\"T_02857_row329_col7\" class=\"data row329 col7\" >0.7189</td>\n",
              "      <td id=\"T_02857_row329_col8\" class=\"data row329 col8\" >0.3753</td>\n",
              "      <td id=\"T_02857_row329_col9\" class=\"data row329 col9\" >0.3456</td>\n",
              "      <td id=\"T_02857_row329_col10\" class=\"data row329 col10\" >0.3798</td>\n",
              "      <td id=\"T_02857_row329_col11\" class=\"data row329 col11\" >0.3977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row330\" class=\"row_heading level0 row330\" >330</th>\n",
              "      <td id=\"T_02857_row330_col0\" class=\"data row330 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row330_col1\" class=\"data row330 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row330_col2\" class=\"data row330 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row330_col3\" class=\"data row330 col3\" >0.6553</td>\n",
              "      <td id=\"T_02857_row330_col4\" class=\"data row330 col4\" >0.7940</td>\n",
              "      <td id=\"T_02857_row330_col5\" class=\"data row330 col5\" >0.7808</td>\n",
              "      <td id=\"T_02857_row330_col6\" class=\"data row330 col6\" >0.6841</td>\n",
              "      <td id=\"T_02857_row330_col7\" class=\"data row330 col7\" >0.7189</td>\n",
              "      <td id=\"T_02857_row330_col8\" class=\"data row330 col8\" >0.3753</td>\n",
              "      <td id=\"T_02857_row330_col9\" class=\"data row330 col9\" >0.3456</td>\n",
              "      <td id=\"T_02857_row330_col10\" class=\"data row330 col10\" >0.3798</td>\n",
              "      <td id=\"T_02857_row330_col11\" class=\"data row330 col11\" >0.3977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row331\" class=\"row_heading level0 row331\" >331</th>\n",
              "      <td id=\"T_02857_row331_col0\" class=\"data row331 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row331_col1\" class=\"data row331 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row331_col2\" class=\"data row331 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row331_col3\" class=\"data row331 col3\" >0.3642</td>\n",
              "      <td id=\"T_02857_row331_col4\" class=\"data row331 col4\" >0.3887</td>\n",
              "      <td id=\"T_02857_row331_col5\" class=\"data row331 col5\" >0.4372</td>\n",
              "      <td id=\"T_02857_row331_col6\" class=\"data row331 col6\" >0.4131</td>\n",
              "      <td id=\"T_02857_row331_col7\" class=\"data row331 col7\" >0.4289</td>\n",
              "      <td id=\"T_02857_row331_col8\" class=\"data row331 col8\" >0.4553</td>\n",
              "      <td id=\"T_02857_row331_col9\" class=\"data row331 col9\" >0.4951</td>\n",
              "      <td id=\"T_02857_row331_col10\" class=\"data row331 col10\" >0.4518</td>\n",
              "      <td id=\"T_02857_row331_col11\" class=\"data row331 col11\" >0.2334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row332\" class=\"row_heading level0 row332\" >332</th>\n",
              "      <td id=\"T_02857_row332_col0\" class=\"data row332 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row332_col1\" class=\"data row332 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row332_col2\" class=\"data row332 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row332_col3\" class=\"data row332 col3\" >0.3642</td>\n",
              "      <td id=\"T_02857_row332_col4\" class=\"data row332 col4\" >0.3887</td>\n",
              "      <td id=\"T_02857_row332_col5\" class=\"data row332 col5\" >0.4372</td>\n",
              "      <td id=\"T_02857_row332_col6\" class=\"data row332 col6\" >0.4131</td>\n",
              "      <td id=\"T_02857_row332_col7\" class=\"data row332 col7\" >0.4289</td>\n",
              "      <td id=\"T_02857_row332_col8\" class=\"data row332 col8\" >0.4553</td>\n",
              "      <td id=\"T_02857_row332_col9\" class=\"data row332 col9\" >0.4951</td>\n",
              "      <td id=\"T_02857_row332_col10\" class=\"data row332 col10\" >0.4518</td>\n",
              "      <td id=\"T_02857_row332_col11\" class=\"data row332 col11\" >0.2334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row333\" class=\"row_heading level0 row333\" >333</th>\n",
              "      <td id=\"T_02857_row333_col0\" class=\"data row333 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row333_col1\" class=\"data row333 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row333_col2\" class=\"data row333 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row333_col3\" class=\"data row333 col3\" >0.3753</td>\n",
              "      <td id=\"T_02857_row333_col4\" class=\"data row333 col4\" >0.3874</td>\n",
              "      <td id=\"T_02857_row333_col5\" class=\"data row333 col5\" >0.5147</td>\n",
              "      <td id=\"T_02857_row333_col6\" class=\"data row333 col6\" >0.4148</td>\n",
              "      <td id=\"T_02857_row333_col7\" class=\"data row333 col7\" >0.4490</td>\n",
              "      <td id=\"T_02857_row333_col8\" class=\"data row333 col8\" >0.5071</td>\n",
              "      <td id=\"T_02857_row333_col9\" class=\"data row333 col9\" >0.4860</td>\n",
              "      <td id=\"T_02857_row333_col10\" class=\"data row333 col10\" >0.4456</td>\n",
              "      <td id=\"T_02857_row333_col11\" class=\"data row333 col11\" >0.2232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row334\" class=\"row_heading level0 row334\" >334</th>\n",
              "      <td id=\"T_02857_row334_col0\" class=\"data row334 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row334_col1\" class=\"data row334 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row334_col2\" class=\"data row334 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row334_col3\" class=\"data row334 col3\" >0.3753</td>\n",
              "      <td id=\"T_02857_row334_col4\" class=\"data row334 col4\" >0.3874</td>\n",
              "      <td id=\"T_02857_row334_col5\" class=\"data row334 col5\" >0.5147</td>\n",
              "      <td id=\"T_02857_row334_col6\" class=\"data row334 col6\" >0.4148</td>\n",
              "      <td id=\"T_02857_row334_col7\" class=\"data row334 col7\" >0.4490</td>\n",
              "      <td id=\"T_02857_row334_col8\" class=\"data row334 col8\" >0.5071</td>\n",
              "      <td id=\"T_02857_row334_col9\" class=\"data row334 col9\" >0.4860</td>\n",
              "      <td id=\"T_02857_row334_col10\" class=\"data row334 col10\" >0.4456</td>\n",
              "      <td id=\"T_02857_row334_col11\" class=\"data row334 col11\" >0.2232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row335\" class=\"row_heading level0 row335\" >335</th>\n",
              "      <td id=\"T_02857_row335_col0\" class=\"data row335 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row335_col1\" class=\"data row335 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row335_col2\" class=\"data row335 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row335_col3\" class=\"data row335 col3\" >0.6571</td>\n",
              "      <td id=\"T_02857_row335_col4\" class=\"data row335 col4\" >0.7322</td>\n",
              "      <td id=\"T_02857_row335_col5\" class=\"data row335 col5\" >0.7682</td>\n",
              "      <td id=\"T_02857_row335_col6\" class=\"data row335 col6\" >0.6993</td>\n",
              "      <td id=\"T_02857_row335_col7\" class=\"data row335 col7\" >0.7275</td>\n",
              "      <td id=\"T_02857_row335_col8\" class=\"data row335 col8\" >0.3919</td>\n",
              "      <td id=\"T_02857_row335_col9\" class=\"data row335 col9\" >0.3722</td>\n",
              "      <td id=\"T_02857_row335_col10\" class=\"data row335 col10\" >0.4151</td>\n",
              "      <td id=\"T_02857_row335_col11\" class=\"data row335 col11\" >0.3881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row336\" class=\"row_heading level0 row336\" >336</th>\n",
              "      <td id=\"T_02857_row336_col0\" class=\"data row336 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row336_col1\" class=\"data row336 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row336_col2\" class=\"data row336 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row336_col3\" class=\"data row336 col3\" >0.6571</td>\n",
              "      <td id=\"T_02857_row336_col4\" class=\"data row336 col4\" >0.7322</td>\n",
              "      <td id=\"T_02857_row336_col5\" class=\"data row336 col5\" >0.7682</td>\n",
              "      <td id=\"T_02857_row336_col6\" class=\"data row336 col6\" >0.6993</td>\n",
              "      <td id=\"T_02857_row336_col7\" class=\"data row336 col7\" >0.7275</td>\n",
              "      <td id=\"T_02857_row336_col8\" class=\"data row336 col8\" >0.3919</td>\n",
              "      <td id=\"T_02857_row336_col9\" class=\"data row336 col9\" >0.3722</td>\n",
              "      <td id=\"T_02857_row336_col10\" class=\"data row336 col10\" >0.4151</td>\n",
              "      <td id=\"T_02857_row336_col11\" class=\"data row336 col11\" >0.3881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row337\" class=\"row_heading level0 row337\" >337</th>\n",
              "      <td id=\"T_02857_row337_col0\" class=\"data row337 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row337_col1\" class=\"data row337 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row337_col2\" class=\"data row337 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row337_col3\" class=\"data row337 col3\" >0.4342</td>\n",
              "      <td id=\"T_02857_row337_col4\" class=\"data row337 col4\" >0.5051</td>\n",
              "      <td id=\"T_02857_row337_col5\" class=\"data row337 col5\" >0.6726</td>\n",
              "      <td id=\"T_02857_row337_col6\" class=\"data row337 col6\" >0.6230</td>\n",
              "      <td id=\"T_02857_row337_col7\" class=\"data row337 col7\" >0.7695</td>\n",
              "      <td id=\"T_02857_row337_col8\" class=\"data row337 col8\" >0.6648</td>\n",
              "      <td id=\"T_02857_row337_col9\" class=\"data row337 col9\" >0.3618</td>\n",
              "      <td id=\"T_02857_row337_col10\" class=\"data row337 col10\" >0.6033</td>\n",
              "      <td id=\"T_02857_row337_col11\" class=\"data row337 col11\" >0.3542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row338\" class=\"row_heading level0 row338\" >338</th>\n",
              "      <td id=\"T_02857_row338_col0\" class=\"data row338 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row338_col1\" class=\"data row338 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row338_col2\" class=\"data row338 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row338_col3\" class=\"data row338 col3\" >0.4342</td>\n",
              "      <td id=\"T_02857_row338_col4\" class=\"data row338 col4\" >0.5051</td>\n",
              "      <td id=\"T_02857_row338_col5\" class=\"data row338 col5\" >0.6726</td>\n",
              "      <td id=\"T_02857_row338_col6\" class=\"data row338 col6\" >0.6230</td>\n",
              "      <td id=\"T_02857_row338_col7\" class=\"data row338 col7\" >0.7695</td>\n",
              "      <td id=\"T_02857_row338_col8\" class=\"data row338 col8\" >0.6648</td>\n",
              "      <td id=\"T_02857_row338_col9\" class=\"data row338 col9\" >0.3618</td>\n",
              "      <td id=\"T_02857_row338_col10\" class=\"data row338 col10\" >0.6033</td>\n",
              "      <td id=\"T_02857_row338_col11\" class=\"data row338 col11\" >0.3542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row339\" class=\"row_heading level0 row339\" >339</th>\n",
              "      <td id=\"T_02857_row339_col0\" class=\"data row339 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row339_col1\" class=\"data row339 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row339_col2\" class=\"data row339 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row339_col3\" class=\"data row339 col3\" >0.4907</td>\n",
              "      <td id=\"T_02857_row339_col4\" class=\"data row339 col4\" >0.4846</td>\n",
              "      <td id=\"T_02857_row339_col5\" class=\"data row339 col5\" >0.5439</td>\n",
              "      <td id=\"T_02857_row339_col6\" class=\"data row339 col6\" >0.4830</td>\n",
              "      <td id=\"T_02857_row339_col7\" class=\"data row339 col7\" >0.5025</td>\n",
              "      <td id=\"T_02857_row339_col8\" class=\"data row339 col8\" >0.3512</td>\n",
              "      <td id=\"T_02857_row339_col9\" class=\"data row339 col9\" >0.4156</td>\n",
              "      <td id=\"T_02857_row339_col10\" class=\"data row339 col10\" >0.4845</td>\n",
              "      <td id=\"T_02857_row339_col11\" class=\"data row339 col11\" >0.4631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row340\" class=\"row_heading level0 row340\" >340</th>\n",
              "      <td id=\"T_02857_row340_col0\" class=\"data row340 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row340_col1\" class=\"data row340 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row340_col2\" class=\"data row340 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row340_col3\" class=\"data row340 col3\" >0.4907</td>\n",
              "      <td id=\"T_02857_row340_col4\" class=\"data row340 col4\" >0.4846</td>\n",
              "      <td id=\"T_02857_row340_col5\" class=\"data row340 col5\" >0.5439</td>\n",
              "      <td id=\"T_02857_row340_col6\" class=\"data row340 col6\" >0.4830</td>\n",
              "      <td id=\"T_02857_row340_col7\" class=\"data row340 col7\" >0.5025</td>\n",
              "      <td id=\"T_02857_row340_col8\" class=\"data row340 col8\" >0.3512</td>\n",
              "      <td id=\"T_02857_row340_col9\" class=\"data row340 col9\" >0.4156</td>\n",
              "      <td id=\"T_02857_row340_col10\" class=\"data row340 col10\" >0.4845</td>\n",
              "      <td id=\"T_02857_row340_col11\" class=\"data row340 col11\" >0.4631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row341\" class=\"row_heading level0 row341\" >341</th>\n",
              "      <td id=\"T_02857_row341_col0\" class=\"data row341 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row341_col1\" class=\"data row341 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row341_col2\" class=\"data row341 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row341_col3\" class=\"data row341 col3\" >0.4626</td>\n",
              "      <td id=\"T_02857_row341_col4\" class=\"data row341 col4\" >0.5923</td>\n",
              "      <td id=\"T_02857_row341_col5\" class=\"data row341 col5\" >0.7099</td>\n",
              "      <td id=\"T_02857_row341_col6\" class=\"data row341 col6\" >0.6210</td>\n",
              "      <td id=\"T_02857_row341_col7\" class=\"data row341 col7\" >0.7232</td>\n",
              "      <td id=\"T_02857_row341_col8\" class=\"data row341 col8\" >0.6351</td>\n",
              "      <td id=\"T_02857_row341_col9\" class=\"data row341 col9\" >0.3623</td>\n",
              "      <td id=\"T_02857_row341_col10\" class=\"data row341 col10\" >0.4792</td>\n",
              "      <td id=\"T_02857_row341_col11\" class=\"data row341 col11\" >0.3584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row342\" class=\"row_heading level0 row342\" >342</th>\n",
              "      <td id=\"T_02857_row342_col0\" class=\"data row342 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row342_col1\" class=\"data row342 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row342_col2\" class=\"data row342 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row342_col3\" class=\"data row342 col3\" >0.4626</td>\n",
              "      <td id=\"T_02857_row342_col4\" class=\"data row342 col4\" >0.5923</td>\n",
              "      <td id=\"T_02857_row342_col5\" class=\"data row342 col5\" >0.7099</td>\n",
              "      <td id=\"T_02857_row342_col6\" class=\"data row342 col6\" >0.6210</td>\n",
              "      <td id=\"T_02857_row342_col7\" class=\"data row342 col7\" >0.7232</td>\n",
              "      <td id=\"T_02857_row342_col8\" class=\"data row342 col8\" >0.6351</td>\n",
              "      <td id=\"T_02857_row342_col9\" class=\"data row342 col9\" >0.3623</td>\n",
              "      <td id=\"T_02857_row342_col10\" class=\"data row342 col10\" >0.4792</td>\n",
              "      <td id=\"T_02857_row342_col11\" class=\"data row342 col11\" >0.3584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row343\" class=\"row_heading level0 row343\" >343</th>\n",
              "      <td id=\"T_02857_row343_col0\" class=\"data row343 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row343_col1\" class=\"data row343 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row343_col2\" class=\"data row343 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row343_col3\" class=\"data row343 col3\" >0.5417</td>\n",
              "      <td id=\"T_02857_row343_col4\" class=\"data row343 col4\" >0.5444</td>\n",
              "      <td id=\"T_02857_row343_col5\" class=\"data row343 col5\" >0.5322</td>\n",
              "      <td id=\"T_02857_row343_col6\" class=\"data row343 col6\" >0.5429</td>\n",
              "      <td id=\"T_02857_row343_col7\" class=\"data row343 col7\" >0.5321</td>\n",
              "      <td id=\"T_02857_row343_col8\" class=\"data row343 col8\" >0.4338</td>\n",
              "      <td id=\"T_02857_row343_col9\" class=\"data row343 col9\" >0.4956</td>\n",
              "      <td id=\"T_02857_row343_col10\" class=\"data row343 col10\" >0.4570</td>\n",
              "      <td id=\"T_02857_row343_col11\" class=\"data row343 col11\" >0.5642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row344\" class=\"row_heading level0 row344\" >344</th>\n",
              "      <td id=\"T_02857_row344_col0\" class=\"data row344 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row344_col1\" class=\"data row344 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row344_col2\" class=\"data row344 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row344_col3\" class=\"data row344 col3\" >0.5417</td>\n",
              "      <td id=\"T_02857_row344_col4\" class=\"data row344 col4\" >0.5444</td>\n",
              "      <td id=\"T_02857_row344_col5\" class=\"data row344 col5\" >0.5322</td>\n",
              "      <td id=\"T_02857_row344_col6\" class=\"data row344 col6\" >0.5429</td>\n",
              "      <td id=\"T_02857_row344_col7\" class=\"data row344 col7\" >0.5321</td>\n",
              "      <td id=\"T_02857_row344_col8\" class=\"data row344 col8\" >0.4338</td>\n",
              "      <td id=\"T_02857_row344_col9\" class=\"data row344 col9\" >0.4956</td>\n",
              "      <td id=\"T_02857_row344_col10\" class=\"data row344 col10\" >0.4570</td>\n",
              "      <td id=\"T_02857_row344_col11\" class=\"data row344 col11\" >0.5642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row345\" class=\"row_heading level0 row345\" >345</th>\n",
              "      <td id=\"T_02857_row345_col0\" class=\"data row345 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row345_col1\" class=\"data row345 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row345_col2\" class=\"data row345 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row345_col3\" class=\"data row345 col3\" >0.4605</td>\n",
              "      <td id=\"T_02857_row345_col4\" class=\"data row345 col4\" >0.4599</td>\n",
              "      <td id=\"T_02857_row345_col5\" class=\"data row345 col5\" >0.4613</td>\n",
              "      <td id=\"T_02857_row345_col6\" class=\"data row345 col6\" >0.4680</td>\n",
              "      <td id=\"T_02857_row345_col7\" class=\"data row345 col7\" >0.4708</td>\n",
              "      <td id=\"T_02857_row345_col8\" class=\"data row345 col8\" >0.4705</td>\n",
              "      <td id=\"T_02857_row345_col9\" class=\"data row345 col9\" >0.4852</td>\n",
              "      <td id=\"T_02857_row345_col10\" class=\"data row345 col10\" >0.4774</td>\n",
              "      <td id=\"T_02857_row345_col11\" class=\"data row345 col11\" >0.5131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row346\" class=\"row_heading level0 row346\" >346</th>\n",
              "      <td id=\"T_02857_row346_col0\" class=\"data row346 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row346_col1\" class=\"data row346 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row346_col2\" class=\"data row346 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row346_col3\" class=\"data row346 col3\" >0.4605</td>\n",
              "      <td id=\"T_02857_row346_col4\" class=\"data row346 col4\" >0.4599</td>\n",
              "      <td id=\"T_02857_row346_col5\" class=\"data row346 col5\" >0.4613</td>\n",
              "      <td id=\"T_02857_row346_col6\" class=\"data row346 col6\" >0.4680</td>\n",
              "      <td id=\"T_02857_row346_col7\" class=\"data row346 col7\" >0.4708</td>\n",
              "      <td id=\"T_02857_row346_col8\" class=\"data row346 col8\" >0.4705</td>\n",
              "      <td id=\"T_02857_row346_col9\" class=\"data row346 col9\" >0.4852</td>\n",
              "      <td id=\"T_02857_row346_col10\" class=\"data row346 col10\" >0.4774</td>\n",
              "      <td id=\"T_02857_row346_col11\" class=\"data row346 col11\" >0.5131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row347\" class=\"row_heading level0 row347\" >347</th>\n",
              "      <td id=\"T_02857_row347_col0\" class=\"data row347 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row347_col1\" class=\"data row347 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row347_col2\" class=\"data row347 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row347_col3\" class=\"data row347 col3\" >0.4809</td>\n",
              "      <td id=\"T_02857_row347_col4\" class=\"data row347 col4\" >0.4965</td>\n",
              "      <td id=\"T_02857_row347_col5\" class=\"data row347 col5\" >0.4400</td>\n",
              "      <td id=\"T_02857_row347_col6\" class=\"data row347 col6\" >0.5477</td>\n",
              "      <td id=\"T_02857_row347_col7\" class=\"data row347 col7\" >0.5031</td>\n",
              "      <td id=\"T_02857_row347_col8\" class=\"data row347 col8\" >0.5139</td>\n",
              "      <td id=\"T_02857_row347_col9\" class=\"data row347 col9\" >0.6459</td>\n",
              "      <td id=\"T_02857_row347_col10\" class=\"data row347 col10\" >0.4519</td>\n",
              "      <td id=\"T_02857_row347_col11\" class=\"data row347 col11\" >0.2993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row348\" class=\"row_heading level0 row348\" >348</th>\n",
              "      <td id=\"T_02857_row348_col0\" class=\"data row348 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row348_col1\" class=\"data row348 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row348_col2\" class=\"data row348 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row348_col3\" class=\"data row348 col3\" >0.4809</td>\n",
              "      <td id=\"T_02857_row348_col4\" class=\"data row348 col4\" >0.4965</td>\n",
              "      <td id=\"T_02857_row348_col5\" class=\"data row348 col5\" >0.4400</td>\n",
              "      <td id=\"T_02857_row348_col6\" class=\"data row348 col6\" >0.5477</td>\n",
              "      <td id=\"T_02857_row348_col7\" class=\"data row348 col7\" >0.5031</td>\n",
              "      <td id=\"T_02857_row348_col8\" class=\"data row348 col8\" >0.5139</td>\n",
              "      <td id=\"T_02857_row348_col9\" class=\"data row348 col9\" >0.6459</td>\n",
              "      <td id=\"T_02857_row348_col10\" class=\"data row348 col10\" >0.4519</td>\n",
              "      <td id=\"T_02857_row348_col11\" class=\"data row348 col11\" >0.2993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row349\" class=\"row_heading level0 row349\" >349</th>\n",
              "      <td id=\"T_02857_row349_col0\" class=\"data row349 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row349_col1\" class=\"data row349 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row349_col2\" class=\"data row349 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row349_col3\" class=\"data row349 col3\" >0.6616</td>\n",
              "      <td id=\"T_02857_row349_col4\" class=\"data row349 col4\" >0.6760</td>\n",
              "      <td id=\"T_02857_row349_col5\" class=\"data row349 col5\" >0.6592</td>\n",
              "      <td id=\"T_02857_row349_col6\" class=\"data row349 col6\" >0.6765</td>\n",
              "      <td id=\"T_02857_row349_col7\" class=\"data row349 col7\" >0.6673</td>\n",
              "      <td id=\"T_02857_row349_col8\" class=\"data row349 col8\" >0.5849</td>\n",
              "      <td id=\"T_02857_row349_col9\" class=\"data row349 col9\" >0.5875</td>\n",
              "      <td id=\"T_02857_row349_col10\" class=\"data row349 col10\" >0.5462</td>\n",
              "      <td id=\"T_02857_row349_col11\" class=\"data row349 col11\" >0.6213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row350\" class=\"row_heading level0 row350\" >350</th>\n",
              "      <td id=\"T_02857_row350_col0\" class=\"data row350 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row350_col1\" class=\"data row350 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row350_col2\" class=\"data row350 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row350_col3\" class=\"data row350 col3\" >0.6616</td>\n",
              "      <td id=\"T_02857_row350_col4\" class=\"data row350 col4\" >0.6760</td>\n",
              "      <td id=\"T_02857_row350_col5\" class=\"data row350 col5\" >0.6592</td>\n",
              "      <td id=\"T_02857_row350_col6\" class=\"data row350 col6\" >0.6765</td>\n",
              "      <td id=\"T_02857_row350_col7\" class=\"data row350 col7\" >0.6673</td>\n",
              "      <td id=\"T_02857_row350_col8\" class=\"data row350 col8\" >0.5849</td>\n",
              "      <td id=\"T_02857_row350_col9\" class=\"data row350 col9\" >0.5875</td>\n",
              "      <td id=\"T_02857_row350_col10\" class=\"data row350 col10\" >0.5462</td>\n",
              "      <td id=\"T_02857_row350_col11\" class=\"data row350 col11\" >0.6213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row351\" class=\"row_heading level0 row351\" >351</th>\n",
              "      <td id=\"T_02857_row351_col0\" class=\"data row351 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row351_col1\" class=\"data row351 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row351_col2\" class=\"data row351 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row351_col3\" class=\"data row351 col3\" >0.7582</td>\n",
              "      <td id=\"T_02857_row351_col4\" class=\"data row351 col4\" >0.8001</td>\n",
              "      <td id=\"T_02857_row351_col5\" class=\"data row351 col5\" >0.8342</td>\n",
              "      <td id=\"T_02857_row351_col6\" class=\"data row351 col6\" >0.8957</td>\n",
              "      <td id=\"T_02857_row351_col7\" class=\"data row351 col7\" >0.8513</td>\n",
              "      <td id=\"T_02857_row351_col8\" class=\"data row351 col8\" >0.9080</td>\n",
              "      <td id=\"T_02857_row351_col9\" class=\"data row351 col9\" >0.6075</td>\n",
              "      <td id=\"T_02857_row351_col10\" class=\"data row351 col10\" >0.7289</td>\n",
              "      <td id=\"T_02857_row351_col11\" class=\"data row351 col11\" >0.6228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row352\" class=\"row_heading level0 row352\" >352</th>\n",
              "      <td id=\"T_02857_row352_col0\" class=\"data row352 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row352_col1\" class=\"data row352 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row352_col2\" class=\"data row352 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row352_col3\" class=\"data row352 col3\" >0.7582</td>\n",
              "      <td id=\"T_02857_row352_col4\" class=\"data row352 col4\" >0.8001</td>\n",
              "      <td id=\"T_02857_row352_col5\" class=\"data row352 col5\" >0.8342</td>\n",
              "      <td id=\"T_02857_row352_col6\" class=\"data row352 col6\" >0.8957</td>\n",
              "      <td id=\"T_02857_row352_col7\" class=\"data row352 col7\" >0.8513</td>\n",
              "      <td id=\"T_02857_row352_col8\" class=\"data row352 col8\" >0.9080</td>\n",
              "      <td id=\"T_02857_row352_col9\" class=\"data row352 col9\" >0.6075</td>\n",
              "      <td id=\"T_02857_row352_col10\" class=\"data row352 col10\" >0.7289</td>\n",
              "      <td id=\"T_02857_row352_col11\" class=\"data row352 col11\" >0.6228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row353\" class=\"row_heading level0 row353\" >353</th>\n",
              "      <td id=\"T_02857_row353_col0\" class=\"data row353 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row353_col1\" class=\"data row353 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row353_col2\" class=\"data row353 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row353_col3\" class=\"data row353 col3\" >0.5146</td>\n",
              "      <td id=\"T_02857_row353_col4\" class=\"data row353 col4\" >0.5137</td>\n",
              "      <td id=\"T_02857_row353_col5\" class=\"data row353 col5\" >0.4998</td>\n",
              "      <td id=\"T_02857_row353_col6\" class=\"data row353 col6\" >0.5269</td>\n",
              "      <td id=\"T_02857_row353_col7\" class=\"data row353 col7\" >0.5088</td>\n",
              "      <td id=\"T_02857_row353_col8\" class=\"data row353 col8\" >0.4461</td>\n",
              "      <td id=\"T_02857_row353_col9\" class=\"data row353 col9\" >0.5206</td>\n",
              "      <td id=\"T_02857_row353_col10\" class=\"data row353 col10\" >0.4873</td>\n",
              "      <td id=\"T_02857_row353_col11\" class=\"data row353 col11\" >0.5022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row354\" class=\"row_heading level0 row354\" >354</th>\n",
              "      <td id=\"T_02857_row354_col0\" class=\"data row354 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row354_col1\" class=\"data row354 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row354_col2\" class=\"data row354 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row354_col3\" class=\"data row354 col3\" >0.5146</td>\n",
              "      <td id=\"T_02857_row354_col4\" class=\"data row354 col4\" >0.5137</td>\n",
              "      <td id=\"T_02857_row354_col5\" class=\"data row354 col5\" >0.4998</td>\n",
              "      <td id=\"T_02857_row354_col6\" class=\"data row354 col6\" >0.5269</td>\n",
              "      <td id=\"T_02857_row354_col7\" class=\"data row354 col7\" >0.5088</td>\n",
              "      <td id=\"T_02857_row354_col8\" class=\"data row354 col8\" >0.4461</td>\n",
              "      <td id=\"T_02857_row354_col9\" class=\"data row354 col9\" >0.5206</td>\n",
              "      <td id=\"T_02857_row354_col10\" class=\"data row354 col10\" >0.4873</td>\n",
              "      <td id=\"T_02857_row354_col11\" class=\"data row354 col11\" >0.5022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row355\" class=\"row_heading level0 row355\" >355</th>\n",
              "      <td id=\"T_02857_row355_col0\" class=\"data row355 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row355_col1\" class=\"data row355 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row355_col2\" class=\"data row355 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row355_col3\" class=\"data row355 col3\" >0.7992</td>\n",
              "      <td id=\"T_02857_row355_col4\" class=\"data row355 col4\" >0.8318</td>\n",
              "      <td id=\"T_02857_row355_col5\" class=\"data row355 col5\" >0.8429</td>\n",
              "      <td id=\"T_02857_row355_col6\" class=\"data row355 col6\" >0.8852</td>\n",
              "      <td id=\"T_02857_row355_col7\" class=\"data row355 col7\" >0.8220</td>\n",
              "      <td id=\"T_02857_row355_col8\" class=\"data row355 col8\" >0.7620</td>\n",
              "      <td id=\"T_02857_row355_col9\" class=\"data row355 col9\" >0.5332</td>\n",
              "      <td id=\"T_02857_row355_col10\" class=\"data row355 col10\" >0.5935</td>\n",
              "      <td id=\"T_02857_row355_col11\" class=\"data row355 col11\" >0.5941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row356\" class=\"row_heading level0 row356\" >356</th>\n",
              "      <td id=\"T_02857_row356_col0\" class=\"data row356 col0\" >ru</td>\n",
              "      <td id=\"T_02857_row356_col1\" class=\"data row356 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row356_col2\" class=\"data row356 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row356_col3\" class=\"data row356 col3\" >0.7992</td>\n",
              "      <td id=\"T_02857_row356_col4\" class=\"data row356 col4\" >0.8318</td>\n",
              "      <td id=\"T_02857_row356_col5\" class=\"data row356 col5\" >0.8429</td>\n",
              "      <td id=\"T_02857_row356_col6\" class=\"data row356 col6\" >0.8852</td>\n",
              "      <td id=\"T_02857_row356_col7\" class=\"data row356 col7\" >0.8220</td>\n",
              "      <td id=\"T_02857_row356_col8\" class=\"data row356 col8\" >0.7620</td>\n",
              "      <td id=\"T_02857_row356_col9\" class=\"data row356 col9\" >0.5332</td>\n",
              "      <td id=\"T_02857_row356_col10\" class=\"data row356 col10\" >0.5935</td>\n",
              "      <td id=\"T_02857_row356_col11\" class=\"data row356 col11\" >0.5941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row357\" class=\"row_heading level0 row357\" >357</th>\n",
              "      <td id=\"T_02857_row357_col0\" class=\"data row357 col0\" >all</td>\n",
              "      <td id=\"T_02857_row357_col1\" class=\"data row357 col1\" >all</td>\n",
              "      <td id=\"T_02857_row357_col2\" class=\"data row357 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row357_col3\" class=\"data row357 col3\" >0.7420</td>\n",
              "      <td id=\"T_02857_row357_col4\" class=\"data row357 col4\" >0.7430</td>\n",
              "      <td id=\"T_02857_row357_col5\" class=\"data row357 col5\" >0.7393</td>\n",
              "      <td id=\"T_02857_row357_col6\" class=\"data row357 col6\" >0.7391</td>\n",
              "      <td id=\"T_02857_row357_col7\" class=\"data row357 col7\" >0.7256</td>\n",
              "      <td id=\"T_02857_row357_col8\" class=\"data row357 col8\" >0.7133</td>\n",
              "      <td id=\"T_02857_row357_col9\" class=\"data row357 col9\" >0.7339</td>\n",
              "      <td id=\"T_02857_row357_col10\" class=\"data row357 col10\" >0.6857</td>\n",
              "      <td id=\"T_02857_row357_col11\" class=\"data row357 col11\" >0.7563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row358\" class=\"row_heading level0 row358\" >358</th>\n",
              "      <td id=\"T_02857_row358_col0\" class=\"data row358 col0\" >all</td>\n",
              "      <td id=\"T_02857_row358_col1\" class=\"data row358 col1\" >all</td>\n",
              "      <td id=\"T_02857_row358_col2\" class=\"data row358 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row358_col3\" class=\"data row358 col3\" >0.4201</td>\n",
              "      <td id=\"T_02857_row358_col4\" class=\"data row358 col4\" >0.4174</td>\n",
              "      <td id=\"T_02857_row358_col5\" class=\"data row358 col5\" >0.4151</td>\n",
              "      <td id=\"T_02857_row358_col6\" class=\"data row358 col6\" >0.4226</td>\n",
              "      <td id=\"T_02857_row358_col7\" class=\"data row358 col7\" >0.4209</td>\n",
              "      <td id=\"T_02857_row358_col8\" class=\"data row358 col8\" >0.4216</td>\n",
              "      <td id=\"T_02857_row358_col9\" class=\"data row358 col9\" >0.4397</td>\n",
              "      <td id=\"T_02857_row358_col10\" class=\"data row358 col10\" >0.4278</td>\n",
              "      <td id=\"T_02857_row358_col11\" class=\"data row358 col11\" >0.5059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row359\" class=\"row_heading level0 row359\" >359</th>\n",
              "      <td id=\"T_02857_row359_col0\" class=\"data row359 col0\" >all</td>\n",
              "      <td id=\"T_02857_row359_col1\" class=\"data row359 col1\" >all</td>\n",
              "      <td id=\"T_02857_row359_col2\" class=\"data row359 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row359_col3\" class=\"data row359 col3\" >0.6393</td>\n",
              "      <td id=\"T_02857_row359_col4\" class=\"data row359 col4\" >0.6626</td>\n",
              "      <td id=\"T_02857_row359_col5\" class=\"data row359 col5\" >0.6507</td>\n",
              "      <td id=\"T_02857_row359_col6\" class=\"data row359 col6\" >0.6509</td>\n",
              "      <td id=\"T_02857_row359_col7\" class=\"data row359 col7\" >0.6599</td>\n",
              "      <td id=\"T_02857_row359_col8\" class=\"data row359 col8\" >0.6505</td>\n",
              "      <td id=\"T_02857_row359_col9\" class=\"data row359 col9\" >0.6626</td>\n",
              "      <td id=\"T_02857_row359_col10\" class=\"data row359 col10\" >0.5908</td>\n",
              "      <td id=\"T_02857_row359_col11\" class=\"data row359 col11\" >0.6646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row360\" class=\"row_heading level0 row360\" >360</th>\n",
              "      <td id=\"T_02857_row360_col0\" class=\"data row360 col0\" >all</td>\n",
              "      <td id=\"T_02857_row360_col1\" class=\"data row360 col1\" >all</td>\n",
              "      <td id=\"T_02857_row360_col2\" class=\"data row360 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row360_col3\" class=\"data row360 col3\" >0.7181</td>\n",
              "      <td id=\"T_02857_row360_col4\" class=\"data row360 col4\" >0.7316</td>\n",
              "      <td id=\"T_02857_row360_col5\" class=\"data row360 col5\" >0.7250</td>\n",
              "      <td id=\"T_02857_row360_col6\" class=\"data row360 col6\" >0.7259</td>\n",
              "      <td id=\"T_02857_row360_col7\" class=\"data row360 col7\" >0.7226</td>\n",
              "      <td id=\"T_02857_row360_col8\" class=\"data row360 col8\" >0.7139</td>\n",
              "      <td id=\"T_02857_row360_col9\" class=\"data row360 col9\" >0.7265</td>\n",
              "      <td id=\"T_02857_row360_col10\" class=\"data row360 col10\" >0.6142</td>\n",
              "      <td id=\"T_02857_row360_col11\" class=\"data row360 col11\" >0.6959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row361\" class=\"row_heading level0 row361\" >361</th>\n",
              "      <td id=\"T_02857_row361_col0\" class=\"data row361 col0\" >all</td>\n",
              "      <td id=\"T_02857_row361_col1\" class=\"data row361 col1\" >all</td>\n",
              "      <td id=\"T_02857_row361_col2\" class=\"data row361 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row361_col3\" class=\"data row361 col3\" >0.8552</td>\n",
              "      <td id=\"T_02857_row361_col4\" class=\"data row361 col4\" >0.8442</td>\n",
              "      <td id=\"T_02857_row361_col5\" class=\"data row361 col5\" >0.8500</td>\n",
              "      <td id=\"T_02857_row361_col6\" class=\"data row361 col6\" >0.8438</td>\n",
              "      <td id=\"T_02857_row361_col7\" class=\"data row361 col7\" >0.8548</td>\n",
              "      <td id=\"T_02857_row361_col8\" class=\"data row361 col8\" >0.8623</td>\n",
              "      <td id=\"T_02857_row361_col9\" class=\"data row361 col9\" >0.8600</td>\n",
              "      <td id=\"T_02857_row361_col10\" class=\"data row361 col10\" >0.8295</td>\n",
              "      <td id=\"T_02857_row361_col11\" class=\"data row361 col11\" >0.8480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row362\" class=\"row_heading level0 row362\" >362</th>\n",
              "      <td id=\"T_02857_row362_col0\" class=\"data row362 col0\" >all</td>\n",
              "      <td id=\"T_02857_row362_col1\" class=\"data row362 col1\" >all</td>\n",
              "      <td id=\"T_02857_row362_col2\" class=\"data row362 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row362_col3\" class=\"data row362 col3\" >0.7323</td>\n",
              "      <td id=\"T_02857_row362_col4\" class=\"data row362 col4\" >0.7402</td>\n",
              "      <td id=\"T_02857_row362_col5\" class=\"data row362 col5\" >0.7241</td>\n",
              "      <td id=\"T_02857_row362_col6\" class=\"data row362 col6\" >0.7414</td>\n",
              "      <td id=\"T_02857_row362_col7\" class=\"data row362 col7\" >0.7276</td>\n",
              "      <td id=\"T_02857_row362_col8\" class=\"data row362 col8\" >0.7654</td>\n",
              "      <td id=\"T_02857_row362_col9\" class=\"data row362 col9\" >0.7667</td>\n",
              "      <td id=\"T_02857_row362_col10\" class=\"data row362 col10\" >0.7362</td>\n",
              "      <td id=\"T_02857_row362_col11\" class=\"data row362 col11\" >0.7360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row363\" class=\"row_heading level0 row363\" >363</th>\n",
              "      <td id=\"T_02857_row363_col0\" class=\"data row363 col0\" >all</td>\n",
              "      <td id=\"T_02857_row363_col1\" class=\"data row363 col1\" >all</td>\n",
              "      <td id=\"T_02857_row363_col2\" class=\"data row363 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row363_col3\" class=\"data row363 col3\" >0.7801</td>\n",
              "      <td id=\"T_02857_row363_col4\" class=\"data row363 col4\" >0.7806</td>\n",
              "      <td id=\"T_02857_row363_col5\" class=\"data row363 col5\" >0.7788</td>\n",
              "      <td id=\"T_02857_row363_col6\" class=\"data row363 col6\" >0.7792</td>\n",
              "      <td id=\"T_02857_row363_col7\" class=\"data row363 col7\" >0.7754</td>\n",
              "      <td id=\"T_02857_row363_col8\" class=\"data row363 col8\" >0.7778</td>\n",
              "      <td id=\"T_02857_row363_col9\" class=\"data row363 col9\" >0.7760</td>\n",
              "      <td id=\"T_02857_row363_col10\" class=\"data row363 col10\" >0.7484</td>\n",
              "      <td id=\"T_02857_row363_col11\" class=\"data row363 col11\" >0.8240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row364\" class=\"row_heading level0 row364\" >364</th>\n",
              "      <td id=\"T_02857_row364_col0\" class=\"data row364 col0\" >all</td>\n",
              "      <td id=\"T_02857_row364_col1\" class=\"data row364 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row364_col2\" class=\"data row364 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row364_col3\" class=\"data row364 col3\" >0.8832</td>\n",
              "      <td id=\"T_02857_row364_col4\" class=\"data row364 col4\" >0.9281</td>\n",
              "      <td id=\"T_02857_row364_col5\" class=\"data row364 col5\" >0.9005</td>\n",
              "      <td id=\"T_02857_row364_col6\" class=\"data row364 col6\" >0.8645</td>\n",
              "      <td id=\"T_02857_row364_col7\" class=\"data row364 col7\" >0.8698</td>\n",
              "      <td id=\"T_02857_row364_col8\" class=\"data row364 col8\" >0.4810</td>\n",
              "      <td id=\"T_02857_row364_col9\" class=\"data row364 col9\" >0.5032</td>\n",
              "      <td id=\"T_02857_row364_col10\" class=\"data row364 col10\" >0.4212</td>\n",
              "      <td id=\"T_02857_row364_col11\" class=\"data row364 col11\" >0.5528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row365\" class=\"row_heading level0 row365\" >365</th>\n",
              "      <td id=\"T_02857_row365_col0\" class=\"data row365 col0\" >all</td>\n",
              "      <td id=\"T_02857_row365_col1\" class=\"data row365 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row365_col2\" class=\"data row365 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row365_col3\" class=\"data row365 col3\" >0.8832</td>\n",
              "      <td id=\"T_02857_row365_col4\" class=\"data row365 col4\" >0.9281</td>\n",
              "      <td id=\"T_02857_row365_col5\" class=\"data row365 col5\" >0.9005</td>\n",
              "      <td id=\"T_02857_row365_col6\" class=\"data row365 col6\" >0.8645</td>\n",
              "      <td id=\"T_02857_row365_col7\" class=\"data row365 col7\" >0.8698</td>\n",
              "      <td id=\"T_02857_row365_col8\" class=\"data row365 col8\" >0.4810</td>\n",
              "      <td id=\"T_02857_row365_col9\" class=\"data row365 col9\" >0.5032</td>\n",
              "      <td id=\"T_02857_row365_col10\" class=\"data row365 col10\" >0.4212</td>\n",
              "      <td id=\"T_02857_row365_col11\" class=\"data row365 col11\" >0.5528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row366\" class=\"row_heading level0 row366\" >366</th>\n",
              "      <td id=\"T_02857_row366_col0\" class=\"data row366 col0\" >all</td>\n",
              "      <td id=\"T_02857_row366_col1\" class=\"data row366 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row366_col2\" class=\"data row366 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row366_col3\" class=\"data row366 col3\" >0.5822</td>\n",
              "      <td id=\"T_02857_row366_col4\" class=\"data row366 col4\" >0.6278</td>\n",
              "      <td id=\"T_02857_row366_col5\" class=\"data row366 col5\" >0.6007</td>\n",
              "      <td id=\"T_02857_row366_col6\" class=\"data row366 col6\" >0.6223</td>\n",
              "      <td id=\"T_02857_row366_col7\" class=\"data row366 col7\" >0.6507</td>\n",
              "      <td id=\"T_02857_row366_col8\" class=\"data row366 col8\" >0.4472</td>\n",
              "      <td id=\"T_02857_row366_col9\" class=\"data row366 col9\" >0.4489</td>\n",
              "      <td id=\"T_02857_row366_col10\" class=\"data row366 col10\" >0.4038</td>\n",
              "      <td id=\"T_02857_row366_col11\" class=\"data row366 col11\" >0.3214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row367\" class=\"row_heading level0 row367\" >367</th>\n",
              "      <td id=\"T_02857_row367_col0\" class=\"data row367 col0\" >all</td>\n",
              "      <td id=\"T_02857_row367_col1\" class=\"data row367 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row367_col2\" class=\"data row367 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row367_col3\" class=\"data row367 col3\" >0.5822</td>\n",
              "      <td id=\"T_02857_row367_col4\" class=\"data row367 col4\" >0.6278</td>\n",
              "      <td id=\"T_02857_row367_col5\" class=\"data row367 col5\" >0.6007</td>\n",
              "      <td id=\"T_02857_row367_col6\" class=\"data row367 col6\" >0.6223</td>\n",
              "      <td id=\"T_02857_row367_col7\" class=\"data row367 col7\" >0.6507</td>\n",
              "      <td id=\"T_02857_row367_col8\" class=\"data row367 col8\" >0.4472</td>\n",
              "      <td id=\"T_02857_row367_col9\" class=\"data row367 col9\" >0.4489</td>\n",
              "      <td id=\"T_02857_row367_col10\" class=\"data row367 col10\" >0.4038</td>\n",
              "      <td id=\"T_02857_row367_col11\" class=\"data row367 col11\" >0.3214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row368\" class=\"row_heading level0 row368\" >368</th>\n",
              "      <td id=\"T_02857_row368_col0\" class=\"data row368 col0\" >all</td>\n",
              "      <td id=\"T_02857_row368_col1\" class=\"data row368 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row368_col2\" class=\"data row368 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row368_col3\" class=\"data row368 col3\" >0.5215</td>\n",
              "      <td id=\"T_02857_row368_col4\" class=\"data row368 col4\" >0.6725</td>\n",
              "      <td id=\"T_02857_row368_col5\" class=\"data row368 col5\" >0.6214</td>\n",
              "      <td id=\"T_02857_row368_col6\" class=\"data row368 col6\" >0.5983</td>\n",
              "      <td id=\"T_02857_row368_col7\" class=\"data row368 col7\" >0.7961</td>\n",
              "      <td id=\"T_02857_row368_col8\" class=\"data row368 col8\" >0.5728</td>\n",
              "      <td id=\"T_02857_row368_col9\" class=\"data row368 col9\" >0.5651</td>\n",
              "      <td id=\"T_02857_row368_col10\" class=\"data row368 col10\" >0.4509</td>\n",
              "      <td id=\"T_02857_row368_col11\" class=\"data row368 col11\" >0.3800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row369\" class=\"row_heading level0 row369\" >369</th>\n",
              "      <td id=\"T_02857_row369_col0\" class=\"data row369 col0\" >all</td>\n",
              "      <td id=\"T_02857_row369_col1\" class=\"data row369 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row369_col2\" class=\"data row369 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row369_col3\" class=\"data row369 col3\" >0.5215</td>\n",
              "      <td id=\"T_02857_row369_col4\" class=\"data row369 col4\" >0.6725</td>\n",
              "      <td id=\"T_02857_row369_col5\" class=\"data row369 col5\" >0.6214</td>\n",
              "      <td id=\"T_02857_row369_col6\" class=\"data row369 col6\" >0.5983</td>\n",
              "      <td id=\"T_02857_row369_col7\" class=\"data row369 col7\" >0.7961</td>\n",
              "      <td id=\"T_02857_row369_col8\" class=\"data row369 col8\" >0.5728</td>\n",
              "      <td id=\"T_02857_row369_col9\" class=\"data row369 col9\" >0.5651</td>\n",
              "      <td id=\"T_02857_row369_col10\" class=\"data row369 col10\" >0.4509</td>\n",
              "      <td id=\"T_02857_row369_col11\" class=\"data row369 col11\" >0.3800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row370\" class=\"row_heading level0 row370\" >370</th>\n",
              "      <td id=\"T_02857_row370_col0\" class=\"data row370 col0\" >all</td>\n",
              "      <td id=\"T_02857_row370_col1\" class=\"data row370 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row370_col2\" class=\"data row370 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row370_col3\" class=\"data row370 col3\" >0.7473</td>\n",
              "      <td id=\"T_02857_row370_col4\" class=\"data row370 col4\" >0.8360</td>\n",
              "      <td id=\"T_02857_row370_col5\" class=\"data row370 col5\" >0.8100</td>\n",
              "      <td id=\"T_02857_row370_col6\" class=\"data row370 col6\" >0.7800</td>\n",
              "      <td id=\"T_02857_row370_col7\" class=\"data row370 col7\" >0.8414</td>\n",
              "      <td id=\"T_02857_row370_col8\" class=\"data row370 col8\" >0.4638</td>\n",
              "      <td id=\"T_02857_row370_col9\" class=\"data row370 col9\" >0.5736</td>\n",
              "      <td id=\"T_02857_row370_col10\" class=\"data row370 col10\" >0.4625</td>\n",
              "      <td id=\"T_02857_row370_col11\" class=\"data row370 col11\" >0.5230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row371\" class=\"row_heading level0 row371\" >371</th>\n",
              "      <td id=\"T_02857_row371_col0\" class=\"data row371 col0\" >all</td>\n",
              "      <td id=\"T_02857_row371_col1\" class=\"data row371 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row371_col2\" class=\"data row371 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row371_col3\" class=\"data row371 col3\" >0.7473</td>\n",
              "      <td id=\"T_02857_row371_col4\" class=\"data row371 col4\" >0.8360</td>\n",
              "      <td id=\"T_02857_row371_col5\" class=\"data row371 col5\" >0.8100</td>\n",
              "      <td id=\"T_02857_row371_col6\" class=\"data row371 col6\" >0.7800</td>\n",
              "      <td id=\"T_02857_row371_col7\" class=\"data row371 col7\" >0.8414</td>\n",
              "      <td id=\"T_02857_row371_col8\" class=\"data row371 col8\" >0.4638</td>\n",
              "      <td id=\"T_02857_row371_col9\" class=\"data row371 col9\" >0.5736</td>\n",
              "      <td id=\"T_02857_row371_col10\" class=\"data row371 col10\" >0.4625</td>\n",
              "      <td id=\"T_02857_row371_col11\" class=\"data row371 col11\" >0.5230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row372\" class=\"row_heading level0 row372\" >372</th>\n",
              "      <td id=\"T_02857_row372_col0\" class=\"data row372 col0\" >all</td>\n",
              "      <td id=\"T_02857_row372_col1\" class=\"data row372 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row372_col2\" class=\"data row372 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row372_col3\" class=\"data row372 col3\" >0.8603</td>\n",
              "      <td id=\"T_02857_row372_col4\" class=\"data row372 col4\" >0.8684</td>\n",
              "      <td id=\"T_02857_row372_col5\" class=\"data row372 col5\" >0.8707</td>\n",
              "      <td id=\"T_02857_row372_col6\" class=\"data row372 col6\" >0.8262</td>\n",
              "      <td id=\"T_02857_row372_col7\" class=\"data row372 col7\" >0.8542</td>\n",
              "      <td id=\"T_02857_row372_col8\" class=\"data row372 col8\" >0.5780</td>\n",
              "      <td id=\"T_02857_row372_col9\" class=\"data row372 col9\" >0.5289</td>\n",
              "      <td id=\"T_02857_row372_col10\" class=\"data row372 col10\" >0.5209</td>\n",
              "      <td id=\"T_02857_row372_col11\" class=\"data row372 col11\" >0.5519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row373\" class=\"row_heading level0 row373\" >373</th>\n",
              "      <td id=\"T_02857_row373_col0\" class=\"data row373 col0\" >all</td>\n",
              "      <td id=\"T_02857_row373_col1\" class=\"data row373 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row373_col2\" class=\"data row373 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row373_col3\" class=\"data row373 col3\" >0.8603</td>\n",
              "      <td id=\"T_02857_row373_col4\" class=\"data row373 col4\" >0.8684</td>\n",
              "      <td id=\"T_02857_row373_col5\" class=\"data row373 col5\" >0.8707</td>\n",
              "      <td id=\"T_02857_row373_col6\" class=\"data row373 col6\" >0.8262</td>\n",
              "      <td id=\"T_02857_row373_col7\" class=\"data row373 col7\" >0.8542</td>\n",
              "      <td id=\"T_02857_row373_col8\" class=\"data row373 col8\" >0.5780</td>\n",
              "      <td id=\"T_02857_row373_col9\" class=\"data row373 col9\" >0.5289</td>\n",
              "      <td id=\"T_02857_row373_col10\" class=\"data row373 col10\" >0.5209</td>\n",
              "      <td id=\"T_02857_row373_col11\" class=\"data row373 col11\" >0.5519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row374\" class=\"row_heading level0 row374\" >374</th>\n",
              "      <td id=\"T_02857_row374_col0\" class=\"data row374 col0\" >all</td>\n",
              "      <td id=\"T_02857_row374_col1\" class=\"data row374 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row374_col2\" class=\"data row374 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row374_col3\" class=\"data row374 col3\" >0.7406</td>\n",
              "      <td id=\"T_02857_row374_col4\" class=\"data row374 col4\" >0.8254</td>\n",
              "      <td id=\"T_02857_row374_col5\" class=\"data row374 col5\" >0.7908</td>\n",
              "      <td id=\"T_02857_row374_col6\" class=\"data row374 col6\" >0.7696</td>\n",
              "      <td id=\"T_02857_row374_col7\" class=\"data row374 col7\" >0.8475</td>\n",
              "      <td id=\"T_02857_row374_col8\" class=\"data row374 col8\" >0.5387</td>\n",
              "      <td id=\"T_02857_row374_col9\" class=\"data row374 col9\" >0.5131</td>\n",
              "      <td id=\"T_02857_row374_col10\" class=\"data row374 col10\" >0.5450</td>\n",
              "      <td id=\"T_02857_row374_col11\" class=\"data row374 col11\" >0.5132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row375\" class=\"row_heading level0 row375\" >375</th>\n",
              "      <td id=\"T_02857_row375_col0\" class=\"data row375 col0\" >all</td>\n",
              "      <td id=\"T_02857_row375_col1\" class=\"data row375 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row375_col2\" class=\"data row375 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row375_col3\" class=\"data row375 col3\" >0.7406</td>\n",
              "      <td id=\"T_02857_row375_col4\" class=\"data row375 col4\" >0.8254</td>\n",
              "      <td id=\"T_02857_row375_col5\" class=\"data row375 col5\" >0.7908</td>\n",
              "      <td id=\"T_02857_row375_col6\" class=\"data row375 col6\" >0.7696</td>\n",
              "      <td id=\"T_02857_row375_col7\" class=\"data row375 col7\" >0.8475</td>\n",
              "      <td id=\"T_02857_row375_col8\" class=\"data row375 col8\" >0.5387</td>\n",
              "      <td id=\"T_02857_row375_col9\" class=\"data row375 col9\" >0.5131</td>\n",
              "      <td id=\"T_02857_row375_col10\" class=\"data row375 col10\" >0.5450</td>\n",
              "      <td id=\"T_02857_row375_col11\" class=\"data row375 col11\" >0.5132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row376\" class=\"row_heading level0 row376\" >376</th>\n",
              "      <td id=\"T_02857_row376_col0\" class=\"data row376 col0\" >all</td>\n",
              "      <td id=\"T_02857_row376_col1\" class=\"data row376 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row376_col2\" class=\"data row376 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row376_col3\" class=\"data row376 col3\" >0.8333</td>\n",
              "      <td id=\"T_02857_row376_col4\" class=\"data row376 col4\" >0.9141</td>\n",
              "      <td id=\"T_02857_row376_col5\" class=\"data row376 col5\" >0.8972</td>\n",
              "      <td id=\"T_02857_row376_col6\" class=\"data row376 col6\" >0.8737</td>\n",
              "      <td id=\"T_02857_row376_col7\" class=\"data row376 col7\" >0.9224</td>\n",
              "      <td id=\"T_02857_row376_col8\" class=\"data row376 col8\" >0.5018</td>\n",
              "      <td id=\"T_02857_row376_col9\" class=\"data row376 col9\" >0.4641</td>\n",
              "      <td id=\"T_02857_row376_col10\" class=\"data row376 col10\" >0.4316</td>\n",
              "      <td id=\"T_02857_row376_col11\" class=\"data row376 col11\" >0.5419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row377\" class=\"row_heading level0 row377\" >377</th>\n",
              "      <td id=\"T_02857_row377_col0\" class=\"data row377 col0\" >all</td>\n",
              "      <td id=\"T_02857_row377_col1\" class=\"data row377 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row377_col2\" class=\"data row377 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row377_col3\" class=\"data row377 col3\" >0.8333</td>\n",
              "      <td id=\"T_02857_row377_col4\" class=\"data row377 col4\" >0.9141</td>\n",
              "      <td id=\"T_02857_row377_col5\" class=\"data row377 col5\" >0.8972</td>\n",
              "      <td id=\"T_02857_row377_col6\" class=\"data row377 col6\" >0.8737</td>\n",
              "      <td id=\"T_02857_row377_col7\" class=\"data row377 col7\" >0.9224</td>\n",
              "      <td id=\"T_02857_row377_col8\" class=\"data row377 col8\" >0.5018</td>\n",
              "      <td id=\"T_02857_row377_col9\" class=\"data row377 col9\" >0.4641</td>\n",
              "      <td id=\"T_02857_row377_col10\" class=\"data row377 col10\" >0.4316</td>\n",
              "      <td id=\"T_02857_row377_col11\" class=\"data row377 col11\" >0.5419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row378\" class=\"row_heading level0 row378\" >378</th>\n",
              "      <td id=\"T_02857_row378_col0\" class=\"data row378 col0\" >all</td>\n",
              "      <td id=\"T_02857_row378_col1\" class=\"data row378 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row378_col2\" class=\"data row378 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row378_col3\" class=\"data row378 col3\" >0.8954</td>\n",
              "      <td id=\"T_02857_row378_col4\" class=\"data row378 col4\" >0.9472</td>\n",
              "      <td id=\"T_02857_row378_col5\" class=\"data row378 col5\" >0.8561</td>\n",
              "      <td id=\"T_02857_row378_col6\" class=\"data row378 col6\" >0.8536</td>\n",
              "      <td id=\"T_02857_row378_col7\" class=\"data row378 col7\" >0.7597</td>\n",
              "      <td id=\"T_02857_row378_col8\" class=\"data row378 col8\" >0.3724</td>\n",
              "      <td id=\"T_02857_row378_col9\" class=\"data row378 col9\" >0.3657</td>\n",
              "      <td id=\"T_02857_row378_col10\" class=\"data row378 col10\" >0.3607</td>\n",
              "      <td id=\"T_02857_row378_col11\" class=\"data row378 col11\" >0.4892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row379\" class=\"row_heading level0 row379\" >379</th>\n",
              "      <td id=\"T_02857_row379_col0\" class=\"data row379 col0\" >all</td>\n",
              "      <td id=\"T_02857_row379_col1\" class=\"data row379 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row379_col2\" class=\"data row379 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row379_col3\" class=\"data row379 col3\" >0.8954</td>\n",
              "      <td id=\"T_02857_row379_col4\" class=\"data row379 col4\" >0.9472</td>\n",
              "      <td id=\"T_02857_row379_col5\" class=\"data row379 col5\" >0.8561</td>\n",
              "      <td id=\"T_02857_row379_col6\" class=\"data row379 col6\" >0.8536</td>\n",
              "      <td id=\"T_02857_row379_col7\" class=\"data row379 col7\" >0.7597</td>\n",
              "      <td id=\"T_02857_row379_col8\" class=\"data row379 col8\" >0.3724</td>\n",
              "      <td id=\"T_02857_row379_col9\" class=\"data row379 col9\" >0.3657</td>\n",
              "      <td id=\"T_02857_row379_col10\" class=\"data row379 col10\" >0.3607</td>\n",
              "      <td id=\"T_02857_row379_col11\" class=\"data row379 col11\" >0.4892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row380\" class=\"row_heading level0 row380\" >380</th>\n",
              "      <td id=\"T_02857_row380_col0\" class=\"data row380 col0\" >all</td>\n",
              "      <td id=\"T_02857_row380_col1\" class=\"data row380 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row380_col2\" class=\"data row380 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row380_col3\" class=\"data row380 col3\" >0.5481</td>\n",
              "      <td id=\"T_02857_row380_col4\" class=\"data row380 col4\" >0.6022</td>\n",
              "      <td id=\"T_02857_row380_col5\" class=\"data row380 col5\" >0.5509</td>\n",
              "      <td id=\"T_02857_row380_col6\" class=\"data row380 col6\" >0.5787</td>\n",
              "      <td id=\"T_02857_row380_col7\" class=\"data row380 col7\" >0.5143</td>\n",
              "      <td id=\"T_02857_row380_col8\" class=\"data row380 col8\" >0.3702</td>\n",
              "      <td id=\"T_02857_row380_col9\" class=\"data row380 col9\" >0.3592</td>\n",
              "      <td id=\"T_02857_row380_col10\" class=\"data row380 col10\" >0.3606</td>\n",
              "      <td id=\"T_02857_row380_col11\" class=\"data row380 col11\" >0.2594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row381\" class=\"row_heading level0 row381\" >381</th>\n",
              "      <td id=\"T_02857_row381_col0\" class=\"data row381 col0\" >all</td>\n",
              "      <td id=\"T_02857_row381_col1\" class=\"data row381 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row381_col2\" class=\"data row381 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row381_col3\" class=\"data row381 col3\" >0.5481</td>\n",
              "      <td id=\"T_02857_row381_col4\" class=\"data row381 col4\" >0.6022</td>\n",
              "      <td id=\"T_02857_row381_col5\" class=\"data row381 col5\" >0.5509</td>\n",
              "      <td id=\"T_02857_row381_col6\" class=\"data row381 col6\" >0.5787</td>\n",
              "      <td id=\"T_02857_row381_col7\" class=\"data row381 col7\" >0.5143</td>\n",
              "      <td id=\"T_02857_row381_col8\" class=\"data row381 col8\" >0.3702</td>\n",
              "      <td id=\"T_02857_row381_col9\" class=\"data row381 col9\" >0.3592</td>\n",
              "      <td id=\"T_02857_row381_col10\" class=\"data row381 col10\" >0.3606</td>\n",
              "      <td id=\"T_02857_row381_col11\" class=\"data row381 col11\" >0.2594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row382\" class=\"row_heading level0 row382\" >382</th>\n",
              "      <td id=\"T_02857_row382_col0\" class=\"data row382 col0\" >all</td>\n",
              "      <td id=\"T_02857_row382_col1\" class=\"data row382 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row382_col2\" class=\"data row382 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row382_col3\" class=\"data row382 col3\" >0.6708</td>\n",
              "      <td id=\"T_02857_row382_col4\" class=\"data row382 col4\" >0.7654</td>\n",
              "      <td id=\"T_02857_row382_col5\" class=\"data row382 col5\" >0.6504</td>\n",
              "      <td id=\"T_02857_row382_col6\" class=\"data row382 col6\" >0.6692</td>\n",
              "      <td id=\"T_02857_row382_col7\" class=\"data row382 col7\" >0.6108</td>\n",
              "      <td id=\"T_02857_row382_col8\" class=\"data row382 col8\" >0.4914</td>\n",
              "      <td id=\"T_02857_row382_col9\" class=\"data row382 col9\" >0.3676</td>\n",
              "      <td id=\"T_02857_row382_col10\" class=\"data row382 col10\" >0.3752</td>\n",
              "      <td id=\"T_02857_row382_col11\" class=\"data row382 col11\" >0.3551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row383\" class=\"row_heading level0 row383\" >383</th>\n",
              "      <td id=\"T_02857_row383_col0\" class=\"data row383 col0\" >all</td>\n",
              "      <td id=\"T_02857_row383_col1\" class=\"data row383 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row383_col2\" class=\"data row383 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row383_col3\" class=\"data row383 col3\" >0.6708</td>\n",
              "      <td id=\"T_02857_row383_col4\" class=\"data row383 col4\" >0.7654</td>\n",
              "      <td id=\"T_02857_row383_col5\" class=\"data row383 col5\" >0.6504</td>\n",
              "      <td id=\"T_02857_row383_col6\" class=\"data row383 col6\" >0.6692</td>\n",
              "      <td id=\"T_02857_row383_col7\" class=\"data row383 col7\" >0.6108</td>\n",
              "      <td id=\"T_02857_row383_col8\" class=\"data row383 col8\" >0.4914</td>\n",
              "      <td id=\"T_02857_row383_col9\" class=\"data row383 col9\" >0.3676</td>\n",
              "      <td id=\"T_02857_row383_col10\" class=\"data row383 col10\" >0.3752</td>\n",
              "      <td id=\"T_02857_row383_col11\" class=\"data row383 col11\" >0.3551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row384\" class=\"row_heading level0 row384\" >384</th>\n",
              "      <td id=\"T_02857_row384_col0\" class=\"data row384 col0\" >all</td>\n",
              "      <td id=\"T_02857_row384_col1\" class=\"data row384 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row384_col2\" class=\"data row384 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row384_col3\" class=\"data row384 col3\" >0.7367</td>\n",
              "      <td id=\"T_02857_row384_col4\" class=\"data row384 col4\" >0.8562</td>\n",
              "      <td id=\"T_02857_row384_col5\" class=\"data row384 col5\" >0.7667</td>\n",
              "      <td id=\"T_02857_row384_col6\" class=\"data row384 col6\" >0.7370</td>\n",
              "      <td id=\"T_02857_row384_col7\" class=\"data row384 col7\" >0.7020</td>\n",
              "      <td id=\"T_02857_row384_col8\" class=\"data row384 col8\" >0.3418</td>\n",
              "      <td id=\"T_02857_row384_col9\" class=\"data row384 col9\" >0.3600</td>\n",
              "      <td id=\"T_02857_row384_col10\" class=\"data row384 col10\" >0.3621</td>\n",
              "      <td id=\"T_02857_row384_col11\" class=\"data row384 col11\" >0.4171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row385\" class=\"row_heading level0 row385\" >385</th>\n",
              "      <td id=\"T_02857_row385_col0\" class=\"data row385 col0\" >all</td>\n",
              "      <td id=\"T_02857_row385_col1\" class=\"data row385 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row385_col2\" class=\"data row385 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row385_col3\" class=\"data row385 col3\" >0.7367</td>\n",
              "      <td id=\"T_02857_row385_col4\" class=\"data row385 col4\" >0.8562</td>\n",
              "      <td id=\"T_02857_row385_col5\" class=\"data row385 col5\" >0.7667</td>\n",
              "      <td id=\"T_02857_row385_col6\" class=\"data row385 col6\" >0.7370</td>\n",
              "      <td id=\"T_02857_row385_col7\" class=\"data row385 col7\" >0.7020</td>\n",
              "      <td id=\"T_02857_row385_col8\" class=\"data row385 col8\" >0.3418</td>\n",
              "      <td id=\"T_02857_row385_col9\" class=\"data row385 col9\" >0.3600</td>\n",
              "      <td id=\"T_02857_row385_col10\" class=\"data row385 col10\" >0.3621</td>\n",
              "      <td id=\"T_02857_row385_col11\" class=\"data row385 col11\" >0.4171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row386\" class=\"row_heading level0 row386\" >386</th>\n",
              "      <td id=\"T_02857_row386_col0\" class=\"data row386 col0\" >all</td>\n",
              "      <td id=\"T_02857_row386_col1\" class=\"data row386 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row386_col2\" class=\"data row386 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row386_col3\" class=\"data row386 col3\" >0.9263</td>\n",
              "      <td id=\"T_02857_row386_col4\" class=\"data row386 col4\" >0.9247</td>\n",
              "      <td id=\"T_02857_row386_col5\" class=\"data row386 col5\" >0.8518</td>\n",
              "      <td id=\"T_02857_row386_col6\" class=\"data row386 col6\" >0.8265</td>\n",
              "      <td id=\"T_02857_row386_col7\" class=\"data row386 col7\" >0.7857</td>\n",
              "      <td id=\"T_02857_row386_col8\" class=\"data row386 col8\" >0.3370</td>\n",
              "      <td id=\"T_02857_row386_col9\" class=\"data row386 col9\" >0.3614</td>\n",
              "      <td id=\"T_02857_row386_col10\" class=\"data row386 col10\" >0.3454</td>\n",
              "      <td id=\"T_02857_row386_col11\" class=\"data row386 col11\" >0.5011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row387\" class=\"row_heading level0 row387\" >387</th>\n",
              "      <td id=\"T_02857_row387_col0\" class=\"data row387 col0\" >all</td>\n",
              "      <td id=\"T_02857_row387_col1\" class=\"data row387 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row387_col2\" class=\"data row387 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row387_col3\" class=\"data row387 col3\" >0.9263</td>\n",
              "      <td id=\"T_02857_row387_col4\" class=\"data row387 col4\" >0.9247</td>\n",
              "      <td id=\"T_02857_row387_col5\" class=\"data row387 col5\" >0.8518</td>\n",
              "      <td id=\"T_02857_row387_col6\" class=\"data row387 col6\" >0.8265</td>\n",
              "      <td id=\"T_02857_row387_col7\" class=\"data row387 col7\" >0.7857</td>\n",
              "      <td id=\"T_02857_row387_col8\" class=\"data row387 col8\" >0.3370</td>\n",
              "      <td id=\"T_02857_row387_col9\" class=\"data row387 col9\" >0.3614</td>\n",
              "      <td id=\"T_02857_row387_col10\" class=\"data row387 col10\" >0.3454</td>\n",
              "      <td id=\"T_02857_row387_col11\" class=\"data row387 col11\" >0.5011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row388\" class=\"row_heading level0 row388\" >388</th>\n",
              "      <td id=\"T_02857_row388_col0\" class=\"data row388 col0\" >all</td>\n",
              "      <td id=\"T_02857_row388_col1\" class=\"data row388 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row388_col2\" class=\"data row388 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row388_col3\" class=\"data row388 col3\" >0.6609</td>\n",
              "      <td id=\"T_02857_row388_col4\" class=\"data row388 col4\" >0.7375</td>\n",
              "      <td id=\"T_02857_row388_col5\" class=\"data row388 col5\" >0.6515</td>\n",
              "      <td id=\"T_02857_row388_col6\" class=\"data row388 col6\" >0.6580</td>\n",
              "      <td id=\"T_02857_row388_col7\" class=\"data row388 col7\" >0.6131</td>\n",
              "      <td id=\"T_02857_row388_col8\" class=\"data row388 col8\" >0.3795</td>\n",
              "      <td id=\"T_02857_row388_col9\" class=\"data row388 col9\" >0.3652</td>\n",
              "      <td id=\"T_02857_row388_col10\" class=\"data row388 col10\" >0.3762</td>\n",
              "      <td id=\"T_02857_row388_col11\" class=\"data row388 col11\" >0.3388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row389\" class=\"row_heading level0 row389\" >389</th>\n",
              "      <td id=\"T_02857_row389_col0\" class=\"data row389 col0\" >all</td>\n",
              "      <td id=\"T_02857_row389_col1\" class=\"data row389 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row389_col2\" class=\"data row389 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row389_col3\" class=\"data row389 col3\" >0.6609</td>\n",
              "      <td id=\"T_02857_row389_col4\" class=\"data row389 col4\" >0.7375</td>\n",
              "      <td id=\"T_02857_row389_col5\" class=\"data row389 col5\" >0.6515</td>\n",
              "      <td id=\"T_02857_row389_col6\" class=\"data row389 col6\" >0.6580</td>\n",
              "      <td id=\"T_02857_row389_col7\" class=\"data row389 col7\" >0.6131</td>\n",
              "      <td id=\"T_02857_row389_col8\" class=\"data row389 col8\" >0.3795</td>\n",
              "      <td id=\"T_02857_row389_col9\" class=\"data row389 col9\" >0.3652</td>\n",
              "      <td id=\"T_02857_row389_col10\" class=\"data row389 col10\" >0.3762</td>\n",
              "      <td id=\"T_02857_row389_col11\" class=\"data row389 col11\" >0.3388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row390\" class=\"row_heading level0 row390\" >390</th>\n",
              "      <td id=\"T_02857_row390_col0\" class=\"data row390 col0\" >all</td>\n",
              "      <td id=\"T_02857_row390_col1\" class=\"data row390 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row390_col2\" class=\"data row390 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row390_col3\" class=\"data row390 col3\" >0.9394</td>\n",
              "      <td id=\"T_02857_row390_col4\" class=\"data row390 col4\" >0.9673</td>\n",
              "      <td id=\"T_02857_row390_col5\" class=\"data row390 col5\" >0.8759</td>\n",
              "      <td id=\"T_02857_row390_col6\" class=\"data row390 col6\" >0.8386</td>\n",
              "      <td id=\"T_02857_row390_col7\" class=\"data row390 col7\" >0.7625</td>\n",
              "      <td id=\"T_02857_row390_col8\" class=\"data row390 col8\" >0.3582</td>\n",
              "      <td id=\"T_02857_row390_col9\" class=\"data row390 col9\" >0.3599</td>\n",
              "      <td id=\"T_02857_row390_col10\" class=\"data row390 col10\" >0.3587</td>\n",
              "      <td id=\"T_02857_row390_col11\" class=\"data row390 col11\" >0.4940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row391\" class=\"row_heading level0 row391\" >391</th>\n",
              "      <td id=\"T_02857_row391_col0\" class=\"data row391 col0\" >all</td>\n",
              "      <td id=\"T_02857_row391_col1\" class=\"data row391 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row391_col2\" class=\"data row391 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row391_col3\" class=\"data row391 col3\" >0.9394</td>\n",
              "      <td id=\"T_02857_row391_col4\" class=\"data row391 col4\" >0.9673</td>\n",
              "      <td id=\"T_02857_row391_col5\" class=\"data row391 col5\" >0.8759</td>\n",
              "      <td id=\"T_02857_row391_col6\" class=\"data row391 col6\" >0.8386</td>\n",
              "      <td id=\"T_02857_row391_col7\" class=\"data row391 col7\" >0.7625</td>\n",
              "      <td id=\"T_02857_row391_col8\" class=\"data row391 col8\" >0.3582</td>\n",
              "      <td id=\"T_02857_row391_col9\" class=\"data row391 col9\" >0.3599</td>\n",
              "      <td id=\"T_02857_row391_col10\" class=\"data row391 col10\" >0.3587</td>\n",
              "      <td id=\"T_02857_row391_col11\" class=\"data row391 col11\" >0.4940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row392\" class=\"row_heading level0 row392\" >392</th>\n",
              "      <td id=\"T_02857_row392_col0\" class=\"data row392 col0\" >all</td>\n",
              "      <td id=\"T_02857_row392_col1\" class=\"data row392 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row392_col2\" class=\"data row392 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row392_col3\" class=\"data row392 col3\" >0.9464</td>\n",
              "      <td id=\"T_02857_row392_col4\" class=\"data row392 col4\" >0.9277</td>\n",
              "      <td id=\"T_02857_row392_col5\" class=\"data row392 col5\" >0.8474</td>\n",
              "      <td id=\"T_02857_row392_col6\" class=\"data row392 col6\" >0.8366</td>\n",
              "      <td id=\"T_02857_row392_col7\" class=\"data row392 col7\" >0.7401</td>\n",
              "      <td id=\"T_02857_row392_col8\" class=\"data row392 col8\" >0.3555</td>\n",
              "      <td id=\"T_02857_row392_col9\" class=\"data row392 col9\" >0.3777</td>\n",
              "      <td id=\"T_02857_row392_col10\" class=\"data row392 col10\" >0.3681</td>\n",
              "      <td id=\"T_02857_row392_col11\" class=\"data row392 col11\" >0.4974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row393\" class=\"row_heading level0 row393\" >393</th>\n",
              "      <td id=\"T_02857_row393_col0\" class=\"data row393 col0\" >all</td>\n",
              "      <td id=\"T_02857_row393_col1\" class=\"data row393 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row393_col2\" class=\"data row393 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row393_col3\" class=\"data row393 col3\" >0.9464</td>\n",
              "      <td id=\"T_02857_row393_col4\" class=\"data row393 col4\" >0.9277</td>\n",
              "      <td id=\"T_02857_row393_col5\" class=\"data row393 col5\" >0.8474</td>\n",
              "      <td id=\"T_02857_row393_col6\" class=\"data row393 col6\" >0.8366</td>\n",
              "      <td id=\"T_02857_row393_col7\" class=\"data row393 col7\" >0.7401</td>\n",
              "      <td id=\"T_02857_row393_col8\" class=\"data row393 col8\" >0.3555</td>\n",
              "      <td id=\"T_02857_row393_col9\" class=\"data row393 col9\" >0.3777</td>\n",
              "      <td id=\"T_02857_row393_col10\" class=\"data row393 col10\" >0.3681</td>\n",
              "      <td id=\"T_02857_row393_col11\" class=\"data row393 col11\" >0.4974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row394\" class=\"row_heading level0 row394\" >394</th>\n",
              "      <td id=\"T_02857_row394_col0\" class=\"data row394 col0\" >all</td>\n",
              "      <td id=\"T_02857_row394_col1\" class=\"data row394 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row394_col2\" class=\"data row394 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row394_col3\" class=\"data row394 col3\" >0.6665</td>\n",
              "      <td id=\"T_02857_row394_col4\" class=\"data row394 col4\" >0.6454</td>\n",
              "      <td id=\"T_02857_row394_col5\" class=\"data row394 col5\" >0.5948</td>\n",
              "      <td id=\"T_02857_row394_col6\" class=\"data row394 col6\" >0.6315</td>\n",
              "      <td id=\"T_02857_row394_col7\" class=\"data row394 col7\" >0.5565</td>\n",
              "      <td id=\"T_02857_row394_col8\" class=\"data row394 col8\" >0.3799</td>\n",
              "      <td id=\"T_02857_row394_col9\" class=\"data row394 col9\" >0.3737</td>\n",
              "      <td id=\"T_02857_row394_col10\" class=\"data row394 col10\" >0.3731</td>\n",
              "      <td id=\"T_02857_row394_col11\" class=\"data row394 col11\" >0.3035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row395\" class=\"row_heading level0 row395\" >395</th>\n",
              "      <td id=\"T_02857_row395_col0\" class=\"data row395 col0\" >all</td>\n",
              "      <td id=\"T_02857_row395_col1\" class=\"data row395 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row395_col2\" class=\"data row395 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row395_col3\" class=\"data row395 col3\" >0.6665</td>\n",
              "      <td id=\"T_02857_row395_col4\" class=\"data row395 col4\" >0.6454</td>\n",
              "      <td id=\"T_02857_row395_col5\" class=\"data row395 col5\" >0.5948</td>\n",
              "      <td id=\"T_02857_row395_col6\" class=\"data row395 col6\" >0.6315</td>\n",
              "      <td id=\"T_02857_row395_col7\" class=\"data row395 col7\" >0.5565</td>\n",
              "      <td id=\"T_02857_row395_col8\" class=\"data row395 col8\" >0.3799</td>\n",
              "      <td id=\"T_02857_row395_col9\" class=\"data row395 col9\" >0.3737</td>\n",
              "      <td id=\"T_02857_row395_col10\" class=\"data row395 col10\" >0.3731</td>\n",
              "      <td id=\"T_02857_row395_col11\" class=\"data row395 col11\" >0.3035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row396\" class=\"row_heading level0 row396\" >396</th>\n",
              "      <td id=\"T_02857_row396_col0\" class=\"data row396 col0\" >all</td>\n",
              "      <td id=\"T_02857_row396_col1\" class=\"data row396 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row396_col2\" class=\"data row396 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row396_col3\" class=\"data row396 col3\" >0.7867</td>\n",
              "      <td id=\"T_02857_row396_col4\" class=\"data row396 col4\" >0.7226</td>\n",
              "      <td id=\"T_02857_row396_col5\" class=\"data row396 col5\" >0.5813</td>\n",
              "      <td id=\"T_02857_row396_col6\" class=\"data row396 col6\" >0.6552</td>\n",
              "      <td id=\"T_02857_row396_col7\" class=\"data row396 col7\" >0.5793</td>\n",
              "      <td id=\"T_02857_row396_col8\" class=\"data row396 col8\" >0.5005</td>\n",
              "      <td id=\"T_02857_row396_col9\" class=\"data row396 col9\" >0.4103</td>\n",
              "      <td id=\"T_02857_row396_col10\" class=\"data row396 col10\" >0.4177</td>\n",
              "      <td id=\"T_02857_row396_col11\" class=\"data row396 col11\" >0.3669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row397\" class=\"row_heading level0 row397\" >397</th>\n",
              "      <td id=\"T_02857_row397_col0\" class=\"data row397 col0\" >all</td>\n",
              "      <td id=\"T_02857_row397_col1\" class=\"data row397 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row397_col2\" class=\"data row397 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row397_col3\" class=\"data row397 col3\" >0.7867</td>\n",
              "      <td id=\"T_02857_row397_col4\" class=\"data row397 col4\" >0.7226</td>\n",
              "      <td id=\"T_02857_row397_col5\" class=\"data row397 col5\" >0.5813</td>\n",
              "      <td id=\"T_02857_row397_col6\" class=\"data row397 col6\" >0.6552</td>\n",
              "      <td id=\"T_02857_row397_col7\" class=\"data row397 col7\" >0.5793</td>\n",
              "      <td id=\"T_02857_row397_col8\" class=\"data row397 col8\" >0.5005</td>\n",
              "      <td id=\"T_02857_row397_col9\" class=\"data row397 col9\" >0.4103</td>\n",
              "      <td id=\"T_02857_row397_col10\" class=\"data row397 col10\" >0.4177</td>\n",
              "      <td id=\"T_02857_row397_col11\" class=\"data row397 col11\" >0.3669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row398\" class=\"row_heading level0 row398\" >398</th>\n",
              "      <td id=\"T_02857_row398_col0\" class=\"data row398 col0\" >all</td>\n",
              "      <td id=\"T_02857_row398_col1\" class=\"data row398 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row398_col2\" class=\"data row398 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row398_col3\" class=\"data row398 col3\" >0.8784</td>\n",
              "      <td id=\"T_02857_row398_col4\" class=\"data row398 col4\" >0.8711</td>\n",
              "      <td id=\"T_02857_row398_col5\" class=\"data row398 col5\" >0.7934</td>\n",
              "      <td id=\"T_02857_row398_col6\" class=\"data row398 col6\" >0.7802</td>\n",
              "      <td id=\"T_02857_row398_col7\" class=\"data row398 col7\" >0.6971</td>\n",
              "      <td id=\"T_02857_row398_col8\" class=\"data row398 col8\" >0.3527</td>\n",
              "      <td id=\"T_02857_row398_col9\" class=\"data row398 col9\" >0.3851</td>\n",
              "      <td id=\"T_02857_row398_col10\" class=\"data row398 col10\" >0.3829</td>\n",
              "      <td id=\"T_02857_row398_col11\" class=\"data row398 col11\" >0.4529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row399\" class=\"row_heading level0 row399\" >399</th>\n",
              "      <td id=\"T_02857_row399_col0\" class=\"data row399 col0\" >all</td>\n",
              "      <td id=\"T_02857_row399_col1\" class=\"data row399 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row399_col2\" class=\"data row399 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row399_col3\" class=\"data row399 col3\" >0.8784</td>\n",
              "      <td id=\"T_02857_row399_col4\" class=\"data row399 col4\" >0.8711</td>\n",
              "      <td id=\"T_02857_row399_col5\" class=\"data row399 col5\" >0.7934</td>\n",
              "      <td id=\"T_02857_row399_col6\" class=\"data row399 col6\" >0.7802</td>\n",
              "      <td id=\"T_02857_row399_col7\" class=\"data row399 col7\" >0.6971</td>\n",
              "      <td id=\"T_02857_row399_col8\" class=\"data row399 col8\" >0.3527</td>\n",
              "      <td id=\"T_02857_row399_col9\" class=\"data row399 col9\" >0.3851</td>\n",
              "      <td id=\"T_02857_row399_col10\" class=\"data row399 col10\" >0.3829</td>\n",
              "      <td id=\"T_02857_row399_col11\" class=\"data row399 col11\" >0.4529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row400\" class=\"row_heading level0 row400\" >400</th>\n",
              "      <td id=\"T_02857_row400_col0\" class=\"data row400 col0\" >all</td>\n",
              "      <td id=\"T_02857_row400_col1\" class=\"data row400 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row400_col2\" class=\"data row400 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row400_col3\" class=\"data row400 col3\" >0.9443</td>\n",
              "      <td id=\"T_02857_row400_col4\" class=\"data row400 col4\" >0.9295</td>\n",
              "      <td id=\"T_02857_row400_col5\" class=\"data row400 col5\" >0.8388</td>\n",
              "      <td id=\"T_02857_row400_col6\" class=\"data row400 col6\" >0.8106</td>\n",
              "      <td id=\"T_02857_row400_col7\" class=\"data row400 col7\" >0.7345</td>\n",
              "      <td id=\"T_02857_row400_col8\" class=\"data row400 col8\" >0.3415</td>\n",
              "      <td id=\"T_02857_row400_col9\" class=\"data row400 col9\" >0.3562</td>\n",
              "      <td id=\"T_02857_row400_col10\" class=\"data row400 col10\" >0.3526</td>\n",
              "      <td id=\"T_02857_row400_col11\" class=\"data row400 col11\" >0.4868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row401\" class=\"row_heading level0 row401\" >401</th>\n",
              "      <td id=\"T_02857_row401_col0\" class=\"data row401 col0\" >all</td>\n",
              "      <td id=\"T_02857_row401_col1\" class=\"data row401 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row401_col2\" class=\"data row401 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row401_col3\" class=\"data row401 col3\" >0.9443</td>\n",
              "      <td id=\"T_02857_row401_col4\" class=\"data row401 col4\" >0.9295</td>\n",
              "      <td id=\"T_02857_row401_col5\" class=\"data row401 col5\" >0.8388</td>\n",
              "      <td id=\"T_02857_row401_col6\" class=\"data row401 col6\" >0.8106</td>\n",
              "      <td id=\"T_02857_row401_col7\" class=\"data row401 col7\" >0.7345</td>\n",
              "      <td id=\"T_02857_row401_col8\" class=\"data row401 col8\" >0.3415</td>\n",
              "      <td id=\"T_02857_row401_col9\" class=\"data row401 col9\" >0.3562</td>\n",
              "      <td id=\"T_02857_row401_col10\" class=\"data row401 col10\" >0.3526</td>\n",
              "      <td id=\"T_02857_row401_col11\" class=\"data row401 col11\" >0.4868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row402\" class=\"row_heading level0 row402\" >402</th>\n",
              "      <td id=\"T_02857_row402_col0\" class=\"data row402 col0\" >all</td>\n",
              "      <td id=\"T_02857_row402_col1\" class=\"data row402 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row402_col2\" class=\"data row402 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row402_col3\" class=\"data row402 col3\" >0.8435</td>\n",
              "      <td id=\"T_02857_row402_col4\" class=\"data row402 col4\" >0.8119</td>\n",
              "      <td id=\"T_02857_row402_col5\" class=\"data row402 col5\" >0.7343</td>\n",
              "      <td id=\"T_02857_row402_col6\" class=\"data row402 col6\" >0.7385</td>\n",
              "      <td id=\"T_02857_row402_col7\" class=\"data row402 col7\" >0.6519</td>\n",
              "      <td id=\"T_02857_row402_col8\" class=\"data row402 col8\" >0.3896</td>\n",
              "      <td id=\"T_02857_row402_col9\" class=\"data row402 col9\" >0.3797</td>\n",
              "      <td id=\"T_02857_row402_col10\" class=\"data row402 col10\" >0.4037</td>\n",
              "      <td id=\"T_02857_row402_col11\" class=\"data row402 col11\" >0.4219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row403\" class=\"row_heading level0 row403\" >403</th>\n",
              "      <td id=\"T_02857_row403_col0\" class=\"data row403 col0\" >all</td>\n",
              "      <td id=\"T_02857_row403_col1\" class=\"data row403 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row403_col2\" class=\"data row403 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row403_col3\" class=\"data row403 col3\" >0.8435</td>\n",
              "      <td id=\"T_02857_row403_col4\" class=\"data row403 col4\" >0.8119</td>\n",
              "      <td id=\"T_02857_row403_col5\" class=\"data row403 col5\" >0.7343</td>\n",
              "      <td id=\"T_02857_row403_col6\" class=\"data row403 col6\" >0.7385</td>\n",
              "      <td id=\"T_02857_row403_col7\" class=\"data row403 col7\" >0.6519</td>\n",
              "      <td id=\"T_02857_row403_col8\" class=\"data row403 col8\" >0.3896</td>\n",
              "      <td id=\"T_02857_row403_col9\" class=\"data row403 col9\" >0.3797</td>\n",
              "      <td id=\"T_02857_row403_col10\" class=\"data row403 col10\" >0.4037</td>\n",
              "      <td id=\"T_02857_row403_col11\" class=\"data row403 col11\" >0.4219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row404\" class=\"row_heading level0 row404\" >404</th>\n",
              "      <td id=\"T_02857_row404_col0\" class=\"data row404 col0\" >all</td>\n",
              "      <td id=\"T_02857_row404_col1\" class=\"data row404 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row404_col2\" class=\"data row404 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row404_col3\" class=\"data row404 col3\" >0.9734</td>\n",
              "      <td id=\"T_02857_row404_col4\" class=\"data row404 col4\" >0.9532</td>\n",
              "      <td id=\"T_02857_row404_col5\" class=\"data row404 col5\" >0.8614</td>\n",
              "      <td id=\"T_02857_row404_col6\" class=\"data row404 col6\" >0.8381</td>\n",
              "      <td id=\"T_02857_row404_col7\" class=\"data row404 col7\" >0.7428</td>\n",
              "      <td id=\"T_02857_row404_col8\" class=\"data row404 col8\" >0.3503</td>\n",
              "      <td id=\"T_02857_row404_col9\" class=\"data row404 col9\" >0.3636</td>\n",
              "      <td id=\"T_02857_row404_col10\" class=\"data row404 col10\" >0.3560</td>\n",
              "      <td id=\"T_02857_row404_col11\" class=\"data row404 col11\" >0.4949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row405\" class=\"row_heading level0 row405\" >405</th>\n",
              "      <td id=\"T_02857_row405_col0\" class=\"data row405 col0\" >all</td>\n",
              "      <td id=\"T_02857_row405_col1\" class=\"data row405 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row405_col2\" class=\"data row405 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row405_col3\" class=\"data row405 col3\" >0.9734</td>\n",
              "      <td id=\"T_02857_row405_col4\" class=\"data row405 col4\" >0.9532</td>\n",
              "      <td id=\"T_02857_row405_col5\" class=\"data row405 col5\" >0.8614</td>\n",
              "      <td id=\"T_02857_row405_col6\" class=\"data row405 col6\" >0.8381</td>\n",
              "      <td id=\"T_02857_row405_col7\" class=\"data row405 col7\" >0.7428</td>\n",
              "      <td id=\"T_02857_row405_col8\" class=\"data row405 col8\" >0.3503</td>\n",
              "      <td id=\"T_02857_row405_col9\" class=\"data row405 col9\" >0.3636</td>\n",
              "      <td id=\"T_02857_row405_col10\" class=\"data row405 col10\" >0.3560</td>\n",
              "      <td id=\"T_02857_row405_col11\" class=\"data row405 col11\" >0.4949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row406\" class=\"row_heading level0 row406\" >406</th>\n",
              "      <td id=\"T_02857_row406_col0\" class=\"data row406 col0\" >all</td>\n",
              "      <td id=\"T_02857_row406_col1\" class=\"data row406 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row406_col2\" class=\"data row406 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row406_col3\" class=\"data row406 col3\" >0.5227</td>\n",
              "      <td id=\"T_02857_row406_col4\" class=\"data row406 col4\" >0.3974</td>\n",
              "      <td id=\"T_02857_row406_col5\" class=\"data row406 col5\" >0.3335</td>\n",
              "      <td id=\"T_02857_row406_col6\" class=\"data row406 col6\" >0.5261</td>\n",
              "      <td id=\"T_02857_row406_col7\" class=\"data row406 col7\" >0.4534</td>\n",
              "      <td id=\"T_02857_row406_col8\" class=\"data row406 col8\" >0.7655</td>\n",
              "      <td id=\"T_02857_row406_col9\" class=\"data row406 col9\" >0.8754</td>\n",
              "      <td id=\"T_02857_row406_col10\" class=\"data row406 col10\" >0.7091</td>\n",
              "      <td id=\"T_02857_row406_col11\" class=\"data row406 col11\" >0.4114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row407\" class=\"row_heading level0 row407\" >407</th>\n",
              "      <td id=\"T_02857_row407_col0\" class=\"data row407 col0\" >all</td>\n",
              "      <td id=\"T_02857_row407_col1\" class=\"data row407 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row407_col2\" class=\"data row407 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row407_col3\" class=\"data row407 col3\" >0.5227</td>\n",
              "      <td id=\"T_02857_row407_col4\" class=\"data row407 col4\" >0.3974</td>\n",
              "      <td id=\"T_02857_row407_col5\" class=\"data row407 col5\" >0.3335</td>\n",
              "      <td id=\"T_02857_row407_col6\" class=\"data row407 col6\" >0.5261</td>\n",
              "      <td id=\"T_02857_row407_col7\" class=\"data row407 col7\" >0.4534</td>\n",
              "      <td id=\"T_02857_row407_col8\" class=\"data row407 col8\" >0.7655</td>\n",
              "      <td id=\"T_02857_row407_col9\" class=\"data row407 col9\" >0.8754</td>\n",
              "      <td id=\"T_02857_row407_col10\" class=\"data row407 col10\" >0.7091</td>\n",
              "      <td id=\"T_02857_row407_col11\" class=\"data row407 col11\" >0.4114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row408\" class=\"row_heading level0 row408\" >408</th>\n",
              "      <td id=\"T_02857_row408_col0\" class=\"data row408 col0\" >all</td>\n",
              "      <td id=\"T_02857_row408_col1\" class=\"data row408 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row408_col2\" class=\"data row408 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row408_col3\" class=\"data row408 col3\" >0.4035</td>\n",
              "      <td id=\"T_02857_row408_col4\" class=\"data row408 col4\" >0.3768</td>\n",
              "      <td id=\"T_02857_row408_col5\" class=\"data row408 col5\" >0.3744</td>\n",
              "      <td id=\"T_02857_row408_col6\" class=\"data row408 col6\" >0.4771</td>\n",
              "      <td id=\"T_02857_row408_col7\" class=\"data row408 col7\" >0.4733</td>\n",
              "      <td id=\"T_02857_row408_col8\" class=\"data row408 col8\" >0.6337</td>\n",
              "      <td id=\"T_02857_row408_col9\" class=\"data row408 col9\" >0.8947</td>\n",
              "      <td id=\"T_02857_row408_col10\" class=\"data row408 col10\" >0.5945</td>\n",
              "      <td id=\"T_02857_row408_col11\" class=\"data row408 col11\" >0.3311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row409\" class=\"row_heading level0 row409\" >409</th>\n",
              "      <td id=\"T_02857_row409_col0\" class=\"data row409 col0\" >all</td>\n",
              "      <td id=\"T_02857_row409_col1\" class=\"data row409 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row409_col2\" class=\"data row409 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row409_col3\" class=\"data row409 col3\" >0.4035</td>\n",
              "      <td id=\"T_02857_row409_col4\" class=\"data row409 col4\" >0.3768</td>\n",
              "      <td id=\"T_02857_row409_col5\" class=\"data row409 col5\" >0.3744</td>\n",
              "      <td id=\"T_02857_row409_col6\" class=\"data row409 col6\" >0.4771</td>\n",
              "      <td id=\"T_02857_row409_col7\" class=\"data row409 col7\" >0.4733</td>\n",
              "      <td id=\"T_02857_row409_col8\" class=\"data row409 col8\" >0.6337</td>\n",
              "      <td id=\"T_02857_row409_col9\" class=\"data row409 col9\" >0.8947</td>\n",
              "      <td id=\"T_02857_row409_col10\" class=\"data row409 col10\" >0.5945</td>\n",
              "      <td id=\"T_02857_row409_col11\" class=\"data row409 col11\" >0.3311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row410\" class=\"row_heading level0 row410\" >410</th>\n",
              "      <td id=\"T_02857_row410_col0\" class=\"data row410 col0\" >all</td>\n",
              "      <td id=\"T_02857_row410_col1\" class=\"data row410 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row410_col2\" class=\"data row410 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row410_col3\" class=\"data row410 col3\" >0.4348</td>\n",
              "      <td id=\"T_02857_row410_col4\" class=\"data row410 col4\" >0.4280</td>\n",
              "      <td id=\"T_02857_row410_col5\" class=\"data row410 col5\" >0.4067</td>\n",
              "      <td id=\"T_02857_row410_col6\" class=\"data row410 col6\" >0.4871</td>\n",
              "      <td id=\"T_02857_row410_col7\" class=\"data row410 col7\" >0.4811</td>\n",
              "      <td id=\"T_02857_row410_col8\" class=\"data row410 col8\" >0.6822</td>\n",
              "      <td id=\"T_02857_row410_col9\" class=\"data row410 col9\" >0.9044</td>\n",
              "      <td id=\"T_02857_row410_col10\" class=\"data row410 col10\" >0.6296</td>\n",
              "      <td id=\"T_02857_row410_col11\" class=\"data row410 col11\" >0.3660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row411\" class=\"row_heading level0 row411\" >411</th>\n",
              "      <td id=\"T_02857_row411_col0\" class=\"data row411 col0\" >all</td>\n",
              "      <td id=\"T_02857_row411_col1\" class=\"data row411 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row411_col2\" class=\"data row411 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row411_col3\" class=\"data row411 col3\" >0.4348</td>\n",
              "      <td id=\"T_02857_row411_col4\" class=\"data row411 col4\" >0.4280</td>\n",
              "      <td id=\"T_02857_row411_col5\" class=\"data row411 col5\" >0.4067</td>\n",
              "      <td id=\"T_02857_row411_col6\" class=\"data row411 col6\" >0.4871</td>\n",
              "      <td id=\"T_02857_row411_col7\" class=\"data row411 col7\" >0.4811</td>\n",
              "      <td id=\"T_02857_row411_col8\" class=\"data row411 col8\" >0.6822</td>\n",
              "      <td id=\"T_02857_row411_col9\" class=\"data row411 col9\" >0.9044</td>\n",
              "      <td id=\"T_02857_row411_col10\" class=\"data row411 col10\" >0.6296</td>\n",
              "      <td id=\"T_02857_row411_col11\" class=\"data row411 col11\" >0.3660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row412\" class=\"row_heading level0 row412\" >412</th>\n",
              "      <td id=\"T_02857_row412_col0\" class=\"data row412 col0\" >all</td>\n",
              "      <td id=\"T_02857_row412_col1\" class=\"data row412 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row412_col2\" class=\"data row412 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row412_col3\" class=\"data row412 col3\" >0.5578</td>\n",
              "      <td id=\"T_02857_row412_col4\" class=\"data row412 col4\" >0.4820</td>\n",
              "      <td id=\"T_02857_row412_col5\" class=\"data row412 col5\" >0.4148</td>\n",
              "      <td id=\"T_02857_row412_col6\" class=\"data row412 col6\" >0.5935</td>\n",
              "      <td id=\"T_02857_row412_col7\" class=\"data row412 col7\" >0.5154</td>\n",
              "      <td id=\"T_02857_row412_col8\" class=\"data row412 col8\" >0.7969</td>\n",
              "      <td id=\"T_02857_row412_col9\" class=\"data row412 col9\" >0.8902</td>\n",
              "      <td id=\"T_02857_row412_col10\" class=\"data row412 col10\" >0.6573</td>\n",
              "      <td id=\"T_02857_row412_col11\" class=\"data row412 col11\" >0.4418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row413\" class=\"row_heading level0 row413\" >413</th>\n",
              "      <td id=\"T_02857_row413_col0\" class=\"data row413 col0\" >all</td>\n",
              "      <td id=\"T_02857_row413_col1\" class=\"data row413 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row413_col2\" class=\"data row413 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row413_col3\" class=\"data row413 col3\" >0.5578</td>\n",
              "      <td id=\"T_02857_row413_col4\" class=\"data row413 col4\" >0.4820</td>\n",
              "      <td id=\"T_02857_row413_col5\" class=\"data row413 col5\" >0.4148</td>\n",
              "      <td id=\"T_02857_row413_col6\" class=\"data row413 col6\" >0.5935</td>\n",
              "      <td id=\"T_02857_row413_col7\" class=\"data row413 col7\" >0.5154</td>\n",
              "      <td id=\"T_02857_row413_col8\" class=\"data row413 col8\" >0.7969</td>\n",
              "      <td id=\"T_02857_row413_col9\" class=\"data row413 col9\" >0.8902</td>\n",
              "      <td id=\"T_02857_row413_col10\" class=\"data row413 col10\" >0.6573</td>\n",
              "      <td id=\"T_02857_row413_col11\" class=\"data row413 col11\" >0.4418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row414\" class=\"row_heading level0 row414\" >414</th>\n",
              "      <td id=\"T_02857_row414_col0\" class=\"data row414 col0\" >all</td>\n",
              "      <td id=\"T_02857_row414_col1\" class=\"data row414 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row414_col2\" class=\"data row414 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row414_col3\" class=\"data row414 col3\" >0.5339</td>\n",
              "      <td id=\"T_02857_row414_col4\" class=\"data row414 col4\" >0.4541</td>\n",
              "      <td id=\"T_02857_row414_col5\" class=\"data row414 col5\" >0.4823</td>\n",
              "      <td id=\"T_02857_row414_col6\" class=\"data row414 col6\" >0.6771</td>\n",
              "      <td id=\"T_02857_row414_col7\" class=\"data row414 col7\" >0.5919</td>\n",
              "      <td id=\"T_02857_row414_col8\" class=\"data row414 col8\" >0.8593</td>\n",
              "      <td id=\"T_02857_row414_col9\" class=\"data row414 col9\" >0.8710</td>\n",
              "      <td id=\"T_02857_row414_col10\" class=\"data row414 col10\" >0.7922</td>\n",
              "      <td id=\"T_02857_row414_col11\" class=\"data row414 col11\" >0.5067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row415\" class=\"row_heading level0 row415\" >415</th>\n",
              "      <td id=\"T_02857_row415_col0\" class=\"data row415 col0\" >all</td>\n",
              "      <td id=\"T_02857_row415_col1\" class=\"data row415 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row415_col2\" class=\"data row415 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row415_col3\" class=\"data row415 col3\" >0.5339</td>\n",
              "      <td id=\"T_02857_row415_col4\" class=\"data row415 col4\" >0.4541</td>\n",
              "      <td id=\"T_02857_row415_col5\" class=\"data row415 col5\" >0.4823</td>\n",
              "      <td id=\"T_02857_row415_col6\" class=\"data row415 col6\" >0.6771</td>\n",
              "      <td id=\"T_02857_row415_col7\" class=\"data row415 col7\" >0.5919</td>\n",
              "      <td id=\"T_02857_row415_col8\" class=\"data row415 col8\" >0.8593</td>\n",
              "      <td id=\"T_02857_row415_col9\" class=\"data row415 col9\" >0.8710</td>\n",
              "      <td id=\"T_02857_row415_col10\" class=\"data row415 col10\" >0.7922</td>\n",
              "      <td id=\"T_02857_row415_col11\" class=\"data row415 col11\" >0.5067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row416\" class=\"row_heading level0 row416\" >416</th>\n",
              "      <td id=\"T_02857_row416_col0\" class=\"data row416 col0\" >all</td>\n",
              "      <td id=\"T_02857_row416_col1\" class=\"data row416 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row416_col2\" class=\"data row416 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row416_col3\" class=\"data row416 col3\" >0.5905</td>\n",
              "      <td id=\"T_02857_row416_col4\" class=\"data row416 col4\" >0.5654</td>\n",
              "      <td id=\"T_02857_row416_col5\" class=\"data row416 col5\" >0.5468</td>\n",
              "      <td id=\"T_02857_row416_col6\" class=\"data row416 col6\" >0.6444</td>\n",
              "      <td id=\"T_02857_row416_col7\" class=\"data row416 col7\" >0.5565</td>\n",
              "      <td id=\"T_02857_row416_col8\" class=\"data row416 col8\" >0.7521</td>\n",
              "      <td id=\"T_02857_row416_col9\" class=\"data row416 col9\" >0.8927</td>\n",
              "      <td id=\"T_02857_row416_col10\" class=\"data row416 col10\" >0.7183</td>\n",
              "      <td id=\"T_02857_row416_col11\" class=\"data row416 col11\" >0.4813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row417\" class=\"row_heading level0 row417\" >417</th>\n",
              "      <td id=\"T_02857_row417_col0\" class=\"data row417 col0\" >all</td>\n",
              "      <td id=\"T_02857_row417_col1\" class=\"data row417 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row417_col2\" class=\"data row417 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row417_col3\" class=\"data row417 col3\" >0.5905</td>\n",
              "      <td id=\"T_02857_row417_col4\" class=\"data row417 col4\" >0.5654</td>\n",
              "      <td id=\"T_02857_row417_col5\" class=\"data row417 col5\" >0.5468</td>\n",
              "      <td id=\"T_02857_row417_col6\" class=\"data row417 col6\" >0.6444</td>\n",
              "      <td id=\"T_02857_row417_col7\" class=\"data row417 col7\" >0.5565</td>\n",
              "      <td id=\"T_02857_row417_col8\" class=\"data row417 col8\" >0.7521</td>\n",
              "      <td id=\"T_02857_row417_col9\" class=\"data row417 col9\" >0.8927</td>\n",
              "      <td id=\"T_02857_row417_col10\" class=\"data row417 col10\" >0.7183</td>\n",
              "      <td id=\"T_02857_row417_col11\" class=\"data row417 col11\" >0.4813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row418\" class=\"row_heading level0 row418\" >418</th>\n",
              "      <td id=\"T_02857_row418_col0\" class=\"data row418 col0\" >all</td>\n",
              "      <td id=\"T_02857_row418_col1\" class=\"data row418 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row418_col2\" class=\"data row418 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row418_col3\" class=\"data row418 col3\" >0.5043</td>\n",
              "      <td id=\"T_02857_row418_col4\" class=\"data row418 col4\" >0.4131</td>\n",
              "      <td id=\"T_02857_row418_col5\" class=\"data row418 col5\" >0.3973</td>\n",
              "      <td id=\"T_02857_row418_col6\" class=\"data row418 col6\" >0.5985</td>\n",
              "      <td id=\"T_02857_row418_col7\" class=\"data row418 col7\" >0.5208</td>\n",
              "      <td id=\"T_02857_row418_col8\" class=\"data row418 col8\" >0.8136</td>\n",
              "      <td id=\"T_02857_row418_col9\" class=\"data row418 col9\" >0.8507</td>\n",
              "      <td id=\"T_02857_row418_col10\" class=\"data row418 col10\" >0.7333</td>\n",
              "      <td id=\"T_02857_row418_col11\" class=\"data row418 col11\" >0.4628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row419\" class=\"row_heading level0 row419\" >419</th>\n",
              "      <td id=\"T_02857_row419_col0\" class=\"data row419 col0\" >all</td>\n",
              "      <td id=\"T_02857_row419_col1\" class=\"data row419 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row419_col2\" class=\"data row419 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row419_col3\" class=\"data row419 col3\" >0.5043</td>\n",
              "      <td id=\"T_02857_row419_col4\" class=\"data row419 col4\" >0.4131</td>\n",
              "      <td id=\"T_02857_row419_col5\" class=\"data row419 col5\" >0.3973</td>\n",
              "      <td id=\"T_02857_row419_col6\" class=\"data row419 col6\" >0.5985</td>\n",
              "      <td id=\"T_02857_row419_col7\" class=\"data row419 col7\" >0.5208</td>\n",
              "      <td id=\"T_02857_row419_col8\" class=\"data row419 col8\" >0.8136</td>\n",
              "      <td id=\"T_02857_row419_col9\" class=\"data row419 col9\" >0.8507</td>\n",
              "      <td id=\"T_02857_row419_col10\" class=\"data row419 col10\" >0.7333</td>\n",
              "      <td id=\"T_02857_row419_col11\" class=\"data row419 col11\" >0.4628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row420\" class=\"row_heading level0 row420\" >420</th>\n",
              "      <td id=\"T_02857_row420_col0\" class=\"data row420 col0\" >all</td>\n",
              "      <td id=\"T_02857_row420_col1\" class=\"data row420 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row420_col2\" class=\"data row420 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row420_col3\" class=\"data row420 col3\" >0.5236</td>\n",
              "      <td id=\"T_02857_row420_col4\" class=\"data row420 col4\" >0.4947</td>\n",
              "      <td id=\"T_02857_row420_col5\" class=\"data row420 col5\" >0.4613</td>\n",
              "      <td id=\"T_02857_row420_col6\" class=\"data row420 col6\" >0.6089</td>\n",
              "      <td id=\"T_02857_row420_col7\" class=\"data row420 col7\" >0.4959</td>\n",
              "      <td id=\"T_02857_row420_col8\" class=\"data row420 col8\" >0.8342</td>\n",
              "      <td id=\"T_02857_row420_col9\" class=\"data row420 col9\" >0.8585</td>\n",
              "      <td id=\"T_02857_row420_col10\" class=\"data row420 col10\" >0.8001</td>\n",
              "      <td id=\"T_02857_row420_col11\" class=\"data row420 col11\" >0.4517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row421\" class=\"row_heading level0 row421\" >421</th>\n",
              "      <td id=\"T_02857_row421_col0\" class=\"data row421 col0\" >all</td>\n",
              "      <td id=\"T_02857_row421_col1\" class=\"data row421 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row421_col2\" class=\"data row421 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row421_col3\" class=\"data row421 col3\" >0.5236</td>\n",
              "      <td id=\"T_02857_row421_col4\" class=\"data row421 col4\" >0.4947</td>\n",
              "      <td id=\"T_02857_row421_col5\" class=\"data row421 col5\" >0.4613</td>\n",
              "      <td id=\"T_02857_row421_col6\" class=\"data row421 col6\" >0.6089</td>\n",
              "      <td id=\"T_02857_row421_col7\" class=\"data row421 col7\" >0.4959</td>\n",
              "      <td id=\"T_02857_row421_col8\" class=\"data row421 col8\" >0.8342</td>\n",
              "      <td id=\"T_02857_row421_col9\" class=\"data row421 col9\" >0.8585</td>\n",
              "      <td id=\"T_02857_row421_col10\" class=\"data row421 col10\" >0.8001</td>\n",
              "      <td id=\"T_02857_row421_col11\" class=\"data row421 col11\" >0.4517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row422\" class=\"row_heading level0 row422\" >422</th>\n",
              "      <td id=\"T_02857_row422_col0\" class=\"data row422 col0\" >all</td>\n",
              "      <td id=\"T_02857_row422_col1\" class=\"data row422 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row422_col2\" class=\"data row422 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row422_col3\" class=\"data row422 col3\" >0.5391</td>\n",
              "      <td id=\"T_02857_row422_col4\" class=\"data row422 col4\" >0.4762</td>\n",
              "      <td id=\"T_02857_row422_col5\" class=\"data row422 col5\" >0.4728</td>\n",
              "      <td id=\"T_02857_row422_col6\" class=\"data row422 col6\" >0.5681</td>\n",
              "      <td id=\"T_02857_row422_col7\" class=\"data row422 col7\" >0.5145</td>\n",
              "      <td id=\"T_02857_row422_col8\" class=\"data row422 col8\" >0.7123</td>\n",
              "      <td id=\"T_02857_row422_col9\" class=\"data row422 col9\" >0.7580</td>\n",
              "      <td id=\"T_02857_row422_col10\" class=\"data row422 col10\" >0.6660</td>\n",
              "      <td id=\"T_02857_row422_col11\" class=\"data row422 col11\" >0.4015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row423\" class=\"row_heading level0 row423\" >423</th>\n",
              "      <td id=\"T_02857_row423_col0\" class=\"data row423 col0\" >all</td>\n",
              "      <td id=\"T_02857_row423_col1\" class=\"data row423 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row423_col2\" class=\"data row423 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row423_col3\" class=\"data row423 col3\" >0.5391</td>\n",
              "      <td id=\"T_02857_row423_col4\" class=\"data row423 col4\" >0.4762</td>\n",
              "      <td id=\"T_02857_row423_col5\" class=\"data row423 col5\" >0.4728</td>\n",
              "      <td id=\"T_02857_row423_col6\" class=\"data row423 col6\" >0.5681</td>\n",
              "      <td id=\"T_02857_row423_col7\" class=\"data row423 col7\" >0.5145</td>\n",
              "      <td id=\"T_02857_row423_col8\" class=\"data row423 col8\" >0.7123</td>\n",
              "      <td id=\"T_02857_row423_col9\" class=\"data row423 col9\" >0.7580</td>\n",
              "      <td id=\"T_02857_row423_col10\" class=\"data row423 col10\" >0.6660</td>\n",
              "      <td id=\"T_02857_row423_col11\" class=\"data row423 col11\" >0.4015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row424\" class=\"row_heading level0 row424\" >424</th>\n",
              "      <td id=\"T_02857_row424_col0\" class=\"data row424 col0\" >all</td>\n",
              "      <td id=\"T_02857_row424_col1\" class=\"data row424 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row424_col2\" class=\"data row424 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row424_col3\" class=\"data row424 col3\" >0.4618</td>\n",
              "      <td id=\"T_02857_row424_col4\" class=\"data row424 col4\" >0.5060</td>\n",
              "      <td id=\"T_02857_row424_col5\" class=\"data row424 col5\" >0.5778</td>\n",
              "      <td id=\"T_02857_row424_col6\" class=\"data row424 col6\" >0.5924</td>\n",
              "      <td id=\"T_02857_row424_col7\" class=\"data row424 col7\" >0.5363</td>\n",
              "      <td id=\"T_02857_row424_col8\" class=\"data row424 col8\" >0.7940</td>\n",
              "      <td id=\"T_02857_row424_col9\" class=\"data row424 col9\" >0.6472</td>\n",
              "      <td id=\"T_02857_row424_col10\" class=\"data row424 col10\" >0.7592</td>\n",
              "      <td id=\"T_02857_row424_col11\" class=\"data row424 col11\" >0.4313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row425\" class=\"row_heading level0 row425\" >425</th>\n",
              "      <td id=\"T_02857_row425_col0\" class=\"data row425 col0\" >all</td>\n",
              "      <td id=\"T_02857_row425_col1\" class=\"data row425 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row425_col2\" class=\"data row425 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row425_col3\" class=\"data row425 col3\" >0.4618</td>\n",
              "      <td id=\"T_02857_row425_col4\" class=\"data row425 col4\" >0.5060</td>\n",
              "      <td id=\"T_02857_row425_col5\" class=\"data row425 col5\" >0.5778</td>\n",
              "      <td id=\"T_02857_row425_col6\" class=\"data row425 col6\" >0.5924</td>\n",
              "      <td id=\"T_02857_row425_col7\" class=\"data row425 col7\" >0.5363</td>\n",
              "      <td id=\"T_02857_row425_col8\" class=\"data row425 col8\" >0.7940</td>\n",
              "      <td id=\"T_02857_row425_col9\" class=\"data row425 col9\" >0.6472</td>\n",
              "      <td id=\"T_02857_row425_col10\" class=\"data row425 col10\" >0.7592</td>\n",
              "      <td id=\"T_02857_row425_col11\" class=\"data row425 col11\" >0.4313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row426\" class=\"row_heading level0 row426\" >426</th>\n",
              "      <td id=\"T_02857_row426_col0\" class=\"data row426 col0\" >all</td>\n",
              "      <td id=\"T_02857_row426_col1\" class=\"data row426 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row426_col2\" class=\"data row426 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row426_col3\" class=\"data row426 col3\" >0.4982</td>\n",
              "      <td id=\"T_02857_row426_col4\" class=\"data row426 col4\" >0.5293</td>\n",
              "      <td id=\"T_02857_row426_col5\" class=\"data row426 col5\" >0.5907</td>\n",
              "      <td id=\"T_02857_row426_col6\" class=\"data row426 col6\" >0.7005</td>\n",
              "      <td id=\"T_02857_row426_col7\" class=\"data row426 col7\" >0.6595</td>\n",
              "      <td id=\"T_02857_row426_col8\" class=\"data row426 col8\" >0.9022</td>\n",
              "      <td id=\"T_02857_row426_col9\" class=\"data row426 col9\" >0.7957</td>\n",
              "      <td id=\"T_02857_row426_col10\" class=\"data row426 col10\" >0.8511</td>\n",
              "      <td id=\"T_02857_row426_col11\" class=\"data row426 col11\" >0.5223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row427\" class=\"row_heading level0 row427\" >427</th>\n",
              "      <td id=\"T_02857_row427_col0\" class=\"data row427 col0\" >all</td>\n",
              "      <td id=\"T_02857_row427_col1\" class=\"data row427 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row427_col2\" class=\"data row427 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row427_col3\" class=\"data row427 col3\" >0.4982</td>\n",
              "      <td id=\"T_02857_row427_col4\" class=\"data row427 col4\" >0.5293</td>\n",
              "      <td id=\"T_02857_row427_col5\" class=\"data row427 col5\" >0.5907</td>\n",
              "      <td id=\"T_02857_row427_col6\" class=\"data row427 col6\" >0.7005</td>\n",
              "      <td id=\"T_02857_row427_col7\" class=\"data row427 col7\" >0.6595</td>\n",
              "      <td id=\"T_02857_row427_col8\" class=\"data row427 col8\" >0.9022</td>\n",
              "      <td id=\"T_02857_row427_col9\" class=\"data row427 col9\" >0.7957</td>\n",
              "      <td id=\"T_02857_row427_col10\" class=\"data row427 col10\" >0.8511</td>\n",
              "      <td id=\"T_02857_row427_col11\" class=\"data row427 col11\" >0.5223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row428\" class=\"row_heading level0 row428\" >428</th>\n",
              "      <td id=\"T_02857_row428_col0\" class=\"data row428 col0\" >all</td>\n",
              "      <td id=\"T_02857_row428_col1\" class=\"data row428 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row428_col2\" class=\"data row428 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row428_col3\" class=\"data row428 col3\" >0.4866</td>\n",
              "      <td id=\"T_02857_row428_col4\" class=\"data row428 col4\" >0.4413</td>\n",
              "      <td id=\"T_02857_row428_col5\" class=\"data row428 col5\" >0.6195</td>\n",
              "      <td id=\"T_02857_row428_col6\" class=\"data row428 col6\" >0.7703</td>\n",
              "      <td id=\"T_02857_row428_col7\" class=\"data row428 col7\" >0.7361</td>\n",
              "      <td id=\"T_02857_row428_col8\" class=\"data row428 col8\" >0.9144</td>\n",
              "      <td id=\"T_02857_row428_col9\" class=\"data row428 col9\" >0.8752</td>\n",
              "      <td id=\"T_02857_row428_col10\" class=\"data row428 col10\" >0.9084</td>\n",
              "      <td id=\"T_02857_row428_col11\" class=\"data row428 col11\" >0.5560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row429\" class=\"row_heading level0 row429\" >429</th>\n",
              "      <td id=\"T_02857_row429_col0\" class=\"data row429 col0\" >all</td>\n",
              "      <td id=\"T_02857_row429_col1\" class=\"data row429 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row429_col2\" class=\"data row429 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row429_col3\" class=\"data row429 col3\" >0.4866</td>\n",
              "      <td id=\"T_02857_row429_col4\" class=\"data row429 col4\" >0.4413</td>\n",
              "      <td id=\"T_02857_row429_col5\" class=\"data row429 col5\" >0.6195</td>\n",
              "      <td id=\"T_02857_row429_col6\" class=\"data row429 col6\" >0.7703</td>\n",
              "      <td id=\"T_02857_row429_col7\" class=\"data row429 col7\" >0.7361</td>\n",
              "      <td id=\"T_02857_row429_col8\" class=\"data row429 col8\" >0.9144</td>\n",
              "      <td id=\"T_02857_row429_col9\" class=\"data row429 col9\" >0.8752</td>\n",
              "      <td id=\"T_02857_row429_col10\" class=\"data row429 col10\" >0.9084</td>\n",
              "      <td id=\"T_02857_row429_col11\" class=\"data row429 col11\" >0.5560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row430\" class=\"row_heading level0 row430\" >430</th>\n",
              "      <td id=\"T_02857_row430_col0\" class=\"data row430 col0\" >all</td>\n",
              "      <td id=\"T_02857_row430_col1\" class=\"data row430 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row430_col2\" class=\"data row430 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row430_col3\" class=\"data row430 col3\" >0.5135</td>\n",
              "      <td id=\"T_02857_row430_col4\" class=\"data row430 col4\" >0.5530</td>\n",
              "      <td id=\"T_02857_row430_col5\" class=\"data row430 col5\" >0.6193</td>\n",
              "      <td id=\"T_02857_row430_col6\" class=\"data row430 col6\" >0.6426</td>\n",
              "      <td id=\"T_02857_row430_col7\" class=\"data row430 col7\" >0.6191</td>\n",
              "      <td id=\"T_02857_row430_col8\" class=\"data row430 col8\" >0.8475</td>\n",
              "      <td id=\"T_02857_row430_col9\" class=\"data row430 col9\" >0.6727</td>\n",
              "      <td id=\"T_02857_row430_col10\" class=\"data row430 col10\" >0.8145</td>\n",
              "      <td id=\"T_02857_row430_col11\" class=\"data row430 col11\" >0.4851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row431\" class=\"row_heading level0 row431\" >431</th>\n",
              "      <td id=\"T_02857_row431_col0\" class=\"data row431 col0\" >all</td>\n",
              "      <td id=\"T_02857_row431_col1\" class=\"data row431 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row431_col2\" class=\"data row431 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row431_col3\" class=\"data row431 col3\" >0.5135</td>\n",
              "      <td id=\"T_02857_row431_col4\" class=\"data row431 col4\" >0.5530</td>\n",
              "      <td id=\"T_02857_row431_col5\" class=\"data row431 col5\" >0.6193</td>\n",
              "      <td id=\"T_02857_row431_col6\" class=\"data row431 col6\" >0.6426</td>\n",
              "      <td id=\"T_02857_row431_col7\" class=\"data row431 col7\" >0.6191</td>\n",
              "      <td id=\"T_02857_row431_col8\" class=\"data row431 col8\" >0.8475</td>\n",
              "      <td id=\"T_02857_row431_col9\" class=\"data row431 col9\" >0.6727</td>\n",
              "      <td id=\"T_02857_row431_col10\" class=\"data row431 col10\" >0.8145</td>\n",
              "      <td id=\"T_02857_row431_col11\" class=\"data row431 col11\" >0.4851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row432\" class=\"row_heading level0 row432\" >432</th>\n",
              "      <td id=\"T_02857_row432_col0\" class=\"data row432 col0\" >all</td>\n",
              "      <td id=\"T_02857_row432_col1\" class=\"data row432 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row432_col2\" class=\"data row432 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row432_col3\" class=\"data row432 col3\" >0.4347</td>\n",
              "      <td id=\"T_02857_row432_col4\" class=\"data row432 col4\" >0.3781</td>\n",
              "      <td id=\"T_02857_row432_col5\" class=\"data row432 col5\" >0.4410</td>\n",
              "      <td id=\"T_02857_row432_col6\" class=\"data row432 col6\" >0.5760</td>\n",
              "      <td id=\"T_02857_row432_col7\" class=\"data row432 col7\" >0.5879</td>\n",
              "      <td id=\"T_02857_row432_col8\" class=\"data row432 col8\" >0.9625</td>\n",
              "      <td id=\"T_02857_row432_col9\" class=\"data row432 col9\" >0.8868</td>\n",
              "      <td id=\"T_02857_row432_col10\" class=\"data row432 col10\" >0.9149</td>\n",
              "      <td id=\"T_02857_row432_col11\" class=\"data row432 col11\" >0.4588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row433\" class=\"row_heading level0 row433\" >433</th>\n",
              "      <td id=\"T_02857_row433_col0\" class=\"data row433 col0\" >all</td>\n",
              "      <td id=\"T_02857_row433_col1\" class=\"data row433 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row433_col2\" class=\"data row433 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row433_col3\" class=\"data row433 col3\" >0.4347</td>\n",
              "      <td id=\"T_02857_row433_col4\" class=\"data row433 col4\" >0.3781</td>\n",
              "      <td id=\"T_02857_row433_col5\" class=\"data row433 col5\" >0.4410</td>\n",
              "      <td id=\"T_02857_row433_col6\" class=\"data row433 col6\" >0.5760</td>\n",
              "      <td id=\"T_02857_row433_col7\" class=\"data row433 col7\" >0.5879</td>\n",
              "      <td id=\"T_02857_row433_col8\" class=\"data row433 col8\" >0.9625</td>\n",
              "      <td id=\"T_02857_row433_col9\" class=\"data row433 col9\" >0.8868</td>\n",
              "      <td id=\"T_02857_row433_col10\" class=\"data row433 col10\" >0.9149</td>\n",
              "      <td id=\"T_02857_row433_col11\" class=\"data row433 col11\" >0.4588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row434\" class=\"row_heading level0 row434\" >434</th>\n",
              "      <td id=\"T_02857_row434_col0\" class=\"data row434 col0\" >all</td>\n",
              "      <td id=\"T_02857_row434_col1\" class=\"data row434 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row434_col2\" class=\"data row434 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row434_col3\" class=\"data row434 col3\" >0.5275</td>\n",
              "      <td id=\"T_02857_row434_col4\" class=\"data row434 col4\" >0.5216</td>\n",
              "      <td id=\"T_02857_row434_col5\" class=\"data row434 col5\" >0.4821</td>\n",
              "      <td id=\"T_02857_row434_col6\" class=\"data row434 col6\" >0.6899</td>\n",
              "      <td id=\"T_02857_row434_col7\" class=\"data row434 col7\" >0.5866</td>\n",
              "      <td id=\"T_02857_row434_col8\" class=\"data row434 col8\" >0.9337</td>\n",
              "      <td id=\"T_02857_row434_col9\" class=\"data row434 col9\" >0.8565</td>\n",
              "      <td id=\"T_02857_row434_col10\" class=\"data row434 col10\" >0.8145</td>\n",
              "      <td id=\"T_02857_row434_col11\" class=\"data row434 col11\" >0.4882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row435\" class=\"row_heading level0 row435\" >435</th>\n",
              "      <td id=\"T_02857_row435_col0\" class=\"data row435 col0\" >all</td>\n",
              "      <td id=\"T_02857_row435_col1\" class=\"data row435 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row435_col2\" class=\"data row435 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row435_col3\" class=\"data row435 col3\" >0.5275</td>\n",
              "      <td id=\"T_02857_row435_col4\" class=\"data row435 col4\" >0.5216</td>\n",
              "      <td id=\"T_02857_row435_col5\" class=\"data row435 col5\" >0.4821</td>\n",
              "      <td id=\"T_02857_row435_col6\" class=\"data row435 col6\" >0.6899</td>\n",
              "      <td id=\"T_02857_row435_col7\" class=\"data row435 col7\" >0.5866</td>\n",
              "      <td id=\"T_02857_row435_col8\" class=\"data row435 col8\" >0.9337</td>\n",
              "      <td id=\"T_02857_row435_col9\" class=\"data row435 col9\" >0.8565</td>\n",
              "      <td id=\"T_02857_row435_col10\" class=\"data row435 col10\" >0.8145</td>\n",
              "      <td id=\"T_02857_row435_col11\" class=\"data row435 col11\" >0.4882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row436\" class=\"row_heading level0 row436\" >436</th>\n",
              "      <td id=\"T_02857_row436_col0\" class=\"data row436 col0\" >all</td>\n",
              "      <td id=\"T_02857_row436_col1\" class=\"data row436 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row436_col2\" class=\"data row436 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row436_col3\" class=\"data row436 col3\" >0.3681</td>\n",
              "      <td id=\"T_02857_row436_col4\" class=\"data row436 col4\" >0.4000</td>\n",
              "      <td id=\"T_02857_row436_col5\" class=\"data row436 col5\" >0.3981</td>\n",
              "      <td id=\"T_02857_row436_col6\" class=\"data row436 col6\" >0.5169</td>\n",
              "      <td id=\"T_02857_row436_col7\" class=\"data row436 col7\" >0.4687</td>\n",
              "      <td id=\"T_02857_row436_col8\" class=\"data row436 col8\" >0.7700</td>\n",
              "      <td id=\"T_02857_row436_col9\" class=\"data row436 col9\" >0.7727</td>\n",
              "      <td id=\"T_02857_row436_col10\" class=\"data row436 col10\" >0.6129</td>\n",
              "      <td id=\"T_02857_row436_col11\" class=\"data row436 col11\" >0.3244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row437\" class=\"row_heading level0 row437\" >437</th>\n",
              "      <td id=\"T_02857_row437_col0\" class=\"data row437 col0\" >all</td>\n",
              "      <td id=\"T_02857_row437_col1\" class=\"data row437 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row437_col2\" class=\"data row437 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row437_col3\" class=\"data row437 col3\" >0.3681</td>\n",
              "      <td id=\"T_02857_row437_col4\" class=\"data row437 col4\" >0.4000</td>\n",
              "      <td id=\"T_02857_row437_col5\" class=\"data row437 col5\" >0.3981</td>\n",
              "      <td id=\"T_02857_row437_col6\" class=\"data row437 col6\" >0.5169</td>\n",
              "      <td id=\"T_02857_row437_col7\" class=\"data row437 col7\" >0.4687</td>\n",
              "      <td id=\"T_02857_row437_col8\" class=\"data row437 col8\" >0.7700</td>\n",
              "      <td id=\"T_02857_row437_col9\" class=\"data row437 col9\" >0.7727</td>\n",
              "      <td id=\"T_02857_row437_col10\" class=\"data row437 col10\" >0.6129</td>\n",
              "      <td id=\"T_02857_row437_col11\" class=\"data row437 col11\" >0.3244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row438\" class=\"row_heading level0 row438\" >438</th>\n",
              "      <td id=\"T_02857_row438_col0\" class=\"data row438 col0\" >all</td>\n",
              "      <td id=\"T_02857_row438_col1\" class=\"data row438 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row438_col2\" class=\"data row438 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row438_col3\" class=\"data row438 col3\" >0.4614</td>\n",
              "      <td id=\"T_02857_row438_col4\" class=\"data row438 col4\" >0.5002</td>\n",
              "      <td id=\"T_02857_row438_col5\" class=\"data row438 col5\" >0.5167</td>\n",
              "      <td id=\"T_02857_row438_col6\" class=\"data row438 col6\" >0.6042</td>\n",
              "      <td id=\"T_02857_row438_col7\" class=\"data row438 col7\" >0.5312</td>\n",
              "      <td id=\"T_02857_row438_col8\" class=\"data row438 col8\" >0.8767</td>\n",
              "      <td id=\"T_02857_row438_col9\" class=\"data row438 col9\" >0.6347</td>\n",
              "      <td id=\"T_02857_row438_col10\" class=\"data row438 col10\" >0.6588</td>\n",
              "      <td id=\"T_02857_row438_col11\" class=\"data row438 col11\" >0.3957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row439\" class=\"row_heading level0 row439\" >439</th>\n",
              "      <td id=\"T_02857_row439_col0\" class=\"data row439 col0\" >all</td>\n",
              "      <td id=\"T_02857_row439_col1\" class=\"data row439 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row439_col2\" class=\"data row439 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row439_col3\" class=\"data row439 col3\" >0.4614</td>\n",
              "      <td id=\"T_02857_row439_col4\" class=\"data row439 col4\" >0.5002</td>\n",
              "      <td id=\"T_02857_row439_col5\" class=\"data row439 col5\" >0.5167</td>\n",
              "      <td id=\"T_02857_row439_col6\" class=\"data row439 col6\" >0.6042</td>\n",
              "      <td id=\"T_02857_row439_col7\" class=\"data row439 col7\" >0.5312</td>\n",
              "      <td id=\"T_02857_row439_col8\" class=\"data row439 col8\" >0.8767</td>\n",
              "      <td id=\"T_02857_row439_col9\" class=\"data row439 col9\" >0.6347</td>\n",
              "      <td id=\"T_02857_row439_col10\" class=\"data row439 col10\" >0.6588</td>\n",
              "      <td id=\"T_02857_row439_col11\" class=\"data row439 col11\" >0.3957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row440\" class=\"row_heading level0 row440\" >440</th>\n",
              "      <td id=\"T_02857_row440_col0\" class=\"data row440 col0\" >all</td>\n",
              "      <td id=\"T_02857_row440_col1\" class=\"data row440 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row440_col2\" class=\"data row440 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row440_col3\" class=\"data row440 col3\" >0.4262</td>\n",
              "      <td id=\"T_02857_row440_col4\" class=\"data row440 col4\" >0.4698</td>\n",
              "      <td id=\"T_02857_row440_col5\" class=\"data row440 col5\" >0.4910</td>\n",
              "      <td id=\"T_02857_row440_col6\" class=\"data row440 col6\" >0.6765</td>\n",
              "      <td id=\"T_02857_row440_col7\" class=\"data row440 col7\" >0.5921</td>\n",
              "      <td id=\"T_02857_row440_col8\" class=\"data row440 col8\" >0.9495</td>\n",
              "      <td id=\"T_02857_row440_col9\" class=\"data row440 col9\" >0.7749</td>\n",
              "      <td id=\"T_02857_row440_col10\" class=\"data row440 col10\" >0.6646</td>\n",
              "      <td id=\"T_02857_row440_col11\" class=\"data row440 col11\" >0.4276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row441\" class=\"row_heading level0 row441\" >441</th>\n",
              "      <td id=\"T_02857_row441_col0\" class=\"data row441 col0\" >all</td>\n",
              "      <td id=\"T_02857_row441_col1\" class=\"data row441 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row441_col2\" class=\"data row441 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row441_col3\" class=\"data row441 col3\" >0.4262</td>\n",
              "      <td id=\"T_02857_row441_col4\" class=\"data row441 col4\" >0.4698</td>\n",
              "      <td id=\"T_02857_row441_col5\" class=\"data row441 col5\" >0.4910</td>\n",
              "      <td id=\"T_02857_row441_col6\" class=\"data row441 col6\" >0.6765</td>\n",
              "      <td id=\"T_02857_row441_col7\" class=\"data row441 col7\" >0.5921</td>\n",
              "      <td id=\"T_02857_row441_col8\" class=\"data row441 col8\" >0.9495</td>\n",
              "      <td id=\"T_02857_row441_col9\" class=\"data row441 col9\" >0.7749</td>\n",
              "      <td id=\"T_02857_row441_col10\" class=\"data row441 col10\" >0.6646</td>\n",
              "      <td id=\"T_02857_row441_col11\" class=\"data row441 col11\" >0.4276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row442\" class=\"row_heading level0 row442\" >442</th>\n",
              "      <td id=\"T_02857_row442_col0\" class=\"data row442 col0\" >all</td>\n",
              "      <td id=\"T_02857_row442_col1\" class=\"data row442 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row442_col2\" class=\"data row442 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row442_col3\" class=\"data row442 col3\" >0.4340</td>\n",
              "      <td id=\"T_02857_row442_col4\" class=\"data row442 col4\" >0.4644</td>\n",
              "      <td id=\"T_02857_row442_col5\" class=\"data row442 col5\" >0.5467</td>\n",
              "      <td id=\"T_02857_row442_col6\" class=\"data row442 col6\" >0.7414</td>\n",
              "      <td id=\"T_02857_row442_col7\" class=\"data row442 col7\" >0.6589</td>\n",
              "      <td id=\"T_02857_row442_col8\" class=\"data row442 col8\" >0.9607</td>\n",
              "      <td id=\"T_02857_row442_col9\" class=\"data row442 col9\" >0.8180</td>\n",
              "      <td id=\"T_02857_row442_col10\" class=\"data row442 col10\" >0.8595</td>\n",
              "      <td id=\"T_02857_row442_col11\" class=\"data row442 col11\" >0.4871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row443\" class=\"row_heading level0 row443\" >443</th>\n",
              "      <td id=\"T_02857_row443_col0\" class=\"data row443 col0\" >all</td>\n",
              "      <td id=\"T_02857_row443_col1\" class=\"data row443 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row443_col2\" class=\"data row443 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row443_col3\" class=\"data row443 col3\" >0.4340</td>\n",
              "      <td id=\"T_02857_row443_col4\" class=\"data row443 col4\" >0.4644</td>\n",
              "      <td id=\"T_02857_row443_col5\" class=\"data row443 col5\" >0.5467</td>\n",
              "      <td id=\"T_02857_row443_col6\" class=\"data row443 col6\" >0.7414</td>\n",
              "      <td id=\"T_02857_row443_col7\" class=\"data row443 col7\" >0.6589</td>\n",
              "      <td id=\"T_02857_row443_col8\" class=\"data row443 col8\" >0.9607</td>\n",
              "      <td id=\"T_02857_row443_col9\" class=\"data row443 col9\" >0.8180</td>\n",
              "      <td id=\"T_02857_row443_col10\" class=\"data row443 col10\" >0.8595</td>\n",
              "      <td id=\"T_02857_row443_col11\" class=\"data row443 col11\" >0.4871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row444\" class=\"row_heading level0 row444\" >444</th>\n",
              "      <td id=\"T_02857_row444_col0\" class=\"data row444 col0\" >all</td>\n",
              "      <td id=\"T_02857_row444_col1\" class=\"data row444 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row444_col2\" class=\"data row444 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row444_col3\" class=\"data row444 col3\" >0.5242</td>\n",
              "      <td id=\"T_02857_row444_col4\" class=\"data row444 col4\" >0.5681</td>\n",
              "      <td id=\"T_02857_row444_col5\" class=\"data row444 col5\" >0.6388</td>\n",
              "      <td id=\"T_02857_row444_col6\" class=\"data row444 col6\" >0.6227</td>\n",
              "      <td id=\"T_02857_row444_col7\" class=\"data row444 col7\" >0.5893</td>\n",
              "      <td id=\"T_02857_row444_col8\" class=\"data row444 col8\" >0.9036</td>\n",
              "      <td id=\"T_02857_row444_col9\" class=\"data row444 col9\" >0.5407</td>\n",
              "      <td id=\"T_02857_row444_col10\" class=\"data row444 col10\" >0.6842</td>\n",
              "      <td id=\"T_02857_row444_col11\" class=\"data row444 col11\" >0.4460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row445\" class=\"row_heading level0 row445\" >445</th>\n",
              "      <td id=\"T_02857_row445_col0\" class=\"data row445 col0\" >all</td>\n",
              "      <td id=\"T_02857_row445_col1\" class=\"data row445 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row445_col2\" class=\"data row445 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row445_col3\" class=\"data row445 col3\" >0.5242</td>\n",
              "      <td id=\"T_02857_row445_col4\" class=\"data row445 col4\" >0.5681</td>\n",
              "      <td id=\"T_02857_row445_col5\" class=\"data row445 col5\" >0.6388</td>\n",
              "      <td id=\"T_02857_row445_col6\" class=\"data row445 col6\" >0.6227</td>\n",
              "      <td id=\"T_02857_row445_col7\" class=\"data row445 col7\" >0.5893</td>\n",
              "      <td id=\"T_02857_row445_col8\" class=\"data row445 col8\" >0.9036</td>\n",
              "      <td id=\"T_02857_row445_col9\" class=\"data row445 col9\" >0.5407</td>\n",
              "      <td id=\"T_02857_row445_col10\" class=\"data row445 col10\" >0.6842</td>\n",
              "      <td id=\"T_02857_row445_col11\" class=\"data row445 col11\" >0.4460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row446\" class=\"row_heading level0 row446\" >446</th>\n",
              "      <td id=\"T_02857_row446_col0\" class=\"data row446 col0\" >all</td>\n",
              "      <td id=\"T_02857_row446_col1\" class=\"data row446 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row446_col2\" class=\"data row446 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row446_col3\" class=\"data row446 col3\" >0.3801</td>\n",
              "      <td id=\"T_02857_row446_col4\" class=\"data row446 col4\" >0.3644</td>\n",
              "      <td id=\"T_02857_row446_col5\" class=\"data row446 col5\" >0.3826</td>\n",
              "      <td id=\"T_02857_row446_col6\" class=\"data row446 col6\" >0.5273</td>\n",
              "      <td id=\"T_02857_row446_col7\" class=\"data row446 col7\" >0.5037</td>\n",
              "      <td id=\"T_02857_row446_col8\" class=\"data row446 col8\" >0.9661</td>\n",
              "      <td id=\"T_02857_row446_col9\" class=\"data row446 col9\" >0.5722</td>\n",
              "      <td id=\"T_02857_row446_col10\" class=\"data row446 col10\" >0.6960</td>\n",
              "      <td id=\"T_02857_row446_col11\" class=\"data row446 col11\" >0.3456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row447\" class=\"row_heading level0 row447\" >447</th>\n",
              "      <td id=\"T_02857_row447_col0\" class=\"data row447 col0\" >all</td>\n",
              "      <td id=\"T_02857_row447_col1\" class=\"data row447 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row447_col2\" class=\"data row447 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row447_col3\" class=\"data row447 col3\" >0.3801</td>\n",
              "      <td id=\"T_02857_row447_col4\" class=\"data row447 col4\" >0.3644</td>\n",
              "      <td id=\"T_02857_row447_col5\" class=\"data row447 col5\" >0.3826</td>\n",
              "      <td id=\"T_02857_row447_col6\" class=\"data row447 col6\" >0.5273</td>\n",
              "      <td id=\"T_02857_row447_col7\" class=\"data row447 col7\" >0.5037</td>\n",
              "      <td id=\"T_02857_row447_col8\" class=\"data row447 col8\" >0.9661</td>\n",
              "      <td id=\"T_02857_row447_col9\" class=\"data row447 col9\" >0.5722</td>\n",
              "      <td id=\"T_02857_row447_col10\" class=\"data row447 col10\" >0.6960</td>\n",
              "      <td id=\"T_02857_row447_col11\" class=\"data row447 col11\" >0.3456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row448\" class=\"row_heading level0 row448\" >448</th>\n",
              "      <td id=\"T_02857_row448_col0\" class=\"data row448 col0\" >all</td>\n",
              "      <td id=\"T_02857_row448_col1\" class=\"data row448 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row448_col2\" class=\"data row448 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row448_col3\" class=\"data row448 col3\" >0.8784</td>\n",
              "      <td id=\"T_02857_row448_col4\" class=\"data row448 col4\" >0.9426</td>\n",
              "      <td id=\"T_02857_row448_col5\" class=\"data row448 col5\" >0.9272</td>\n",
              "      <td id=\"T_02857_row448_col6\" class=\"data row448 col6\" >0.8379</td>\n",
              "      <td id=\"T_02857_row448_col7\" class=\"data row448 col7\" >0.8140</td>\n",
              "      <td id=\"T_02857_row448_col8\" class=\"data row448 col8\" >0.3919</td>\n",
              "      <td id=\"T_02857_row448_col9\" class=\"data row448 col9\" >0.3564</td>\n",
              "      <td id=\"T_02857_row448_col10\" class=\"data row448 col10\" >0.3867</td>\n",
              "      <td id=\"T_02857_row448_col11\" class=\"data row448 col11\" >0.5161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row449\" class=\"row_heading level0 row449\" >449</th>\n",
              "      <td id=\"T_02857_row449_col0\" class=\"data row449 col0\" >all</td>\n",
              "      <td id=\"T_02857_row449_col1\" class=\"data row449 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row449_col2\" class=\"data row449 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row449_col3\" class=\"data row449 col3\" >0.8784</td>\n",
              "      <td id=\"T_02857_row449_col4\" class=\"data row449 col4\" >0.9426</td>\n",
              "      <td id=\"T_02857_row449_col5\" class=\"data row449 col5\" >0.9272</td>\n",
              "      <td id=\"T_02857_row449_col6\" class=\"data row449 col6\" >0.8379</td>\n",
              "      <td id=\"T_02857_row449_col7\" class=\"data row449 col7\" >0.8140</td>\n",
              "      <td id=\"T_02857_row449_col8\" class=\"data row449 col8\" >0.3919</td>\n",
              "      <td id=\"T_02857_row449_col9\" class=\"data row449 col9\" >0.3564</td>\n",
              "      <td id=\"T_02857_row449_col10\" class=\"data row449 col10\" >0.3867</td>\n",
              "      <td id=\"T_02857_row449_col11\" class=\"data row449 col11\" >0.5161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row450\" class=\"row_heading level0 row450\" >450</th>\n",
              "      <td id=\"T_02857_row450_col0\" class=\"data row450 col0\" >all</td>\n",
              "      <td id=\"T_02857_row450_col1\" class=\"data row450 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row450_col2\" class=\"data row450 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row450_col3\" class=\"data row450 col3\" >0.5669</td>\n",
              "      <td id=\"T_02857_row450_col4\" class=\"data row450 col4\" >0.6344</td>\n",
              "      <td id=\"T_02857_row450_col5\" class=\"data row450 col5\" >0.6075</td>\n",
              "      <td id=\"T_02857_row450_col6\" class=\"data row450 col6\" >0.6112</td>\n",
              "      <td id=\"T_02857_row450_col7\" class=\"data row450 col7\" >0.5534</td>\n",
              "      <td id=\"T_02857_row450_col8\" class=\"data row450 col8\" >0.4012</td>\n",
              "      <td id=\"T_02857_row450_col9\" class=\"data row450 col9\" >0.3853</td>\n",
              "      <td id=\"T_02857_row450_col10\" class=\"data row450 col10\" >0.3818</td>\n",
              "      <td id=\"T_02857_row450_col11\" class=\"data row450 col11\" >0.2909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row451\" class=\"row_heading level0 row451\" >451</th>\n",
              "      <td id=\"T_02857_row451_col0\" class=\"data row451 col0\" >all</td>\n",
              "      <td id=\"T_02857_row451_col1\" class=\"data row451 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row451_col2\" class=\"data row451 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row451_col3\" class=\"data row451 col3\" >0.5669</td>\n",
              "      <td id=\"T_02857_row451_col4\" class=\"data row451 col4\" >0.6344</td>\n",
              "      <td id=\"T_02857_row451_col5\" class=\"data row451 col5\" >0.6075</td>\n",
              "      <td id=\"T_02857_row451_col6\" class=\"data row451 col6\" >0.6112</td>\n",
              "      <td id=\"T_02857_row451_col7\" class=\"data row451 col7\" >0.5534</td>\n",
              "      <td id=\"T_02857_row451_col8\" class=\"data row451 col8\" >0.4012</td>\n",
              "      <td id=\"T_02857_row451_col9\" class=\"data row451 col9\" >0.3853</td>\n",
              "      <td id=\"T_02857_row451_col10\" class=\"data row451 col10\" >0.3818</td>\n",
              "      <td id=\"T_02857_row451_col11\" class=\"data row451 col11\" >0.2909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row452\" class=\"row_heading level0 row452\" >452</th>\n",
              "      <td id=\"T_02857_row452_col0\" class=\"data row452 col0\" >all</td>\n",
              "      <td id=\"T_02857_row452_col1\" class=\"data row452 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row452_col2\" class=\"data row452 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row452_col3\" class=\"data row452 col3\" >0.5244</td>\n",
              "      <td id=\"T_02857_row452_col4\" class=\"data row452 col4\" >0.6947</td>\n",
              "      <td id=\"T_02857_row452_col5\" class=\"data row452 col5\" >0.8014</td>\n",
              "      <td id=\"T_02857_row452_col6\" class=\"data row452 col6\" >0.6827</td>\n",
              "      <td id=\"T_02857_row452_col7\" class=\"data row452 col7\" >0.7033</td>\n",
              "      <td id=\"T_02857_row452_col8\" class=\"data row452 col8\" >0.6392</td>\n",
              "      <td id=\"T_02857_row452_col9\" class=\"data row452 col9\" >0.4440</td>\n",
              "      <td id=\"T_02857_row452_col10\" class=\"data row452 col10\" >0.4939</td>\n",
              "      <td id=\"T_02857_row452_col11\" class=\"data row452 col11\" >0.4140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row453\" class=\"row_heading level0 row453\" >453</th>\n",
              "      <td id=\"T_02857_row453_col0\" class=\"data row453 col0\" >all</td>\n",
              "      <td id=\"T_02857_row453_col1\" class=\"data row453 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row453_col2\" class=\"data row453 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row453_col3\" class=\"data row453 col3\" >0.5244</td>\n",
              "      <td id=\"T_02857_row453_col4\" class=\"data row453 col4\" >0.6947</td>\n",
              "      <td id=\"T_02857_row453_col5\" class=\"data row453 col5\" >0.8014</td>\n",
              "      <td id=\"T_02857_row453_col6\" class=\"data row453 col6\" >0.6827</td>\n",
              "      <td id=\"T_02857_row453_col7\" class=\"data row453 col7\" >0.7033</td>\n",
              "      <td id=\"T_02857_row453_col8\" class=\"data row453 col8\" >0.6392</td>\n",
              "      <td id=\"T_02857_row453_col9\" class=\"data row453 col9\" >0.4440</td>\n",
              "      <td id=\"T_02857_row453_col10\" class=\"data row453 col10\" >0.4939</td>\n",
              "      <td id=\"T_02857_row453_col11\" class=\"data row453 col11\" >0.4140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row454\" class=\"row_heading level0 row454\" >454</th>\n",
              "      <td id=\"T_02857_row454_col0\" class=\"data row454 col0\" >all</td>\n",
              "      <td id=\"T_02857_row454_col1\" class=\"data row454 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row454_col2\" class=\"data row454 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row454_col3\" class=\"data row454 col3\" >0.7930</td>\n",
              "      <td id=\"T_02857_row454_col4\" class=\"data row454 col4\" >0.8809</td>\n",
              "      <td id=\"T_02857_row454_col5\" class=\"data row454 col5\" >0.8635</td>\n",
              "      <td id=\"T_02857_row454_col6\" class=\"data row454 col6\" >0.7960</td>\n",
              "      <td id=\"T_02857_row454_col7\" class=\"data row454 col7\" >0.7408</td>\n",
              "      <td id=\"T_02857_row454_col8\" class=\"data row454 col8\" >0.3769</td>\n",
              "      <td id=\"T_02857_row454_col9\" class=\"data row454 col9\" >0.3650</td>\n",
              "      <td id=\"T_02857_row454_col10\" class=\"data row454 col10\" >0.3949</td>\n",
              "      <td id=\"T_02857_row454_col11\" class=\"data row454 col11\" >0.4580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row455\" class=\"row_heading level0 row455\" >455</th>\n",
              "      <td id=\"T_02857_row455_col0\" class=\"data row455 col0\" >all</td>\n",
              "      <td id=\"T_02857_row455_col1\" class=\"data row455 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row455_col2\" class=\"data row455 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row455_col3\" class=\"data row455 col3\" >0.7930</td>\n",
              "      <td id=\"T_02857_row455_col4\" class=\"data row455 col4\" >0.8809</td>\n",
              "      <td id=\"T_02857_row455_col5\" class=\"data row455 col5\" >0.8635</td>\n",
              "      <td id=\"T_02857_row455_col6\" class=\"data row455 col6\" >0.7960</td>\n",
              "      <td id=\"T_02857_row455_col7\" class=\"data row455 col7\" >0.7408</td>\n",
              "      <td id=\"T_02857_row455_col8\" class=\"data row455 col8\" >0.3769</td>\n",
              "      <td id=\"T_02857_row455_col9\" class=\"data row455 col9\" >0.3650</td>\n",
              "      <td id=\"T_02857_row455_col10\" class=\"data row455 col10\" >0.3949</td>\n",
              "      <td id=\"T_02857_row455_col11\" class=\"data row455 col11\" >0.4580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row456\" class=\"row_heading level0 row456\" >456</th>\n",
              "      <td id=\"T_02857_row456_col0\" class=\"data row456 col0\" >all</td>\n",
              "      <td id=\"T_02857_row456_col1\" class=\"data row456 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row456_col2\" class=\"data row456 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row456_col3\" class=\"data row456 col3\" >0.8901</td>\n",
              "      <td id=\"T_02857_row456_col4\" class=\"data row456 col4\" >0.8858</td>\n",
              "      <td id=\"T_02857_row456_col5\" class=\"data row456 col5\" >0.8894</td>\n",
              "      <td id=\"T_02857_row456_col6\" class=\"data row456 col6\" >0.8095</td>\n",
              "      <td id=\"T_02857_row456_col7\" class=\"data row456 col7\" >0.8190</td>\n",
              "      <td id=\"T_02857_row456_col8\" class=\"data row456 col8\" >0.3805</td>\n",
              "      <td id=\"T_02857_row456_col9\" class=\"data row456 col9\" >0.3641</td>\n",
              "      <td id=\"T_02857_row456_col10\" class=\"data row456 col10\" >0.3833</td>\n",
              "      <td id=\"T_02857_row456_col11\" class=\"data row456 col11\" >0.4878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row457\" class=\"row_heading level0 row457\" >457</th>\n",
              "      <td id=\"T_02857_row457_col0\" class=\"data row457 col0\" >all</td>\n",
              "      <td id=\"T_02857_row457_col1\" class=\"data row457 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row457_col2\" class=\"data row457 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row457_col3\" class=\"data row457 col3\" >0.8901</td>\n",
              "      <td id=\"T_02857_row457_col4\" class=\"data row457 col4\" >0.8858</td>\n",
              "      <td id=\"T_02857_row457_col5\" class=\"data row457 col5\" >0.8894</td>\n",
              "      <td id=\"T_02857_row457_col6\" class=\"data row457 col6\" >0.8095</td>\n",
              "      <td id=\"T_02857_row457_col7\" class=\"data row457 col7\" >0.8190</td>\n",
              "      <td id=\"T_02857_row457_col8\" class=\"data row457 col8\" >0.3805</td>\n",
              "      <td id=\"T_02857_row457_col9\" class=\"data row457 col9\" >0.3641</td>\n",
              "      <td id=\"T_02857_row457_col10\" class=\"data row457 col10\" >0.3833</td>\n",
              "      <td id=\"T_02857_row457_col11\" class=\"data row457 col11\" >0.4878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row458\" class=\"row_heading level0 row458\" >458</th>\n",
              "      <td id=\"T_02857_row458_col0\" class=\"data row458 col0\" >all</td>\n",
              "      <td id=\"T_02857_row458_col1\" class=\"data row458 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row458_col2\" class=\"data row458 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row458_col3\" class=\"data row458 col3\" >0.7210</td>\n",
              "      <td id=\"T_02857_row458_col4\" class=\"data row458 col4\" >0.7622</td>\n",
              "      <td id=\"T_02857_row458_col5\" class=\"data row458 col5\" >0.8276</td>\n",
              "      <td id=\"T_02857_row458_col6\" class=\"data row458 col6\" >0.7496</td>\n",
              "      <td id=\"T_02857_row458_col7\" class=\"data row458 col7\" >0.7419</td>\n",
              "      <td id=\"T_02857_row458_col8\" class=\"data row458 col8\" >0.5490</td>\n",
              "      <td id=\"T_02857_row458_col9\" class=\"data row458 col9\" >0.4477</td>\n",
              "      <td id=\"T_02857_row458_col10\" class=\"data row458 col10\" >0.5874</td>\n",
              "      <td id=\"T_02857_row458_col11\" class=\"data row458 col11\" >0.5215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row459\" class=\"row_heading level0 row459\" >459</th>\n",
              "      <td id=\"T_02857_row459_col0\" class=\"data row459 col0\" >all</td>\n",
              "      <td id=\"T_02857_row459_col1\" class=\"data row459 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row459_col2\" class=\"data row459 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row459_col3\" class=\"data row459 col3\" >0.7210</td>\n",
              "      <td id=\"T_02857_row459_col4\" class=\"data row459 col4\" >0.7622</td>\n",
              "      <td id=\"T_02857_row459_col5\" class=\"data row459 col5\" >0.8276</td>\n",
              "      <td id=\"T_02857_row459_col6\" class=\"data row459 col6\" >0.7496</td>\n",
              "      <td id=\"T_02857_row459_col7\" class=\"data row459 col7\" >0.7419</td>\n",
              "      <td id=\"T_02857_row459_col8\" class=\"data row459 col8\" >0.5490</td>\n",
              "      <td id=\"T_02857_row459_col9\" class=\"data row459 col9\" >0.4477</td>\n",
              "      <td id=\"T_02857_row459_col10\" class=\"data row459 col10\" >0.5874</td>\n",
              "      <td id=\"T_02857_row459_col11\" class=\"data row459 col11\" >0.5215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row460\" class=\"row_heading level0 row460\" >460</th>\n",
              "      <td id=\"T_02857_row460_col0\" class=\"data row460 col0\" >all</td>\n",
              "      <td id=\"T_02857_row460_col1\" class=\"data row460 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row460_col2\" class=\"data row460 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row460_col3\" class=\"data row460 col3\" >0.7660</td>\n",
              "      <td id=\"T_02857_row460_col4\" class=\"data row460 col4\" >0.8664</td>\n",
              "      <td id=\"T_02857_row460_col5\" class=\"data row460 col5\" >0.8621</td>\n",
              "      <td id=\"T_02857_row460_col6\" class=\"data row460 col6\" >0.7742</td>\n",
              "      <td id=\"T_02857_row460_col7\" class=\"data row460 col7\" >0.7566</td>\n",
              "      <td id=\"T_02857_row460_col8\" class=\"data row460 col8\" >0.4245</td>\n",
              "      <td id=\"T_02857_row460_col9\" class=\"data row460 col9\" >0.3698</td>\n",
              "      <td id=\"T_02857_row460_col10\" class=\"data row460 col10\" >0.3828</td>\n",
              "      <td id=\"T_02857_row460_col11\" class=\"data row460 col11\" >0.4543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row461\" class=\"row_heading level0 row461\" >461</th>\n",
              "      <td id=\"T_02857_row461_col0\" class=\"data row461 col0\" >all</td>\n",
              "      <td id=\"T_02857_row461_col1\" class=\"data row461 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row461_col2\" class=\"data row461 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row461_col3\" class=\"data row461 col3\" >0.7660</td>\n",
              "      <td id=\"T_02857_row461_col4\" class=\"data row461 col4\" >0.8664</td>\n",
              "      <td id=\"T_02857_row461_col5\" class=\"data row461 col5\" >0.8621</td>\n",
              "      <td id=\"T_02857_row461_col6\" class=\"data row461 col6\" >0.7742</td>\n",
              "      <td id=\"T_02857_row461_col7\" class=\"data row461 col7\" >0.7566</td>\n",
              "      <td id=\"T_02857_row461_col8\" class=\"data row461 col8\" >0.4245</td>\n",
              "      <td id=\"T_02857_row461_col9\" class=\"data row461 col9\" >0.3698</td>\n",
              "      <td id=\"T_02857_row461_col10\" class=\"data row461 col10\" >0.3828</td>\n",
              "      <td id=\"T_02857_row461_col11\" class=\"data row461 col11\" >0.4543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row462\" class=\"row_heading level0 row462\" >462</th>\n",
              "      <td id=\"T_02857_row462_col0\" class=\"data row462 col0\" >all</td>\n",
              "      <td id=\"T_02857_row462_col1\" class=\"data row462 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row462_col2\" class=\"data row462 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row462_col3\" class=\"data row462 col3\" >0.8905</td>\n",
              "      <td id=\"T_02857_row462_col4\" class=\"data row462 col4\" >0.9223</td>\n",
              "      <td id=\"T_02857_row462_col5\" class=\"data row462 col5\" >0.8597</td>\n",
              "      <td id=\"T_02857_row462_col6\" class=\"data row462 col6\" >0.9031</td>\n",
              "      <td id=\"T_02857_row462_col7\" class=\"data row462 col7\" >0.8074</td>\n",
              "      <td id=\"T_02857_row462_col8\" class=\"data row462 col8\" >0.4816</td>\n",
              "      <td id=\"T_02857_row462_col9\" class=\"data row462 col9\" >0.4869</td>\n",
              "      <td id=\"T_02857_row462_col10\" class=\"data row462 col10\" >0.4093</td>\n",
              "      <td id=\"T_02857_row462_col11\" class=\"data row462 col11\" >0.5446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row463\" class=\"row_heading level0 row463\" >463</th>\n",
              "      <td id=\"T_02857_row463_col0\" class=\"data row463 col0\" >all</td>\n",
              "      <td id=\"T_02857_row463_col1\" class=\"data row463 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row463_col2\" class=\"data row463 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row463_col3\" class=\"data row463 col3\" >0.8905</td>\n",
              "      <td id=\"T_02857_row463_col4\" class=\"data row463 col4\" >0.9223</td>\n",
              "      <td id=\"T_02857_row463_col5\" class=\"data row463 col5\" >0.8597</td>\n",
              "      <td id=\"T_02857_row463_col6\" class=\"data row463 col6\" >0.9031</td>\n",
              "      <td id=\"T_02857_row463_col7\" class=\"data row463 col7\" >0.8074</td>\n",
              "      <td id=\"T_02857_row463_col8\" class=\"data row463 col8\" >0.4816</td>\n",
              "      <td id=\"T_02857_row463_col9\" class=\"data row463 col9\" >0.4869</td>\n",
              "      <td id=\"T_02857_row463_col10\" class=\"data row463 col10\" >0.4093</td>\n",
              "      <td id=\"T_02857_row463_col11\" class=\"data row463 col11\" >0.5446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row464\" class=\"row_heading level0 row464\" >464</th>\n",
              "      <td id=\"T_02857_row464_col0\" class=\"data row464 col0\" >all</td>\n",
              "      <td id=\"T_02857_row464_col1\" class=\"data row464 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row464_col2\" class=\"data row464 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row464_col3\" class=\"data row464 col3\" >0.6023</td>\n",
              "      <td id=\"T_02857_row464_col4\" class=\"data row464 col4\" >0.6382</td>\n",
              "      <td id=\"T_02857_row464_col5\" class=\"data row464 col5\" >0.5912</td>\n",
              "      <td id=\"T_02857_row464_col6\" class=\"data row464 col6\" >0.6581</td>\n",
              "      <td id=\"T_02857_row464_col7\" class=\"data row464 col7\" >0.5892</td>\n",
              "      <td id=\"T_02857_row464_col8\" class=\"data row464 col8\" >0.4560</td>\n",
              "      <td id=\"T_02857_row464_col9\" class=\"data row464 col9\" >0.5024</td>\n",
              "      <td id=\"T_02857_row464_col10\" class=\"data row464 col10\" >0.4106</td>\n",
              "      <td id=\"T_02857_row464_col11\" class=\"data row464 col11\" >0.3292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row465\" class=\"row_heading level0 row465\" >465</th>\n",
              "      <td id=\"T_02857_row465_col0\" class=\"data row465 col0\" >all</td>\n",
              "      <td id=\"T_02857_row465_col1\" class=\"data row465 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row465_col2\" class=\"data row465 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row465_col3\" class=\"data row465 col3\" >0.6023</td>\n",
              "      <td id=\"T_02857_row465_col4\" class=\"data row465 col4\" >0.6382</td>\n",
              "      <td id=\"T_02857_row465_col5\" class=\"data row465 col5\" >0.5912</td>\n",
              "      <td id=\"T_02857_row465_col6\" class=\"data row465 col6\" >0.6581</td>\n",
              "      <td id=\"T_02857_row465_col7\" class=\"data row465 col7\" >0.5892</td>\n",
              "      <td id=\"T_02857_row465_col8\" class=\"data row465 col8\" >0.4560</td>\n",
              "      <td id=\"T_02857_row465_col9\" class=\"data row465 col9\" >0.5024</td>\n",
              "      <td id=\"T_02857_row465_col10\" class=\"data row465 col10\" >0.4106</td>\n",
              "      <td id=\"T_02857_row465_col11\" class=\"data row465 col11\" >0.3292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row466\" class=\"row_heading level0 row466\" >466</th>\n",
              "      <td id=\"T_02857_row466_col0\" class=\"data row466 col0\" >all</td>\n",
              "      <td id=\"T_02857_row466_col1\" class=\"data row466 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row466_col2\" class=\"data row466 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row466_col3\" class=\"data row466 col3\" >0.6246</td>\n",
              "      <td id=\"T_02857_row466_col4\" class=\"data row466 col4\" >0.7113</td>\n",
              "      <td id=\"T_02857_row466_col5\" class=\"data row466 col5\" >0.6496</td>\n",
              "      <td id=\"T_02857_row466_col6\" class=\"data row466 col6\" >0.7758</td>\n",
              "      <td id=\"T_02857_row466_col7\" class=\"data row466 col7\" >0.6697</td>\n",
              "      <td id=\"T_02857_row466_col8\" class=\"data row466 col8\" >0.6400</td>\n",
              "      <td id=\"T_02857_row466_col9\" class=\"data row466 col9\" >0.4753</td>\n",
              "      <td id=\"T_02857_row466_col10\" class=\"data row466 col10\" >0.4681</td>\n",
              "      <td id=\"T_02857_row466_col11\" class=\"data row466 col11\" >0.4056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row467\" class=\"row_heading level0 row467\" >467</th>\n",
              "      <td id=\"T_02857_row467_col0\" class=\"data row467 col0\" >all</td>\n",
              "      <td id=\"T_02857_row467_col1\" class=\"data row467 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row467_col2\" class=\"data row467 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row467_col3\" class=\"data row467 col3\" >0.6246</td>\n",
              "      <td id=\"T_02857_row467_col4\" class=\"data row467 col4\" >0.7113</td>\n",
              "      <td id=\"T_02857_row467_col5\" class=\"data row467 col5\" >0.6496</td>\n",
              "      <td id=\"T_02857_row467_col6\" class=\"data row467 col6\" >0.7758</td>\n",
              "      <td id=\"T_02857_row467_col7\" class=\"data row467 col7\" >0.6697</td>\n",
              "      <td id=\"T_02857_row467_col8\" class=\"data row467 col8\" >0.6400</td>\n",
              "      <td id=\"T_02857_row467_col9\" class=\"data row467 col9\" >0.4753</td>\n",
              "      <td id=\"T_02857_row467_col10\" class=\"data row467 col10\" >0.4681</td>\n",
              "      <td id=\"T_02857_row467_col11\" class=\"data row467 col11\" >0.4056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row468\" class=\"row_heading level0 row468\" >468</th>\n",
              "      <td id=\"T_02857_row468_col0\" class=\"data row468 col0\" >all</td>\n",
              "      <td id=\"T_02857_row468_col1\" class=\"data row468 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row468_col2\" class=\"data row468 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row468_col3\" class=\"data row468 col3\" >0.8238</td>\n",
              "      <td id=\"T_02857_row468_col4\" class=\"data row468 col4\" >0.8660</td>\n",
              "      <td id=\"T_02857_row468_col5\" class=\"data row468 col5\" >0.8176</td>\n",
              "      <td id=\"T_02857_row468_col6\" class=\"data row468 col6\" >0.8529</td>\n",
              "      <td id=\"T_02857_row468_col7\" class=\"data row468 col7\" >0.7949</td>\n",
              "      <td id=\"T_02857_row468_col8\" class=\"data row468 col8\" >0.4407</td>\n",
              "      <td id=\"T_02857_row468_col9\" class=\"data row468 col9\" >0.5522</td>\n",
              "      <td id=\"T_02857_row468_col10\" class=\"data row468 col10\" >0.4397</td>\n",
              "      <td id=\"T_02857_row468_col11\" class=\"data row468 col11\" >0.5223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row469\" class=\"row_heading level0 row469\" >469</th>\n",
              "      <td id=\"T_02857_row469_col0\" class=\"data row469 col0\" >all</td>\n",
              "      <td id=\"T_02857_row469_col1\" class=\"data row469 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row469_col2\" class=\"data row469 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row469_col3\" class=\"data row469 col3\" >0.8238</td>\n",
              "      <td id=\"T_02857_row469_col4\" class=\"data row469 col4\" >0.8660</td>\n",
              "      <td id=\"T_02857_row469_col5\" class=\"data row469 col5\" >0.8176</td>\n",
              "      <td id=\"T_02857_row469_col6\" class=\"data row469 col6\" >0.8529</td>\n",
              "      <td id=\"T_02857_row469_col7\" class=\"data row469 col7\" >0.7949</td>\n",
              "      <td id=\"T_02857_row469_col8\" class=\"data row469 col8\" >0.4407</td>\n",
              "      <td id=\"T_02857_row469_col9\" class=\"data row469 col9\" >0.5522</td>\n",
              "      <td id=\"T_02857_row469_col10\" class=\"data row469 col10\" >0.4397</td>\n",
              "      <td id=\"T_02857_row469_col11\" class=\"data row469 col11\" >0.5223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row470\" class=\"row_heading level0 row470\" >470</th>\n",
              "      <td id=\"T_02857_row470_col0\" class=\"data row470 col0\" >all</td>\n",
              "      <td id=\"T_02857_row470_col1\" class=\"data row470 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row470_col2\" class=\"data row470 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row470_col3\" class=\"data row470 col3\" >0.9036</td>\n",
              "      <td id=\"T_02857_row470_col4\" class=\"data row470 col4\" >0.8833</td>\n",
              "      <td id=\"T_02857_row470_col5\" class=\"data row470 col5\" >0.8597</td>\n",
              "      <td id=\"T_02857_row470_col6\" class=\"data row470 col6\" >0.8655</td>\n",
              "      <td id=\"T_02857_row470_col7\" class=\"data row470 col7\" >0.8215</td>\n",
              "      <td id=\"T_02857_row470_col8\" class=\"data row470 col8\" >0.4913</td>\n",
              "      <td id=\"T_02857_row470_col9\" class=\"data row470 col9\" >0.4851</td>\n",
              "      <td id=\"T_02857_row470_col10\" class=\"data row470 col10\" >0.4231</td>\n",
              "      <td id=\"T_02857_row470_col11\" class=\"data row470 col11\" >0.5323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row471\" class=\"row_heading level0 row471\" >471</th>\n",
              "      <td id=\"T_02857_row471_col0\" class=\"data row471 col0\" >all</td>\n",
              "      <td id=\"T_02857_row471_col1\" class=\"data row471 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row471_col2\" class=\"data row471 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row471_col3\" class=\"data row471 col3\" >0.9036</td>\n",
              "      <td id=\"T_02857_row471_col4\" class=\"data row471 col4\" >0.8833</td>\n",
              "      <td id=\"T_02857_row471_col5\" class=\"data row471 col5\" >0.8597</td>\n",
              "      <td id=\"T_02857_row471_col6\" class=\"data row471 col6\" >0.8655</td>\n",
              "      <td id=\"T_02857_row471_col7\" class=\"data row471 col7\" >0.8215</td>\n",
              "      <td id=\"T_02857_row471_col8\" class=\"data row471 col8\" >0.4913</td>\n",
              "      <td id=\"T_02857_row471_col9\" class=\"data row471 col9\" >0.4851</td>\n",
              "      <td id=\"T_02857_row471_col10\" class=\"data row471 col10\" >0.4231</td>\n",
              "      <td id=\"T_02857_row471_col11\" class=\"data row471 col11\" >0.5323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row472\" class=\"row_heading level0 row472\" >472</th>\n",
              "      <td id=\"T_02857_row472_col0\" class=\"data row472 col0\" >all</td>\n",
              "      <td id=\"T_02857_row472_col1\" class=\"data row472 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row472_col2\" class=\"data row472 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row472_col3\" class=\"data row472 col3\" >0.8172</td>\n",
              "      <td id=\"T_02857_row472_col4\" class=\"data row472 col4\" >0.8624</td>\n",
              "      <td id=\"T_02857_row472_col5\" class=\"data row472 col5\" >0.8172</td>\n",
              "      <td id=\"T_02857_row472_col6\" class=\"data row472 col6\" >0.8713</td>\n",
              "      <td id=\"T_02857_row472_col7\" class=\"data row472 col7\" >0.7963</td>\n",
              "      <td id=\"T_02857_row472_col8\" class=\"data row472 col8\" >0.5268</td>\n",
              "      <td id=\"T_02857_row472_col9\" class=\"data row472 col9\" >0.5663</td>\n",
              "      <td id=\"T_02857_row472_col10\" class=\"data row472 col10\" >0.5183</td>\n",
              "      <td id=\"T_02857_row472_col11\" class=\"data row472 col11\" >0.5291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row473\" class=\"row_heading level0 row473\" >473</th>\n",
              "      <td id=\"T_02857_row473_col0\" class=\"data row473 col0\" >all</td>\n",
              "      <td id=\"T_02857_row473_col1\" class=\"data row473 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row473_col2\" class=\"data row473 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row473_col3\" class=\"data row473 col3\" >0.8172</td>\n",
              "      <td id=\"T_02857_row473_col4\" class=\"data row473 col4\" >0.8624</td>\n",
              "      <td id=\"T_02857_row473_col5\" class=\"data row473 col5\" >0.8172</td>\n",
              "      <td id=\"T_02857_row473_col6\" class=\"data row473 col6\" >0.8713</td>\n",
              "      <td id=\"T_02857_row473_col7\" class=\"data row473 col7\" >0.7963</td>\n",
              "      <td id=\"T_02857_row473_col8\" class=\"data row473 col8\" >0.5268</td>\n",
              "      <td id=\"T_02857_row473_col9\" class=\"data row473 col9\" >0.5663</td>\n",
              "      <td id=\"T_02857_row473_col10\" class=\"data row473 col10\" >0.5183</td>\n",
              "      <td id=\"T_02857_row473_col11\" class=\"data row473 col11\" >0.5291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row474\" class=\"row_heading level0 row474\" >474</th>\n",
              "      <td id=\"T_02857_row474_col0\" class=\"data row474 col0\" >all</td>\n",
              "      <td id=\"T_02857_row474_col1\" class=\"data row474 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row474_col2\" class=\"data row474 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row474_col3\" class=\"data row474 col3\" >0.9202</td>\n",
              "      <td id=\"T_02857_row474_col4\" class=\"data row474 col4\" >0.9338</td>\n",
              "      <td id=\"T_02857_row474_col5\" class=\"data row474 col5\" >0.8938</td>\n",
              "      <td id=\"T_02857_row474_col6\" class=\"data row474 col6\" >0.9269</td>\n",
              "      <td id=\"T_02857_row474_col7\" class=\"data row474 col7\" >0.8345</td>\n",
              "      <td id=\"T_02857_row474_col8\" class=\"data row474 col8\" >0.5455</td>\n",
              "      <td id=\"T_02857_row474_col9\" class=\"data row474 col9\" >0.5322</td>\n",
              "      <td id=\"T_02857_row474_col10\" class=\"data row474 col10\" >0.4640</td>\n",
              "      <td id=\"T_02857_row474_col11\" class=\"data row474 col11\" >0.5792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row475\" class=\"row_heading level0 row475\" >475</th>\n",
              "      <td id=\"T_02857_row475_col0\" class=\"data row475 col0\" >all</td>\n",
              "      <td id=\"T_02857_row475_col1\" class=\"data row475 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row475_col2\" class=\"data row475 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row475_col3\" class=\"data row475 col3\" >0.9202</td>\n",
              "      <td id=\"T_02857_row475_col4\" class=\"data row475 col4\" >0.9338</td>\n",
              "      <td id=\"T_02857_row475_col5\" class=\"data row475 col5\" >0.8938</td>\n",
              "      <td id=\"T_02857_row475_col6\" class=\"data row475 col6\" >0.9269</td>\n",
              "      <td id=\"T_02857_row475_col7\" class=\"data row475 col7\" >0.8345</td>\n",
              "      <td id=\"T_02857_row475_col8\" class=\"data row475 col8\" >0.5455</td>\n",
              "      <td id=\"T_02857_row475_col9\" class=\"data row475 col9\" >0.5322</td>\n",
              "      <td id=\"T_02857_row475_col10\" class=\"data row475 col10\" >0.4640</td>\n",
              "      <td id=\"T_02857_row475_col11\" class=\"data row475 col11\" >0.5792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row476\" class=\"row_heading level0 row476\" >476</th>\n",
              "      <td id=\"T_02857_row476_col0\" class=\"data row476 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row476_col1\" class=\"data row476 col1\" >all</td>\n",
              "      <td id=\"T_02857_row476_col2\" class=\"data row476 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row476_col3\" class=\"data row476 col3\" >0.3968</td>\n",
              "      <td id=\"T_02857_row476_col4\" class=\"data row476 col4\" >0.3966</td>\n",
              "      <td id=\"T_02857_row476_col5\" class=\"data row476 col5\" >0.3967</td>\n",
              "      <td id=\"T_02857_row476_col6\" class=\"data row476 col6\" >0.3961</td>\n",
              "      <td id=\"T_02857_row476_col7\" class=\"data row476 col7\" >0.3964</td>\n",
              "      <td id=\"T_02857_row476_col8\" class=\"data row476 col8\" >0.3932</td>\n",
              "      <td id=\"T_02857_row476_col9\" class=\"data row476 col9\" >0.3942</td>\n",
              "      <td id=\"T_02857_row476_col10\" class=\"data row476 col10\" >0.3942</td>\n",
              "      <td id=\"T_02857_row476_col11\" class=\"data row476 col11\" >0.5264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row477\" class=\"row_heading level0 row477\" >477</th>\n",
              "      <td id=\"T_02857_row477_col0\" class=\"data row477 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row477_col1\" class=\"data row477 col1\" >all</td>\n",
              "      <td id=\"T_02857_row477_col2\" class=\"data row477 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row477_col3\" class=\"data row477 col3\" >0.4150</td>\n",
              "      <td id=\"T_02857_row477_col4\" class=\"data row477 col4\" >0.4137</td>\n",
              "      <td id=\"T_02857_row477_col5\" class=\"data row477 col5\" >0.4151</td>\n",
              "      <td id=\"T_02857_row477_col6\" class=\"data row477 col6\" >0.4150</td>\n",
              "      <td id=\"T_02857_row477_col7\" class=\"data row477 col7\" >0.4152</td>\n",
              "      <td id=\"T_02857_row477_col8\" class=\"data row477 col8\" >0.4117</td>\n",
              "      <td id=\"T_02857_row477_col9\" class=\"data row477 col9\" >0.4124</td>\n",
              "      <td id=\"T_02857_row477_col10\" class=\"data row477 col10\" >0.4114</td>\n",
              "      <td id=\"T_02857_row477_col11\" class=\"data row477 col11\" >0.5418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row478\" class=\"row_heading level0 row478\" >478</th>\n",
              "      <td id=\"T_02857_row478_col0\" class=\"data row478 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row478_col1\" class=\"data row478 col1\" >all</td>\n",
              "      <td id=\"T_02857_row478_col2\" class=\"data row478 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row478_col3\" class=\"data row478 col3\" >0.4382</td>\n",
              "      <td id=\"T_02857_row478_col4\" class=\"data row478 col4\" >0.4230</td>\n",
              "      <td id=\"T_02857_row478_col5\" class=\"data row478 col5\" >0.4138</td>\n",
              "      <td id=\"T_02857_row478_col6\" class=\"data row478 col6\" >0.4725</td>\n",
              "      <td id=\"T_02857_row478_col7\" class=\"data row478 col7\" >0.4275</td>\n",
              "      <td id=\"T_02857_row478_col8\" class=\"data row478 col8\" >0.5317</td>\n",
              "      <td id=\"T_02857_row478_col9\" class=\"data row478 col9\" >0.5906</td>\n",
              "      <td id=\"T_02857_row478_col10\" class=\"data row478 col10\" >0.5214</td>\n",
              "      <td id=\"T_02857_row478_col11\" class=\"data row478 col11\" >0.4619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row479\" class=\"row_heading level0 row479\" >479</th>\n",
              "      <td id=\"T_02857_row479_col0\" class=\"data row479 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row479_col1\" class=\"data row479 col1\" >all</td>\n",
              "      <td id=\"T_02857_row479_col2\" class=\"data row479 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row479_col3\" class=\"data row479 col3\" >0.5690</td>\n",
              "      <td id=\"T_02857_row479_col4\" class=\"data row479 col4\" >0.5641</td>\n",
              "      <td id=\"T_02857_row479_col5\" class=\"data row479 col5\" >0.5516</td>\n",
              "      <td id=\"T_02857_row479_col6\" class=\"data row479 col6\" >0.5687</td>\n",
              "      <td id=\"T_02857_row479_col7\" class=\"data row479 col7\" >0.5355</td>\n",
              "      <td id=\"T_02857_row479_col8\" class=\"data row479 col8\" >0.4149</td>\n",
              "      <td id=\"T_02857_row479_col9\" class=\"data row479 col9\" >0.5707</td>\n",
              "      <td id=\"T_02857_row479_col10\" class=\"data row479 col10\" >0.5049</td>\n",
              "      <td id=\"T_02857_row479_col11\" class=\"data row479 col11\" >0.5602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row480\" class=\"row_heading level0 row480\" >480</th>\n",
              "      <td id=\"T_02857_row480_col0\" class=\"data row480 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row480_col1\" class=\"data row480 col1\" >all</td>\n",
              "      <td id=\"T_02857_row480_col2\" class=\"data row480 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row480_col3\" class=\"data row480 col3\" >0.4561</td>\n",
              "      <td id=\"T_02857_row480_col4\" class=\"data row480 col4\" >0.4559</td>\n",
              "      <td id=\"T_02857_row480_col5\" class=\"data row480 col5\" >0.4568</td>\n",
              "      <td id=\"T_02857_row480_col6\" class=\"data row480 col6\" >0.4563</td>\n",
              "      <td id=\"T_02857_row480_col7\" class=\"data row480 col7\" >0.4557</td>\n",
              "      <td id=\"T_02857_row480_col8\" class=\"data row480 col8\" >0.4511</td>\n",
              "      <td id=\"T_02857_row480_col9\" class=\"data row480 col9\" >0.4522</td>\n",
              "      <td id=\"T_02857_row480_col10\" class=\"data row480 col10\" >0.4453</td>\n",
              "      <td id=\"T_02857_row480_col11\" class=\"data row480 col11\" >0.5737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row481\" class=\"row_heading level0 row481\" >481</th>\n",
              "      <td id=\"T_02857_row481_col0\" class=\"data row481 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row481_col1\" class=\"data row481 col1\" >all</td>\n",
              "      <td id=\"T_02857_row481_col2\" class=\"data row481 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row481_col3\" class=\"data row481 col3\" >0.5063</td>\n",
              "      <td id=\"T_02857_row481_col4\" class=\"data row481 col4\" >0.5043</td>\n",
              "      <td id=\"T_02857_row481_col5\" class=\"data row481 col5\" >0.5008</td>\n",
              "      <td id=\"T_02857_row481_col6\" class=\"data row481 col6\" >0.5040</td>\n",
              "      <td id=\"T_02857_row481_col7\" class=\"data row481 col7\" >0.5020</td>\n",
              "      <td id=\"T_02857_row481_col8\" class=\"data row481 col8\" >0.5052</td>\n",
              "      <td id=\"T_02857_row481_col9\" class=\"data row481 col9\" >0.5039</td>\n",
              "      <td id=\"T_02857_row481_col10\" class=\"data row481 col10\" >0.5035</td>\n",
              "      <td id=\"T_02857_row481_col11\" class=\"data row481 col11\" >0.6106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row482\" class=\"row_heading level0 row482\" >482</th>\n",
              "      <td id=\"T_02857_row482_col0\" class=\"data row482 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row482_col1\" class=\"data row482 col1\" >all</td>\n",
              "      <td id=\"T_02857_row482_col2\" class=\"data row482 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row482_col3\" class=\"data row482 col3\" >0.5094</td>\n",
              "      <td id=\"T_02857_row482_col4\" class=\"data row482 col4\" >0.5093</td>\n",
              "      <td id=\"T_02857_row482_col5\" class=\"data row482 col5\" >0.5090</td>\n",
              "      <td id=\"T_02857_row482_col6\" class=\"data row482 col6\" >0.5092</td>\n",
              "      <td id=\"T_02857_row482_col7\" class=\"data row482 col7\" >0.5054</td>\n",
              "      <td id=\"T_02857_row482_col8\" class=\"data row482 col8\" >0.4821</td>\n",
              "      <td id=\"T_02857_row482_col9\" class=\"data row482 col9\" >0.5043</td>\n",
              "      <td id=\"T_02857_row482_col10\" class=\"data row482 col10\" >0.4874</td>\n",
              "      <td id=\"T_02857_row482_col11\" class=\"data row482 col11\" >0.6066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row483\" class=\"row_heading level0 row483\" >483</th>\n",
              "      <td id=\"T_02857_row483_col0\" class=\"data row483 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row483_col1\" class=\"data row483 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row483_col2\" class=\"data row483 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row483_col3\" class=\"data row483 col3\" >0.8151</td>\n",
              "      <td id=\"T_02857_row483_col4\" class=\"data row483 col4\" >0.8543</td>\n",
              "      <td id=\"T_02857_row483_col5\" class=\"data row483 col5\" >0.8351</td>\n",
              "      <td id=\"T_02857_row483_col6\" class=\"data row483 col6\" >0.7879</td>\n",
              "      <td id=\"T_02857_row483_col7\" class=\"data row483 col7\" >0.8076</td>\n",
              "      <td id=\"T_02857_row483_col8\" class=\"data row483 col8\" >0.4268</td>\n",
              "      <td id=\"T_02857_row483_col9\" class=\"data row483 col9\" >0.4969</td>\n",
              "      <td id=\"T_02857_row483_col10\" class=\"data row483 col10\" >0.4106</td>\n",
              "      <td id=\"T_02857_row483_col11\" class=\"data row483 col11\" >0.5392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row484\" class=\"row_heading level0 row484\" >484</th>\n",
              "      <td id=\"T_02857_row484_col0\" class=\"data row484 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row484_col1\" class=\"data row484 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row484_col2\" class=\"data row484 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row484_col3\" class=\"data row484 col3\" >0.8151</td>\n",
              "      <td id=\"T_02857_row484_col4\" class=\"data row484 col4\" >0.8543</td>\n",
              "      <td id=\"T_02857_row484_col5\" class=\"data row484 col5\" >0.8351</td>\n",
              "      <td id=\"T_02857_row484_col6\" class=\"data row484 col6\" >0.7879</td>\n",
              "      <td id=\"T_02857_row484_col7\" class=\"data row484 col7\" >0.8076</td>\n",
              "      <td id=\"T_02857_row484_col8\" class=\"data row484 col8\" >0.4268</td>\n",
              "      <td id=\"T_02857_row484_col9\" class=\"data row484 col9\" >0.4969</td>\n",
              "      <td id=\"T_02857_row484_col10\" class=\"data row484 col10\" >0.4106</td>\n",
              "      <td id=\"T_02857_row484_col11\" class=\"data row484 col11\" >0.5392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row485\" class=\"row_heading level0 row485\" >485</th>\n",
              "      <td id=\"T_02857_row485_col0\" class=\"data row485 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row485_col1\" class=\"data row485 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row485_col2\" class=\"data row485 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row485_col3\" class=\"data row485 col3\" >0.5400</td>\n",
              "      <td id=\"T_02857_row485_col4\" class=\"data row485 col4\" >0.5471</td>\n",
              "      <td id=\"T_02857_row485_col5\" class=\"data row485 col5\" >0.5396</td>\n",
              "      <td id=\"T_02857_row485_col6\" class=\"data row485 col6\" >0.5547</td>\n",
              "      <td id=\"T_02857_row485_col7\" class=\"data row485 col7\" >0.5655</td>\n",
              "      <td id=\"T_02857_row485_col8\" class=\"data row485 col8\" >0.5207</td>\n",
              "      <td id=\"T_02857_row485_col9\" class=\"data row485 col9\" >0.4650</td>\n",
              "      <td id=\"T_02857_row485_col10\" class=\"data row485 col10\" >0.4882</td>\n",
              "      <td id=\"T_02857_row485_col11\" class=\"data row485 col11\" >0.3887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row486\" class=\"row_heading level0 row486\" >486</th>\n",
              "      <td id=\"T_02857_row486_col0\" class=\"data row486 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row486_col1\" class=\"data row486 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row486_col2\" class=\"data row486 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row486_col3\" class=\"data row486 col3\" >0.5400</td>\n",
              "      <td id=\"T_02857_row486_col4\" class=\"data row486 col4\" >0.5471</td>\n",
              "      <td id=\"T_02857_row486_col5\" class=\"data row486 col5\" >0.5396</td>\n",
              "      <td id=\"T_02857_row486_col6\" class=\"data row486 col6\" >0.5547</td>\n",
              "      <td id=\"T_02857_row486_col7\" class=\"data row486 col7\" >0.5655</td>\n",
              "      <td id=\"T_02857_row486_col8\" class=\"data row486 col8\" >0.5207</td>\n",
              "      <td id=\"T_02857_row486_col9\" class=\"data row486 col9\" >0.4650</td>\n",
              "      <td id=\"T_02857_row486_col10\" class=\"data row486 col10\" >0.4882</td>\n",
              "      <td id=\"T_02857_row486_col11\" class=\"data row486 col11\" >0.3887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row487\" class=\"row_heading level0 row487\" >487</th>\n",
              "      <td id=\"T_02857_row487_col0\" class=\"data row487 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row487_col1\" class=\"data row487 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row487_col2\" class=\"data row487 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row487_col3\" class=\"data row487 col3\" >0.4348</td>\n",
              "      <td id=\"T_02857_row487_col4\" class=\"data row487 col4\" >0.4299</td>\n",
              "      <td id=\"T_02857_row487_col5\" class=\"data row487 col5\" >0.4243</td>\n",
              "      <td id=\"T_02857_row487_col6\" class=\"data row487 col6\" >0.4534</td>\n",
              "      <td id=\"T_02857_row487_col7\" class=\"data row487 col7\" >0.4813</td>\n",
              "      <td id=\"T_02857_row487_col8\" class=\"data row487 col8\" >0.4284</td>\n",
              "      <td id=\"T_02857_row487_col9\" class=\"data row487 col9\" >0.5194</td>\n",
              "      <td id=\"T_02857_row487_col10\" class=\"data row487 col10\" >0.3787</td>\n",
              "      <td id=\"T_02857_row487_col11\" class=\"data row487 col11\" >0.2113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row488\" class=\"row_heading level0 row488\" >488</th>\n",
              "      <td id=\"T_02857_row488_col0\" class=\"data row488 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row488_col1\" class=\"data row488 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row488_col2\" class=\"data row488 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row488_col3\" class=\"data row488 col3\" >0.4348</td>\n",
              "      <td id=\"T_02857_row488_col4\" class=\"data row488 col4\" >0.4299</td>\n",
              "      <td id=\"T_02857_row488_col5\" class=\"data row488 col5\" >0.4243</td>\n",
              "      <td id=\"T_02857_row488_col6\" class=\"data row488 col6\" >0.4534</td>\n",
              "      <td id=\"T_02857_row488_col7\" class=\"data row488 col7\" >0.4813</td>\n",
              "      <td id=\"T_02857_row488_col8\" class=\"data row488 col8\" >0.4284</td>\n",
              "      <td id=\"T_02857_row488_col9\" class=\"data row488 col9\" >0.5194</td>\n",
              "      <td id=\"T_02857_row488_col10\" class=\"data row488 col10\" >0.3787</td>\n",
              "      <td id=\"T_02857_row488_col11\" class=\"data row488 col11\" >0.2113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row489\" class=\"row_heading level0 row489\" >489</th>\n",
              "      <td id=\"T_02857_row489_col0\" class=\"data row489 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row489_col1\" class=\"data row489 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row489_col2\" class=\"data row489 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row489_col3\" class=\"data row489 col3\" >0.7409</td>\n",
              "      <td id=\"T_02857_row489_col4\" class=\"data row489 col4\" >0.7746</td>\n",
              "      <td id=\"T_02857_row489_col5\" class=\"data row489 col5\" >0.7435</td>\n",
              "      <td id=\"T_02857_row489_col6\" class=\"data row489 col6\" >0.7279</td>\n",
              "      <td id=\"T_02857_row489_col7\" class=\"data row489 col7\" >0.7345</td>\n",
              "      <td id=\"T_02857_row489_col8\" class=\"data row489 col8\" >0.3377</td>\n",
              "      <td id=\"T_02857_row489_col9\" class=\"data row489 col9\" >0.5127</td>\n",
              "      <td id=\"T_02857_row489_col10\" class=\"data row489 col10\" >0.4332</td>\n",
              "      <td id=\"T_02857_row489_col11\" class=\"data row489 col11\" >0.4948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row490\" class=\"row_heading level0 row490\" >490</th>\n",
              "      <td id=\"T_02857_row490_col0\" class=\"data row490 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row490_col1\" class=\"data row490 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row490_col2\" class=\"data row490 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row490_col3\" class=\"data row490 col3\" >0.7409</td>\n",
              "      <td id=\"T_02857_row490_col4\" class=\"data row490 col4\" >0.7746</td>\n",
              "      <td id=\"T_02857_row490_col5\" class=\"data row490 col5\" >0.7435</td>\n",
              "      <td id=\"T_02857_row490_col6\" class=\"data row490 col6\" >0.7279</td>\n",
              "      <td id=\"T_02857_row490_col7\" class=\"data row490 col7\" >0.7345</td>\n",
              "      <td id=\"T_02857_row490_col8\" class=\"data row490 col8\" >0.3377</td>\n",
              "      <td id=\"T_02857_row490_col9\" class=\"data row490 col9\" >0.5127</td>\n",
              "      <td id=\"T_02857_row490_col10\" class=\"data row490 col10\" >0.4332</td>\n",
              "      <td id=\"T_02857_row490_col11\" class=\"data row490 col11\" >0.4948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row491\" class=\"row_heading level0 row491\" >491</th>\n",
              "      <td id=\"T_02857_row491_col0\" class=\"data row491 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row491_col1\" class=\"data row491 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row491_col2\" class=\"data row491 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row491_col3\" class=\"data row491 col3\" >0.7325</td>\n",
              "      <td id=\"T_02857_row491_col4\" class=\"data row491 col4\" >0.7243</td>\n",
              "      <td id=\"T_02857_row491_col5\" class=\"data row491 col5\" >0.7209</td>\n",
              "      <td id=\"T_02857_row491_col6\" class=\"data row491 col6\" >0.6931</td>\n",
              "      <td id=\"T_02857_row491_col7\" class=\"data row491 col7\" >0.6814</td>\n",
              "      <td id=\"T_02857_row491_col8\" class=\"data row491 col8\" >0.3014</td>\n",
              "      <td id=\"T_02857_row491_col9\" class=\"data row491 col9\" >0.4174</td>\n",
              "      <td id=\"T_02857_row491_col10\" class=\"data row491 col10\" >0.3414</td>\n",
              "      <td id=\"T_02857_row491_col11\" class=\"data row491 col11\" >0.4968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row492\" class=\"row_heading level0 row492\" >492</th>\n",
              "      <td id=\"T_02857_row492_col0\" class=\"data row492 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row492_col1\" class=\"data row492 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row492_col2\" class=\"data row492 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row492_col3\" class=\"data row492 col3\" >0.7325</td>\n",
              "      <td id=\"T_02857_row492_col4\" class=\"data row492 col4\" >0.7243</td>\n",
              "      <td id=\"T_02857_row492_col5\" class=\"data row492 col5\" >0.7209</td>\n",
              "      <td id=\"T_02857_row492_col6\" class=\"data row492 col6\" >0.6931</td>\n",
              "      <td id=\"T_02857_row492_col7\" class=\"data row492 col7\" >0.6814</td>\n",
              "      <td id=\"T_02857_row492_col8\" class=\"data row492 col8\" >0.3014</td>\n",
              "      <td id=\"T_02857_row492_col9\" class=\"data row492 col9\" >0.4174</td>\n",
              "      <td id=\"T_02857_row492_col10\" class=\"data row492 col10\" >0.3414</td>\n",
              "      <td id=\"T_02857_row492_col11\" class=\"data row492 col11\" >0.4968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row493\" class=\"row_heading level0 row493\" >493</th>\n",
              "      <td id=\"T_02857_row493_col0\" class=\"data row493 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row493_col1\" class=\"data row493 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row493_col2\" class=\"data row493 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row493_col3\" class=\"data row493 col3\" >0.5076</td>\n",
              "      <td id=\"T_02857_row493_col4\" class=\"data row493 col4\" >0.5087</td>\n",
              "      <td id=\"T_02857_row493_col5\" class=\"data row493 col5\" >0.5084</td>\n",
              "      <td id=\"T_02857_row493_col6\" class=\"data row493 col6\" >0.5015</td>\n",
              "      <td id=\"T_02857_row493_col7\" class=\"data row493 col7\" >0.5073</td>\n",
              "      <td id=\"T_02857_row493_col8\" class=\"data row493 col8\" >0.4801</td>\n",
              "      <td id=\"T_02857_row493_col9\" class=\"data row493 col9\" >0.4343</td>\n",
              "      <td id=\"T_02857_row493_col10\" class=\"data row493 col10\" >0.4800</td>\n",
              "      <td id=\"T_02857_row493_col11\" class=\"data row493 col11\" >0.5771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row494\" class=\"row_heading level0 row494\" >494</th>\n",
              "      <td id=\"T_02857_row494_col0\" class=\"data row494 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row494_col1\" class=\"data row494 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row494_col2\" class=\"data row494 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row494_col3\" class=\"data row494 col3\" >0.5076</td>\n",
              "      <td id=\"T_02857_row494_col4\" class=\"data row494 col4\" >0.5087</td>\n",
              "      <td id=\"T_02857_row494_col5\" class=\"data row494 col5\" >0.5084</td>\n",
              "      <td id=\"T_02857_row494_col6\" class=\"data row494 col6\" >0.5015</td>\n",
              "      <td id=\"T_02857_row494_col7\" class=\"data row494 col7\" >0.5073</td>\n",
              "      <td id=\"T_02857_row494_col8\" class=\"data row494 col8\" >0.4801</td>\n",
              "      <td id=\"T_02857_row494_col9\" class=\"data row494 col9\" >0.4343</td>\n",
              "      <td id=\"T_02857_row494_col10\" class=\"data row494 col10\" >0.4800</td>\n",
              "      <td id=\"T_02857_row494_col11\" class=\"data row494 col11\" >0.5771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row495\" class=\"row_heading level0 row495\" >495</th>\n",
              "      <td id=\"T_02857_row495_col0\" class=\"data row495 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row495_col1\" class=\"data row495 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row495_col2\" class=\"data row495 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row495_col3\" class=\"data row495 col3\" >0.8422</td>\n",
              "      <td id=\"T_02857_row495_col4\" class=\"data row495 col4\" >0.8490</td>\n",
              "      <td id=\"T_02857_row495_col5\" class=\"data row495 col5\" >0.8340</td>\n",
              "      <td id=\"T_02857_row495_col6\" class=\"data row495 col6\" >0.7973</td>\n",
              "      <td id=\"T_02857_row495_col7\" class=\"data row495 col7\" >0.7933</td>\n",
              "      <td id=\"T_02857_row495_col8\" class=\"data row495 col8\" >0.3941</td>\n",
              "      <td id=\"T_02857_row495_col9\" class=\"data row495 col9\" >0.4535</td>\n",
              "      <td id=\"T_02857_row495_col10\" class=\"data row495 col10\" >0.4135</td>\n",
              "      <td id=\"T_02857_row495_col11\" class=\"data row495 col11\" >0.5376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row496\" class=\"row_heading level0 row496\" >496</th>\n",
              "      <td id=\"T_02857_row496_col0\" class=\"data row496 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row496_col1\" class=\"data row496 col1\" >alpaca-lora-30b</td>\n",
              "      <td id=\"T_02857_row496_col2\" class=\"data row496 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row496_col3\" class=\"data row496 col3\" >0.8422</td>\n",
              "      <td id=\"T_02857_row496_col4\" class=\"data row496 col4\" >0.8490</td>\n",
              "      <td id=\"T_02857_row496_col5\" class=\"data row496 col5\" >0.8340</td>\n",
              "      <td id=\"T_02857_row496_col6\" class=\"data row496 col6\" >0.7973</td>\n",
              "      <td id=\"T_02857_row496_col7\" class=\"data row496 col7\" >0.7933</td>\n",
              "      <td id=\"T_02857_row496_col8\" class=\"data row496 col8\" >0.3941</td>\n",
              "      <td id=\"T_02857_row496_col9\" class=\"data row496 col9\" >0.4535</td>\n",
              "      <td id=\"T_02857_row496_col10\" class=\"data row496 col10\" >0.4135</td>\n",
              "      <td id=\"T_02857_row496_col11\" class=\"data row496 col11\" >0.5376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row497\" class=\"row_heading level0 row497\" >497</th>\n",
              "      <td id=\"T_02857_row497_col0\" class=\"data row497 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row497_col1\" class=\"data row497 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row497_col2\" class=\"data row497 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row497_col3\" class=\"data row497 col3\" >0.8979</td>\n",
              "      <td id=\"T_02857_row497_col4\" class=\"data row497 col4\" >0.9247</td>\n",
              "      <td id=\"T_02857_row497_col5\" class=\"data row497 col5\" >0.8589</td>\n",
              "      <td id=\"T_02857_row497_col6\" class=\"data row497 col6\" >0.8269</td>\n",
              "      <td id=\"T_02857_row497_col7\" class=\"data row497 col7\" >0.7837</td>\n",
              "      <td id=\"T_02857_row497_col8\" class=\"data row497 col8\" >0.3818</td>\n",
              "      <td id=\"T_02857_row497_col9\" class=\"data row497 col9\" >0.3740</td>\n",
              "      <td id=\"T_02857_row497_col10\" class=\"data row497 col10\" >0.3913</td>\n",
              "      <td id=\"T_02857_row497_col11\" class=\"data row497 col11\" >0.5084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row498\" class=\"row_heading level0 row498\" >498</th>\n",
              "      <td id=\"T_02857_row498_col0\" class=\"data row498 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row498_col1\" class=\"data row498 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row498_col2\" class=\"data row498 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row498_col3\" class=\"data row498 col3\" >0.8979</td>\n",
              "      <td id=\"T_02857_row498_col4\" class=\"data row498 col4\" >0.9247</td>\n",
              "      <td id=\"T_02857_row498_col5\" class=\"data row498 col5\" >0.8589</td>\n",
              "      <td id=\"T_02857_row498_col6\" class=\"data row498 col6\" >0.8269</td>\n",
              "      <td id=\"T_02857_row498_col7\" class=\"data row498 col7\" >0.7837</td>\n",
              "      <td id=\"T_02857_row498_col8\" class=\"data row498 col8\" >0.3818</td>\n",
              "      <td id=\"T_02857_row498_col9\" class=\"data row498 col9\" >0.3740</td>\n",
              "      <td id=\"T_02857_row498_col10\" class=\"data row498 col10\" >0.3913</td>\n",
              "      <td id=\"T_02857_row498_col11\" class=\"data row498 col11\" >0.5084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row499\" class=\"row_heading level0 row499\" >499</th>\n",
              "      <td id=\"T_02857_row499_col0\" class=\"data row499 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row499_col1\" class=\"data row499 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row499_col2\" class=\"data row499 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row499_col3\" class=\"data row499 col3\" >0.4776</td>\n",
              "      <td id=\"T_02857_row499_col4\" class=\"data row499 col4\" >0.5008</td>\n",
              "      <td id=\"T_02857_row499_col5\" class=\"data row499 col5\" >0.4987</td>\n",
              "      <td id=\"T_02857_row499_col6\" class=\"data row499 col6\" >0.5280</td>\n",
              "      <td id=\"T_02857_row499_col7\" class=\"data row499 col7\" >0.5082</td>\n",
              "      <td id=\"T_02857_row499_col8\" class=\"data row499 col8\" >0.4400</td>\n",
              "      <td id=\"T_02857_row499_col9\" class=\"data row499 col9\" >0.3978</td>\n",
              "      <td id=\"T_02857_row499_col10\" class=\"data row499 col10\" >0.4154</td>\n",
              "      <td id=\"T_02857_row499_col11\" class=\"data row499 col11\" >0.2664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row500\" class=\"row_heading level0 row500\" >500</th>\n",
              "      <td id=\"T_02857_row500_col0\" class=\"data row500 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row500_col1\" class=\"data row500 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row500_col2\" class=\"data row500 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row500_col3\" class=\"data row500 col3\" >0.4776</td>\n",
              "      <td id=\"T_02857_row500_col4\" class=\"data row500 col4\" >0.5008</td>\n",
              "      <td id=\"T_02857_row500_col5\" class=\"data row500 col5\" >0.4987</td>\n",
              "      <td id=\"T_02857_row500_col6\" class=\"data row500 col6\" >0.5280</td>\n",
              "      <td id=\"T_02857_row500_col7\" class=\"data row500 col7\" >0.5082</td>\n",
              "      <td id=\"T_02857_row500_col8\" class=\"data row500 col8\" >0.4400</td>\n",
              "      <td id=\"T_02857_row500_col9\" class=\"data row500 col9\" >0.3978</td>\n",
              "      <td id=\"T_02857_row500_col10\" class=\"data row500 col10\" >0.4154</td>\n",
              "      <td id=\"T_02857_row500_col11\" class=\"data row500 col11\" >0.2664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row501\" class=\"row_heading level0 row501\" >501</th>\n",
              "      <td id=\"T_02857_row501_col0\" class=\"data row501 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row501_col1\" class=\"data row501 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row501_col2\" class=\"data row501 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row501_col3\" class=\"data row501 col3\" >0.4305</td>\n",
              "      <td id=\"T_02857_row501_col4\" class=\"data row501 col4\" >0.4284</td>\n",
              "      <td id=\"T_02857_row501_col5\" class=\"data row501 col5\" >0.4119</td>\n",
              "      <td id=\"T_02857_row501_col6\" class=\"data row501 col6\" >0.4438</td>\n",
              "      <td id=\"T_02857_row501_col7\" class=\"data row501 col7\" >0.4111</td>\n",
              "      <td id=\"T_02857_row501_col8\" class=\"data row501 col8\" >0.3825</td>\n",
              "      <td id=\"T_02857_row501_col9\" class=\"data row501 col9\" >0.4405</td>\n",
              "      <td id=\"T_02857_row501_col10\" class=\"data row501 col10\" >0.3495</td>\n",
              "      <td id=\"T_02857_row501_col11\" class=\"data row501 col11\" >0.1789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row502\" class=\"row_heading level0 row502\" >502</th>\n",
              "      <td id=\"T_02857_row502_col0\" class=\"data row502 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row502_col1\" class=\"data row502 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row502_col2\" class=\"data row502 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row502_col3\" class=\"data row502 col3\" >0.4305</td>\n",
              "      <td id=\"T_02857_row502_col4\" class=\"data row502 col4\" >0.4284</td>\n",
              "      <td id=\"T_02857_row502_col5\" class=\"data row502 col5\" >0.4119</td>\n",
              "      <td id=\"T_02857_row502_col6\" class=\"data row502 col6\" >0.4438</td>\n",
              "      <td id=\"T_02857_row502_col7\" class=\"data row502 col7\" >0.4111</td>\n",
              "      <td id=\"T_02857_row502_col8\" class=\"data row502 col8\" >0.3825</td>\n",
              "      <td id=\"T_02857_row502_col9\" class=\"data row502 col9\" >0.4405</td>\n",
              "      <td id=\"T_02857_row502_col10\" class=\"data row502 col10\" >0.3495</td>\n",
              "      <td id=\"T_02857_row502_col11\" class=\"data row502 col11\" >0.1789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row503\" class=\"row_heading level0 row503\" >503</th>\n",
              "      <td id=\"T_02857_row503_col0\" class=\"data row503 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row503_col1\" class=\"data row503 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row503_col2\" class=\"data row503 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row503_col3\" class=\"data row503 col3\" >0.7658</td>\n",
              "      <td id=\"T_02857_row503_col4\" class=\"data row503 col4\" >0.8302</td>\n",
              "      <td id=\"T_02857_row503_col5\" class=\"data row503 col5\" >0.7375</td>\n",
              "      <td id=\"T_02857_row503_col6\" class=\"data row503 col6\" >0.7148</td>\n",
              "      <td id=\"T_02857_row503_col7\" class=\"data row503 col7\" >0.6393</td>\n",
              "      <td id=\"T_02857_row503_col8\" class=\"data row503 col8\" >0.3223</td>\n",
              "      <td id=\"T_02857_row503_col9\" class=\"data row503 col9\" >0.3518</td>\n",
              "      <td id=\"T_02857_row503_col10\" class=\"data row503 col10\" >0.3565</td>\n",
              "      <td id=\"T_02857_row503_col11\" class=\"data row503 col11\" >0.4170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row504\" class=\"row_heading level0 row504\" >504</th>\n",
              "      <td id=\"T_02857_row504_col0\" class=\"data row504 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row504_col1\" class=\"data row504 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row504_col2\" class=\"data row504 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row504_col3\" class=\"data row504 col3\" >0.7658</td>\n",
              "      <td id=\"T_02857_row504_col4\" class=\"data row504 col4\" >0.8302</td>\n",
              "      <td id=\"T_02857_row504_col5\" class=\"data row504 col5\" >0.7375</td>\n",
              "      <td id=\"T_02857_row504_col6\" class=\"data row504 col6\" >0.7148</td>\n",
              "      <td id=\"T_02857_row504_col7\" class=\"data row504 col7\" >0.6393</td>\n",
              "      <td id=\"T_02857_row504_col8\" class=\"data row504 col8\" >0.3223</td>\n",
              "      <td id=\"T_02857_row504_col9\" class=\"data row504 col9\" >0.3518</td>\n",
              "      <td id=\"T_02857_row504_col10\" class=\"data row504 col10\" >0.3565</td>\n",
              "      <td id=\"T_02857_row504_col11\" class=\"data row504 col11\" >0.4170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row505\" class=\"row_heading level0 row505\" >505</th>\n",
              "      <td id=\"T_02857_row505_col0\" class=\"data row505 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row505_col1\" class=\"data row505 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row505_col2\" class=\"data row505 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row505_col3\" class=\"data row505 col3\" >0.7957</td>\n",
              "      <td id=\"T_02857_row505_col4\" class=\"data row505 col4\" >0.7862</td>\n",
              "      <td id=\"T_02857_row505_col5\" class=\"data row505 col5\" >0.7232</td>\n",
              "      <td id=\"T_02857_row505_col6\" class=\"data row505 col6\" >0.7055</td>\n",
              "      <td id=\"T_02857_row505_col7\" class=\"data row505 col7\" >0.6741</td>\n",
              "      <td id=\"T_02857_row505_col8\" class=\"data row505 col8\" >0.2717</td>\n",
              "      <td id=\"T_02857_row505_col9\" class=\"data row505 col9\" >0.3442</td>\n",
              "      <td id=\"T_02857_row505_col10\" class=\"data row505 col10\" >0.3028</td>\n",
              "      <td id=\"T_02857_row505_col11\" class=\"data row505 col11\" >0.4716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row506\" class=\"row_heading level0 row506\" >506</th>\n",
              "      <td id=\"T_02857_row506_col0\" class=\"data row506 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row506_col1\" class=\"data row506 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row506_col2\" class=\"data row506 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row506_col3\" class=\"data row506 col3\" >0.7957</td>\n",
              "      <td id=\"T_02857_row506_col4\" class=\"data row506 col4\" >0.7862</td>\n",
              "      <td id=\"T_02857_row506_col5\" class=\"data row506 col5\" >0.7232</td>\n",
              "      <td id=\"T_02857_row506_col6\" class=\"data row506 col6\" >0.7055</td>\n",
              "      <td id=\"T_02857_row506_col7\" class=\"data row506 col7\" >0.6741</td>\n",
              "      <td id=\"T_02857_row506_col8\" class=\"data row506 col8\" >0.2717</td>\n",
              "      <td id=\"T_02857_row506_col9\" class=\"data row506 col9\" >0.3442</td>\n",
              "      <td id=\"T_02857_row506_col10\" class=\"data row506 col10\" >0.3028</td>\n",
              "      <td id=\"T_02857_row506_col11\" class=\"data row506 col11\" >0.4716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row507\" class=\"row_heading level0 row507\" >507</th>\n",
              "      <td id=\"T_02857_row507_col0\" class=\"data row507 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row507_col1\" class=\"data row507 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row507_col2\" class=\"data row507 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row507_col3\" class=\"data row507 col3\" >0.7438</td>\n",
              "      <td id=\"T_02857_row507_col4\" class=\"data row507 col4\" >0.7688</td>\n",
              "      <td id=\"T_02857_row507_col5\" class=\"data row507 col5\" >0.7638</td>\n",
              "      <td id=\"T_02857_row507_col6\" class=\"data row507 col6\" >0.7448</td>\n",
              "      <td id=\"T_02857_row507_col7\" class=\"data row507 col7\" >0.7219</td>\n",
              "      <td id=\"T_02857_row507_col8\" class=\"data row507 col8\" >0.5160</td>\n",
              "      <td id=\"T_02857_row507_col9\" class=\"data row507 col9\" >0.4643</td>\n",
              "      <td id=\"T_02857_row507_col10\" class=\"data row507 col10\" >0.5791</td>\n",
              "      <td id=\"T_02857_row507_col11\" class=\"data row507 col11\" >0.5217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row508\" class=\"row_heading level0 row508\" >508</th>\n",
              "      <td id=\"T_02857_row508_col0\" class=\"data row508 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row508_col1\" class=\"data row508 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row508_col2\" class=\"data row508 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row508_col3\" class=\"data row508 col3\" >0.7438</td>\n",
              "      <td id=\"T_02857_row508_col4\" class=\"data row508 col4\" >0.7688</td>\n",
              "      <td id=\"T_02857_row508_col5\" class=\"data row508 col5\" >0.7638</td>\n",
              "      <td id=\"T_02857_row508_col6\" class=\"data row508 col6\" >0.7448</td>\n",
              "      <td id=\"T_02857_row508_col7\" class=\"data row508 col7\" >0.7219</td>\n",
              "      <td id=\"T_02857_row508_col8\" class=\"data row508 col8\" >0.5160</td>\n",
              "      <td id=\"T_02857_row508_col9\" class=\"data row508 col9\" >0.4643</td>\n",
              "      <td id=\"T_02857_row508_col10\" class=\"data row508 col10\" >0.5791</td>\n",
              "      <td id=\"T_02857_row508_col11\" class=\"data row508 col11\" >0.5217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row509\" class=\"row_heading level0 row509\" >509</th>\n",
              "      <td id=\"T_02857_row509_col0\" class=\"data row509 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row509_col1\" class=\"data row509 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row509_col2\" class=\"data row509 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row509_col3\" class=\"data row509 col3\" >0.9154</td>\n",
              "      <td id=\"T_02857_row509_col4\" class=\"data row509 col4\" >0.9367</td>\n",
              "      <td id=\"T_02857_row509_col5\" class=\"data row509 col5\" >0.8283</td>\n",
              "      <td id=\"T_02857_row509_col6\" class=\"data row509 col6\" >0.8110</td>\n",
              "      <td id=\"T_02857_row509_col7\" class=\"data row509 col7\" >0.7439</td>\n",
              "      <td id=\"T_02857_row509_col8\" class=\"data row509 col8\" >0.3780</td>\n",
              "      <td id=\"T_02857_row509_col9\" class=\"data row509 col9\" >0.3729</td>\n",
              "      <td id=\"T_02857_row509_col10\" class=\"data row509 col10\" >0.3675</td>\n",
              "      <td id=\"T_02857_row509_col11\" class=\"data row509 col11\" >0.4894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row510\" class=\"row_heading level0 row510\" >510</th>\n",
              "      <td id=\"T_02857_row510_col0\" class=\"data row510 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row510_col1\" class=\"data row510 col1\" >gpt-3.5-turbo</td>\n",
              "      <td id=\"T_02857_row510_col2\" class=\"data row510 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row510_col3\" class=\"data row510 col3\" >0.9154</td>\n",
              "      <td id=\"T_02857_row510_col4\" class=\"data row510 col4\" >0.9367</td>\n",
              "      <td id=\"T_02857_row510_col5\" class=\"data row510 col5\" >0.8283</td>\n",
              "      <td id=\"T_02857_row510_col6\" class=\"data row510 col6\" >0.8110</td>\n",
              "      <td id=\"T_02857_row510_col7\" class=\"data row510 col7\" >0.7439</td>\n",
              "      <td id=\"T_02857_row510_col8\" class=\"data row510 col8\" >0.3780</td>\n",
              "      <td id=\"T_02857_row510_col9\" class=\"data row510 col9\" >0.3729</td>\n",
              "      <td id=\"T_02857_row510_col10\" class=\"data row510 col10\" >0.3675</td>\n",
              "      <td id=\"T_02857_row510_col11\" class=\"data row510 col11\" >0.4894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row511\" class=\"row_heading level0 row511\" >511</th>\n",
              "      <td id=\"T_02857_row511_col0\" class=\"data row511 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row511_col1\" class=\"data row511 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row511_col2\" class=\"data row511 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row511_col3\" class=\"data row511 col3\" >0.8780</td>\n",
              "      <td id=\"T_02857_row511_col4\" class=\"data row511 col4\" >0.8399</td>\n",
              "      <td id=\"T_02857_row511_col5\" class=\"data row511 col5\" >0.7670</td>\n",
              "      <td id=\"T_02857_row511_col6\" class=\"data row511 col6\" >0.7482</td>\n",
              "      <td id=\"T_02857_row511_col7\" class=\"data row511 col7\" >0.6940</td>\n",
              "      <td id=\"T_02857_row511_col8\" class=\"data row511 col8\" >0.3493</td>\n",
              "      <td id=\"T_02857_row511_col9\" class=\"data row511 col9\" >0.3725</td>\n",
              "      <td id=\"T_02857_row511_col10\" class=\"data row511 col10\" >0.3957</td>\n",
              "      <td id=\"T_02857_row511_col11\" class=\"data row511 col11\" >0.4749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row512\" class=\"row_heading level0 row512\" >512</th>\n",
              "      <td id=\"T_02857_row512_col0\" class=\"data row512 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row512_col1\" class=\"data row512 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row512_col2\" class=\"data row512 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row512_col3\" class=\"data row512 col3\" >0.8780</td>\n",
              "      <td id=\"T_02857_row512_col4\" class=\"data row512 col4\" >0.8399</td>\n",
              "      <td id=\"T_02857_row512_col5\" class=\"data row512 col5\" >0.7670</td>\n",
              "      <td id=\"T_02857_row512_col6\" class=\"data row512 col6\" >0.7482</td>\n",
              "      <td id=\"T_02857_row512_col7\" class=\"data row512 col7\" >0.6940</td>\n",
              "      <td id=\"T_02857_row512_col8\" class=\"data row512 col8\" >0.3493</td>\n",
              "      <td id=\"T_02857_row512_col9\" class=\"data row512 col9\" >0.3725</td>\n",
              "      <td id=\"T_02857_row512_col10\" class=\"data row512 col10\" >0.3957</td>\n",
              "      <td id=\"T_02857_row512_col11\" class=\"data row512 col11\" >0.4749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row513\" class=\"row_heading level0 row513\" >513</th>\n",
              "      <td id=\"T_02857_row513_col0\" class=\"data row513 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row513_col1\" class=\"data row513 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row513_col2\" class=\"data row513 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row513_col3\" class=\"data row513 col3\" >0.5808</td>\n",
              "      <td id=\"T_02857_row513_col4\" class=\"data row513 col4\" >0.5443</td>\n",
              "      <td id=\"T_02857_row513_col5\" class=\"data row513 col5\" >0.5301</td>\n",
              "      <td id=\"T_02857_row513_col6\" class=\"data row513 col6\" >0.5618</td>\n",
              "      <td id=\"T_02857_row513_col7\" class=\"data row513 col7\" >0.5403</td>\n",
              "      <td id=\"T_02857_row513_col8\" class=\"data row513 col8\" >0.4435</td>\n",
              "      <td id=\"T_02857_row513_col9\" class=\"data row513 col9\" >0.3991</td>\n",
              "      <td id=\"T_02857_row513_col10\" class=\"data row513 col10\" >0.4183</td>\n",
              "      <td id=\"T_02857_row513_col11\" class=\"data row513 col11\" >0.2942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row514\" class=\"row_heading level0 row514\" >514</th>\n",
              "      <td id=\"T_02857_row514_col0\" class=\"data row514 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row514_col1\" class=\"data row514 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row514_col2\" class=\"data row514 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row514_col3\" class=\"data row514 col3\" >0.5808</td>\n",
              "      <td id=\"T_02857_row514_col4\" class=\"data row514 col4\" >0.5443</td>\n",
              "      <td id=\"T_02857_row514_col5\" class=\"data row514 col5\" >0.5301</td>\n",
              "      <td id=\"T_02857_row514_col6\" class=\"data row514 col6\" >0.5618</td>\n",
              "      <td id=\"T_02857_row514_col7\" class=\"data row514 col7\" >0.5403</td>\n",
              "      <td id=\"T_02857_row514_col8\" class=\"data row514 col8\" >0.4435</td>\n",
              "      <td id=\"T_02857_row514_col9\" class=\"data row514 col9\" >0.3991</td>\n",
              "      <td id=\"T_02857_row514_col10\" class=\"data row514 col10\" >0.4183</td>\n",
              "      <td id=\"T_02857_row514_col11\" class=\"data row514 col11\" >0.2942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row515\" class=\"row_heading level0 row515\" >515</th>\n",
              "      <td id=\"T_02857_row515_col0\" class=\"data row515 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row515_col1\" class=\"data row515 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row515_col2\" class=\"data row515 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row515_col3\" class=\"data row515 col3\" >0.4360</td>\n",
              "      <td id=\"T_02857_row515_col4\" class=\"data row515 col4\" >0.4274</td>\n",
              "      <td id=\"T_02857_row515_col5\" class=\"data row515 col5\" >0.4131</td>\n",
              "      <td id=\"T_02857_row515_col6\" class=\"data row515 col6\" >0.4405</td>\n",
              "      <td id=\"T_02857_row515_col7\" class=\"data row515 col7\" >0.4165</td>\n",
              "      <td id=\"T_02857_row515_col8\" class=\"data row515 col8\" >0.3899</td>\n",
              "      <td id=\"T_02857_row515_col9\" class=\"data row515 col9\" >0.4641</td>\n",
              "      <td id=\"T_02857_row515_col10\" class=\"data row515 col10\" >0.3610</td>\n",
              "      <td id=\"T_02857_row515_col11\" class=\"data row515 col11\" >0.1849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row516\" class=\"row_heading level0 row516\" >516</th>\n",
              "      <td id=\"T_02857_row516_col0\" class=\"data row516 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row516_col1\" class=\"data row516 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row516_col2\" class=\"data row516 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row516_col3\" class=\"data row516 col3\" >0.4360</td>\n",
              "      <td id=\"T_02857_row516_col4\" class=\"data row516 col4\" >0.4274</td>\n",
              "      <td id=\"T_02857_row516_col5\" class=\"data row516 col5\" >0.4131</td>\n",
              "      <td id=\"T_02857_row516_col6\" class=\"data row516 col6\" >0.4405</td>\n",
              "      <td id=\"T_02857_row516_col7\" class=\"data row516 col7\" >0.4165</td>\n",
              "      <td id=\"T_02857_row516_col8\" class=\"data row516 col8\" >0.3899</td>\n",
              "      <td id=\"T_02857_row516_col9\" class=\"data row516 col9\" >0.4641</td>\n",
              "      <td id=\"T_02857_row516_col10\" class=\"data row516 col10\" >0.3610</td>\n",
              "      <td id=\"T_02857_row516_col11\" class=\"data row516 col11\" >0.1849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row517\" class=\"row_heading level0 row517\" >517</th>\n",
              "      <td id=\"T_02857_row517_col0\" class=\"data row517 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row517_col1\" class=\"data row517 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row517_col2\" class=\"data row517 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row517_col3\" class=\"data row517 col3\" >0.8353</td>\n",
              "      <td id=\"T_02857_row517_col4\" class=\"data row517 col4\" >0.7972</td>\n",
              "      <td id=\"T_02857_row517_col5\" class=\"data row517 col5\" >0.6787</td>\n",
              "      <td id=\"T_02857_row517_col6\" class=\"data row517 col6\" >0.6849</td>\n",
              "      <td id=\"T_02857_row517_col7\" class=\"data row517 col7\" >0.6009</td>\n",
              "      <td id=\"T_02857_row517_col8\" class=\"data row517 col8\" >0.3319</td>\n",
              "      <td id=\"T_02857_row517_col9\" class=\"data row517 col9\" >0.3581</td>\n",
              "      <td id=\"T_02857_row517_col10\" class=\"data row517 col10\" >0.3732</td>\n",
              "      <td id=\"T_02857_row517_col11\" class=\"data row517 col11\" >0.4024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row518\" class=\"row_heading level0 row518\" >518</th>\n",
              "      <td id=\"T_02857_row518_col0\" class=\"data row518 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row518_col1\" class=\"data row518 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row518_col2\" class=\"data row518 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row518_col3\" class=\"data row518 col3\" >0.8353</td>\n",
              "      <td id=\"T_02857_row518_col4\" class=\"data row518 col4\" >0.7972</td>\n",
              "      <td id=\"T_02857_row518_col5\" class=\"data row518 col5\" >0.6787</td>\n",
              "      <td id=\"T_02857_row518_col6\" class=\"data row518 col6\" >0.6849</td>\n",
              "      <td id=\"T_02857_row518_col7\" class=\"data row518 col7\" >0.6009</td>\n",
              "      <td id=\"T_02857_row518_col8\" class=\"data row518 col8\" >0.3319</td>\n",
              "      <td id=\"T_02857_row518_col9\" class=\"data row518 col9\" >0.3581</td>\n",
              "      <td id=\"T_02857_row518_col10\" class=\"data row518 col10\" >0.3732</td>\n",
              "      <td id=\"T_02857_row518_col11\" class=\"data row518 col11\" >0.4024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row519\" class=\"row_heading level0 row519\" >519</th>\n",
              "      <td id=\"T_02857_row519_col0\" class=\"data row519 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row519_col1\" class=\"data row519 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row519_col2\" class=\"data row519 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row519_col3\" class=\"data row519 col3\" >0.7715</td>\n",
              "      <td id=\"T_02857_row519_col4\" class=\"data row519 col4\" >0.7939</td>\n",
              "      <td id=\"T_02857_row519_col5\" class=\"data row519 col5\" >0.7170</td>\n",
              "      <td id=\"T_02857_row519_col6\" class=\"data row519 col6\" >0.6981</td>\n",
              "      <td id=\"T_02857_row519_col7\" class=\"data row519 col7\" >0.6605</td>\n",
              "      <td id=\"T_02857_row519_col8\" class=\"data row519 col8\" >0.2911</td>\n",
              "      <td id=\"T_02857_row519_col9\" class=\"data row519 col9\" >0.3493</td>\n",
              "      <td id=\"T_02857_row519_col10\" class=\"data row519 col10\" >0.3226</td>\n",
              "      <td id=\"T_02857_row519_col11\" class=\"data row519 col11\" >0.4547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row520\" class=\"row_heading level0 row520\" >520</th>\n",
              "      <td id=\"T_02857_row520_col0\" class=\"data row520 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row520_col1\" class=\"data row520 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row520_col2\" class=\"data row520 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row520_col3\" class=\"data row520 col3\" >0.7715</td>\n",
              "      <td id=\"T_02857_row520_col4\" class=\"data row520 col4\" >0.7939</td>\n",
              "      <td id=\"T_02857_row520_col5\" class=\"data row520 col5\" >0.7170</td>\n",
              "      <td id=\"T_02857_row520_col6\" class=\"data row520 col6\" >0.6981</td>\n",
              "      <td id=\"T_02857_row520_col7\" class=\"data row520 col7\" >0.6605</td>\n",
              "      <td id=\"T_02857_row520_col8\" class=\"data row520 col8\" >0.2911</td>\n",
              "      <td id=\"T_02857_row520_col9\" class=\"data row520 col9\" >0.3493</td>\n",
              "      <td id=\"T_02857_row520_col10\" class=\"data row520 col10\" >0.3226</td>\n",
              "      <td id=\"T_02857_row520_col11\" class=\"data row520 col11\" >0.4547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row521\" class=\"row_heading level0 row521\" >521</th>\n",
              "      <td id=\"T_02857_row521_col0\" class=\"data row521 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row521_col1\" class=\"data row521 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row521_col2\" class=\"data row521 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row521_col3\" class=\"data row521 col3\" >0.6501</td>\n",
              "      <td id=\"T_02857_row521_col4\" class=\"data row521 col4\" >0.6471</td>\n",
              "      <td id=\"T_02857_row521_col5\" class=\"data row521 col5\" >0.6394</td>\n",
              "      <td id=\"T_02857_row521_col6\" class=\"data row521 col6\" >0.6294</td>\n",
              "      <td id=\"T_02857_row521_col7\" class=\"data row521 col7\" >0.6128</td>\n",
              "      <td id=\"T_02857_row521_col8\" class=\"data row521 col8\" >0.3543</td>\n",
              "      <td id=\"T_02857_row521_col9\" class=\"data row521 col9\" >0.3969</td>\n",
              "      <td id=\"T_02857_row521_col10\" class=\"data row521 col10\" >0.5274</td>\n",
              "      <td id=\"T_02857_row521_col11\" class=\"data row521 col11\" >0.4668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row522\" class=\"row_heading level0 row522\" >522</th>\n",
              "      <td id=\"T_02857_row522_col0\" class=\"data row522 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row522_col1\" class=\"data row522 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row522_col2\" class=\"data row522 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row522_col3\" class=\"data row522 col3\" >0.6501</td>\n",
              "      <td id=\"T_02857_row522_col4\" class=\"data row522 col4\" >0.6471</td>\n",
              "      <td id=\"T_02857_row522_col5\" class=\"data row522 col5\" >0.6394</td>\n",
              "      <td id=\"T_02857_row522_col6\" class=\"data row522 col6\" >0.6294</td>\n",
              "      <td id=\"T_02857_row522_col7\" class=\"data row522 col7\" >0.6128</td>\n",
              "      <td id=\"T_02857_row522_col8\" class=\"data row522 col8\" >0.3543</td>\n",
              "      <td id=\"T_02857_row522_col9\" class=\"data row522 col9\" >0.3969</td>\n",
              "      <td id=\"T_02857_row522_col10\" class=\"data row522 col10\" >0.5274</td>\n",
              "      <td id=\"T_02857_row522_col11\" class=\"data row522 col11\" >0.4668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row523\" class=\"row_heading level0 row523\" >523</th>\n",
              "      <td id=\"T_02857_row523_col0\" class=\"data row523 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row523_col1\" class=\"data row523 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row523_col2\" class=\"data row523 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row523_col3\" class=\"data row523 col3\" >0.9127</td>\n",
              "      <td id=\"T_02857_row523_col4\" class=\"data row523 col4\" >0.8878</td>\n",
              "      <td id=\"T_02857_row523_col5\" class=\"data row523 col5\" >0.8060</td>\n",
              "      <td id=\"T_02857_row523_col6\" class=\"data row523 col6\" >0.7937</td>\n",
              "      <td id=\"T_02857_row523_col7\" class=\"data row523 col7\" >0.7587</td>\n",
              "      <td id=\"T_02857_row523_col8\" class=\"data row523 col8\" >0.3723</td>\n",
              "      <td id=\"T_02857_row523_col9\" class=\"data row523 col9\" >0.3995</td>\n",
              "      <td id=\"T_02857_row523_col10\" class=\"data row523 col10\" >0.4030</td>\n",
              "      <td id=\"T_02857_row523_col11\" class=\"data row523 col11\" >0.4952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row524\" class=\"row_heading level0 row524\" >524</th>\n",
              "      <td id=\"T_02857_row524_col0\" class=\"data row524 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row524_col1\" class=\"data row524 col1\" >gpt-4</td>\n",
              "      <td id=\"T_02857_row524_col2\" class=\"data row524 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row524_col3\" class=\"data row524 col3\" >0.9127</td>\n",
              "      <td id=\"T_02857_row524_col4\" class=\"data row524 col4\" >0.8878</td>\n",
              "      <td id=\"T_02857_row524_col5\" class=\"data row524 col5\" >0.8060</td>\n",
              "      <td id=\"T_02857_row524_col6\" class=\"data row524 col6\" >0.7937</td>\n",
              "      <td id=\"T_02857_row524_col7\" class=\"data row524 col7\" >0.7587</td>\n",
              "      <td id=\"T_02857_row524_col8\" class=\"data row524 col8\" >0.3723</td>\n",
              "      <td id=\"T_02857_row524_col9\" class=\"data row524 col9\" >0.3995</td>\n",
              "      <td id=\"T_02857_row524_col10\" class=\"data row524 col10\" >0.4030</td>\n",
              "      <td id=\"T_02857_row524_col11\" class=\"data row524 col11\" >0.4952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row525\" class=\"row_heading level0 row525\" >525</th>\n",
              "      <td id=\"T_02857_row525_col0\" class=\"data row525 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row525_col1\" class=\"data row525 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row525_col2\" class=\"data row525 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row525_col3\" class=\"data row525 col3\" >0.5031</td>\n",
              "      <td id=\"T_02857_row525_col4\" class=\"data row525 col4\" >0.3670</td>\n",
              "      <td id=\"T_02857_row525_col5\" class=\"data row525 col5\" >0.3187</td>\n",
              "      <td id=\"T_02857_row525_col6\" class=\"data row525 col6\" >0.4918</td>\n",
              "      <td id=\"T_02857_row525_col7\" class=\"data row525 col7\" >0.4144</td>\n",
              "      <td id=\"T_02857_row525_col8\" class=\"data row525 col8\" >0.6656</td>\n",
              "      <td id=\"T_02857_row525_col9\" class=\"data row525 col9\" >0.7246</td>\n",
              "      <td id=\"T_02857_row525_col10\" class=\"data row525 col10\" >0.6322</td>\n",
              "      <td id=\"T_02857_row525_col11\" class=\"data row525 col11\" >0.4324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row526\" class=\"row_heading level0 row526\" >526</th>\n",
              "      <td id=\"T_02857_row526_col0\" class=\"data row526 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row526_col1\" class=\"data row526 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row526_col2\" class=\"data row526 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row526_col3\" class=\"data row526 col3\" >0.5031</td>\n",
              "      <td id=\"T_02857_row526_col4\" class=\"data row526 col4\" >0.3670</td>\n",
              "      <td id=\"T_02857_row526_col5\" class=\"data row526 col5\" >0.3187</td>\n",
              "      <td id=\"T_02857_row526_col6\" class=\"data row526 col6\" >0.4918</td>\n",
              "      <td id=\"T_02857_row526_col7\" class=\"data row526 col7\" >0.4144</td>\n",
              "      <td id=\"T_02857_row526_col8\" class=\"data row526 col8\" >0.6656</td>\n",
              "      <td id=\"T_02857_row526_col9\" class=\"data row526 col9\" >0.7246</td>\n",
              "      <td id=\"T_02857_row526_col10\" class=\"data row526 col10\" >0.6322</td>\n",
              "      <td id=\"T_02857_row526_col11\" class=\"data row526 col11\" >0.4324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row527\" class=\"row_heading level0 row527\" >527</th>\n",
              "      <td id=\"T_02857_row527_col0\" class=\"data row527 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row527_col1\" class=\"data row527 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row527_col2\" class=\"data row527 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row527_col3\" class=\"data row527 col3\" >0.4220</td>\n",
              "      <td id=\"T_02857_row527_col4\" class=\"data row527 col4\" >0.4194</td>\n",
              "      <td id=\"T_02857_row527_col5\" class=\"data row527 col5\" >0.4231</td>\n",
              "      <td id=\"T_02857_row527_col6\" class=\"data row527 col6\" >0.4205</td>\n",
              "      <td id=\"T_02857_row527_col7\" class=\"data row527 col7\" >0.4223</td>\n",
              "      <td id=\"T_02857_row527_col8\" class=\"data row527 col8\" >0.4197</td>\n",
              "      <td id=\"T_02857_row527_col9\" class=\"data row527 col9\" >0.4203</td>\n",
              "      <td id=\"T_02857_row527_col10\" class=\"data row527 col10\" >0.4188</td>\n",
              "      <td id=\"T_02857_row527_col11\" class=\"data row527 col11\" >0.5447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row528\" class=\"row_heading level0 row528\" >528</th>\n",
              "      <td id=\"T_02857_row528_col0\" class=\"data row528 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row528_col1\" class=\"data row528 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row528_col2\" class=\"data row528 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row528_col3\" class=\"data row528 col3\" >0.4220</td>\n",
              "      <td id=\"T_02857_row528_col4\" class=\"data row528 col4\" >0.4194</td>\n",
              "      <td id=\"T_02857_row528_col5\" class=\"data row528 col5\" >0.4231</td>\n",
              "      <td id=\"T_02857_row528_col6\" class=\"data row528 col6\" >0.4205</td>\n",
              "      <td id=\"T_02857_row528_col7\" class=\"data row528 col7\" >0.4223</td>\n",
              "      <td id=\"T_02857_row528_col8\" class=\"data row528 col8\" >0.4197</td>\n",
              "      <td id=\"T_02857_row528_col9\" class=\"data row528 col9\" >0.4203</td>\n",
              "      <td id=\"T_02857_row528_col10\" class=\"data row528 col10\" >0.4188</td>\n",
              "      <td id=\"T_02857_row528_col11\" class=\"data row528 col11\" >0.5447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row529\" class=\"row_heading level0 row529\" >529</th>\n",
              "      <td id=\"T_02857_row529_col0\" class=\"data row529 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row529_col1\" class=\"data row529 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row529_col2\" class=\"data row529 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row529_col3\" class=\"data row529 col3\" >0.4169</td>\n",
              "      <td id=\"T_02857_row529_col4\" class=\"data row529 col4\" >0.4162</td>\n",
              "      <td id=\"T_02857_row529_col5\" class=\"data row529 col5\" >0.4353</td>\n",
              "      <td id=\"T_02857_row529_col6\" class=\"data row529 col6\" >0.4505</td>\n",
              "      <td id=\"T_02857_row529_col7\" class=\"data row529 col7\" >0.4104</td>\n",
              "      <td id=\"T_02857_row529_col8\" class=\"data row529 col8\" >0.4763</td>\n",
              "      <td id=\"T_02857_row529_col9\" class=\"data row529 col9\" >0.4983</td>\n",
              "      <td id=\"T_02857_row529_col10\" class=\"data row529 col10\" >0.4771</td>\n",
              "      <td id=\"T_02857_row529_col11\" class=\"data row529 col11\" >0.4952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row530\" class=\"row_heading level0 row530\" >530</th>\n",
              "      <td id=\"T_02857_row530_col0\" class=\"data row530 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row530_col1\" class=\"data row530 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row530_col2\" class=\"data row530 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row530_col3\" class=\"data row530 col3\" >0.4169</td>\n",
              "      <td id=\"T_02857_row530_col4\" class=\"data row530 col4\" >0.4162</td>\n",
              "      <td id=\"T_02857_row530_col5\" class=\"data row530 col5\" >0.4353</td>\n",
              "      <td id=\"T_02857_row530_col6\" class=\"data row530 col6\" >0.4505</td>\n",
              "      <td id=\"T_02857_row530_col7\" class=\"data row530 col7\" >0.4104</td>\n",
              "      <td id=\"T_02857_row530_col8\" class=\"data row530 col8\" >0.4763</td>\n",
              "      <td id=\"T_02857_row530_col9\" class=\"data row530 col9\" >0.4983</td>\n",
              "      <td id=\"T_02857_row530_col10\" class=\"data row530 col10\" >0.4771</td>\n",
              "      <td id=\"T_02857_row530_col11\" class=\"data row530 col11\" >0.4952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row531\" class=\"row_heading level0 row531\" >531</th>\n",
              "      <td id=\"T_02857_row531_col0\" class=\"data row531 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row531_col1\" class=\"data row531 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row531_col2\" class=\"data row531 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row531_col3\" class=\"data row531 col3\" >0.5497</td>\n",
              "      <td id=\"T_02857_row531_col4\" class=\"data row531 col4\" >0.5172</td>\n",
              "      <td id=\"T_02857_row531_col5\" class=\"data row531 col5\" >0.5011</td>\n",
              "      <td id=\"T_02857_row531_col6\" class=\"data row531 col6\" >0.5502</td>\n",
              "      <td id=\"T_02857_row531_col7\" class=\"data row531 col7\" >0.4923</td>\n",
              "      <td id=\"T_02857_row531_col8\" class=\"data row531 col8\" >0.5247</td>\n",
              "      <td id=\"T_02857_row531_col9\" class=\"data row531 col9\" >0.5897</td>\n",
              "      <td id=\"T_02857_row531_col10\" class=\"data row531 col10\" >0.5448</td>\n",
              "      <td id=\"T_02857_row531_col11\" class=\"data row531 col11\" >0.5419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row532\" class=\"row_heading level0 row532\" >532</th>\n",
              "      <td id=\"T_02857_row532_col0\" class=\"data row532 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row532_col1\" class=\"data row532 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row532_col2\" class=\"data row532 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row532_col3\" class=\"data row532 col3\" >0.5497</td>\n",
              "      <td id=\"T_02857_row532_col4\" class=\"data row532 col4\" >0.5172</td>\n",
              "      <td id=\"T_02857_row532_col5\" class=\"data row532 col5\" >0.5011</td>\n",
              "      <td id=\"T_02857_row532_col6\" class=\"data row532 col6\" >0.5502</td>\n",
              "      <td id=\"T_02857_row532_col7\" class=\"data row532 col7\" >0.4923</td>\n",
              "      <td id=\"T_02857_row532_col8\" class=\"data row532 col8\" >0.5247</td>\n",
              "      <td id=\"T_02857_row532_col9\" class=\"data row532 col9\" >0.5897</td>\n",
              "      <td id=\"T_02857_row532_col10\" class=\"data row532 col10\" >0.5448</td>\n",
              "      <td id=\"T_02857_row532_col11\" class=\"data row532 col11\" >0.5419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row533\" class=\"row_heading level0 row533\" >533</th>\n",
              "      <td id=\"T_02857_row533_col0\" class=\"data row533 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row533_col1\" class=\"data row533 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row533_col2\" class=\"data row533 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row533_col3\" class=\"data row533 col3\" >0.5089</td>\n",
              "      <td id=\"T_02857_row533_col4\" class=\"data row533 col4\" >0.4283</td>\n",
              "      <td id=\"T_02857_row533_col5\" class=\"data row533 col5\" >0.3880</td>\n",
              "      <td id=\"T_02857_row533_col6\" class=\"data row533 col6\" >0.6115</td>\n",
              "      <td id=\"T_02857_row533_col7\" class=\"data row533 col7\" >0.4688</td>\n",
              "      <td id=\"T_02857_row533_col8\" class=\"data row533 col8\" >0.7534</td>\n",
              "      <td id=\"T_02857_row533_col9\" class=\"data row533 col9\" >0.7759</td>\n",
              "      <td id=\"T_02857_row533_col10\" class=\"data row533 col10\" >0.6835</td>\n",
              "      <td id=\"T_02857_row533_col11\" class=\"data row533 col11\" >0.4680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row534\" class=\"row_heading level0 row534\" >534</th>\n",
              "      <td id=\"T_02857_row534_col0\" class=\"data row534 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row534_col1\" class=\"data row534 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row534_col2\" class=\"data row534 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row534_col3\" class=\"data row534 col3\" >0.5089</td>\n",
              "      <td id=\"T_02857_row534_col4\" class=\"data row534 col4\" >0.4283</td>\n",
              "      <td id=\"T_02857_row534_col5\" class=\"data row534 col5\" >0.3880</td>\n",
              "      <td id=\"T_02857_row534_col6\" class=\"data row534 col6\" >0.6115</td>\n",
              "      <td id=\"T_02857_row534_col7\" class=\"data row534 col7\" >0.4688</td>\n",
              "      <td id=\"T_02857_row534_col8\" class=\"data row534 col8\" >0.7534</td>\n",
              "      <td id=\"T_02857_row534_col9\" class=\"data row534 col9\" >0.7759</td>\n",
              "      <td id=\"T_02857_row534_col10\" class=\"data row534 col10\" >0.6835</td>\n",
              "      <td id=\"T_02857_row534_col11\" class=\"data row534 col11\" >0.4680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row535\" class=\"row_heading level0 row535\" >535</th>\n",
              "      <td id=\"T_02857_row535_col0\" class=\"data row535 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row535_col1\" class=\"data row535 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row535_col2\" class=\"data row535 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row535_col3\" class=\"data row535 col3\" >0.4293</td>\n",
              "      <td id=\"T_02857_row535_col4\" class=\"data row535 col4\" >0.4279</td>\n",
              "      <td id=\"T_02857_row535_col5\" class=\"data row535 col5\" >0.4290</td>\n",
              "      <td id=\"T_02857_row535_col6\" class=\"data row535 col6\" >0.4270</td>\n",
              "      <td id=\"T_02857_row535_col7\" class=\"data row535 col7\" >0.4266</td>\n",
              "      <td id=\"T_02857_row535_col8\" class=\"data row535 col8\" >0.4255</td>\n",
              "      <td id=\"T_02857_row535_col9\" class=\"data row535 col9\" >0.4266</td>\n",
              "      <td id=\"T_02857_row535_col10\" class=\"data row535 col10\" >0.4253</td>\n",
              "      <td id=\"T_02857_row535_col11\" class=\"data row535 col11\" >0.5511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row536\" class=\"row_heading level0 row536\" >536</th>\n",
              "      <td id=\"T_02857_row536_col0\" class=\"data row536 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row536_col1\" class=\"data row536 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row536_col2\" class=\"data row536 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row536_col3\" class=\"data row536 col3\" >0.4293</td>\n",
              "      <td id=\"T_02857_row536_col4\" class=\"data row536 col4\" >0.4279</td>\n",
              "      <td id=\"T_02857_row536_col5\" class=\"data row536 col5\" >0.4290</td>\n",
              "      <td id=\"T_02857_row536_col6\" class=\"data row536 col6\" >0.4270</td>\n",
              "      <td id=\"T_02857_row536_col7\" class=\"data row536 col7\" >0.4266</td>\n",
              "      <td id=\"T_02857_row536_col8\" class=\"data row536 col8\" >0.4255</td>\n",
              "      <td id=\"T_02857_row536_col9\" class=\"data row536 col9\" >0.4266</td>\n",
              "      <td id=\"T_02857_row536_col10\" class=\"data row536 col10\" >0.4253</td>\n",
              "      <td id=\"T_02857_row536_col11\" class=\"data row536 col11\" >0.5511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row537\" class=\"row_heading level0 row537\" >537</th>\n",
              "      <td id=\"T_02857_row537_col0\" class=\"data row537 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row537_col1\" class=\"data row537 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row537_col2\" class=\"data row537 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row537_col3\" class=\"data row537 col3\" >0.5440</td>\n",
              "      <td id=\"T_02857_row537_col4\" class=\"data row537 col4\" >0.5121</td>\n",
              "      <td id=\"T_02857_row537_col5\" class=\"data row537 col5\" >0.5097</td>\n",
              "      <td id=\"T_02857_row537_col6\" class=\"data row537 col6\" >0.5627</td>\n",
              "      <td id=\"T_02857_row537_col7\" class=\"data row537 col7\" >0.5389</td>\n",
              "      <td id=\"T_02857_row537_col8\" class=\"data row537 col8\" >0.6168</td>\n",
              "      <td id=\"T_02857_row537_col9\" class=\"data row537 col9\" >0.6284</td>\n",
              "      <td id=\"T_02857_row537_col10\" class=\"data row537 col10\" >0.5837</td>\n",
              "      <td id=\"T_02857_row537_col11\" class=\"data row537 col11\" >0.5602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row538\" class=\"row_heading level0 row538\" >538</th>\n",
              "      <td id=\"T_02857_row538_col0\" class=\"data row538 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row538_col1\" class=\"data row538 col1\" >llama-65b</td>\n",
              "      <td id=\"T_02857_row538_col2\" class=\"data row538 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row538_col3\" class=\"data row538 col3\" >0.5440</td>\n",
              "      <td id=\"T_02857_row538_col4\" class=\"data row538 col4\" >0.5121</td>\n",
              "      <td id=\"T_02857_row538_col5\" class=\"data row538 col5\" >0.5097</td>\n",
              "      <td id=\"T_02857_row538_col6\" class=\"data row538 col6\" >0.5627</td>\n",
              "      <td id=\"T_02857_row538_col7\" class=\"data row538 col7\" >0.5389</td>\n",
              "      <td id=\"T_02857_row538_col8\" class=\"data row538 col8\" >0.6168</td>\n",
              "      <td id=\"T_02857_row538_col9\" class=\"data row538 col9\" >0.6284</td>\n",
              "      <td id=\"T_02857_row538_col10\" class=\"data row538 col10\" >0.5837</td>\n",
              "      <td id=\"T_02857_row538_col11\" class=\"data row538 col11\" >0.5602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row539\" class=\"row_heading level0 row539\" >539</th>\n",
              "      <td id=\"T_02857_row539_col0\" class=\"data row539 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row539_col1\" class=\"data row539 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row539_col2\" class=\"data row539 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row539_col3\" class=\"data row539 col3\" >0.5914</td>\n",
              "      <td id=\"T_02857_row539_col4\" class=\"data row539 col4\" >0.5545</td>\n",
              "      <td id=\"T_02857_row539_col5\" class=\"data row539 col5\" >0.5067</td>\n",
              "      <td id=\"T_02857_row539_col6\" class=\"data row539 col6\" >0.6055</td>\n",
              "      <td id=\"T_02857_row539_col7\" class=\"data row539 col7\" >0.5326</td>\n",
              "      <td id=\"T_02857_row539_col8\" class=\"data row539 col8\" >0.5987</td>\n",
              "      <td id=\"T_02857_row539_col9\" class=\"data row539 col9\" >0.6977</td>\n",
              "      <td id=\"T_02857_row539_col10\" class=\"data row539 col10\" >0.6128</td>\n",
              "      <td id=\"T_02857_row539_col11\" class=\"data row539 col11\" >0.5165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row540\" class=\"row_heading level0 row540\" >540</th>\n",
              "      <td id=\"T_02857_row540_col0\" class=\"data row540 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row540_col1\" class=\"data row540 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row540_col2\" class=\"data row540 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row540_col3\" class=\"data row540 col3\" >0.5914</td>\n",
              "      <td id=\"T_02857_row540_col4\" class=\"data row540 col4\" >0.5545</td>\n",
              "      <td id=\"T_02857_row540_col5\" class=\"data row540 col5\" >0.5067</td>\n",
              "      <td id=\"T_02857_row540_col6\" class=\"data row540 col6\" >0.6055</td>\n",
              "      <td id=\"T_02857_row540_col7\" class=\"data row540 col7\" >0.5326</td>\n",
              "      <td id=\"T_02857_row540_col8\" class=\"data row540 col8\" >0.5987</td>\n",
              "      <td id=\"T_02857_row540_col9\" class=\"data row540 col9\" >0.6977</td>\n",
              "      <td id=\"T_02857_row540_col10\" class=\"data row540 col10\" >0.6128</td>\n",
              "      <td id=\"T_02857_row540_col11\" class=\"data row540 col11\" >0.5165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row541\" class=\"row_heading level0 row541\" >541</th>\n",
              "      <td id=\"T_02857_row541_col0\" class=\"data row541 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row541_col1\" class=\"data row541 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row541_col2\" class=\"data row541 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row541_col3\" class=\"data row541 col3\" >0.5937</td>\n",
              "      <td id=\"T_02857_row541_col4\" class=\"data row541 col4\" >0.5328</td>\n",
              "      <td id=\"T_02857_row541_col5\" class=\"data row541 col5\" >0.5065</td>\n",
              "      <td id=\"T_02857_row541_col6\" class=\"data row541 col6\" >0.5910</td>\n",
              "      <td id=\"T_02857_row541_col7\" class=\"data row541 col7\" >0.5283</td>\n",
              "      <td id=\"T_02857_row541_col8\" class=\"data row541 col8\" >0.6740</td>\n",
              "      <td id=\"T_02857_row541_col9\" class=\"data row541 col9\" >0.7075</td>\n",
              "      <td id=\"T_02857_row541_col10\" class=\"data row541 col10\" >0.6282</td>\n",
              "      <td id=\"T_02857_row541_col11\" class=\"data row541 col11\" >0.5294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row542\" class=\"row_heading level0 row542\" >542</th>\n",
              "      <td id=\"T_02857_row542_col0\" class=\"data row542 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row542_col1\" class=\"data row542 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row542_col2\" class=\"data row542 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row542_col3\" class=\"data row542 col3\" >0.5937</td>\n",
              "      <td id=\"T_02857_row542_col4\" class=\"data row542 col4\" >0.5328</td>\n",
              "      <td id=\"T_02857_row542_col5\" class=\"data row542 col5\" >0.5065</td>\n",
              "      <td id=\"T_02857_row542_col6\" class=\"data row542 col6\" >0.5910</td>\n",
              "      <td id=\"T_02857_row542_col7\" class=\"data row542 col7\" >0.5283</td>\n",
              "      <td id=\"T_02857_row542_col8\" class=\"data row542 col8\" >0.6740</td>\n",
              "      <td id=\"T_02857_row542_col9\" class=\"data row542 col9\" >0.7075</td>\n",
              "      <td id=\"T_02857_row542_col10\" class=\"data row542 col10\" >0.6282</td>\n",
              "      <td id=\"T_02857_row542_col11\" class=\"data row542 col11\" >0.5294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row543\" class=\"row_heading level0 row543\" >543</th>\n",
              "      <td id=\"T_02857_row543_col0\" class=\"data row543 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row543_col1\" class=\"data row543 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row543_col2\" class=\"data row543 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row543_col3\" class=\"data row543 col3\" >0.4763</td>\n",
              "      <td id=\"T_02857_row543_col4\" class=\"data row543 col4\" >0.4699</td>\n",
              "      <td id=\"T_02857_row543_col5\" class=\"data row543 col5\" >0.5001</td>\n",
              "      <td id=\"T_02857_row543_col6\" class=\"data row543 col6\" >0.5261</td>\n",
              "      <td id=\"T_02857_row543_col7\" class=\"data row543 col7\" >0.4859</td>\n",
              "      <td id=\"T_02857_row543_col8\" class=\"data row543 col8\" >0.5970</td>\n",
              "      <td id=\"T_02857_row543_col9\" class=\"data row543 col9\" >0.7058</td>\n",
              "      <td id=\"T_02857_row543_col10\" class=\"data row543 col10\" >0.5837</td>\n",
              "      <td id=\"T_02857_row543_col11\" class=\"data row543 col11\" >0.4099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row544\" class=\"row_heading level0 row544\" >544</th>\n",
              "      <td id=\"T_02857_row544_col0\" class=\"data row544 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row544_col1\" class=\"data row544 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row544_col2\" class=\"data row544 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row544_col3\" class=\"data row544 col3\" >0.4763</td>\n",
              "      <td id=\"T_02857_row544_col4\" class=\"data row544 col4\" >0.4699</td>\n",
              "      <td id=\"T_02857_row544_col5\" class=\"data row544 col5\" >0.5001</td>\n",
              "      <td id=\"T_02857_row544_col6\" class=\"data row544 col6\" >0.5261</td>\n",
              "      <td id=\"T_02857_row544_col7\" class=\"data row544 col7\" >0.4859</td>\n",
              "      <td id=\"T_02857_row544_col8\" class=\"data row544 col8\" >0.5970</td>\n",
              "      <td id=\"T_02857_row544_col9\" class=\"data row544 col9\" >0.7058</td>\n",
              "      <td id=\"T_02857_row544_col10\" class=\"data row544 col10\" >0.5837</td>\n",
              "      <td id=\"T_02857_row544_col11\" class=\"data row544 col11\" >0.4099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row545\" class=\"row_heading level0 row545\" >545</th>\n",
              "      <td id=\"T_02857_row545_col0\" class=\"data row545 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row545_col1\" class=\"data row545 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row545_col2\" class=\"data row545 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row545_col3\" class=\"data row545 col3\" >0.6479</td>\n",
              "      <td id=\"T_02857_row545_col4\" class=\"data row545 col4\" >0.6341</td>\n",
              "      <td id=\"T_02857_row545_col5\" class=\"data row545 col5\" >0.5932</td>\n",
              "      <td id=\"T_02857_row545_col6\" class=\"data row545 col6\" >0.6250</td>\n",
              "      <td id=\"T_02857_row545_col7\" class=\"data row545 col7\" >0.5000</td>\n",
              "      <td id=\"T_02857_row545_col8\" class=\"data row545 col8\" >0.4601</td>\n",
              "      <td id=\"T_02857_row545_col9\" class=\"data row545 col9\" >0.6377</td>\n",
              "      <td id=\"T_02857_row545_col10\" class=\"data row545 col10\" >0.5635</td>\n",
              "      <td id=\"T_02857_row545_col11\" class=\"data row545 col11\" >0.5042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row546\" class=\"row_heading level0 row546\" >546</th>\n",
              "      <td id=\"T_02857_row546_col0\" class=\"data row546 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row546_col1\" class=\"data row546 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row546_col2\" class=\"data row546 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row546_col3\" class=\"data row546 col3\" >0.6479</td>\n",
              "      <td id=\"T_02857_row546_col4\" class=\"data row546 col4\" >0.6341</td>\n",
              "      <td id=\"T_02857_row546_col5\" class=\"data row546 col5\" >0.5932</td>\n",
              "      <td id=\"T_02857_row546_col6\" class=\"data row546 col6\" >0.6250</td>\n",
              "      <td id=\"T_02857_row546_col7\" class=\"data row546 col7\" >0.5000</td>\n",
              "      <td id=\"T_02857_row546_col8\" class=\"data row546 col8\" >0.4601</td>\n",
              "      <td id=\"T_02857_row546_col9\" class=\"data row546 col9\" >0.6377</td>\n",
              "      <td id=\"T_02857_row546_col10\" class=\"data row546 col10\" >0.5635</td>\n",
              "      <td id=\"T_02857_row546_col11\" class=\"data row546 col11\" >0.5042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row547\" class=\"row_heading level0 row547\" >547</th>\n",
              "      <td id=\"T_02857_row547_col0\" class=\"data row547 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row547_col1\" class=\"data row547 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row547_col2\" class=\"data row547 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row547_col3\" class=\"data row547 col3\" >0.6136</td>\n",
              "      <td id=\"T_02857_row547_col4\" class=\"data row547 col4\" >0.6105</td>\n",
              "      <td id=\"T_02857_row547_col5\" class=\"data row547 col5\" >0.5734</td>\n",
              "      <td id=\"T_02857_row547_col6\" class=\"data row547 col6\" >0.5865</td>\n",
              "      <td id=\"T_02857_row547_col7\" class=\"data row547 col7\" >0.5084</td>\n",
              "      <td id=\"T_02857_row547_col8\" class=\"data row547 col8\" >0.4740</td>\n",
              "      <td id=\"T_02857_row547_col9\" class=\"data row547 col9\" >0.5826</td>\n",
              "      <td id=\"T_02857_row547_col10\" class=\"data row547 col10\" >0.5027</td>\n",
              "      <td id=\"T_02857_row547_col11\" class=\"data row547 col11\" >0.5309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row548\" class=\"row_heading level0 row548\" >548</th>\n",
              "      <td id=\"T_02857_row548_col0\" class=\"data row548 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row548_col1\" class=\"data row548 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row548_col2\" class=\"data row548 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row548_col3\" class=\"data row548 col3\" >0.6136</td>\n",
              "      <td id=\"T_02857_row548_col4\" class=\"data row548 col4\" >0.6105</td>\n",
              "      <td id=\"T_02857_row548_col5\" class=\"data row548 col5\" >0.5734</td>\n",
              "      <td id=\"T_02857_row548_col6\" class=\"data row548 col6\" >0.5865</td>\n",
              "      <td id=\"T_02857_row548_col7\" class=\"data row548 col7\" >0.5084</td>\n",
              "      <td id=\"T_02857_row548_col8\" class=\"data row548 col8\" >0.4740</td>\n",
              "      <td id=\"T_02857_row548_col9\" class=\"data row548 col9\" >0.5826</td>\n",
              "      <td id=\"T_02857_row548_col10\" class=\"data row548 col10\" >0.5027</td>\n",
              "      <td id=\"T_02857_row548_col11\" class=\"data row548 col11\" >0.5309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row549\" class=\"row_heading level0 row549\" >549</th>\n",
              "      <td id=\"T_02857_row549_col0\" class=\"data row549 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row549_col1\" class=\"data row549 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row549_col2\" class=\"data row549 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row549_col3\" class=\"data row549 col3\" >0.5232</td>\n",
              "      <td id=\"T_02857_row549_col4\" class=\"data row549 col4\" >0.5205</td>\n",
              "      <td id=\"T_02857_row549_col5\" class=\"data row549 col5\" >0.5193</td>\n",
              "      <td id=\"T_02857_row549_col6\" class=\"data row549 col6\" >0.5160</td>\n",
              "      <td id=\"T_02857_row549_col7\" class=\"data row549 col7\" >0.5112</td>\n",
              "      <td id=\"T_02857_row549_col8\" class=\"data row549 col8\" >0.5173</td>\n",
              "      <td id=\"T_02857_row549_col9\" class=\"data row549 col9\" >0.5093</td>\n",
              "      <td id=\"T_02857_row549_col10\" class=\"data row549 col10\" >0.5178</td>\n",
              "      <td id=\"T_02857_row549_col11\" class=\"data row549 col11\" >0.6132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row550\" class=\"row_heading level0 row550\" >550</th>\n",
              "      <td id=\"T_02857_row550_col0\" class=\"data row550 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row550_col1\" class=\"data row550 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row550_col2\" class=\"data row550 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row550_col3\" class=\"data row550 col3\" >0.5232</td>\n",
              "      <td id=\"T_02857_row550_col4\" class=\"data row550 col4\" >0.5205</td>\n",
              "      <td id=\"T_02857_row550_col5\" class=\"data row550 col5\" >0.5193</td>\n",
              "      <td id=\"T_02857_row550_col6\" class=\"data row550 col6\" >0.5160</td>\n",
              "      <td id=\"T_02857_row550_col7\" class=\"data row550 col7\" >0.5112</td>\n",
              "      <td id=\"T_02857_row550_col8\" class=\"data row550 col8\" >0.5173</td>\n",
              "      <td id=\"T_02857_row550_col9\" class=\"data row550 col9\" >0.5093</td>\n",
              "      <td id=\"T_02857_row550_col10\" class=\"data row550 col10\" >0.5178</td>\n",
              "      <td id=\"T_02857_row550_col11\" class=\"data row550 col11\" >0.6132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row551\" class=\"row_heading level0 row551\" >551</th>\n",
              "      <td id=\"T_02857_row551_col0\" class=\"data row551 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row551_col1\" class=\"data row551 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row551_col2\" class=\"data row551 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row551_col3\" class=\"data row551 col3\" >0.5730</td>\n",
              "      <td id=\"T_02857_row551_col4\" class=\"data row551 col4\" >0.5137</td>\n",
              "      <td id=\"T_02857_row551_col5\" class=\"data row551 col5\" >0.4619</td>\n",
              "      <td id=\"T_02857_row551_col6\" class=\"data row551 col6\" >0.5960</td>\n",
              "      <td id=\"T_02857_row551_col7\" class=\"data row551 col7\" >0.4469</td>\n",
              "      <td id=\"T_02857_row551_col8\" class=\"data row551 col8\" >0.5489</td>\n",
              "      <td id=\"T_02857_row551_col9\" class=\"data row551 col9\" >0.6883</td>\n",
              "      <td id=\"T_02857_row551_col10\" class=\"data row551 col10\" >0.5823</td>\n",
              "      <td id=\"T_02857_row551_col11\" class=\"data row551 col11\" >0.4797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row552\" class=\"row_heading level0 row552\" >552</th>\n",
              "      <td id=\"T_02857_row552_col0\" class=\"data row552 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row552_col1\" class=\"data row552 col1\" >opt-66b</td>\n",
              "      <td id=\"T_02857_row552_col2\" class=\"data row552 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row552_col3\" class=\"data row552 col3\" >0.5730</td>\n",
              "      <td id=\"T_02857_row552_col4\" class=\"data row552 col4\" >0.5137</td>\n",
              "      <td id=\"T_02857_row552_col5\" class=\"data row552 col5\" >0.4619</td>\n",
              "      <td id=\"T_02857_row552_col6\" class=\"data row552 col6\" >0.5960</td>\n",
              "      <td id=\"T_02857_row552_col7\" class=\"data row552 col7\" >0.4469</td>\n",
              "      <td id=\"T_02857_row552_col8\" class=\"data row552 col8\" >0.5489</td>\n",
              "      <td id=\"T_02857_row552_col9\" class=\"data row552 col9\" >0.6883</td>\n",
              "      <td id=\"T_02857_row552_col10\" class=\"data row552 col10\" >0.5823</td>\n",
              "      <td id=\"T_02857_row552_col11\" class=\"data row552 col11\" >0.4797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row553\" class=\"row_heading level0 row553\" >553</th>\n",
              "      <td id=\"T_02857_row553_col0\" class=\"data row553 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row553_col1\" class=\"data row553 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row553_col2\" class=\"data row553 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row553_col3\" class=\"data row553 col3\" >0.6083</td>\n",
              "      <td id=\"T_02857_row553_col4\" class=\"data row553 col4\" >0.6039</td>\n",
              "      <td id=\"T_02857_row553_col5\" class=\"data row553 col5\" >0.5743</td>\n",
              "      <td id=\"T_02857_row553_col6\" class=\"data row553 col6\" >0.6566</td>\n",
              "      <td id=\"T_02857_row553_col7\" class=\"data row553 col7\" >0.5785</td>\n",
              "      <td id=\"T_02857_row553_col8\" class=\"data row553 col8\" >0.6602</td>\n",
              "      <td id=\"T_02857_row553_col9\" class=\"data row553 col9\" >0.7016</td>\n",
              "      <td id=\"T_02857_row553_col10\" class=\"data row553 col10\" >0.6453</td>\n",
              "      <td id=\"T_02857_row553_col11\" class=\"data row553 col11\" >0.5673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row554\" class=\"row_heading level0 row554\" >554</th>\n",
              "      <td id=\"T_02857_row554_col0\" class=\"data row554 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row554_col1\" class=\"data row554 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row554_col2\" class=\"data row554 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row554_col3\" class=\"data row554 col3\" >0.6083</td>\n",
              "      <td id=\"T_02857_row554_col4\" class=\"data row554 col4\" >0.6039</td>\n",
              "      <td id=\"T_02857_row554_col5\" class=\"data row554 col5\" >0.5743</td>\n",
              "      <td id=\"T_02857_row554_col6\" class=\"data row554 col6\" >0.6566</td>\n",
              "      <td id=\"T_02857_row554_col7\" class=\"data row554 col7\" >0.5785</td>\n",
              "      <td id=\"T_02857_row554_col8\" class=\"data row554 col8\" >0.6602</td>\n",
              "      <td id=\"T_02857_row554_col9\" class=\"data row554 col9\" >0.7016</td>\n",
              "      <td id=\"T_02857_row554_col10\" class=\"data row554 col10\" >0.6453</td>\n",
              "      <td id=\"T_02857_row554_col11\" class=\"data row554 col11\" >0.5673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row555\" class=\"row_heading level0 row555\" >555</th>\n",
              "      <td id=\"T_02857_row555_col0\" class=\"data row555 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row555_col1\" class=\"data row555 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row555_col2\" class=\"data row555 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row555_col3\" class=\"data row555 col3\" >0.4963</td>\n",
              "      <td id=\"T_02857_row555_col4\" class=\"data row555 col4\" >0.4613</td>\n",
              "      <td id=\"T_02857_row555_col5\" class=\"data row555 col5\" >0.4714</td>\n",
              "      <td id=\"T_02857_row555_col6\" class=\"data row555 col6\" >0.5101</td>\n",
              "      <td id=\"T_02857_row555_col7\" class=\"data row555 col7\" >0.5024</td>\n",
              "      <td id=\"T_02857_row555_col8\" class=\"data row555 col8\" >0.6649</td>\n",
              "      <td id=\"T_02857_row555_col9\" class=\"data row555 col9\" >0.6412</td>\n",
              "      <td id=\"T_02857_row555_col10\" class=\"data row555 col10\" >0.6001</td>\n",
              "      <td id=\"T_02857_row555_col11\" class=\"data row555 col11\" >0.4999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row556\" class=\"row_heading level0 row556\" >556</th>\n",
              "      <td id=\"T_02857_row556_col0\" class=\"data row556 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row556_col1\" class=\"data row556 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row556_col2\" class=\"data row556 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row556_col3\" class=\"data row556 col3\" >0.4963</td>\n",
              "      <td id=\"T_02857_row556_col4\" class=\"data row556 col4\" >0.4613</td>\n",
              "      <td id=\"T_02857_row556_col5\" class=\"data row556 col5\" >0.4714</td>\n",
              "      <td id=\"T_02857_row556_col6\" class=\"data row556 col6\" >0.5101</td>\n",
              "      <td id=\"T_02857_row556_col7\" class=\"data row556 col7\" >0.5024</td>\n",
              "      <td id=\"T_02857_row556_col8\" class=\"data row556 col8\" >0.6649</td>\n",
              "      <td id=\"T_02857_row556_col9\" class=\"data row556 col9\" >0.6412</td>\n",
              "      <td id=\"T_02857_row556_col10\" class=\"data row556 col10\" >0.6001</td>\n",
              "      <td id=\"T_02857_row556_col11\" class=\"data row556 col11\" >0.4999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row557\" class=\"row_heading level0 row557\" >557</th>\n",
              "      <td id=\"T_02857_row557_col0\" class=\"data row557 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row557_col1\" class=\"data row557 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row557_col2\" class=\"data row557 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row557_col3\" class=\"data row557 col3\" >0.5137</td>\n",
              "      <td id=\"T_02857_row557_col4\" class=\"data row557 col4\" >0.5134</td>\n",
              "      <td id=\"T_02857_row557_col5\" class=\"data row557 col5\" >0.5096</td>\n",
              "      <td id=\"T_02857_row557_col6\" class=\"data row557 col6\" >0.5544</td>\n",
              "      <td id=\"T_02857_row557_col7\" class=\"data row557 col7\" >0.5052</td>\n",
              "      <td id=\"T_02857_row557_col8\" class=\"data row557 col8\" >0.6557</td>\n",
              "      <td id=\"T_02857_row557_col9\" class=\"data row557 col9\" >0.6785</td>\n",
              "      <td id=\"T_02857_row557_col10\" class=\"data row557 col10\" >0.5742</td>\n",
              "      <td id=\"T_02857_row557_col11\" class=\"data row557 col11\" >0.4216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row558\" class=\"row_heading level0 row558\" >558</th>\n",
              "      <td id=\"T_02857_row558_col0\" class=\"data row558 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row558_col1\" class=\"data row558 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row558_col2\" class=\"data row558 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row558_col3\" class=\"data row558 col3\" >0.5137</td>\n",
              "      <td id=\"T_02857_row558_col4\" class=\"data row558 col4\" >0.5134</td>\n",
              "      <td id=\"T_02857_row558_col5\" class=\"data row558 col5\" >0.5096</td>\n",
              "      <td id=\"T_02857_row558_col6\" class=\"data row558 col6\" >0.5544</td>\n",
              "      <td id=\"T_02857_row558_col7\" class=\"data row558 col7\" >0.5052</td>\n",
              "      <td id=\"T_02857_row558_col8\" class=\"data row558 col8\" >0.6557</td>\n",
              "      <td id=\"T_02857_row558_col9\" class=\"data row558 col9\" >0.6785</td>\n",
              "      <td id=\"T_02857_row558_col10\" class=\"data row558 col10\" >0.5742</td>\n",
              "      <td id=\"T_02857_row558_col11\" class=\"data row558 col11\" >0.4216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row559\" class=\"row_heading level0 row559\" >559</th>\n",
              "      <td id=\"T_02857_row559_col0\" class=\"data row559 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row559_col1\" class=\"data row559 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row559_col2\" class=\"data row559 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row559_col3\" class=\"data row559 col3\" >0.6202</td>\n",
              "      <td id=\"T_02857_row559_col4\" class=\"data row559 col4\" >0.6232</td>\n",
              "      <td id=\"T_02857_row559_col5\" class=\"data row559 col5\" >0.6024</td>\n",
              "      <td id=\"T_02857_row559_col6\" class=\"data row559 col6\" >0.6378</td>\n",
              "      <td id=\"T_02857_row559_col7\" class=\"data row559 col7\" >0.5682</td>\n",
              "      <td id=\"T_02857_row559_col8\" class=\"data row559 col8\" >0.5302</td>\n",
              "      <td id=\"T_02857_row559_col9\" class=\"data row559 col9\" >0.6345</td>\n",
              "      <td id=\"T_02857_row559_col10\" class=\"data row559 col10\" >0.5826</td>\n",
              "      <td id=\"T_02857_row559_col11\" class=\"data row559 col11\" >0.5648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row560\" class=\"row_heading level0 row560\" >560</th>\n",
              "      <td id=\"T_02857_row560_col0\" class=\"data row560 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row560_col1\" class=\"data row560 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row560_col2\" class=\"data row560 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row560_col3\" class=\"data row560 col3\" >0.6202</td>\n",
              "      <td id=\"T_02857_row560_col4\" class=\"data row560 col4\" >0.6232</td>\n",
              "      <td id=\"T_02857_row560_col5\" class=\"data row560 col5\" >0.6024</td>\n",
              "      <td id=\"T_02857_row560_col6\" class=\"data row560 col6\" >0.6378</td>\n",
              "      <td id=\"T_02857_row560_col7\" class=\"data row560 col7\" >0.5682</td>\n",
              "      <td id=\"T_02857_row560_col8\" class=\"data row560 col8\" >0.5302</td>\n",
              "      <td id=\"T_02857_row560_col9\" class=\"data row560 col9\" >0.6345</td>\n",
              "      <td id=\"T_02857_row560_col10\" class=\"data row560 col10\" >0.5826</td>\n",
              "      <td id=\"T_02857_row560_col11\" class=\"data row560 col11\" >0.5648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row561\" class=\"row_heading level0 row561\" >561</th>\n",
              "      <td id=\"T_02857_row561_col0\" class=\"data row561 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row561_col1\" class=\"data row561 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row561_col2\" class=\"data row561 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row561_col3\" class=\"data row561 col3\" >0.6091</td>\n",
              "      <td id=\"T_02857_row561_col4\" class=\"data row561 col4\" >0.6437</td>\n",
              "      <td id=\"T_02857_row561_col5\" class=\"data row561 col5\" >0.6864</td>\n",
              "      <td id=\"T_02857_row561_col6\" class=\"data row561 col6\" >0.7819</td>\n",
              "      <td id=\"T_02857_row561_col7\" class=\"data row561 col7\" >0.6829</td>\n",
              "      <td id=\"T_02857_row561_col8\" class=\"data row561 col8\" >0.8214</td>\n",
              "      <td id=\"T_02857_row561_col9\" class=\"data row561 col9\" >0.7460</td>\n",
              "      <td id=\"T_02857_row561_col10\" class=\"data row561 col10\" >0.7701</td>\n",
              "      <td id=\"T_02857_row561_col11\" class=\"data row561 col11\" >0.6069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row562\" class=\"row_heading level0 row562\" >562</th>\n",
              "      <td id=\"T_02857_row562_col0\" class=\"data row562 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row562_col1\" class=\"data row562 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row562_col2\" class=\"data row562 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row562_col3\" class=\"data row562 col3\" >0.6091</td>\n",
              "      <td id=\"T_02857_row562_col4\" class=\"data row562 col4\" >0.6437</td>\n",
              "      <td id=\"T_02857_row562_col5\" class=\"data row562 col5\" >0.6864</td>\n",
              "      <td id=\"T_02857_row562_col6\" class=\"data row562 col6\" >0.7819</td>\n",
              "      <td id=\"T_02857_row562_col7\" class=\"data row562 col7\" >0.6829</td>\n",
              "      <td id=\"T_02857_row562_col8\" class=\"data row562 col8\" >0.8214</td>\n",
              "      <td id=\"T_02857_row562_col9\" class=\"data row562 col9\" >0.7460</td>\n",
              "      <td id=\"T_02857_row562_col10\" class=\"data row562 col10\" >0.7701</td>\n",
              "      <td id=\"T_02857_row562_col11\" class=\"data row562 col11\" >0.6069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row563\" class=\"row_heading level0 row563\" >563</th>\n",
              "      <td id=\"T_02857_row563_col0\" class=\"data row563 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row563_col1\" class=\"data row563 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row563_col2\" class=\"data row563 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row563_col3\" class=\"data row563 col3\" >0.6425</td>\n",
              "      <td id=\"T_02857_row563_col4\" class=\"data row563 col4\" >0.6572</td>\n",
              "      <td id=\"T_02857_row563_col5\" class=\"data row563 col5\" >0.6471</td>\n",
              "      <td id=\"T_02857_row563_col6\" class=\"data row563 col6\" >0.6559</td>\n",
              "      <td id=\"T_02857_row563_col7\" class=\"data row563 col7\" >0.6247</td>\n",
              "      <td id=\"T_02857_row563_col8\" class=\"data row563 col8\" >0.6868</td>\n",
              "      <td id=\"T_02857_row563_col9\" class=\"data row563 col9\" >0.5929</td>\n",
              "      <td id=\"T_02857_row563_col10\" class=\"data row563 col10\" >0.6613</td>\n",
              "      <td id=\"T_02857_row563_col11\" class=\"data row563 col11\" >0.6426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row564\" class=\"row_heading level0 row564\" >564</th>\n",
              "      <td id=\"T_02857_row564_col0\" class=\"data row564 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row564_col1\" class=\"data row564 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row564_col2\" class=\"data row564 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row564_col3\" class=\"data row564 col3\" >0.6425</td>\n",
              "      <td id=\"T_02857_row564_col4\" class=\"data row564 col4\" >0.6572</td>\n",
              "      <td id=\"T_02857_row564_col5\" class=\"data row564 col5\" >0.6471</td>\n",
              "      <td id=\"T_02857_row564_col6\" class=\"data row564 col6\" >0.6559</td>\n",
              "      <td id=\"T_02857_row564_col7\" class=\"data row564 col7\" >0.6247</td>\n",
              "      <td id=\"T_02857_row564_col8\" class=\"data row564 col8\" >0.6868</td>\n",
              "      <td id=\"T_02857_row564_col9\" class=\"data row564 col9\" >0.5929</td>\n",
              "      <td id=\"T_02857_row564_col10\" class=\"data row564 col10\" >0.6613</td>\n",
              "      <td id=\"T_02857_row564_col11\" class=\"data row564 col11\" >0.6426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row565\" class=\"row_heading level0 row565\" >565</th>\n",
              "      <td id=\"T_02857_row565_col0\" class=\"data row565 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row565_col1\" class=\"data row565 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row565_col2\" class=\"data row565 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row565_col3\" class=\"data row565 col3\" >0.5853</td>\n",
              "      <td id=\"T_02857_row565_col4\" class=\"data row565 col4\" >0.5686</td>\n",
              "      <td id=\"T_02857_row565_col5\" class=\"data row565 col5\" >0.5519</td>\n",
              "      <td id=\"T_02857_row565_col6\" class=\"data row565 col6\" >0.6704</td>\n",
              "      <td id=\"T_02857_row565_col7\" class=\"data row565 col7\" >0.5645</td>\n",
              "      <td id=\"T_02857_row565_col8\" class=\"data row565 col8\" >0.7147</td>\n",
              "      <td id=\"T_02857_row565_col9\" class=\"data row565 col9\" >0.7462</td>\n",
              "      <td id=\"T_02857_row565_col10\" class=\"data row565 col10\" >0.6800</td>\n",
              "      <td id=\"T_02857_row565_col11\" class=\"data row565 col11\" >0.5194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row566\" class=\"row_heading level0 row566\" >566</th>\n",
              "      <td id=\"T_02857_row566_col0\" class=\"data row566 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row566_col1\" class=\"data row566 col1\" >opt-iml-max-1.3b</td>\n",
              "      <td id=\"T_02857_row566_col2\" class=\"data row566 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row566_col3\" class=\"data row566 col3\" >0.5853</td>\n",
              "      <td id=\"T_02857_row566_col4\" class=\"data row566 col4\" >0.5686</td>\n",
              "      <td id=\"T_02857_row566_col5\" class=\"data row566 col5\" >0.5519</td>\n",
              "      <td id=\"T_02857_row566_col6\" class=\"data row566 col6\" >0.6704</td>\n",
              "      <td id=\"T_02857_row566_col7\" class=\"data row566 col7\" >0.5645</td>\n",
              "      <td id=\"T_02857_row566_col8\" class=\"data row566 col8\" >0.7147</td>\n",
              "      <td id=\"T_02857_row566_col9\" class=\"data row566 col9\" >0.7462</td>\n",
              "      <td id=\"T_02857_row566_col10\" class=\"data row566 col10\" >0.6800</td>\n",
              "      <td id=\"T_02857_row566_col11\" class=\"data row566 col11\" >0.5194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row567\" class=\"row_heading level0 row567\" >567</th>\n",
              "      <td id=\"T_02857_row567_col0\" class=\"data row567 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row567_col1\" class=\"data row567 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row567_col2\" class=\"data row567 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row567_col3\" class=\"data row567 col3\" >0.8238</td>\n",
              "      <td id=\"T_02857_row567_col4\" class=\"data row567 col4\" >0.8777</td>\n",
              "      <td id=\"T_02857_row567_col5\" class=\"data row567 col5\" >0.8564</td>\n",
              "      <td id=\"T_02857_row567_col6\" class=\"data row567 col6\" >0.7813</td>\n",
              "      <td id=\"T_02857_row567_col7\" class=\"data row567 col7\" >0.7659</td>\n",
              "      <td id=\"T_02857_row567_col8\" class=\"data row567 col8\" >0.3726</td>\n",
              "      <td id=\"T_02857_row567_col9\" class=\"data row567 col9\" >0.3524</td>\n",
              "      <td id=\"T_02857_row567_col10\" class=\"data row567 col10\" >0.3905</td>\n",
              "      <td id=\"T_02857_row567_col11\" class=\"data row567 col11\" >0.4875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row568\" class=\"row_heading level0 row568\" >568</th>\n",
              "      <td id=\"T_02857_row568_col0\" class=\"data row568 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row568_col1\" class=\"data row568 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row568_col2\" class=\"data row568 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row568_col3\" class=\"data row568 col3\" >0.8238</td>\n",
              "      <td id=\"T_02857_row568_col4\" class=\"data row568 col4\" >0.8777</td>\n",
              "      <td id=\"T_02857_row568_col5\" class=\"data row568 col5\" >0.8564</td>\n",
              "      <td id=\"T_02857_row568_col6\" class=\"data row568 col6\" >0.7813</td>\n",
              "      <td id=\"T_02857_row568_col7\" class=\"data row568 col7\" >0.7659</td>\n",
              "      <td id=\"T_02857_row568_col8\" class=\"data row568 col8\" >0.3726</td>\n",
              "      <td id=\"T_02857_row568_col9\" class=\"data row568 col9\" >0.3524</td>\n",
              "      <td id=\"T_02857_row568_col10\" class=\"data row568 col10\" >0.3905</td>\n",
              "      <td id=\"T_02857_row568_col11\" class=\"data row568 col11\" >0.4875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row569\" class=\"row_heading level0 row569\" >569</th>\n",
              "      <td id=\"T_02857_row569_col0\" class=\"data row569 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row569_col1\" class=\"data row569 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row569_col2\" class=\"data row569 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row569_col3\" class=\"data row569 col3\" >0.4283</td>\n",
              "      <td id=\"T_02857_row569_col4\" class=\"data row569 col4\" >0.4320</td>\n",
              "      <td id=\"T_02857_row569_col5\" class=\"data row569 col5\" >0.4330</td>\n",
              "      <td id=\"T_02857_row569_col6\" class=\"data row569 col6\" >0.4395</td>\n",
              "      <td id=\"T_02857_row569_col7\" class=\"data row569 col7\" >0.4291</td>\n",
              "      <td id=\"T_02857_row569_col8\" class=\"data row569 col8\" >0.3708</td>\n",
              "      <td id=\"T_02857_row569_col9\" class=\"data row569 col9\" >0.3608</td>\n",
              "      <td id=\"T_02857_row569_col10\" class=\"data row569 col10\" >0.3618</td>\n",
              "      <td id=\"T_02857_row569_col11\" class=\"data row569 col11\" >0.1760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row570\" class=\"row_heading level0 row570\" >570</th>\n",
              "      <td id=\"T_02857_row570_col0\" class=\"data row570 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row570_col1\" class=\"data row570 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row570_col2\" class=\"data row570 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row570_col3\" class=\"data row570 col3\" >0.4283</td>\n",
              "      <td id=\"T_02857_row570_col4\" class=\"data row570 col4\" >0.4320</td>\n",
              "      <td id=\"T_02857_row570_col5\" class=\"data row570 col5\" >0.4330</td>\n",
              "      <td id=\"T_02857_row570_col6\" class=\"data row570 col6\" >0.4395</td>\n",
              "      <td id=\"T_02857_row570_col7\" class=\"data row570 col7\" >0.4291</td>\n",
              "      <td id=\"T_02857_row570_col8\" class=\"data row570 col8\" >0.3708</td>\n",
              "      <td id=\"T_02857_row570_col9\" class=\"data row570 col9\" >0.3608</td>\n",
              "      <td id=\"T_02857_row570_col10\" class=\"data row570 col10\" >0.3618</td>\n",
              "      <td id=\"T_02857_row570_col11\" class=\"data row570 col11\" >0.1760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row571\" class=\"row_heading level0 row571\" >571</th>\n",
              "      <td id=\"T_02857_row571_col0\" class=\"data row571 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row571_col1\" class=\"data row571 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row571_col2\" class=\"data row571 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row571_col3\" class=\"data row571 col3\" >0.4351</td>\n",
              "      <td id=\"T_02857_row571_col4\" class=\"data row571 col4\" >0.4267</td>\n",
              "      <td id=\"T_02857_row571_col5\" class=\"data row571 col5\" >0.4172</td>\n",
              "      <td id=\"T_02857_row571_col6\" class=\"data row571 col6\" >0.4361</td>\n",
              "      <td id=\"T_02857_row571_col7\" class=\"data row571 col7\" >0.4162</td>\n",
              "      <td id=\"T_02857_row571_col8\" class=\"data row571 col8\" >0.4002</td>\n",
              "      <td id=\"T_02857_row571_col9\" class=\"data row571 col9\" >0.4757</td>\n",
              "      <td id=\"T_02857_row571_col10\" class=\"data row571 col10\" >0.3611</td>\n",
              "      <td id=\"T_02857_row571_col11\" class=\"data row571 col11\" >0.1877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row572\" class=\"row_heading level0 row572\" >572</th>\n",
              "      <td id=\"T_02857_row572_col0\" class=\"data row572 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row572_col1\" class=\"data row572 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row572_col2\" class=\"data row572 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row572_col3\" class=\"data row572 col3\" >0.4351</td>\n",
              "      <td id=\"T_02857_row572_col4\" class=\"data row572 col4\" >0.4267</td>\n",
              "      <td id=\"T_02857_row572_col5\" class=\"data row572 col5\" >0.4172</td>\n",
              "      <td id=\"T_02857_row572_col6\" class=\"data row572 col6\" >0.4361</td>\n",
              "      <td id=\"T_02857_row572_col7\" class=\"data row572 col7\" >0.4162</td>\n",
              "      <td id=\"T_02857_row572_col8\" class=\"data row572 col8\" >0.4002</td>\n",
              "      <td id=\"T_02857_row572_col9\" class=\"data row572 col9\" >0.4757</td>\n",
              "      <td id=\"T_02857_row572_col10\" class=\"data row572 col10\" >0.3611</td>\n",
              "      <td id=\"T_02857_row572_col11\" class=\"data row572 col11\" >0.1877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row573\" class=\"row_heading level0 row573\" >573</th>\n",
              "      <td id=\"T_02857_row573_col0\" class=\"data row573 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row573_col1\" class=\"data row573 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row573_col2\" class=\"data row573 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row573_col3\" class=\"data row573 col3\" >0.7918</td>\n",
              "      <td id=\"T_02857_row573_col4\" class=\"data row573 col4\" >0.8413</td>\n",
              "      <td id=\"T_02857_row573_col5\" class=\"data row573 col5\" >0.7684</td>\n",
              "      <td id=\"T_02857_row573_col6\" class=\"data row573 col6\" >0.7266</td>\n",
              "      <td id=\"T_02857_row573_col7\" class=\"data row573 col7\" >0.6655</td>\n",
              "      <td id=\"T_02857_row573_col8\" class=\"data row573 col8\" >0.3339</td>\n",
              "      <td id=\"T_02857_row573_col9\" class=\"data row573 col9\" >0.3537</td>\n",
              "      <td id=\"T_02857_row573_col10\" class=\"data row573 col10\" >0.3662</td>\n",
              "      <td id=\"T_02857_row573_col11\" class=\"data row573 col11\" >0.4276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row574\" class=\"row_heading level0 row574\" >574</th>\n",
              "      <td id=\"T_02857_row574_col0\" class=\"data row574 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row574_col1\" class=\"data row574 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row574_col2\" class=\"data row574 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row574_col3\" class=\"data row574 col3\" >0.7918</td>\n",
              "      <td id=\"T_02857_row574_col4\" class=\"data row574 col4\" >0.8413</td>\n",
              "      <td id=\"T_02857_row574_col5\" class=\"data row574 col5\" >0.7684</td>\n",
              "      <td id=\"T_02857_row574_col6\" class=\"data row574 col6\" >0.7266</td>\n",
              "      <td id=\"T_02857_row574_col7\" class=\"data row574 col7\" >0.6655</td>\n",
              "      <td id=\"T_02857_row574_col8\" class=\"data row574 col8\" >0.3339</td>\n",
              "      <td id=\"T_02857_row574_col9\" class=\"data row574 col9\" >0.3537</td>\n",
              "      <td id=\"T_02857_row574_col10\" class=\"data row574 col10\" >0.3662</td>\n",
              "      <td id=\"T_02857_row574_col11\" class=\"data row574 col11\" >0.4276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row575\" class=\"row_heading level0 row575\" >575</th>\n",
              "      <td id=\"T_02857_row575_col0\" class=\"data row575 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row575_col1\" class=\"data row575 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row575_col2\" class=\"data row575 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row575_col3\" class=\"data row575 col3\" >0.8103</td>\n",
              "      <td id=\"T_02857_row575_col4\" class=\"data row575 col4\" >0.7891</td>\n",
              "      <td id=\"T_02857_row575_col5\" class=\"data row575 col5\" >0.7655</td>\n",
              "      <td id=\"T_02857_row575_col6\" class=\"data row575 col6\" >0.7236</td>\n",
              "      <td id=\"T_02857_row575_col7\" class=\"data row575 col7\" >0.7059</td>\n",
              "      <td id=\"T_02857_row575_col8\" class=\"data row575 col8\" >0.2919</td>\n",
              "      <td id=\"T_02857_row575_col9\" class=\"data row575 col9\" >0.3210</td>\n",
              "      <td id=\"T_02857_row575_col10\" class=\"data row575 col10\" >0.3343</td>\n",
              "      <td id=\"T_02857_row575_col11\" class=\"data row575 col11\" >0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row576\" class=\"row_heading level0 row576\" >576</th>\n",
              "      <td id=\"T_02857_row576_col0\" class=\"data row576 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row576_col1\" class=\"data row576 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row576_col2\" class=\"data row576 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row576_col3\" class=\"data row576 col3\" >0.8103</td>\n",
              "      <td id=\"T_02857_row576_col4\" class=\"data row576 col4\" >0.7891</td>\n",
              "      <td id=\"T_02857_row576_col5\" class=\"data row576 col5\" >0.7655</td>\n",
              "      <td id=\"T_02857_row576_col6\" class=\"data row576 col6\" >0.7236</td>\n",
              "      <td id=\"T_02857_row576_col7\" class=\"data row576 col7\" >0.7059</td>\n",
              "      <td id=\"T_02857_row576_col8\" class=\"data row576 col8\" >0.2919</td>\n",
              "      <td id=\"T_02857_row576_col9\" class=\"data row576 col9\" >0.3210</td>\n",
              "      <td id=\"T_02857_row576_col10\" class=\"data row576 col10\" >0.3343</td>\n",
              "      <td id=\"T_02857_row576_col11\" class=\"data row576 col11\" >0.4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row577\" class=\"row_heading level0 row577\" >577</th>\n",
              "      <td id=\"T_02857_row577_col0\" class=\"data row577 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row577_col1\" class=\"data row577 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row577_col2\" class=\"data row577 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row577_col3\" class=\"data row577 col3\" >0.7133</td>\n",
              "      <td id=\"T_02857_row577_col4\" class=\"data row577 col4\" >0.7534</td>\n",
              "      <td id=\"T_02857_row577_col5\" class=\"data row577 col5\" >0.7455</td>\n",
              "      <td id=\"T_02857_row577_col6\" class=\"data row577 col6\" >0.7361</td>\n",
              "      <td id=\"T_02857_row577_col7\" class=\"data row577 col7\" >0.7146</td>\n",
              "      <td id=\"T_02857_row577_col8\" class=\"data row577 col8\" >0.4962</td>\n",
              "      <td id=\"T_02857_row577_col9\" class=\"data row577 col9\" >0.4018</td>\n",
              "      <td id=\"T_02857_row577_col10\" class=\"data row577 col10\" >0.5438</td>\n",
              "      <td id=\"T_02857_row577_col11\" class=\"data row577 col11\" >0.4625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row578\" class=\"row_heading level0 row578\" >578</th>\n",
              "      <td id=\"T_02857_row578_col0\" class=\"data row578 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row578_col1\" class=\"data row578 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row578_col2\" class=\"data row578 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row578_col3\" class=\"data row578 col3\" >0.7133</td>\n",
              "      <td id=\"T_02857_row578_col4\" class=\"data row578 col4\" >0.7534</td>\n",
              "      <td id=\"T_02857_row578_col5\" class=\"data row578 col5\" >0.7455</td>\n",
              "      <td id=\"T_02857_row578_col6\" class=\"data row578 col6\" >0.7361</td>\n",
              "      <td id=\"T_02857_row578_col7\" class=\"data row578 col7\" >0.7146</td>\n",
              "      <td id=\"T_02857_row578_col8\" class=\"data row578 col8\" >0.4962</td>\n",
              "      <td id=\"T_02857_row578_col9\" class=\"data row578 col9\" >0.4018</td>\n",
              "      <td id=\"T_02857_row578_col10\" class=\"data row578 col10\" >0.5438</td>\n",
              "      <td id=\"T_02857_row578_col11\" class=\"data row578 col11\" >0.4625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row579\" class=\"row_heading level0 row579\" >579</th>\n",
              "      <td id=\"T_02857_row579_col0\" class=\"data row579 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row579_col1\" class=\"data row579 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row579_col2\" class=\"data row579 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row579_col3\" class=\"data row579 col3\" >0.7495</td>\n",
              "      <td id=\"T_02857_row579_col4\" class=\"data row579 col4\" >0.8063</td>\n",
              "      <td id=\"T_02857_row579_col5\" class=\"data row579 col5\" >0.7375</td>\n",
              "      <td id=\"T_02857_row579_col6\" class=\"data row579 col6\" >0.7049</td>\n",
              "      <td id=\"T_02857_row579_col7\" class=\"data row579 col7\" >0.6801</td>\n",
              "      <td id=\"T_02857_row579_col8\" class=\"data row579 col8\" >0.4095</td>\n",
              "      <td id=\"T_02857_row579_col9\" class=\"data row579 col9\" >0.3672</td>\n",
              "      <td id=\"T_02857_row579_col10\" class=\"data row579 col10\" >0.4047</td>\n",
              "      <td id=\"T_02857_row579_col11\" class=\"data row579 col11\" >0.4096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row580\" class=\"row_heading level0 row580\" >580</th>\n",
              "      <td id=\"T_02857_row580_col0\" class=\"data row580 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row580_col1\" class=\"data row580 col1\" >text-davinci-003</td>\n",
              "      <td id=\"T_02857_row580_col2\" class=\"data row580 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row580_col3\" class=\"data row580 col3\" >0.7495</td>\n",
              "      <td id=\"T_02857_row580_col4\" class=\"data row580 col4\" >0.8063</td>\n",
              "      <td id=\"T_02857_row580_col5\" class=\"data row580 col5\" >0.7375</td>\n",
              "      <td id=\"T_02857_row580_col6\" class=\"data row580 col6\" >0.7049</td>\n",
              "      <td id=\"T_02857_row580_col7\" class=\"data row580 col7\" >0.6801</td>\n",
              "      <td id=\"T_02857_row580_col8\" class=\"data row580 col8\" >0.4095</td>\n",
              "      <td id=\"T_02857_row580_col9\" class=\"data row580 col9\" >0.3672</td>\n",
              "      <td id=\"T_02857_row580_col10\" class=\"data row580 col10\" >0.4047</td>\n",
              "      <td id=\"T_02857_row580_col11\" class=\"data row580 col11\" >0.4096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row581\" class=\"row_heading level0 row581\" >581</th>\n",
              "      <td id=\"T_02857_row581_col0\" class=\"data row581 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row581_col1\" class=\"data row581 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row581_col2\" class=\"data row581 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row581_col3\" class=\"data row581 col3\" >0.8599</td>\n",
              "      <td id=\"T_02857_row581_col4\" class=\"data row581 col4\" >0.8619</td>\n",
              "      <td id=\"T_02857_row581_col5\" class=\"data row581 col5\" >0.8215</td>\n",
              "      <td id=\"T_02857_row581_col6\" class=\"data row581 col6\" >0.8347</td>\n",
              "      <td id=\"T_02857_row581_col7\" class=\"data row581 col7\" >0.7681</td>\n",
              "      <td id=\"T_02857_row581_col8\" class=\"data row581 col8\" >0.4598</td>\n",
              "      <td id=\"T_02857_row581_col9\" class=\"data row581 col9\" >0.5385</td>\n",
              "      <td id=\"T_02857_row581_col10\" class=\"data row581 col10\" >0.4496</td>\n",
              "      <td id=\"T_02857_row581_col11\" class=\"data row581 col11\" >0.5631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row582\" class=\"row_heading level0 row582\" >582</th>\n",
              "      <td id=\"T_02857_row582_col0\" class=\"data row582 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row582_col1\" class=\"data row582 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row582_col2\" class=\"data row582 col2\" >bert-base-multilingual-cased</td>\n",
              "      <td id=\"T_02857_row582_col3\" class=\"data row582 col3\" >0.8599</td>\n",
              "      <td id=\"T_02857_row582_col4\" class=\"data row582 col4\" >0.8619</td>\n",
              "      <td id=\"T_02857_row582_col5\" class=\"data row582 col5\" >0.8215</td>\n",
              "      <td id=\"T_02857_row582_col6\" class=\"data row582 col6\" >0.8347</td>\n",
              "      <td id=\"T_02857_row582_col7\" class=\"data row582 col7\" >0.7681</td>\n",
              "      <td id=\"T_02857_row582_col8\" class=\"data row582 col8\" >0.4598</td>\n",
              "      <td id=\"T_02857_row582_col9\" class=\"data row582 col9\" >0.5385</td>\n",
              "      <td id=\"T_02857_row582_col10\" class=\"data row582 col10\" >0.4496</td>\n",
              "      <td id=\"T_02857_row582_col11\" class=\"data row582 col11\" >0.5631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row583\" class=\"row_heading level0 row583\" >583</th>\n",
              "      <td id=\"T_02857_row583_col0\" class=\"data row583 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row583_col1\" class=\"data row583 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row583_col2\" class=\"data row583 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row583_col3\" class=\"data row583 col3\" >0.6033</td>\n",
              "      <td id=\"T_02857_row583_col4\" class=\"data row583 col4\" >0.6081</td>\n",
              "      <td id=\"T_02857_row583_col5\" class=\"data row583 col5\" >0.5785</td>\n",
              "      <td id=\"T_02857_row583_col6\" class=\"data row583 col6\" >0.6328</td>\n",
              "      <td id=\"T_02857_row583_col7\" class=\"data row583 col7\" >0.6077</td>\n",
              "      <td id=\"T_02857_row583_col8\" class=\"data row583 col8\" >0.5936</td>\n",
              "      <td id=\"T_02857_row583_col9\" class=\"data row583 col9\" >0.5292</td>\n",
              "      <td id=\"T_02857_row583_col10\" class=\"data row583 col10\" >0.5099</td>\n",
              "      <td id=\"T_02857_row583_col11\" class=\"data row583 col11\" >0.4431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row584\" class=\"row_heading level0 row584\" >584</th>\n",
              "      <td id=\"T_02857_row584_col0\" class=\"data row584 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row584_col1\" class=\"data row584 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row584_col2\" class=\"data row584 col2\" >electra-large-discriminator</td>\n",
              "      <td id=\"T_02857_row584_col3\" class=\"data row584 col3\" >0.6033</td>\n",
              "      <td id=\"T_02857_row584_col4\" class=\"data row584 col4\" >0.6081</td>\n",
              "      <td id=\"T_02857_row584_col5\" class=\"data row584 col5\" >0.5785</td>\n",
              "      <td id=\"T_02857_row584_col6\" class=\"data row584 col6\" >0.6328</td>\n",
              "      <td id=\"T_02857_row584_col7\" class=\"data row584 col7\" >0.6077</td>\n",
              "      <td id=\"T_02857_row584_col8\" class=\"data row584 col8\" >0.5936</td>\n",
              "      <td id=\"T_02857_row584_col9\" class=\"data row584 col9\" >0.5292</td>\n",
              "      <td id=\"T_02857_row584_col10\" class=\"data row584 col10\" >0.5099</td>\n",
              "      <td id=\"T_02857_row584_col11\" class=\"data row584 col11\" >0.4431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row585\" class=\"row_heading level0 row585\" >585</th>\n",
              "      <td id=\"T_02857_row585_col0\" class=\"data row585 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row585_col1\" class=\"data row585 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row585_col2\" class=\"data row585 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row585_col3\" class=\"data row585 col3\" >0.4815</td>\n",
              "      <td id=\"T_02857_row585_col4\" class=\"data row585 col4\" >0.5030</td>\n",
              "      <td id=\"T_02857_row585_col5\" class=\"data row585 col5\" >0.5517</td>\n",
              "      <td id=\"T_02857_row585_col6\" class=\"data row585 col6\" >0.5490</td>\n",
              "      <td id=\"T_02857_row585_col7\" class=\"data row585 col7\" >0.5444</td>\n",
              "      <td id=\"T_02857_row585_col8\" class=\"data row585 col8\" >0.5433</td>\n",
              "      <td id=\"T_02857_row585_col9\" class=\"data row585 col9\" >0.5459</td>\n",
              "      <td id=\"T_02857_row585_col10\" class=\"data row585 col10\" >0.5119</td>\n",
              "      <td id=\"T_02857_row585_col11\" class=\"data row585 col11\" >0.3570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row586\" class=\"row_heading level0 row586\" >586</th>\n",
              "      <td id=\"T_02857_row586_col0\" class=\"data row586 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row586_col1\" class=\"data row586 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row586_col2\" class=\"data row586 col2\" >gpt2-medium</td>\n",
              "      <td id=\"T_02857_row586_col3\" class=\"data row586 col3\" >0.4815</td>\n",
              "      <td id=\"T_02857_row586_col4\" class=\"data row586 col4\" >0.5030</td>\n",
              "      <td id=\"T_02857_row586_col5\" class=\"data row586 col5\" >0.5517</td>\n",
              "      <td id=\"T_02857_row586_col6\" class=\"data row586 col6\" >0.5490</td>\n",
              "      <td id=\"T_02857_row586_col7\" class=\"data row586 col7\" >0.5444</td>\n",
              "      <td id=\"T_02857_row586_col8\" class=\"data row586 col8\" >0.5433</td>\n",
              "      <td id=\"T_02857_row586_col9\" class=\"data row586 col9\" >0.5459</td>\n",
              "      <td id=\"T_02857_row586_col10\" class=\"data row586 col10\" >0.5119</td>\n",
              "      <td id=\"T_02857_row586_col11\" class=\"data row586 col11\" >0.3570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row587\" class=\"row_heading level0 row587\" >587</th>\n",
              "      <td id=\"T_02857_row587_col0\" class=\"data row587 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row587_col1\" class=\"data row587 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row587_col2\" class=\"data row587 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row587_col3\" class=\"data row587 col3\" >0.7675</td>\n",
              "      <td id=\"T_02857_row587_col4\" class=\"data row587 col4\" >0.7693</td>\n",
              "      <td id=\"T_02857_row587_col5\" class=\"data row587 col5\" >0.7400</td>\n",
              "      <td id=\"T_02857_row587_col6\" class=\"data row587 col6\" >0.7467</td>\n",
              "      <td id=\"T_02857_row587_col7\" class=\"data row587 col7\" >0.6805</td>\n",
              "      <td id=\"T_02857_row587_col8\" class=\"data row587 col8\" >0.3508</td>\n",
              "      <td id=\"T_02857_row587_col9\" class=\"data row587 col9\" >0.4874</td>\n",
              "      <td id=\"T_02857_row587_col10\" class=\"data row587 col10\" >0.4413</td>\n",
              "      <td id=\"T_02857_row587_col11\" class=\"data row587 col11\" >0.5131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row588\" class=\"row_heading level0 row588\" >588</th>\n",
              "      <td id=\"T_02857_row588_col0\" class=\"data row588 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row588_col1\" class=\"data row588 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row588_col2\" class=\"data row588 col2\" >mGPT</td>\n",
              "      <td id=\"T_02857_row588_col3\" class=\"data row588 col3\" >0.7675</td>\n",
              "      <td id=\"T_02857_row588_col4\" class=\"data row588 col4\" >0.7693</td>\n",
              "      <td id=\"T_02857_row588_col5\" class=\"data row588 col5\" >0.7400</td>\n",
              "      <td id=\"T_02857_row588_col6\" class=\"data row588 col6\" >0.7467</td>\n",
              "      <td id=\"T_02857_row588_col7\" class=\"data row588 col7\" >0.6805</td>\n",
              "      <td id=\"T_02857_row588_col8\" class=\"data row588 col8\" >0.3508</td>\n",
              "      <td id=\"T_02857_row588_col9\" class=\"data row588 col9\" >0.4874</td>\n",
              "      <td id=\"T_02857_row588_col10\" class=\"data row588 col10\" >0.4413</td>\n",
              "      <td id=\"T_02857_row588_col11\" class=\"data row588 col11\" >0.5131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row589\" class=\"row_heading level0 row589\" >589</th>\n",
              "      <td id=\"T_02857_row589_col0\" class=\"data row589 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row589_col1\" class=\"data row589 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row589_col2\" class=\"data row589 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row589_col3\" class=\"data row589 col3\" >0.7599</td>\n",
              "      <td id=\"T_02857_row589_col4\" class=\"data row589 col4\" >0.7451</td>\n",
              "      <td id=\"T_02857_row589_col5\" class=\"data row589 col5\" >0.6906</td>\n",
              "      <td id=\"T_02857_row589_col6\" class=\"data row589 col6\" >0.6821</td>\n",
              "      <td id=\"T_02857_row589_col7\" class=\"data row589 col7\" >0.6367</td>\n",
              "      <td id=\"T_02857_row589_col8\" class=\"data row589 col8\" >0.3194</td>\n",
              "      <td id=\"T_02857_row589_col9\" class=\"data row589 col9\" >0.4490</td>\n",
              "      <td id=\"T_02857_row589_col10\" class=\"data row589 col10\" >0.3392</td>\n",
              "      <td id=\"T_02857_row589_col11\" class=\"data row589 col11\" >0.4756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row590\" class=\"row_heading level0 row590\" >590</th>\n",
              "      <td id=\"T_02857_row590_col0\" class=\"data row590 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row590_col1\" class=\"data row590 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row590_col2\" class=\"data row590 col2\" >mdeberta-v3-base</td>\n",
              "      <td id=\"T_02857_row590_col3\" class=\"data row590 col3\" >0.7599</td>\n",
              "      <td id=\"T_02857_row590_col4\" class=\"data row590 col4\" >0.7451</td>\n",
              "      <td id=\"T_02857_row590_col5\" class=\"data row590 col5\" >0.6906</td>\n",
              "      <td id=\"T_02857_row590_col6\" class=\"data row590 col6\" >0.6821</td>\n",
              "      <td id=\"T_02857_row590_col7\" class=\"data row590 col7\" >0.6367</td>\n",
              "      <td id=\"T_02857_row590_col8\" class=\"data row590 col8\" >0.3194</td>\n",
              "      <td id=\"T_02857_row590_col9\" class=\"data row590 col9\" >0.4490</td>\n",
              "      <td id=\"T_02857_row590_col10\" class=\"data row590 col10\" >0.3392</td>\n",
              "      <td id=\"T_02857_row590_col11\" class=\"data row590 col11\" >0.4756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row591\" class=\"row_heading level0 row591\" >591</th>\n",
              "      <td id=\"T_02857_row591_col0\" class=\"data row591 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row591_col1\" class=\"data row591 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row591_col2\" class=\"data row591 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row591_col3\" class=\"data row591 col3\" >0.4982</td>\n",
              "      <td id=\"T_02857_row591_col4\" class=\"data row591 col4\" >0.4981</td>\n",
              "      <td id=\"T_02857_row591_col5\" class=\"data row591 col5\" >0.4978</td>\n",
              "      <td id=\"T_02857_row591_col6\" class=\"data row591 col6\" >0.4969</td>\n",
              "      <td id=\"T_02857_row591_col7\" class=\"data row591 col7\" >0.4962</td>\n",
              "      <td id=\"T_02857_row591_col8\" class=\"data row591 col8\" >0.4546</td>\n",
              "      <td id=\"T_02857_row591_col9\" class=\"data row591 col9\" >0.4327</td>\n",
              "      <td id=\"T_02857_row591_col10\" class=\"data row591 col10\" >0.4770</td>\n",
              "      <td id=\"T_02857_row591_col11\" class=\"data row591 col11\" >0.5725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row592\" class=\"row_heading level0 row592\" >592</th>\n",
              "      <td id=\"T_02857_row592_col0\" class=\"data row592 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row592_col1\" class=\"data row592 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row592_col2\" class=\"data row592 col2\" >roberta-large-openai-detector</td>\n",
              "      <td id=\"T_02857_row592_col3\" class=\"data row592 col3\" >0.4982</td>\n",
              "      <td id=\"T_02857_row592_col4\" class=\"data row592 col4\" >0.4981</td>\n",
              "      <td id=\"T_02857_row592_col5\" class=\"data row592 col5\" >0.4978</td>\n",
              "      <td id=\"T_02857_row592_col6\" class=\"data row592 col6\" >0.4969</td>\n",
              "      <td id=\"T_02857_row592_col7\" class=\"data row592 col7\" >0.4962</td>\n",
              "      <td id=\"T_02857_row592_col8\" class=\"data row592 col8\" >0.4546</td>\n",
              "      <td id=\"T_02857_row592_col9\" class=\"data row592 col9\" >0.4327</td>\n",
              "      <td id=\"T_02857_row592_col10\" class=\"data row592 col10\" >0.4770</td>\n",
              "      <td id=\"T_02857_row592_col11\" class=\"data row592 col11\" >0.5725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row593\" class=\"row_heading level0 row593\" >593</th>\n",
              "      <td id=\"T_02857_row593_col0\" class=\"data row593 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row593_col1\" class=\"data row593 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row593_col2\" class=\"data row593 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row593_col3\" class=\"data row593 col3\" >0.8673</td>\n",
              "      <td id=\"T_02857_row593_col4\" class=\"data row593 col4\" >0.8677</td>\n",
              "      <td id=\"T_02857_row593_col5\" class=\"data row593 col5\" >0.7899</td>\n",
              "      <td id=\"T_02857_row593_col6\" class=\"data row593 col6\" >0.7900</td>\n",
              "      <td id=\"T_02857_row593_col7\" class=\"data row593 col7\" >0.7089</td>\n",
              "      <td id=\"T_02857_row593_col8\" class=\"data row593 col8\" >0.3930</td>\n",
              "      <td id=\"T_02857_row593_col9\" class=\"data row593 col9\" >0.4373</td>\n",
              "      <td id=\"T_02857_row593_col10\" class=\"data row593 col10\" >0.4262</td>\n",
              "      <td id=\"T_02857_row593_col11\" class=\"data row593 col11\" >0.5183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_02857_level0_row594\" class=\"row_heading level0 row594\" >594</th>\n",
              "      <td id=\"T_02857_row594_col0\" class=\"data row594 col0\" >en3</td>\n",
              "      <td id=\"T_02857_row594_col1\" class=\"data row594 col1\" >vicuna-13b</td>\n",
              "      <td id=\"T_02857_row594_col2\" class=\"data row594 col2\" >xlm-roberta-large</td>\n",
              "      <td id=\"T_02857_row594_col3\" class=\"data row594 col3\" >0.8673</td>\n",
              "      <td id=\"T_02857_row594_col4\" class=\"data row594 col4\" >0.8677</td>\n",
              "      <td id=\"T_02857_row594_col5\" class=\"data row594 col5\" >0.7899</td>\n",
              "      <td id=\"T_02857_row594_col6\" class=\"data row594 col6\" >0.7900</td>\n",
              "      <td id=\"T_02857_row594_col7\" class=\"data row594 col7\" >0.7089</td>\n",
              "      <td id=\"T_02857_row594_col8\" class=\"data row594 col8\" >0.3930</td>\n",
              "      <td id=\"T_02857_row594_col9\" class=\"data row594 col9\" >0.4373</td>\n",
              "      <td id=\"T_02857_row594_col10\" class=\"data row594 col10\" >0.4262</td>\n",
              "      <td id=\"T_02857_row594_col11\" class=\"data row594 col11\" >0.5183</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_all.drop(columns=['all'], inplace=True)"
      ],
      "metadata": {
        "id": "-5Py8PUyIpJZ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_all.corr().style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9UNakZuYCxZ1",
        "outputId": "1814e158-456c-4b27-b423-3f40cc0e1d15"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-70-342be14f85a5>:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  results_all.corr().style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001af6140>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_ab148_row0_col0, #T_ab148_row1_col1, #T_ab148_row2_col2, #T_ab148_row3_col3, #T_ab148_row4_col4, #T_ab148_row5_col5, #T_ab148_row6_col6, #T_ab148_row7_col7 {\n",
              "  background-color: #73a9cf;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_ab148_row0_col1, #T_ab148_row1_col0 {\n",
              "  background-color: #79abd0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_ab148_row0_col2, #T_ab148_row2_col0 {\n",
              "  background-color: #86b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_ab148_row0_col3, #T_ab148_row3_col0 {\n",
              "  background-color: #8bb2d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_ab148_row0_col4, #T_ab148_row4_col0 {\n",
              "  background-color: #9ab8d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_ab148_row0_col5, #T_ab148_row0_col6, #T_ab148_row0_col7, #T_ab148_row1_col5, #T_ab148_row1_col6, #T_ab148_row1_col7, #T_ab148_row2_col5, #T_ab148_row2_col6, #T_ab148_row2_col7, #T_ab148_row3_col5, #T_ab148_row3_col6, #T_ab148_row3_col7, #T_ab148_row4_col5, #T_ab148_row4_col6, #T_ab148_row4_col7, #T_ab148_row5_col0, #T_ab148_row5_col1, #T_ab148_row5_col2, #T_ab148_row5_col3, #T_ab148_row5_col4, #T_ab148_row6_col0, #T_ab148_row6_col1, #T_ab148_row6_col2, #T_ab148_row6_col3, #T_ab148_row6_col4, #T_ab148_row7_col0, #T_ab148_row7_col1, #T_ab148_row7_col2, #T_ab148_row7_col3, #T_ab148_row7_col4 {\n",
              "  background-color: #fff7fb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_ab148_row1_col2, #T_ab148_row2_col1 {\n",
              "  background-color: #7bacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_ab148_row1_col3, #T_ab148_row3_col1 {\n",
              "  background-color: #88b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_ab148_row1_col4, #T_ab148_row4_col1 {\n",
              "  background-color: #8eb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_ab148_row2_col3, #T_ab148_row3_col2 {\n",
              "  background-color: #83afd3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_ab148_row2_col4, #T_ab148_row3_col4, #T_ab148_row4_col2, #T_ab148_row4_col3 {\n",
              "  background-color: #81aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_ab148_row5_col6, #T_ab148_row6_col5 {\n",
              "  background-color: #9fbad9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_ab148_row5_col7, #T_ab148_row7_col5 {\n",
              "  background-color: #84b0d3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_ab148_row6_col7, #T_ab148_row7_col6 {\n",
              "  background-color: #94b6d7;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_ab148\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_ab148_level0_col0\" class=\"col_heading level0 col0\" >gpt-4</th>\n",
              "      <th id=\"T_ab148_level0_col1\" class=\"col_heading level0 col1\" >gpt-3.5-turbo</th>\n",
              "      <th id=\"T_ab148_level0_col2\" class=\"col_heading level0 col2\" >text-davinci-003</th>\n",
              "      <th id=\"T_ab148_level0_col3\" class=\"col_heading level0 col3\" >vicuna-13b</th>\n",
              "      <th id=\"T_ab148_level0_col4\" class=\"col_heading level0 col4\" >alpaca-lora-30b</th>\n",
              "      <th id=\"T_ab148_level0_col5\" class=\"col_heading level0 col5\" >opt-iml-max-1.3b</th>\n",
              "      <th id=\"T_ab148_level0_col6\" class=\"col_heading level0 col6\" >llama-65b</th>\n",
              "      <th id=\"T_ab148_level0_col7\" class=\"col_heading level0 col7\" >opt-66b</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_ab148_level0_row0\" class=\"row_heading level0 row0\" >gpt-4</th>\n",
              "      <td id=\"T_ab148_row0_col0\" class=\"data row0 col0\" >1.0000</td>\n",
              "      <td id=\"T_ab148_row0_col1\" class=\"data row0 col1\" >0.9695</td>\n",
              "      <td id=\"T_ab148_row0_col2\" class=\"data row0 col2\" >0.9075</td>\n",
              "      <td id=\"T_ab148_row0_col3\" class=\"data row0 col3\" >0.8830</td>\n",
              "      <td id=\"T_ab148_row0_col4\" class=\"data row0 col4\" >0.8052</td>\n",
              "      <td id=\"T_ab148_row0_col5\" class=\"data row0 col5\" >-0.4841</td>\n",
              "      <td id=\"T_ab148_row0_col6\" class=\"data row0 col6\" >-0.4342</td>\n",
              "      <td id=\"T_ab148_row0_col7\" class=\"data row0 col7\" >-0.4331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ab148_level0_row1\" class=\"row_heading level0 row1\" >gpt-3.5-turbo</th>\n",
              "      <td id=\"T_ab148_row1_col0\" class=\"data row1 col0\" >0.9695</td>\n",
              "      <td id=\"T_ab148_row1_col1\" class=\"data row1 col1\" >1.0000</td>\n",
              "      <td id=\"T_ab148_row1_col2\" class=\"data row1 col2\" >0.9633</td>\n",
              "      <td id=\"T_ab148_row1_col3\" class=\"data row1 col3\" >0.9016</td>\n",
              "      <td id=\"T_ab148_row1_col4\" class=\"data row1 col4\" >0.8706</td>\n",
              "      <td id=\"T_ab148_row1_col5\" class=\"data row1 col5\" >-0.4759</td>\n",
              "      <td id=\"T_ab148_row1_col6\" class=\"data row1 col6\" >-0.4858</td>\n",
              "      <td id=\"T_ab148_row1_col7\" class=\"data row1 col7\" >-0.4592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ab148_level0_row2\" class=\"row_heading level0 row2\" >text-davinci-003</th>\n",
              "      <td id=\"T_ab148_row2_col0\" class=\"data row2 col0\" >0.9075</td>\n",
              "      <td id=\"T_ab148_row2_col1\" class=\"data row2 col1\" >0.9633</td>\n",
              "      <td id=\"T_ab148_row2_col2\" class=\"data row2 col2\" >1.0000</td>\n",
              "      <td id=\"T_ab148_row2_col3\" class=\"data row2 col3\" >0.9260</td>\n",
              "      <td id=\"T_ab148_row2_col4\" class=\"data row2 col4\" >0.9366</td>\n",
              "      <td id=\"T_ab148_row2_col5\" class=\"data row2 col5\" >-0.3625</td>\n",
              "      <td id=\"T_ab148_row2_col6\" class=\"data row2 col6\" >-0.4321</td>\n",
              "      <td id=\"T_ab148_row2_col7\" class=\"data row2 col7\" >-0.3408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ab148_level0_row3\" class=\"row_heading level0 row3\" >vicuna-13b</th>\n",
              "      <td id=\"T_ab148_row3_col0\" class=\"data row3 col0\" >0.8830</td>\n",
              "      <td id=\"T_ab148_row3_col1\" class=\"data row3 col1\" >0.9016</td>\n",
              "      <td id=\"T_ab148_row3_col2\" class=\"data row3 col2\" >0.9260</td>\n",
              "      <td id=\"T_ab148_row3_col3\" class=\"data row3 col3\" >1.0000</td>\n",
              "      <td id=\"T_ab148_row3_col4\" class=\"data row3 col4\" >0.9301</td>\n",
              "      <td id=\"T_ab148_row3_col5\" class=\"data row3 col5\" >-0.1327</td>\n",
              "      <td id=\"T_ab148_row3_col6\" class=\"data row3 col6\" >-0.1718</td>\n",
              "      <td id=\"T_ab148_row3_col7\" class=\"data row3 col7\" >-0.1032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ab148_level0_row4\" class=\"row_heading level0 row4\" >alpaca-lora-30b</th>\n",
              "      <td id=\"T_ab148_row4_col0\" class=\"data row4 col0\" >0.8052</td>\n",
              "      <td id=\"T_ab148_row4_col1\" class=\"data row4 col1\" >0.8706</td>\n",
              "      <td id=\"T_ab148_row4_col2\" class=\"data row4 col2\" >0.9366</td>\n",
              "      <td id=\"T_ab148_row4_col3\" class=\"data row4 col3\" >0.9301</td>\n",
              "      <td id=\"T_ab148_row4_col4\" class=\"data row4 col4\" >1.0000</td>\n",
              "      <td id=\"T_ab148_row4_col5\" class=\"data row4 col5\" >-0.1316</td>\n",
              "      <td id=\"T_ab148_row4_col6\" class=\"data row4 col6\" >-0.2379</td>\n",
              "      <td id=\"T_ab148_row4_col7\" class=\"data row4 col7\" >-0.1300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ab148_level0_row5\" class=\"row_heading level0 row5\" >opt-iml-max-1.3b</th>\n",
              "      <td id=\"T_ab148_row5_col0\" class=\"data row5 col0\" >-0.4841</td>\n",
              "      <td id=\"T_ab148_row5_col1\" class=\"data row5 col1\" >-0.4759</td>\n",
              "      <td id=\"T_ab148_row5_col2\" class=\"data row5 col2\" >-0.3625</td>\n",
              "      <td id=\"T_ab148_row5_col3\" class=\"data row5 col3\" >-0.1327</td>\n",
              "      <td id=\"T_ab148_row5_col4\" class=\"data row5 col4\" >-0.1316</td>\n",
              "      <td id=\"T_ab148_row5_col5\" class=\"data row5 col5\" >1.0000</td>\n",
              "      <td id=\"T_ab148_row5_col6\" class=\"data row5 col6\" >0.7839</td>\n",
              "      <td id=\"T_ab148_row5_col7\" class=\"data row5 col7\" >0.9210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ab148_level0_row6\" class=\"row_heading level0 row6\" >llama-65b</th>\n",
              "      <td id=\"T_ab148_row6_col0\" class=\"data row6 col0\" >-0.4342</td>\n",
              "      <td id=\"T_ab148_row6_col1\" class=\"data row6 col1\" >-0.4858</td>\n",
              "      <td id=\"T_ab148_row6_col2\" class=\"data row6 col2\" >-0.4321</td>\n",
              "      <td id=\"T_ab148_row6_col3\" class=\"data row6 col3\" >-0.1718</td>\n",
              "      <td id=\"T_ab148_row6_col4\" class=\"data row6 col4\" >-0.2379</td>\n",
              "      <td id=\"T_ab148_row6_col5\" class=\"data row6 col5\" >0.7839</td>\n",
              "      <td id=\"T_ab148_row6_col6\" class=\"data row6 col6\" >1.0000</td>\n",
              "      <td id=\"T_ab148_row6_col7\" class=\"data row6 col7\" >0.8419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ab148_level0_row7\" class=\"row_heading level0 row7\" >opt-66b</th>\n",
              "      <td id=\"T_ab148_row7_col0\" class=\"data row7 col0\" >-0.4331</td>\n",
              "      <td id=\"T_ab148_row7_col1\" class=\"data row7 col1\" >-0.4592</td>\n",
              "      <td id=\"T_ab148_row7_col2\" class=\"data row7 col2\" >-0.3408</td>\n",
              "      <td id=\"T_ab148_row7_col3\" class=\"data row7 col3\" >-0.1032</td>\n",
              "      <td id=\"T_ab148_row7_col4\" class=\"data row7 col4\" >-0.1300</td>\n",
              "      <td id=\"T_ab148_row7_col5\" class=\"data row7 col5\" >0.9210</td>\n",
              "      <td id=\"T_ab148_row7_col6\" class=\"data row7 col6\" >0.8419</td>\n",
              "      <td id=\"T_ab148_row7_col7\" class=\"data row7 col7\" >1.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_all = results_all[~results_all['Train Language'].isin(['en3', 'all'])]"
      ],
      "metadata": {
        "id": "0HGvVh12C9gd"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_all = results_all[results_all['Train LLM'] != 'all']"
      ],
      "metadata": {
        "id": "ur5KpxK2I25_"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_all.corr().style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4)"
      ],
      "metadata": {
        "id": "D0QOh7MziIW_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "41cd9335-8284-445f-dde4-e5213d4f7006"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-73-342be14f85a5>:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  results_all.corr().style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7cd001b7e4d0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_c5494_row0_col0, #T_c5494_row1_col1, #T_c5494_row2_col2, #T_c5494_row3_col3, #T_c5494_row4_col4, #T_c5494_row5_col5, #T_c5494_row6_col6, #T_c5494_row7_col7 {\n",
              "  background-color: #73a9cf;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c5494_row0_col1, #T_c5494_row1_col0 {\n",
              "  background-color: #79abd0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c5494_row0_col2, #T_c5494_row1_col3, #T_c5494_row2_col0, #T_c5494_row3_col1, #T_c5494_row5_col7, #T_c5494_row7_col5 {\n",
              "  background-color: #88b1d4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c5494_row0_col3, #T_c5494_row3_col0 {\n",
              "  background-color: #8cb3d5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c5494_row0_col4, #T_c5494_row4_col0 {\n",
              "  background-color: #a1bbda;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c5494_row0_col5, #T_c5494_row0_col6, #T_c5494_row0_col7, #T_c5494_row1_col5, #T_c5494_row1_col6, #T_c5494_row1_col7, #T_c5494_row2_col5, #T_c5494_row2_col6, #T_c5494_row2_col7, #T_c5494_row3_col5, #T_c5494_row3_col6, #T_c5494_row3_col7, #T_c5494_row4_col5, #T_c5494_row4_col6, #T_c5494_row4_col7, #T_c5494_row5_col0, #T_c5494_row5_col1, #T_c5494_row5_col2, #T_c5494_row5_col3, #T_c5494_row5_col4, #T_c5494_row6_col0, #T_c5494_row6_col1, #T_c5494_row6_col2, #T_c5494_row6_col3, #T_c5494_row6_col4, #T_c5494_row7_col0, #T_c5494_row7_col1, #T_c5494_row7_col2, #T_c5494_row7_col3, #T_c5494_row7_col4 {\n",
              "  background-color: #fff7fb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c5494_row1_col2, #T_c5494_row2_col1 {\n",
              "  background-color: #7dacd1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c5494_row1_col4, #T_c5494_row4_col1 {\n",
              "  background-color: #91b5d6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c5494_row2_col3, #T_c5494_row3_col2 {\n",
              "  background-color: #80aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c5494_row2_col4, #T_c5494_row4_col2 {\n",
              "  background-color: #81aed2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c5494_row3_col4, #T_c5494_row4_col3 {\n",
              "  background-color: #83afd3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c5494_row5_col6, #T_c5494_row6_col5 {\n",
              "  background-color: #afc1dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_c5494_row6_col7, #T_c5494_row7_col6 {\n",
              "  background-color: #a2bcda;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_c5494\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_c5494_level0_col0\" class=\"col_heading level0 col0\" >gpt-4</th>\n",
              "      <th id=\"T_c5494_level0_col1\" class=\"col_heading level0 col1\" >gpt-3.5-turbo</th>\n",
              "      <th id=\"T_c5494_level0_col2\" class=\"col_heading level0 col2\" >text-davinci-003</th>\n",
              "      <th id=\"T_c5494_level0_col3\" class=\"col_heading level0 col3\" >vicuna-13b</th>\n",
              "      <th id=\"T_c5494_level0_col4\" class=\"col_heading level0 col4\" >alpaca-lora-30b</th>\n",
              "      <th id=\"T_c5494_level0_col5\" class=\"col_heading level0 col5\" >opt-iml-max-1.3b</th>\n",
              "      <th id=\"T_c5494_level0_col6\" class=\"col_heading level0 col6\" >llama-65b</th>\n",
              "      <th id=\"T_c5494_level0_col7\" class=\"col_heading level0 col7\" >opt-66b</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_c5494_level0_row0\" class=\"row_heading level0 row0\" >gpt-4</th>\n",
              "      <td id=\"T_c5494_row0_col0\" class=\"data row0 col0\" >1.0000</td>\n",
              "      <td id=\"T_c5494_row0_col1\" class=\"data row0 col1\" >0.9712</td>\n",
              "      <td id=\"T_c5494_row0_col2\" class=\"data row0 col2\" >0.9005</td>\n",
              "      <td id=\"T_c5494_row0_col3\" class=\"data row0 col3\" >0.8786</td>\n",
              "      <td id=\"T_c5494_row0_col4\" class=\"data row0 col4\" >0.7781</td>\n",
              "      <td id=\"T_c5494_row0_col5\" class=\"data row0 col5\" >-0.5218</td>\n",
              "      <td id=\"T_c5494_row0_col6\" class=\"data row0 col6\" >-0.4868</td>\n",
              "      <td id=\"T_c5494_row0_col7\" class=\"data row0 col7\" >-0.4779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c5494_level0_row1\" class=\"row_heading level0 row1\" >gpt-3.5-turbo</th>\n",
              "      <td id=\"T_c5494_row1_col0\" class=\"data row1 col0\" >0.9712</td>\n",
              "      <td id=\"T_c5494_row1_col1\" class=\"data row1 col1\" >1.0000</td>\n",
              "      <td id=\"T_c5494_row1_col2\" class=\"data row1 col2\" >0.9585</td>\n",
              "      <td id=\"T_c5494_row1_col3\" class=\"data row1 col3\" >0.9056</td>\n",
              "      <td id=\"T_c5494_row1_col4\" class=\"data row1 col4\" >0.8562</td>\n",
              "      <td id=\"T_c5494_row1_col5\" class=\"data row1 col5\" >-0.4872</td>\n",
              "      <td id=\"T_c5494_row1_col6\" class=\"data row1 col6\" >-0.5131</td>\n",
              "      <td id=\"T_c5494_row1_col7\" class=\"data row1 col7\" >-0.4805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c5494_level0_row2\" class=\"row_heading level0 row2\" >text-davinci-003</th>\n",
              "      <td id=\"T_c5494_row2_col0\" class=\"data row2 col0\" >0.9005</td>\n",
              "      <td id=\"T_c5494_row2_col1\" class=\"data row2 col1\" >0.9585</td>\n",
              "      <td id=\"T_c5494_row2_col2\" class=\"data row2 col2\" >1.0000</td>\n",
              "      <td id=\"T_c5494_row2_col3\" class=\"data row2 col3\" >0.9381</td>\n",
              "      <td id=\"T_c5494_row2_col4\" class=\"data row2 col4\" >0.9357</td>\n",
              "      <td id=\"T_c5494_row2_col5\" class=\"data row2 col5\" >-0.3574</td>\n",
              "      <td id=\"T_c5494_row2_col6\" class=\"data row2 col6\" >-0.4537</td>\n",
              "      <td id=\"T_c5494_row2_col7\" class=\"data row2 col7\" >-0.3444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c5494_level0_row3\" class=\"row_heading level0 row3\" >vicuna-13b</th>\n",
              "      <td id=\"T_c5494_row3_col0\" class=\"data row3 col0\" >0.8786</td>\n",
              "      <td id=\"T_c5494_row3_col1\" class=\"data row3 col1\" >0.9056</td>\n",
              "      <td id=\"T_c5494_row3_col2\" class=\"data row3 col2\" >0.9381</td>\n",
              "      <td id=\"T_c5494_row3_col3\" class=\"data row3 col3\" >1.0000</td>\n",
              "      <td id=\"T_c5494_row3_col4\" class=\"data row3 col4\" >0.9268</td>\n",
              "      <td id=\"T_c5494_row3_col5\" class=\"data row3 col5\" >-0.1632</td>\n",
              "      <td id=\"T_c5494_row3_col6\" class=\"data row3 col6\" >-0.2221</td>\n",
              "      <td id=\"T_c5494_row3_col7\" class=\"data row3 col7\" >-0.1273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c5494_level0_row4\" class=\"row_heading level0 row4\" >alpaca-lora-30b</th>\n",
              "      <td id=\"T_c5494_row4_col0\" class=\"data row4 col0\" >0.7781</td>\n",
              "      <td id=\"T_c5494_row4_col1\" class=\"data row4 col1\" >0.8562</td>\n",
              "      <td id=\"T_c5494_row4_col2\" class=\"data row4 col2\" >0.9357</td>\n",
              "      <td id=\"T_c5494_row4_col3\" class=\"data row4 col3\" >0.9268</td>\n",
              "      <td id=\"T_c5494_row4_col4\" class=\"data row4 col4\" >1.0000</td>\n",
              "      <td id=\"T_c5494_row4_col5\" class=\"data row4 col5\" >-0.1226</td>\n",
              "      <td id=\"T_c5494_row4_col6\" class=\"data row4 col6\" >-0.2870</td>\n",
              "      <td id=\"T_c5494_row4_col7\" class=\"data row4 col7\" >-0.1261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c5494_level0_row5\" class=\"row_heading level0 row5\" >opt-iml-max-1.3b</th>\n",
              "      <td id=\"T_c5494_row5_col0\" class=\"data row5 col0\" >-0.5218</td>\n",
              "      <td id=\"T_c5494_row5_col1\" class=\"data row5 col1\" >-0.4872</td>\n",
              "      <td id=\"T_c5494_row5_col2\" class=\"data row5 col2\" >-0.3574</td>\n",
              "      <td id=\"T_c5494_row5_col3\" class=\"data row5 col3\" >-0.1632</td>\n",
              "      <td id=\"T_c5494_row5_col4\" class=\"data row5 col4\" >-0.1226</td>\n",
              "      <td id=\"T_c5494_row5_col5\" class=\"data row5 col5\" >1.0000</td>\n",
              "      <td id=\"T_c5494_row5_col6\" class=\"data row5 col6\" >0.6990</td>\n",
              "      <td id=\"T_c5494_row5_col7\" class=\"data row5 col7\" >0.9011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c5494_level0_row6\" class=\"row_heading level0 row6\" >llama-65b</th>\n",
              "      <td id=\"T_c5494_row6_col0\" class=\"data row6 col0\" >-0.4868</td>\n",
              "      <td id=\"T_c5494_row6_col1\" class=\"data row6 col1\" >-0.5131</td>\n",
              "      <td id=\"T_c5494_row6_col2\" class=\"data row6 col2\" >-0.4537</td>\n",
              "      <td id=\"T_c5494_row6_col3\" class=\"data row6 col3\" >-0.2221</td>\n",
              "      <td id=\"T_c5494_row6_col4\" class=\"data row6 col4\" >-0.2870</td>\n",
              "      <td id=\"T_c5494_row6_col5\" class=\"data row6 col5\" >0.6990</td>\n",
              "      <td id=\"T_c5494_row6_col6\" class=\"data row6 col6\" >1.0000</td>\n",
              "      <td id=\"T_c5494_row6_col7\" class=\"data row6 col7\" >0.7721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c5494_level0_row7\" class=\"row_heading level0 row7\" >opt-66b</th>\n",
              "      <td id=\"T_c5494_row7_col0\" class=\"data row7 col0\" >-0.4779</td>\n",
              "      <td id=\"T_c5494_row7_col1\" class=\"data row7 col1\" >-0.4805</td>\n",
              "      <td id=\"T_c5494_row7_col2\" class=\"data row7 col2\" >-0.3444</td>\n",
              "      <td id=\"T_c5494_row7_col3\" class=\"data row7 col3\" >-0.1273</td>\n",
              "      <td id=\"T_c5494_row7_col4\" class=\"data row7 col4\" >-0.1261</td>\n",
              "      <td id=\"T_c5494_row7_col5\" class=\"data row7 col5\" >0.9011</td>\n",
              "      <td id=\"T_c5494_row7_col6\" class=\"data row7 col6\" >0.7721</td>\n",
              "      <td id=\"T_c5494_row7_col7\" class=\"data row7 col7\" >1.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tex_temp = results_all.corr().style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4)\n",
        "print(tex_temp.applymap_index(lambda v: \"font-weight: bold;\", axis=0).applymap_index(lambda v: \"font-weight: bold;\", axis=1).to_latex(convert_css=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4ENboULF3iq",
        "outputId": "1a307d9e-0672-4416-bf9f-256fc913def7"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{lrrrrrrrr}\n",
            " & \\bfseries gpt-4 & \\bfseries gpt-3.5-turbo & \\bfseries text-davinci-003 & \\bfseries vicuna-13b & \\bfseries alpaca-lora-30b & \\bfseries opt-iml-max-1.3b & \\bfseries llama-65b & \\bfseries opt-66b \\\\\n",
            "\\bfseries gpt-4 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{79ABD0}} \\color[HTML]{000000} 0.9712 & {\\cellcolor[HTML]{88B1D4}} \\color[HTML]{000000} 0.9005 & {\\cellcolor[HTML]{8CB3D5}} \\color[HTML]{000000} 0.8786 & {\\cellcolor[HTML]{A1BBDA}} \\color[HTML]{000000} 0.7781 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.5218 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.4868 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.4779 \\\\\n",
            "\\bfseries gpt-3.5-turbo & {\\cellcolor[HTML]{79ABD0}} \\color[HTML]{000000} 0.9712 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{7DACD1}} \\color[HTML]{000000} 0.9585 & {\\cellcolor[HTML]{88B1D4}} \\color[HTML]{000000} 0.9056 & {\\cellcolor[HTML]{91B5D6}} \\color[HTML]{000000} 0.8562 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.4872 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.5131 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.4805 \\\\\n",
            "\\bfseries text-davinci-003 & {\\cellcolor[HTML]{88B1D4}} \\color[HTML]{000000} 0.9005 & {\\cellcolor[HTML]{7DACD1}} \\color[HTML]{000000} 0.9585 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{80AED2}} \\color[HTML]{000000} 0.9381 & {\\cellcolor[HTML]{81AED2}} \\color[HTML]{000000} 0.9357 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.3574 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.4537 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.3444 \\\\\n",
            "\\bfseries vicuna-13b & {\\cellcolor[HTML]{8CB3D5}} \\color[HTML]{000000} 0.8786 & {\\cellcolor[HTML]{88B1D4}} \\color[HTML]{000000} 0.9056 & {\\cellcolor[HTML]{80AED2}} \\color[HTML]{000000} 0.9381 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{83AFD3}} \\color[HTML]{000000} 0.9268 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.1632 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.2221 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.1273 \\\\\n",
            "\\bfseries alpaca-lora-30b & {\\cellcolor[HTML]{A1BBDA}} \\color[HTML]{000000} 0.7781 & {\\cellcolor[HTML]{91B5D6}} \\color[HTML]{000000} 0.8562 & {\\cellcolor[HTML]{81AED2}} \\color[HTML]{000000} 0.9357 & {\\cellcolor[HTML]{83AFD3}} \\color[HTML]{000000} 0.9268 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.1226 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.2870 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.1261 \\\\\n",
            "\\bfseries opt-iml-max-1.3b & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.5218 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.4872 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.3574 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.1632 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.1226 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{AFC1DD}} \\color[HTML]{000000} 0.6990 & {\\cellcolor[HTML]{88B1D4}} \\color[HTML]{000000} 0.9011 \\\\\n",
            "\\bfseries llama-65b & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.4868 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.5131 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.4537 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.2221 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.2870 & {\\cellcolor[HTML]{AFC1DD}} \\color[HTML]{000000} 0.6990 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 & {\\cellcolor[HTML]{A2BCDA}} \\color[HTML]{000000} 0.7721 \\\\\n",
            "\\bfseries opt-66b & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.4779 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.4805 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.3444 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.1273 & {\\cellcolor[HTML]{FFF7FB}} \\color[HTML]{000000} -0.1261 & {\\cellcolor[HTML]{88B1D4}} \\color[HTML]{000000} 0.9011 & {\\cellcolor[HTML]{A2BCDA}} \\color[HTML]{000000} 0.7721 & {\\cellcolor[HTML]{73A9CF}} \\color[HTML]{000000} 1.0000 \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-74-0b63dfad15a7>:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  tex_temp = results_all.corr().style.background_gradient(cmap=bg_cmap, vmin=bg_vmin, vmax=bg_vmax, text_color_threshold=bg_text_color_threshold, axis=None).format(na_rep=0, precision=4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = results_all\n",
        "for (src, trg) in itertools.combinations_with_replacement(new_order[:-1], 2):\n",
        "  if src == trg: continue\n",
        "  res = stats.pearsonr(df[src], df[trg])\n",
        "  if (res.pvalue < 0.05): continue #or (res.statistic < 0.1)\n",
        "  print(src, trg)\n",
        "  print(res)\n",
        "  print(res.confidence_interval(0.95))"
      ],
      "metadata": {
        "id": "ciEiVZlWiIdE"
      },
      "execution_count": 75,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZKekSLLTK2T-",
        "iouhTOsXAECh",
        "0vMhiXehCEc0",
        "yw-ewqeieh5d"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}