# MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark
Source code for replication of the experiments in the paper accepted to the EMNLP 2023 conference.

### Install Dependencies
Create a conda environment using the provided .yaml file.
```
conda env create --file=multitude.yaml
conda activate multitude
```
Or at least, install the following python modules/packages:
```
pip install transformers accelerate bitsandbytes peft langcodes[data] nvidia-ml-py3 openai backoff
```

### Source Code Structure
The whole experiments can be run by executing scripts according to their ordinal number in the name.
| # | Description |
| :-: | :-: |
| 01 | Google Colab notebook for dataset creation. The human texts are subsampled from [MassiveSumm](https://github.com/danielvarab/massive-summ), where also author version has been requested. |
| 02 | A script to generate texts from various large language models (HuggingFace pretrained models as well as OpenAI API accessible models). |
| 03 | A script for fine-tuning of the detectors. |
| 04 | A script for inference of fine-tuned detection models to obtain predictions. |
| 05 | Google Colab notebook for results analysis. |

### Running the Experiments

The MULTITuDE dataset can be downloaded from [ToDo]. If downloaded the dataset, continue with the step 4.

1. For dataset reproduction, run the first part of the provided [notebook](01_dataset_creation.ipynb). This downloads the human-text data, preprocesses them, and subsamples them into to file "MassiveSumm_selected.csv".
2. For generation of machine texts for titles (headlines) available for human texts, run the script as provided in the example below, which uses GPT-3 model for text generation.
   ```
   python 02_generate_text.py "text-davinci-003" "MassiveSumm_selected.csv"
   ```
3. After generation of machine texts from all the LLMs as in the paper, run the second part of the provided [notebook](01_dataset_creation.ipynb). This preprocesses generated texts, combines them with the human texts, and removes the duplicates. These precesses result in the "multitude.csv" dataset file generated (required for further experiments).
4. Fine-tune the pre-trained LLMs for machine-generated text detection task using train split of the dataset. Run the script as provided in the example below, which uses mDeBERTa base model and Spanish data generated by GPT-3 model along with Spanish human texts.
   ```
   python 03_finetune_detector.py "microsoft/mdeberta-v3-base" "es" "text-davinci-003"
   ```
5. Generate predictions on the whole test split of the dataset. Run the script as provided in the example below, which uses mDeBERTa base model fine-tuned on English big subset generated by all text-generation LLMs (+ human texts).
   ```
   python 04_test_detector.py "microsoft/mdeberta-v3-base" "en3" "all"
   ```
6. For evaluation of the results, run the provided results analysis [notebook](05_results_analysis.ipynb), which provides various insights and also follows research questions in the paper.

## Cite
```bibtex
@inproceedings{
  macko2023multitude,
  title={{MULTIT}u{DE}: Large-Scale Multilingual Machine-Generated Text Detection Benchmark},
  author={Dominik Macko and Robert Moro and Adaku Uchendu and Jason S Lucas and Michiharu Yamashita and Matúš Pikuliak and Ivan Srba and Thai Le and Dongwon Lee and Jakub Simko and Maria Bielikova},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}
